diff -urN mesa-23.0.0/src/panfrost/base/include/csf/mali_base_csf_kernel.h mesa/src/panfrost/base/include/csf/mali_base_csf_kernel.h
--- mesa-23.0.0/src/panfrost/base/include/csf/mali_base_csf_kernel.h	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/base/include/csf/mali_base_csf_kernel.h	2023-03-06 19:19:32.675308620 +0100
@@ -0,0 +1,594 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ *
+ * (C) COPYRIGHT 2020-2022 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU license.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ */
+
+#ifndef _UAPI_BASE_CSF_KERNEL_H_
+#define _UAPI_BASE_CSF_KERNEL_H_
+
+#include "../mali_base_common_kernel.h"
+#include <linux/types.h>
+
+/* Memory allocation, access/hint flags & mask specific to CSF GPU.
+ *
+ * See base_mem_alloc_flags.
+ */
+
+/* Must be FIXED memory. */
+#define BASE_MEM_FIXED ((base_mem_alloc_flags)1 << 8)
+
+/* CSF event memory
+ *
+ * If Outer shareable coherence is not specified or not available, then on
+ * allocation kbase will automatically use the uncached GPU mapping.
+ * There is no need for the client to specify BASE_MEM_UNCACHED_GPU
+ * themselves when allocating memory with the BASE_MEM_CSF_EVENT flag.
+ *
+ * This memory requires a permanent mapping
+ *
+ * See also kbase_reg_needs_kernel_mapping()
+ */
+#define BASE_MEM_CSF_EVENT ((base_mem_alloc_flags)1 << 19)
+
+#define BASE_MEM_RESERVED_BIT_20 ((base_mem_alloc_flags)1 << 20)
+
+/* Must be FIXABLE memory: its GPU VA will be determined at a later point,
+ * at which time it will be at a fixed GPU VA.
+ */
+#define BASE_MEM_FIXABLE ((base_mem_alloc_flags)1 << 29)
+
+/* Note that the number of bits used for base_mem_alloc_flags
+ * must be less than BASE_MEM_FLAGS_NR_BITS !!!
+ */
+
+/* A mask of all the flags which are only valid for allocations within kbase,
+ * and may not be passed from user space.
+ */
+#define BASEP_MEM_FLAGS_KERNEL_ONLY                                            \
+   (BASEP_MEM_PERMANENT_KERNEL_MAPPING | BASEP_MEM_NO_USER_FREE)
+
+/* A mask of all currently reserved flags
+ */
+#define BASE_MEM_FLAGS_RESERVED BASE_MEM_RESERVED_BIT_20
+
+/* Special base mem handles specific to CSF.
+ */
+#define BASEP_MEM_CSF_USER_REG_PAGE_HANDLE (47ul << LOCAL_PAGE_SHIFT)
+#define BASEP_MEM_CSF_USER_IO_PAGES_HANDLE (48ul << LOCAL_PAGE_SHIFT)
+
+#define KBASE_CSF_NUM_USER_IO_PAGES_HANDLE                                     \
+   ((BASE_MEM_COOKIE_BASE - BASEP_MEM_CSF_USER_IO_PAGES_HANDLE) >>             \
+    LOCAL_PAGE_SHIFT)
+
+/* Valid set of just-in-time memory allocation flags */
+#define BASE_JIT_ALLOC_VALID_FLAGS ((__u8)0)
+
+/* flags for base context specific to CSF */
+
+/* Base context creates a CSF event notification thread.
+ *
+ * The creation of a CSF event notification thread is conditional but
+ * mandatory for the handling of CSF events.
+ */
+#define BASE_CONTEXT_CSF_EVENT_THREAD ((base_context_create_flags)1 << 2)
+
+/* Bitpattern describing the ::base_context_create_flags that can be
+ * passed to base_context_init()
+ */
+#define BASEP_CONTEXT_CREATE_ALLOWED_FLAGS                                     \
+   (BASE_CONTEXT_CCTX_EMBEDDED | BASE_CONTEXT_CSF_EVENT_THREAD |               \
+    BASEP_CONTEXT_CREATE_KERNEL_FLAGS)
+
+/* Flags for base tracepoint specific to CSF */
+
+/* Enable KBase tracepoints for CSF builds */
+#define BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS (1 << 2)
+
+/* Enable additional CSF Firmware side tracepoints */
+#define BASE_TLSTREAM_ENABLE_CSFFW_TRACEPOINTS (1 << 3)
+
+#define BASE_TLSTREAM_FLAGS_MASK                                               \
+   (BASE_TLSTREAM_ENABLE_LATENCY_TRACEPOINTS |                                 \
+    BASE_TLSTREAM_JOB_DUMPING_ENABLED | BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS | \
+    BASE_TLSTREAM_ENABLE_CSFFW_TRACEPOINTS)
+
+/* Number of pages mapped into the process address space for a bound GPU
+ * command queue. A pair of input/output pages and a Hw doorbell page
+ * are mapped to enable direct submission of commands to Hw.
+ */
+#define BASEP_QUEUE_NR_MMAP_USER_PAGES ((size_t)3)
+
+#define BASE_QUEUE_MAX_PRIORITY (15U)
+
+/* CQS Sync object is an array of __u32 event_mem[2], error field index is 1 */
+#define BASEP_EVENT_VAL_INDEX (0U)
+#define BASEP_EVENT_ERR_INDEX (1U)
+
+/* The upper limit for number of objects that could be waited/set per command.
+ * This limit is now enforced as internally the error inherit inputs are
+ * converted to 32-bit flags in a __u32 variable occupying a previously padding
+ * field.
+ */
+#define BASEP_KCPU_CQS_MAX_NUM_OBJS ((size_t)32)
+
+/* CSF CSI EXCEPTION_HANDLER_FLAGS */
+#define BASE_CSF_TILER_OOM_EXCEPTION_FLAG (1u << 0)
+#define BASE_CSF_EXCEPTION_HANDLER_FLAGS_MASK                                  \
+   (BASE_CSF_TILER_OOM_EXCEPTION_FLAG)
+
+/**
+ * enum base_kcpu_command_type - Kernel CPU queue command type.
+ * @BASE_KCPU_COMMAND_TYPE_FENCE_SIGNAL:       fence_signal,
+ * @BASE_KCPU_COMMAND_TYPE_FENCE_WAIT:         fence_wait,
+ * @BASE_KCPU_COMMAND_TYPE_CQS_WAIT:           cqs_wait,
+ * @BASE_KCPU_COMMAND_TYPE_CQS_SET:            cqs_set,
+ * @BASE_KCPU_COMMAND_TYPE_CQS_WAIT_OPERATION: cqs_wait_operation,
+ * @BASE_KCPU_COMMAND_TYPE_CQS_SET_OPERATION:  cqs_set_operation,
+ * @BASE_KCPU_COMMAND_TYPE_MAP_IMPORT:         map_import,
+ * @BASE_KCPU_COMMAND_TYPE_UNMAP_IMPORT:       unmap_import,
+ * @BASE_KCPU_COMMAND_TYPE_UNMAP_IMPORT_FORCE: unmap_import_force,
+ * @BASE_KCPU_COMMAND_TYPE_JIT_ALLOC:          jit_alloc,
+ * @BASE_KCPU_COMMAND_TYPE_JIT_FREE:           jit_free,
+ * @BASE_KCPU_COMMAND_TYPE_GROUP_SUSPEND:      group_suspend,
+ * @BASE_KCPU_COMMAND_TYPE_ERROR_BARRIER:      error_barrier,
+ */
+enum base_kcpu_command_type {
+   BASE_KCPU_COMMAND_TYPE_FENCE_SIGNAL,
+   BASE_KCPU_COMMAND_TYPE_FENCE_WAIT,
+   BASE_KCPU_COMMAND_TYPE_CQS_WAIT,
+   BASE_KCPU_COMMAND_TYPE_CQS_SET,
+   BASE_KCPU_COMMAND_TYPE_CQS_WAIT_OPERATION,
+   BASE_KCPU_COMMAND_TYPE_CQS_SET_OPERATION,
+   BASE_KCPU_COMMAND_TYPE_MAP_IMPORT,
+   BASE_KCPU_COMMAND_TYPE_UNMAP_IMPORT,
+   BASE_KCPU_COMMAND_TYPE_UNMAP_IMPORT_FORCE,
+   BASE_KCPU_COMMAND_TYPE_JIT_ALLOC,
+   BASE_KCPU_COMMAND_TYPE_JIT_FREE,
+   BASE_KCPU_COMMAND_TYPE_GROUP_SUSPEND,
+   BASE_KCPU_COMMAND_TYPE_ERROR_BARRIER
+};
+
+/**
+ * enum base_queue_group_priority - Priority of a GPU Command Queue Group.
+ * @BASE_QUEUE_GROUP_PRIORITY_HIGH:     GPU Command Queue Group is of high
+ *                                      priority.
+ * @BASE_QUEUE_GROUP_PRIORITY_MEDIUM:   GPU Command Queue Group is of medium
+ *                                      priority.
+ * @BASE_QUEUE_GROUP_PRIORITY_LOW:      GPU Command Queue Group is of low
+ *                                      priority.
+ * @BASE_QUEUE_GROUP_PRIORITY_REALTIME: GPU Command Queue Group is of real-time
+ *                                      priority.
+ * @BASE_QUEUE_GROUP_PRIORITY_COUNT:    Number of GPU Command Queue Group
+ *                                      priority levels.
+ *
+ * Currently this is in order of highest to lowest, but if new levels are added
+ * then those new levels may be out of order to preserve the ABI compatibility
+ * with previous releases. At that point, ensure assignment to
+ * the 'priority' member in &kbase_queue_group is updated to ensure it remains
+ * a linear ordering.
+ *
+ * There should be no gaps in the enum, otherwise use of
+ * BASE_QUEUE_GROUP_PRIORITY_COUNT in kbase must be updated.
+ */
+enum base_queue_group_priority {
+   BASE_QUEUE_GROUP_PRIORITY_HIGH = 0,
+   BASE_QUEUE_GROUP_PRIORITY_MEDIUM,
+   BASE_QUEUE_GROUP_PRIORITY_LOW,
+   BASE_QUEUE_GROUP_PRIORITY_REALTIME,
+   BASE_QUEUE_GROUP_PRIORITY_COUNT
+};
+
+struct base_kcpu_command_fence_info {
+   __u64 fence;
+};
+
+struct base_cqs_wait_info {
+   __u64 addr;
+   __u32 val;
+   __u32 padding;
+};
+
+struct base_kcpu_command_cqs_wait_info {
+   __u64 objs;
+   __u32 nr_objs;
+   __u32 inherit_err_flags;
+};
+
+struct base_cqs_set {
+   __u64 addr;
+};
+
+struct base_kcpu_command_cqs_set_info {
+   __u64 objs;
+   __u32 nr_objs;
+   __u32 padding;
+};
+
+/**
+ * typedef basep_cqs_data_type - Enumeration of CQS Data Types
+ *
+ * @BASEP_CQS_DATA_TYPE_U32: The Data Type of a CQS Object's value
+ *                           is an unsigned 32-bit integer
+ * @BASEP_CQS_DATA_TYPE_U64: The Data Type of a CQS Object's value
+ *                           is an unsigned 64-bit integer
+ */
+typedef enum PACKED {
+   BASEP_CQS_DATA_TYPE_U32 = 0,
+   BASEP_CQS_DATA_TYPE_U64 = 1,
+} basep_cqs_data_type;
+
+/**
+ * typedef basep_cqs_wait_operation_op - Enumeration of CQS Object Wait
+ *                                Operation conditions
+ *
+ * @BASEP_CQS_WAIT_OPERATION_LE: CQS Wait Operation indicating that a
+ *                                wait will be satisfied when a CQS Object's
+ *                                value is Less than or Equal to
+ *                                the Wait Operation value
+ * @BASEP_CQS_WAIT_OPERATION_GT: CQS Wait Operation indicating that a
+ *                                wait will be satisfied when a CQS Object's
+ *                                value is Greater than the Wait Operation value
+ */
+typedef enum {
+   BASEP_CQS_WAIT_OPERATION_LE = 0,
+   BASEP_CQS_WAIT_OPERATION_GT = 1,
+} basep_cqs_wait_operation_op;
+
+struct base_cqs_wait_operation_info {
+   __u64 addr;
+   __u64 val;
+   __u8 operation;
+   __u8 data_type;
+   __u8 padding[6];
+};
+
+/**
+ * struct base_kcpu_command_cqs_wait_operation_info - structure which contains
+ *information about the Timeline CQS wait objects
+ *
+ * @objs:              An array of Timeline CQS waits.
+ * @nr_objs:           Number of Timeline CQS waits in the array.
+ * @inherit_err_flags: Bit-pattern for the CQSs in the array who's error field
+ *                     to be served as the source for importing into the
+ *                     queue's error-state.
+ */
+struct base_kcpu_command_cqs_wait_operation_info {
+   __u64 objs;
+   __u32 nr_objs;
+   __u32 inherit_err_flags;
+};
+
+/**
+ * typedef basep_cqs_set_operation_op - Enumeration of CQS Set Operations
+ *
+ * @BASEP_CQS_SET_OPERATION_ADD: CQS Set operation for adding a value
+ *                                to a synchronization object
+ * @BASEP_CQS_SET_OPERATION_SET: CQS Set operation for setting the value
+ *                                of a synchronization object
+ */
+typedef enum {
+   BASEP_CQS_SET_OPERATION_ADD = 0,
+   BASEP_CQS_SET_OPERATION_SET = 1,
+} basep_cqs_set_operation_op;
+
+struct base_cqs_set_operation_info {
+   __u64 addr;
+   __u64 val;
+   __u8 operation;
+   __u8 data_type;
+   __u8 padding[6];
+};
+
+/**
+ * struct base_kcpu_command_cqs_set_operation_info - structure which contains
+ *information about the Timeline CQS set objects
+ *
+ * @objs:    An array of Timeline CQS sets.
+ * @nr_objs: Number of Timeline CQS sets in the array.
+ * @padding: Structure padding, unused bytes.
+ */
+struct base_kcpu_command_cqs_set_operation_info {
+   __u64 objs;
+   __u32 nr_objs;
+   __u32 padding;
+};
+
+/**
+ * struct base_kcpu_command_import_info - structure which contains information
+ *		about the imported buffer.
+ *
+ * @handle:	Address of imported user buffer.
+ */
+struct base_kcpu_command_import_info {
+   __u64 handle;
+};
+
+/**
+ * struct base_kcpu_command_jit_alloc_info - structure which contains
+ *		information about jit memory allocation.
+ *
+ * @info:	An array of elements of the
+ *		struct base_jit_alloc_info type.
+ * @count:	The number of elements in the info array.
+ * @padding:	Padding to a multiple of 64 bits.
+ */
+struct base_kcpu_command_jit_alloc_info {
+   __u64 info;
+   __u8 count;
+   __u8 padding[7];
+};
+
+/**
+ * struct base_kcpu_command_jit_free_info - structure which contains
+ *		information about jit memory which is to be freed.
+ *
+ * @ids:	An array containing the JIT IDs to free.
+ * @count:	The number of elements in the ids array.
+ * @padding:	Padding to a multiple of 64 bits.
+ */
+struct base_kcpu_command_jit_free_info {
+   __u64 ids;
+   __u8 count;
+   __u8 padding[7];
+};
+
+/**
+ * struct base_kcpu_command_group_suspend_info - structure which contains
+ *		suspend buffer data captured for a suspended queue group.
+ *
+ * @buffer:		Pointer to an array of elements of the type char.
+ * @size:		Number of elements in the @buffer array.
+ * @group_handle:	Handle to the mapping of CSG.
+ * @padding:		padding to a multiple of 64 bits.
+ */
+struct base_kcpu_command_group_suspend_info {
+   __u64 buffer;
+   __u32 size;
+   __u8 group_handle;
+   __u8 padding[3];
+};
+
+/**
+ * struct base_kcpu_command - kcpu command.
+ * @type:	type of the kcpu command, one enum base_kcpu_command_type
+ * @padding:	padding to a multiple of 64 bits
+ * @info:	structure which contains information about the kcpu command;
+ *		actual type is determined by @p type
+ * @info.fence:              Fence
+ * @info.cqs_wait:           CQS wait
+ * @info.cqs_set:            CQS set
+ * @info.cqs_wait_operation: CQS wait operation
+ * @info.cqs_set_operation:  CQS set operation
+ * @info.import:             import
+ * @info.jit_alloc:          JIT allocation
+ * @info.jit_free:           JIT deallocation
+ * @info.suspend_buf_copy:   suspend buffer copy
+ * @info.sample_time:        sample time
+ * @info.padding:            padding
+ */
+struct base_kcpu_command {
+   __u8 type;
+   __u8 padding[sizeof(__u64) - sizeof(__u8)];
+   union {
+      struct base_kcpu_command_fence_info fence;
+      struct base_kcpu_command_cqs_wait_info cqs_wait;
+      struct base_kcpu_command_cqs_set_info cqs_set;
+      struct base_kcpu_command_cqs_wait_operation_info cqs_wait_operation;
+      struct base_kcpu_command_cqs_set_operation_info cqs_set_operation;
+      struct base_kcpu_command_import_info import;
+      struct base_kcpu_command_jit_alloc_info jit_alloc;
+      struct base_kcpu_command_jit_free_info jit_free;
+      struct base_kcpu_command_group_suspend_info suspend_buf_copy;
+      __u64 padding[2]; /* No sub-struct should be larger */
+   } info;
+};
+
+/**
+ * struct basep_cs_stream_control - CSI capabilities.
+ *
+ * @features: Features of this stream
+ * @padding:  Padding to a multiple of 64 bits.
+ */
+struct basep_cs_stream_control {
+   __u32 features;
+   __u32 padding;
+};
+
+/**
+ * struct basep_cs_group_control - CSG interface capabilities.
+ *
+ * @features:     Features of this group
+ * @stream_num:   Number of streams in this group
+ * @suspend_size: Size in bytes of the suspend buffer for this group
+ * @padding:      Padding to a multiple of 64 bits.
+ */
+struct basep_cs_group_control {
+   __u32 features;
+   __u32 stream_num;
+   __u32 suspend_size;
+   __u32 padding;
+};
+
+/**
+ * struct base_gpu_queue_group_error_fatal_payload - Unrecoverable fault
+ *        error information associated with GPU command queue group.
+ *
+ * @sideband:     Additional information of the unrecoverable fault.
+ * @status:       Unrecoverable fault information.
+ *                This consists of exception type (least significant byte) and
+ *                data (remaining bytes). One example of exception type is
+ *                CS_INVALID_INSTRUCTION (0x49).
+ * @padding:      Padding to make multiple of 64bits
+ */
+struct base_gpu_queue_group_error_fatal_payload {
+   __u64 sideband;
+   __u32 status;
+   __u32 padding;
+};
+
+/**
+ * struct base_gpu_queue_error_fatal_payload - Unrecoverable fault
+ *        error information related to GPU command queue.
+ *
+ * @sideband:     Additional information about this unrecoverable fault.
+ * @status:       Unrecoverable fault information.
+ *                This consists of exception type (least significant byte) and
+ *                data (remaining bytes). One example of exception type is
+ *                CS_INVALID_INSTRUCTION (0x49).
+ * @csi_index:    Index of the CSF interface the queue is bound to.
+ * @padding:      Padding to make multiple of 64bits
+ */
+struct base_gpu_queue_error_fatal_payload {
+   __u64 sideband;
+   __u32 status;
+   __u8 csi_index;
+   __u8 padding[3];
+};
+
+/**
+ * enum base_gpu_queue_group_error_type - GPU Fatal error type.
+ *
+ * @BASE_GPU_QUEUE_GROUP_ERROR_FATAL:       Fatal error associated with GPU
+ *                                          command queue group.
+ * @BASE_GPU_QUEUE_GROUP_QUEUE_ERROR_FATAL: Fatal error associated with GPU
+ *                                          command queue.
+ * @BASE_GPU_QUEUE_GROUP_ERROR_TIMEOUT:     Fatal error associated with
+ *                                          progress timeout.
+ * @BASE_GPU_QUEUE_GROUP_ERROR_TILER_HEAP_OOM: Fatal error due to running out
+ *                                             of tiler heap memory.
+ * @BASE_GPU_QUEUE_GROUP_ERROR_FATAL_COUNT: The number of fatal error types
+ *
+ * This type is used for &struct_base_gpu_queue_group_error.error_type.
+ */
+enum base_gpu_queue_group_error_type {
+   BASE_GPU_QUEUE_GROUP_ERROR_FATAL = 0,
+   BASE_GPU_QUEUE_GROUP_QUEUE_ERROR_FATAL,
+   BASE_GPU_QUEUE_GROUP_ERROR_TIMEOUT,
+   BASE_GPU_QUEUE_GROUP_ERROR_TILER_HEAP_OOM,
+   BASE_GPU_QUEUE_GROUP_ERROR_FATAL_COUNT
+};
+
+/**
+ * struct base_gpu_queue_group_error - Unrecoverable fault information
+ * @error_type:          Error type of @base_gpu_queue_group_error_type
+ *                       indicating which field in union payload is filled
+ * @padding:             Unused bytes for 64bit boundary
+ * @payload:             Input Payload
+ * @payload.fatal_group: Unrecoverable fault error associated with
+ *                       GPU command queue group
+ * @payload.fatal_queue: Unrecoverable fault error associated with command queue
+ */
+struct base_gpu_queue_group_error {
+   __u8 error_type;
+   __u8 padding[7];
+   union {
+      struct base_gpu_queue_group_error_fatal_payload fatal_group;
+      struct base_gpu_queue_error_fatal_payload fatal_queue;
+   } payload;
+};
+
+/**
+ * enum base_csf_notification_type - Notification type
+ *
+ * @BASE_CSF_NOTIFICATION_EVENT:                 Notification with kernel event
+ * @BASE_CSF_NOTIFICATION_GPU_QUEUE_GROUP_ERROR: Notification with GPU fatal
+ *                                               error
+ * @BASE_CSF_NOTIFICATION_CPU_QUEUE_DUMP:        Notification with dumping cpu
+ *                                               queue
+ * @BASE_CSF_NOTIFICATION_COUNT:                 The number of notification type
+ *
+ * This type is used for &struct_base_csf_notification.type.
+ */
+enum base_csf_notification_type {
+   BASE_CSF_NOTIFICATION_EVENT = 0,
+   BASE_CSF_NOTIFICATION_GPU_QUEUE_GROUP_ERROR,
+   BASE_CSF_NOTIFICATION_CPU_QUEUE_DUMP,
+   BASE_CSF_NOTIFICATION_COUNT
+};
+
+/**
+ * struct base_csf_notification - Event or error notification
+ *
+ * @type:                      Notification type of @base_csf_notification_type
+ * @padding:                   Padding for 64bit boundary
+ * @payload:                   Input Payload
+ * @payload.align:             To fit the struct into a 64-byte cache line
+ * @payload.csg_error:         CSG error
+ * @payload.csg_error.handle:  Handle of GPU command queue group associated with
+ *                             fatal error
+ * @payload.csg_error.padding: Padding
+ * @payload.csg_error.error:   Unrecoverable fault error
+ *
+ */
+struct base_csf_notification {
+   __u8 type;
+   __u8 padding[7];
+   union {
+      struct {
+         __u8 handle;
+         __u8 padding[7];
+         struct base_gpu_queue_group_error error;
+      } csg_error;
+
+      __u8 align[56];
+   } payload;
+};
+
+/**
+ * struct mali_base_gpu_core_props - GPU core props info
+ *
+ * @product_id: Pro specific value.
+ * @version_status: Status of the GPU release. No defined values, but starts at
+ *   0 and increases by one for each release status (alpha, beta, EAC, etc.).
+ *   4 bit values (0-15).
+ * @minor_revision: Minor release number of the GPU. "P" part of an "RnPn"
+ *   release number.
+ *   8 bit values (0-255).
+ * @major_revision: Major release number of the GPU. "R" part of an "RnPn"
+ *   release number.
+ *   4 bit values (0-15).
+ * @padding: padding to align to 8-byte
+ * @gpu_freq_khz_max: The maximum GPU frequency. Reported to applications by
+ *   clGetDeviceInfo()
+ * @log2_program_counter_size: Size of the shader program counter, in bits.
+ * @texture_features: TEXTURE_FEATURES_x registers, as exposed by the GPU. This
+ *   is a bitpattern where a set bit indicates that the format is supported.
+ *   Before using a texture format, it is recommended that the corresponding
+ *   bit be checked.
+ * @gpu_available_memory_size: Theoretical maximum memory available to the GPU.
+ *   It is unlikely that a client will be able to allocate all of this memory
+ *   for their own purposes, but this at least provides an upper bound on the
+ *   memory available to the GPU.
+ *   This is required for OpenCL's clGetDeviceInfo() call when
+ *   CL_DEVICE_GLOBAL_MEM_SIZE is requested, for OpenCL GPU devices. The
+ *   client will not be expecting to allocate anywhere near this value.
+ */
+struct mali_base_gpu_core_props {
+   __u32 product_id;
+   __u16 version_status;
+   __u16 minor_revision;
+   __u16 major_revision;
+   __u16 padding;
+   __u32 gpu_freq_khz_max;
+   __u32 log2_program_counter_size;
+   __u32 texture_features[BASE_GPU_NUM_TEXTURE_FEATURES_REGISTERS];
+   __u64 gpu_available_memory_size;
+};
+
+#endif /* _UAPI_BASE_CSF_KERNEL_H_ */
diff -urN mesa-23.0.0/src/panfrost/base/include/csf/mali_gpu_csf_registers.h mesa/src/panfrost/base/include/csf/mali_gpu_csf_registers.h
--- mesa-23.0.0/src/panfrost/base/include/csf/mali_gpu_csf_registers.h	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/base/include/csf/mali_gpu_csf_registers.h	2023-03-06 19:19:32.676308627 +0100
@@ -0,0 +1,47 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ *
+ * (C) COPYRIGHT 2018-2021 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU license.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ */
+
+/*
+ * This header was originally autogenerated, but it is now ok (and
+ * expected) to have to add to it.
+ */
+
+#ifndef _UAPI_GPU_CSF_REGISTERS_H_
+#define _UAPI_GPU_CSF_REGISTERS_H_
+
+/* Only user block defines are included. HI words have been removed */
+
+/* CS_USER_INPUT_BLOCK register offsets */
+#define CS_INSERT                                                              \
+   0x0000 /* () Current insert offset for ring buffer, low word */
+#define CS_EXTRACT_INIT                                                        \
+   0x0008 /* () Initial extract offset for ring buffer, low word */
+
+/* CS_USER_OUTPUT_BLOCK register offsets */
+#define CS_EXTRACT                                                             \
+   0x0000 /* () Current extract offset for ring buffer, low word */
+#define CS_ACTIVE 0x0008 /* () Initial extract offset when the CS is started */
+
+/* USER register offsets */
+#define LATEST_FLUSH                                                           \
+   0x0000 /* () Flush ID of latest clean-and-invalidate operation */
+
+#endif
diff -urN mesa-23.0.0/src/panfrost/base/include/csf/mali_kbase_csf_ioctl.h mesa/src/panfrost/base/include/csf/mali_kbase_csf_ioctl.h
--- mesa-23.0.0/src/panfrost/base/include/csf/mali_kbase_csf_ioctl.h	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/base/include/csf/mali_kbase_csf_ioctl.h	2023-03-06 19:19:32.679308646 +0100
@@ -0,0 +1,530 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ *
+ * (C) COPYRIGHT 2020-2022 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU license.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ */
+
+#ifndef _UAPI_KBASE_CSF_IOCTL_H_
+#define _UAPI_KBASE_CSF_IOCTL_H_
+
+#include <asm-generic/ioctl.h>
+#include <linux/types.h>
+
+/*
+ * 1.0:
+ * - CSF IOCTL header separated from JM
+ * 1.1:
+ * - Add a new priority level BASE_QUEUE_GROUP_PRIORITY_REALTIME
+ * - Add ioctl 54: This controls the priority setting.
+ * 1.2:
+ * - Add new CSF GPU_FEATURES register into the property structure
+ *   returned by KBASE_IOCTL_GET_GPUPROPS
+ * 1.3:
+ * - Add __u32 group_uid member to
+ *   &struct_kbase_ioctl_cs_queue_group_create.out
+ * 1.4:
+ * - Replace padding in kbase_ioctl_cs_get_glb_iface with
+ *   instr_features member of same size
+ * 1.5:
+ * - Add ioctl 40: kbase_ioctl_cs_queue_register_ex, this is a new
+ *   queue registration call with extended format for supporting CS
+ *   trace configurations with CSF trace_command.
+ * 1.6:
+ * - Added new HW performance counters interface to all GPUs.
+ * 1.7:
+ * - Added reserved field to QUEUE_GROUP_CREATE ioctl for future use
+ * 1.8:
+ * - Removed Kernel legacy HWC interface
+ * 1.9:
+ * - Reorganization of GPU-VA memory zones, including addition of
+ *   FIXED_VA zone and auto-initialization of EXEC_VA zone.
+ * - Added new Base memory allocation interface
+ * 1.10:
+ * - First release of new HW performance counters interface.
+ * 1.11:
+ * - Dummy model (no mali) backend will now clear HWC values after each sample
+ * 1.12:
+ * - Added support for incremental rendering flag in CSG create call
+ */
+
+#define BASE_UK_VERSION_MAJOR 1
+#define BASE_UK_VERSION_MINOR 12
+
+/**
+ * struct kbase_ioctl_version_check - Check version compatibility between
+ * kernel and userspace
+ *
+ * @major: Major version number
+ * @minor: Minor version number
+ */
+struct kbase_ioctl_version_check {
+   __u16 major;
+   __u16 minor;
+};
+
+#define KBASE_IOCTL_VERSION_CHECK_RESERVED                                     \
+   _IOWR(KBASE_IOCTL_TYPE, 0, struct kbase_ioctl_version_check)
+
+/**
+ * struct kbase_ioctl_cs_queue_register - Register a GPU command queue with the
+ *                                        base back-end
+ *
+ * @buffer_gpu_addr: GPU address of the buffer backing the queue
+ * @buffer_size: Size of the buffer in bytes
+ * @priority: Priority of the queue within a group when run within a process
+ * @padding: Currently unused, must be zero
+ *
+ * Note: There is an identical sub-section in kbase_ioctl_cs_queue_register_ex.
+ *        Any change of this struct should also be mirrored to the latter.
+ */
+struct kbase_ioctl_cs_queue_register {
+   __u64 buffer_gpu_addr;
+   __u32 buffer_size;
+   __u8 priority;
+   __u8 padding[3];
+};
+
+#define KBASE_IOCTL_CS_QUEUE_REGISTER                                          \
+   _IOW(KBASE_IOCTL_TYPE, 36, struct kbase_ioctl_cs_queue_register)
+
+/**
+ * struct kbase_ioctl_cs_queue_kick - Kick the GPU command queue group scheduler
+ *                                    to notify that a queue has been updated
+ *
+ * @buffer_gpu_addr: GPU address of the buffer backing the queue
+ */
+struct kbase_ioctl_cs_queue_kick {
+   __u64 buffer_gpu_addr;
+};
+
+#define KBASE_IOCTL_CS_QUEUE_KICK                                              \
+   _IOW(KBASE_IOCTL_TYPE, 37, struct kbase_ioctl_cs_queue_kick)
+
+/**
+ * union kbase_ioctl_cs_queue_bind - Bind a GPU command queue to a group
+ *
+ * @in:                 Input parameters
+ * @in.buffer_gpu_addr: GPU address of the buffer backing the queue
+ * @in.group_handle:    Handle of the group to which the queue should be bound
+ * @in.csi_index:       Index of the CSF interface the queue should be bound to
+ * @in.padding:         Currently unused, must be zero
+ * @out:                Output parameters
+ * @out.mmap_handle:    Handle to be used for creating the mapping of CS
+ *                      input/output pages
+ */
+union kbase_ioctl_cs_queue_bind {
+   struct {
+      __u64 buffer_gpu_addr;
+      __u8 group_handle;
+      __u8 csi_index;
+      __u8 padding[6];
+   } in;
+   struct {
+      __u64 mmap_handle;
+   } out;
+};
+
+#define KBASE_IOCTL_CS_QUEUE_BIND                                              \
+   _IOWR(KBASE_IOCTL_TYPE, 39, union kbase_ioctl_cs_queue_bind)
+
+/**
+ * struct kbase_ioctl_cs_queue_register_ex - Register a GPU command queue with
+ * the base back-end in extended format, involving trace buffer configuration
+ *
+ * @buffer_gpu_addr: GPU address of the buffer backing the queue
+ * @buffer_size: Size of the buffer in bytes
+ * @priority: Priority of the queue within a group when run within a process
+ * @padding: Currently unused, must be zero
+ * @ex_offset_var_addr: GPU address of the trace buffer write offset variable
+ * @ex_buffer_base: Trace buffer GPU base address for the queue
+ * @ex_buffer_size: Size of the trace buffer in bytes
+ * @ex_event_size: Trace event write size, in log2 designation
+ * @ex_event_state: Trace event states configuration
+ * @ex_padding: Currently unused, must be zero
+ *
+ * Note: There is an identical sub-section at the start of this struct to that
+ *        of @ref kbase_ioctl_cs_queue_register. Any change of this sub-section
+ *        must also be mirrored to the latter. Following the said sub-section,
+ *        the remaining fields forms the extension, marked with ex_*.
+ */
+struct kbase_ioctl_cs_queue_register_ex {
+   __u64 buffer_gpu_addr;
+   __u32 buffer_size;
+   __u8 priority;
+   __u8 padding[3];
+   __u64 ex_offset_var_addr;
+   __u64 ex_buffer_base;
+   __u32 ex_buffer_size;
+   __u8 ex_event_size;
+   __u8 ex_event_state;
+   __u8 ex_padding[2];
+};
+
+#define KBASE_IOCTL_CS_QUEUE_REGISTER_EX                                       \
+   _IOW(KBASE_IOCTL_TYPE, 40, struct kbase_ioctl_cs_queue_register_ex)
+
+/**
+ * struct kbase_ioctl_cs_queue_terminate - Terminate a GPU command queue
+ *
+ * @buffer_gpu_addr: GPU address of the buffer backing the queue
+ */
+struct kbase_ioctl_cs_queue_terminate {
+   __u64 buffer_gpu_addr;
+};
+
+#define KBASE_IOCTL_CS_QUEUE_TERMINATE                                         \
+   _IOW(KBASE_IOCTL_TYPE, 41, struct kbase_ioctl_cs_queue_terminate)
+
+/**
+ * union kbase_ioctl_cs_queue_group_create_1_6 - Create a GPU command queue
+ *                                               group
+ * @in:               Input parameters
+ * @in.tiler_mask:    Mask of tiler endpoints the group is allowed to use.
+ * @in.fragment_mask: Mask of fragment endpoints the group is allowed to use.
+ * @in.compute_mask:  Mask of compute endpoints the group is allowed to use.
+ * @in.cs_min:        Minimum number of CSs required.
+ * @in.priority:      Queue group's priority within a process.
+ * @in.tiler_max:     Maximum number of tiler endpoints the group is allowed
+ *                    to use.
+ * @in.fragment_max:  Maximum number of fragment endpoints the group is
+ *                    allowed to use.
+ * @in.compute_max:   Maximum number of compute endpoints the group is allowed
+ *                    to use.
+ * @in.padding:       Currently unused, must be zero
+ * @out:              Output parameters
+ * @out.group_handle: Handle of a newly created queue group.
+ * @out.padding:      Currently unused, must be zero
+ * @out.group_uid:    UID of the queue group available to base.
+ */
+union kbase_ioctl_cs_queue_group_create_1_6 {
+   struct {
+      __u64 tiler_mask;
+      __u64 fragment_mask;
+      __u64 compute_mask;
+      __u8 cs_min;
+      __u8 priority;
+      __u8 tiler_max;
+      __u8 fragment_max;
+      __u8 compute_max;
+      __u8 padding[3];
+
+   } in;
+   struct {
+      __u8 group_handle;
+      __u8 padding[3];
+      __u32 group_uid;
+   } out;
+};
+
+#define KBASE_IOCTL_CS_QUEUE_GROUP_CREATE_1_6                                  \
+   _IOWR(KBASE_IOCTL_TYPE, 42, union kbase_ioctl_cs_queue_group_create_1_6)
+
+/**
+ * union kbase_ioctl_cs_queue_group_create - Create a GPU command queue group
+ * @in:               Input parameters
+ * @in.tiler_mask:    Mask of tiler endpoints the group is allowed to use.
+ * @in.fragment_mask: Mask of fragment endpoints the group is allowed to use.
+ * @in.compute_mask:  Mask of compute endpoints the group is allowed to use.
+ * @in.cs_min:        Minimum number of CSs required.
+ * @in.priority:      Queue group's priority within a process.
+ * @in.tiler_max:     Maximum number of tiler endpoints the group is allowed
+ *                    to use.
+ * @in.fragment_max:  Maximum number of fragment endpoints the group is
+ *                    allowed to use.
+ * @in.compute_max:   Maximum number of compute endpoints the group is allowed
+ *                    to use.
+ * @in.csi_handlers:  Flags to signal that the application intends to use CSI
+ *                    exception handlers in some linear buffers to deal with
+ *                    the given exception types.
+ * @in.padding:       Currently unused, must be zero
+ * @out:              Output parameters
+ * @out.group_handle: Handle of a newly created queue group.
+ * @out.padding:      Currently unused, must be zero
+ * @out.group_uid:    UID of the queue group available to base.
+ */
+union kbase_ioctl_cs_queue_group_create {
+   struct {
+      __u64 tiler_mask;
+      __u64 fragment_mask;
+      __u64 compute_mask;
+      __u8 cs_min;
+      __u8 priority;
+      __u8 tiler_max;
+      __u8 fragment_max;
+      __u8 compute_max;
+      __u8 csi_handlers;
+      __u8 padding[2];
+      /**
+       * @in.reserved: Reserved
+       */
+      __u64 reserved;
+   } in;
+   struct {
+      __u8 group_handle;
+      __u8 padding[3];
+      __u32 group_uid;
+   } out;
+};
+
+#define KBASE_IOCTL_CS_QUEUE_GROUP_CREATE                                      \
+   _IOWR(KBASE_IOCTL_TYPE, 58, union kbase_ioctl_cs_queue_group_create)
+
+/**
+ * struct kbase_ioctl_cs_queue_group_term - Terminate a GPU command queue group
+ *
+ * @group_handle: Handle of the queue group to be terminated
+ * @padding: Padding to round up to a multiple of 8 bytes, must be zero
+ */
+struct kbase_ioctl_cs_queue_group_term {
+   __u8 group_handle;
+   __u8 padding[7];
+};
+
+#define KBASE_IOCTL_CS_QUEUE_GROUP_TERMINATE                                   \
+   _IOW(KBASE_IOCTL_TYPE, 43, struct kbase_ioctl_cs_queue_group_term)
+
+#define KBASE_IOCTL_CS_EVENT_SIGNAL _IO(KBASE_IOCTL_TYPE, 44)
+
+typedef __u8 base_kcpu_queue_id; /* We support up to 256 active KCPU queues */
+
+/**
+ * struct kbase_ioctl_kcpu_queue_new - Create a KCPU command queue
+ *
+ * @id: ID of the new command queue returned by the kernel
+ * @padding: Padding to round up to a multiple of 8 bytes, must be zero
+ */
+struct kbase_ioctl_kcpu_queue_new {
+   base_kcpu_queue_id id;
+   __u8 padding[7];
+};
+
+#define KBASE_IOCTL_KCPU_QUEUE_CREATE                                          \
+   _IOR(KBASE_IOCTL_TYPE, 45, struct kbase_ioctl_kcpu_queue_new)
+
+/**
+ * struct kbase_ioctl_kcpu_queue_delete - Destroy a KCPU command queue
+ *
+ * @id: ID of the command queue to be destroyed
+ * @padding: Padding to round up to a multiple of 8 bytes, must be zero
+ */
+struct kbase_ioctl_kcpu_queue_delete {
+   base_kcpu_queue_id id;
+   __u8 padding[7];
+};
+
+#define KBASE_IOCTL_KCPU_QUEUE_DELETE                                          \
+   _IOW(KBASE_IOCTL_TYPE, 46, struct kbase_ioctl_kcpu_queue_delete)
+
+/**
+ * struct kbase_ioctl_kcpu_queue_enqueue - Enqueue commands into the KCPU queue
+ *
+ * @addr: Memory address of an array of struct base_kcpu_queue_command
+ * @nr_commands: Number of commands in the array
+ * @id: kcpu queue identifier, returned by KBASE_IOCTL_KCPU_QUEUE_CREATE ioctl
+ * @padding: Padding to round up to a multiple of 8 bytes, must be zero
+ */
+struct kbase_ioctl_kcpu_queue_enqueue {
+   __u64 addr;
+   __u32 nr_commands;
+   base_kcpu_queue_id id;
+   __u8 padding[3];
+};
+
+#define KBASE_IOCTL_KCPU_QUEUE_ENQUEUE                                         \
+   _IOW(KBASE_IOCTL_TYPE, 47, struct kbase_ioctl_kcpu_queue_enqueue)
+
+/**
+ * union kbase_ioctl_cs_tiler_heap_init - Initialize chunked tiler memory heap
+ * @in:                Input parameters
+ * @in.chunk_size:     Size of each chunk.
+ * @in.initial_chunks: Initial number of chunks that heap will be created with.
+ * @in.max_chunks:     Maximum number of chunks that the heap is allowed to use.
+ * @in.target_in_flight: Number of render-passes that the driver should attempt
+ * to keep in flight for which allocation of new chunks is allowed.
+ * @in.group_id:       Group ID to be used for physical allocations.
+ * @in.padding:        Padding
+ * @out:               Output parameters
+ * @out.gpu_heap_va:   GPU VA (virtual address) of Heap context that was set up
+ *                     for the heap.
+ * @out.first_chunk_va: GPU VA of the first chunk allocated for the heap,
+ *                     actually points to the header of heap chunk and not to
+ *                     the low address of free memory in the chunk.
+ */
+union kbase_ioctl_cs_tiler_heap_init {
+   struct {
+      __u32 chunk_size;
+      __u32 initial_chunks;
+      __u32 max_chunks;
+      __u16 target_in_flight;
+      __u8 group_id;
+      __u8 padding;
+   } in;
+   struct {
+      __u64 gpu_heap_va;
+      __u64 first_chunk_va;
+   } out;
+};
+
+#define KBASE_IOCTL_CS_TILER_HEAP_INIT                                         \
+   _IOWR(KBASE_IOCTL_TYPE, 48, union kbase_ioctl_cs_tiler_heap_init)
+
+/**
+ * struct kbase_ioctl_cs_tiler_heap_term - Terminate a chunked tiler heap
+ *                                         instance
+ *
+ * @gpu_heap_va: GPU VA of Heap context that was set up for the heap.
+ */
+struct kbase_ioctl_cs_tiler_heap_term {
+   __u64 gpu_heap_va;
+};
+
+#define KBASE_IOCTL_CS_TILER_HEAP_TERM                                         \
+   _IOW(KBASE_IOCTL_TYPE, 49, struct kbase_ioctl_cs_tiler_heap_term)
+
+/**
+ * union kbase_ioctl_cs_get_glb_iface - Request the global control block
+ *                                        of CSF interface capabilities
+ *
+ * @in:                    Input parameters
+ * @in.max_group_num:      The maximum number of groups to be read. Can be 0, in
+ *                         which case groups_ptr is unused.
+ * @in.max_total_stream_num: The maximum number of CSs to be read. Can be 0, in
+ *                         which case streams_ptr is unused.
+ * @in.groups_ptr:         Pointer where to store all the group data
+ * (sequentially).
+ * @in.streams_ptr:        Pointer where to store all the CS data
+ * (sequentially).
+ * @out:                   Output parameters
+ * @out.glb_version:       Global interface version.
+ * @out.features:          Bit mask of features (e.g. whether certain types of
+ * job can be suspended).
+ * @out.group_num:         Number of CSGs supported.
+ * @out.prfcnt_size:       Size of CSF performance counters, in bytes. Bits
+ * 31:16 hold the size of firmware performance counter data and 15:0 hold the
+ * size of hardware performance counter data.
+ * @out.total_stream_num:  Total number of CSs, summed across all groups.
+ * @out.instr_features:    Instrumentation features. Bits 7:4 hold the maximum
+ *                         size of events. Bits 3:0 hold the offset update rate.
+ *                         (csf >= 1.1.0)
+ *
+ */
+union kbase_ioctl_cs_get_glb_iface {
+   struct {
+      __u32 max_group_num;
+      __u32 max_total_stream_num;
+      __u64 groups_ptr;
+      __u64 streams_ptr;
+   } in;
+   struct {
+      __u32 glb_version;
+      __u32 features;
+      __u32 group_num;
+      __u32 prfcnt_size;
+      __u32 total_stream_num;
+      __u32 instr_features;
+   } out;
+};
+
+#define KBASE_IOCTL_CS_GET_GLB_IFACE                                           \
+   _IOWR(KBASE_IOCTL_TYPE, 51, union kbase_ioctl_cs_get_glb_iface)
+
+struct kbase_ioctl_cs_cpu_queue_info {
+   __u64 buffer;
+   __u64 size;
+};
+
+#define KBASE_IOCTL_VERSION_CHECK                                              \
+   _IOWR(KBASE_IOCTL_TYPE, 52, struct kbase_ioctl_version_check)
+
+#define KBASE_IOCTL_CS_CPU_QUEUE_DUMP                                          \
+   _IOW(KBASE_IOCTL_TYPE, 53, struct kbase_ioctl_cs_cpu_queue_info)
+
+/**
+ * union kbase_ioctl_mem_alloc_ex - Allocate memory on the GPU
+ * @in: Input parameters
+ * @in.va_pages: The number of pages of virtual address space to reserve
+ * @in.commit_pages: The number of physical pages to allocate
+ * @in.extension: The number of extra pages to allocate on each GPU fault which
+ * grows the region
+ * @in.flags: Flags
+ * @in.fixed_address: The GPU virtual address requested for the allocation,
+ *                    if the allocation is using the BASE_MEM_FIXED flag.
+ * @in.extra: Space for extra parameters that may be added in the future.
+ * @out: Output parameters
+ * @out.flags: Flags
+ * @out.gpu_va: The GPU virtual address which is allocated
+ */
+union kbase_ioctl_mem_alloc_ex {
+   struct {
+      __u64 va_pages;
+      __u64 commit_pages;
+      __u64 extension;
+      __u64 flags;
+      __u64 fixed_address;
+      __u64 extra[3];
+   } in;
+   struct {
+      __u64 flags;
+      __u64 gpu_va;
+   } out;
+};
+
+#define KBASE_IOCTL_MEM_ALLOC_EX                                               \
+   _IOWR(KBASE_IOCTL_TYPE, 59, union kbase_ioctl_mem_alloc_ex)
+
+/***************
+ * test ioctls *
+ ***************/
+#if MALI_UNIT_TEST
+/* These ioctls are purely for test purposes and are not used in the production
+ * driver, they therefore may change without notice
+ */
+
+/**
+ * struct kbase_ioctl_cs_event_memory_write - Write an event memory address
+ * @cpu_addr: Memory address to write
+ * @value: Value to write
+ * @padding: Currently unused, must be zero
+ */
+struct kbase_ioctl_cs_event_memory_write {
+   __u64 cpu_addr;
+   __u8 value;
+   __u8 padding[7];
+};
+
+/**
+ * union kbase_ioctl_cs_event_memory_read - Read an event memory address
+ * @in: Input parameters
+ * @in.cpu_addr: Memory address to read
+ * @out: Output parameters
+ * @out.value: Value read
+ * @out.padding: Currently unused, must be zero
+ */
+union kbase_ioctl_cs_event_memory_read {
+   struct {
+      __u64 cpu_addr;
+   } in;
+   struct {
+      __u8 value;
+      __u8 padding[7];
+   } out;
+};
+
+#endif /* MALI_UNIT_TEST */
+
+#endif /* _UAPI_KBASE_CSF_IOCTL_H_ */
diff -urN mesa-23.0.0/src/panfrost/base/include/jm/mali_base_jm_kernel.h mesa/src/panfrost/base/include/jm/mali_base_jm_kernel.h
--- mesa-23.0.0/src/panfrost/base/include/jm/mali_base_jm_kernel.h	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/base/include/jm/mali_base_jm_kernel.h	2023-03-06 19:19:32.684308680 +0100
@@ -0,0 +1,1053 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ *
+ * (C) COPYRIGHT 2019-2022 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU license.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ */
+
+#ifndef _UAPI_BASE_JM_KERNEL_H_
+#define _UAPI_BASE_JM_KERNEL_H_
+
+#include "../mali_base_common_kernel.h"
+#include <linux/types.h>
+
+/* Memory allocation, access/hint flags & mask specific to JM GPU.
+ *
+ * See base_mem_alloc_flags.
+ */
+
+/* Used as BASE_MEM_FIXED in other backends */
+#define BASE_MEM_RESERVED_BIT_8 ((base_mem_alloc_flags)1 << 8)
+
+/**
+ * BASE_MEM_RESERVED_BIT_19 - Bit 19 is reserved.
+ *
+ * Do not remove, use the next unreserved bit for new flags
+ */
+#define BASE_MEM_RESERVED_BIT_19 ((base_mem_alloc_flags)1 << 19)
+
+/**
+ * BASE_MEM_TILER_ALIGN_TOP - Memory starting from the end of the initial commit
+ * is aligned to 'extension' pages, where 'extension' must be a power of 2 and
+ * no more than BASE_MEM_TILER_ALIGN_TOP_EXTENSION_MAX_PAGES
+ */
+#define BASE_MEM_TILER_ALIGN_TOP ((base_mem_alloc_flags)1 << 20)
+
+/* Use the GPU VA chosen by the kernel client */
+#define BASE_MEM_FLAG_MAP_FIXED ((base_mem_alloc_flags)1 << 27)
+
+/* Force trimming of JIT allocations when creating a new allocation */
+#define BASEP_MEM_PERFORM_JIT_TRIM ((base_mem_alloc_flags)1 << 29)
+
+/* Note that the number of bits used for base_mem_alloc_flags
+ * must be less than BASE_MEM_FLAGS_NR_BITS !!!
+ */
+
+/* A mask of all the flags which are only valid for allocations within kbase,
+ * and may not be passed from user space.
+ */
+#define BASEP_MEM_FLAGS_KERNEL_ONLY                                            \
+   (BASEP_MEM_PERMANENT_KERNEL_MAPPING | BASEP_MEM_NO_USER_FREE |              \
+    BASE_MEM_FLAG_MAP_FIXED | BASEP_MEM_PERFORM_JIT_TRIM)
+
+/* A mask of all currently reserved flags
+ */
+#define BASE_MEM_FLAGS_RESERVED                                                \
+   (BASE_MEM_RESERVED_BIT_8 | BASE_MEM_RESERVED_BIT_19)
+
+/* Similar to BASE_MEM_TILER_ALIGN_TOP, memory starting from the end of the
+ * initial commit is aligned to 'extension' pages, where 'extension' must be a
+ * power of 2 and no more than BASE_MEM_TILER_ALIGN_TOP_EXTENSION_MAX_PAGES
+ */
+#define BASE_JIT_ALLOC_MEM_TILER_ALIGN_TOP (1 << 0)
+
+/**
+ * BASE_JIT_ALLOC_HEAP_INFO_IS_SIZE - If set, the heap info address points
+ * to a __u32 holding the used size in bytes;
+ * otherwise it points to a __u64 holding the lowest address of unused memory.
+ */
+#define BASE_JIT_ALLOC_HEAP_INFO_IS_SIZE (1 << 1)
+
+/**
+ * BASE_JIT_ALLOC_VALID_FLAGS - Valid set of just-in-time memory allocation flags
+ *
+ * Note: BASE_JIT_ALLOC_HEAP_INFO_IS_SIZE cannot be set if heap_info_gpu_addr
+ * in %base_jit_alloc_info is 0 (atom with BASE_JIT_ALLOC_HEAP_INFO_IS_SIZE set
+ * and heap_info_gpu_addr being 0 will be rejected).
+ */
+#define BASE_JIT_ALLOC_VALID_FLAGS                                             \
+   (BASE_JIT_ALLOC_MEM_TILER_ALIGN_TOP | BASE_JIT_ALLOC_HEAP_INFO_IS_SIZE)
+
+/* Bitpattern describing the ::base_context_create_flags that can be
+ * passed to base_context_init()
+ */
+#define BASEP_CONTEXT_CREATE_ALLOWED_FLAGS                                     \
+   (BASE_CONTEXT_CCTX_EMBEDDED | BASEP_CONTEXT_CREATE_KERNEL_FLAGS)
+
+/*
+ * Private flags used on the base context
+ *
+ * These start at bit 31, and run down to zero.
+ *
+ * They share the same space as base_context_create_flags, and so must
+ * not collide with them.
+ */
+
+/* Private flag tracking whether job descriptor dumping is disabled */
+#define BASEP_CONTEXT_FLAG_JOB_DUMP_DISABLED                                   \
+   ((base_context_create_flags)(1 << 31))
+
+/* Flags for base tracepoint specific to JM */
+#define BASE_TLSTREAM_FLAGS_MASK                                               \
+   (BASE_TLSTREAM_ENABLE_LATENCY_TRACEPOINTS |                                 \
+    BASE_TLSTREAM_JOB_DUMPING_ENABLED)
+/*
+ * Dependency stuff, keep it private for now. May want to expose it if
+ * we decide to make the number of semaphores a configurable
+ * option.
+ */
+#define BASE_JD_ATOM_COUNT 256
+
+/* Maximum number of concurrent render passes.
+ */
+#define BASE_JD_RP_COUNT (256)
+
+/* Set/reset values for a software event */
+#define BASE_JD_SOFT_EVENT_SET   ((unsigned char)1)
+#define BASE_JD_SOFT_EVENT_RESET ((unsigned char)0)
+
+/**
+ * struct base_jd_udata - Per-job data
+ *
+ * @blob: per-job data array
+ *
+ * This structure is used to store per-job data, and is completely unused
+ * by the Base driver. It can be used to store things such as callback
+ * function pointer, data to handle job completion. It is guaranteed to be
+ * untouched by the Base driver.
+ */
+struct base_jd_udata {
+   __u64 blob[2];
+};
+
+/**
+ * typedef base_jd_dep_type - Job dependency type.
+ *
+ * A flags field will be inserted into the atom structure to specify whether a
+ * dependency is a data or ordering dependency (by putting it before/after
+ * 'core_req' in the structure it should be possible to add without changing
+ * the structure size).
+ * When the flag is set for a particular dependency to signal that it is an
+ * ordering only dependency then errors will not be propagated.
+ */
+typedef __u8 base_jd_dep_type;
+
+#define BASE_JD_DEP_TYPE_INVALID (0)       /**< Invalid dependency */
+#define BASE_JD_DEP_TYPE_DATA    (1U << 0) /**< Data dependency */
+#define BASE_JD_DEP_TYPE_ORDER   (1U << 1) /**< Order dependency */
+
+/**
+ * typedef base_jd_core_req - Job chain hardware requirements.
+ *
+ * A job chain must specify what GPU features it needs to allow the
+ * driver to schedule the job correctly.  By not specifying the
+ * correct settings can/will cause an early job termination.  Multiple
+ * values can be ORed together to specify multiple requirements.
+ * Special case is ::BASE_JD_REQ_DEP, which is used to express complex
+ * dependencies, and that doesn't execute anything on the hardware.
+ */
+typedef __u32 base_jd_core_req;
+
+/* Requirements that come from the HW */
+
+/* No requirement, dependency only
+ */
+#define BASE_JD_REQ_DEP ((base_jd_core_req)0)
+
+/* Requires fragment shaders
+ */
+#define BASE_JD_REQ_FS ((base_jd_core_req)1 << 0)
+
+/* Requires compute shaders
+ *
+ * This covers any of the following GPU job types:
+ * - Vertex Shader Job
+ * - Geometry Shader Job
+ * - An actual Compute Shader Job
+ *
+ * Compare this with BASE_JD_REQ_ONLY_COMPUTE, which specifies that the
+ * job is specifically just the "Compute Shader" job type, and not the "Vertex
+ * Shader" nor the "Geometry Shader" job type.
+ */
+#define BASE_JD_REQ_CS ((base_jd_core_req)1 << 1)
+
+/* Requires tiling */
+#define BASE_JD_REQ_T ((base_jd_core_req)1 << 2)
+
+/* Requires cache flushes */
+#define BASE_JD_REQ_CF ((base_jd_core_req)1 << 3)
+
+/* Requires value writeback */
+#define BASE_JD_REQ_V ((base_jd_core_req)1 << 4)
+
+/* SW-only requirements - the HW does not expose these as part of the job slot
+ * capabilities
+ */
+
+/* Requires fragment job with AFBC encoding */
+#define BASE_JD_REQ_FS_AFBC ((base_jd_core_req)1 << 13)
+
+/* SW-only requirement: coalesce completion events.
+ * If this bit is set then completion of this atom will not cause an event to
+ * be sent to userspace, whether successful or not; completion events will be
+ * deferred until an atom completes which does not have this bit set.
+ *
+ * This bit may not be used in combination with BASE_JD_REQ_EXTERNAL_RESOURCES.
+ */
+#define BASE_JD_REQ_EVENT_COALESCE ((base_jd_core_req)1 << 5)
+
+/* SW Only requirement: the job chain requires a coherent core group. We don't
+ * mind which coherent core group is used.
+ */
+#define BASE_JD_REQ_COHERENT_GROUP ((base_jd_core_req)1 << 6)
+
+/* SW Only requirement: The performance counters should be enabled only when
+ * they are needed, to reduce power consumption.
+ */
+#define BASE_JD_REQ_PERMON ((base_jd_core_req)1 << 7)
+
+/* SW Only requirement: External resources are referenced by this atom.
+ *
+ * This bit may not be used in combination with BASE_JD_REQ_EVENT_COALESCE and
+ * BASE_JD_REQ_SOFT_EVENT_WAIT.
+ */
+#define BASE_JD_REQ_EXTERNAL_RESOURCES ((base_jd_core_req)1 << 8)
+
+/* SW Only requirement: Software defined job. Jobs with this bit set will not be
+ * submitted to the hardware but will cause some action to happen within the
+ * driver
+ */
+#define BASE_JD_REQ_SOFT_JOB ((base_jd_core_req)1 << 9)
+
+#define BASE_JD_REQ_SOFT_DUMP_CPU_GPU_TIME (BASE_JD_REQ_SOFT_JOB | 0x1)
+#define BASE_JD_REQ_SOFT_FENCE_TRIGGER     (BASE_JD_REQ_SOFT_JOB | 0x2)
+#define BASE_JD_REQ_SOFT_FENCE_WAIT        (BASE_JD_REQ_SOFT_JOB | 0x3)
+
+/* 0x4 RESERVED for now */
+
+/* SW only requirement: event wait/trigger job.
+ *
+ * - BASE_JD_REQ_SOFT_EVENT_WAIT: this job will block until the event is set.
+ * - BASE_JD_REQ_SOFT_EVENT_SET: this job sets the event, thus unblocks the
+ *   other waiting jobs. It completes immediately.
+ * - BASE_JD_REQ_SOFT_EVENT_RESET: this job resets the event, making it
+ *   possible for other jobs to wait upon. It completes immediately.
+ */
+#define BASE_JD_REQ_SOFT_EVENT_WAIT  (BASE_JD_REQ_SOFT_JOB | 0x5)
+#define BASE_JD_REQ_SOFT_EVENT_SET   (BASE_JD_REQ_SOFT_JOB | 0x6)
+#define BASE_JD_REQ_SOFT_EVENT_RESET (BASE_JD_REQ_SOFT_JOB | 0x7)
+
+#define BASE_JD_REQ_SOFT_DEBUG_COPY (BASE_JD_REQ_SOFT_JOB | 0x8)
+
+/* SW only requirement: Just In Time allocation
+ *
+ * This job requests a single or multiple just-in-time allocations through a
+ * list of base_jit_alloc_info structure which is passed via the jc element of
+ * the atom. The number of base_jit_alloc_info structures present in the
+ * list is passed via the nr_extres element of the atom
+ *
+ * It should be noted that the id entry in base_jit_alloc_info must not
+ * be reused until it has been released via BASE_JD_REQ_SOFT_JIT_FREE.
+ *
+ * Should this soft job fail it is expected that a BASE_JD_REQ_SOFT_JIT_FREE
+ * soft job to free the JIT allocation is still made.
+ *
+ * The job will complete immediately.
+ */
+#define BASE_JD_REQ_SOFT_JIT_ALLOC (BASE_JD_REQ_SOFT_JOB | 0x9)
+
+/* SW only requirement: Just In Time free
+ *
+ * This job requests a single or multiple just-in-time allocations created by
+ * BASE_JD_REQ_SOFT_JIT_ALLOC to be freed. The ID list of the just-in-time
+ * allocations is passed via the jc element of the atom.
+ *
+ * The job will complete immediately.
+ */
+#define BASE_JD_REQ_SOFT_JIT_FREE (BASE_JD_REQ_SOFT_JOB | 0xa)
+
+/* SW only requirement: Map external resource
+ *
+ * This job requests external resource(s) are mapped once the dependencies
+ * of the job have been satisfied. The list of external resources are
+ * passed via the jc element of the atom which is a pointer to a
+ * base_external_resource_list.
+ */
+#define BASE_JD_REQ_SOFT_EXT_RES_MAP (BASE_JD_REQ_SOFT_JOB | 0xb)
+
+/* SW only requirement: Unmap external resource
+ *
+ * This job requests external resource(s) are unmapped once the dependencies
+ * of the job has been satisfied. The list of external resources are
+ * passed via the jc element of the atom which is a pointer to a
+ * base_external_resource_list.
+ */
+#define BASE_JD_REQ_SOFT_EXT_RES_UNMAP (BASE_JD_REQ_SOFT_JOB | 0xc)
+
+/* HW Requirement: Requires Compute shaders (but not Vertex or Geometry Shaders)
+ *
+ * This indicates that the Job Chain contains GPU jobs of the 'Compute
+ * Shaders' type.
+ *
+ * In contrast to BASE_JD_REQ_CS, this does not indicate that the Job
+ * Chain contains 'Geometry Shader' or 'Vertex Shader' jobs.
+ */
+#define BASE_JD_REQ_ONLY_COMPUTE ((base_jd_core_req)1 << 10)
+
+/* HW Requirement: Use the base_jd_atom::device_nr field to specify a
+ * particular core group
+ *
+ * If both BASE_JD_REQ_COHERENT_GROUP and this flag are set, this flag
+ * takes priority
+ *
+ * This is only guaranteed to work for BASE_JD_REQ_ONLY_COMPUTE atoms.
+ */
+#define BASE_JD_REQ_SPECIFIC_COHERENT_GROUP ((base_jd_core_req)1 << 11)
+
+/* SW Flag: If this bit is set then the successful completion of this atom
+ * will not cause an event to be sent to userspace
+ */
+#define BASE_JD_REQ_EVENT_ONLY_ON_FAILURE ((base_jd_core_req)1 << 12)
+
+/* SW Flag: If this bit is set then completion of this atom will not cause an
+ * event to be sent to userspace, whether successful or not.
+ */
+#define BASEP_JD_REQ_EVENT_NEVER ((base_jd_core_req)1 << 14)
+
+/* SW Flag: Skip GPU cache clean and invalidation before starting a GPU job.
+ *
+ * If this bit is set then the GPU's cache will not be cleaned and invalidated
+ * until a GPU job starts which does not have this bit set or a job completes
+ * which does not have the BASE_JD_REQ_SKIP_CACHE_END bit set. Do not use
+ * if the CPU may have written to memory addressed by the job since the last job
+ * without this bit set was submitted.
+ */
+#define BASE_JD_REQ_SKIP_CACHE_START ((base_jd_core_req)1 << 15)
+
+/* SW Flag: Skip GPU cache clean and invalidation after a GPU job completes.
+ *
+ * If this bit is set then the GPU's cache will not be cleaned and invalidated
+ * until a GPU job completes which does not have this bit set or a job starts
+ * which does not have the BASE_JD_REQ_SKIP_CACHE_START bit set. Do not use
+ * if the CPU may read from or partially overwrite memory addressed by the job
+ * before the next job without this bit set completes.
+ */
+#define BASE_JD_REQ_SKIP_CACHE_END ((base_jd_core_req)1 << 16)
+
+/* Request the atom be executed on a specific job slot.
+ *
+ * When this flag is specified, it takes precedence over any existing job slot
+ * selection logic.
+ */
+#define BASE_JD_REQ_JOB_SLOT ((base_jd_core_req)1 << 17)
+
+/* SW-only requirement: The atom is the start of a renderpass.
+ *
+ * If this bit is set then the job chain will be soft-stopped if it causes the
+ * GPU to write beyond the end of the physical pages backing the tiler heap, and
+ * committing more memory to the heap would exceed an internal threshold. It may
+ * be resumed after running one of the job chains attached to an atom with
+ * BASE_JD_REQ_END_RENDERPASS set and the same renderpass ID. It may be
+ * resumed multiple times until it completes without memory usage exceeding the
+ * threshold.
+ *
+ * Usually used with BASE_JD_REQ_T.
+ */
+#define BASE_JD_REQ_START_RENDERPASS ((base_jd_core_req)1 << 18)
+
+/* SW-only requirement: The atom is the end of a renderpass.
+ *
+ * If this bit is set then the atom incorporates the CPU address of a
+ * base_jd_fragment object instead of the GPU address of a job chain.
+ *
+ * Which job chain is run depends upon whether the atom with the same renderpass
+ * ID and the BASE_JD_REQ_START_RENDERPASS bit set completed normally or
+ * was soft-stopped when it exceeded an upper threshold for tiler heap memory
+ * usage.
+ *
+ * It also depends upon whether one of the job chains attached to the atom has
+ * already been run as part of the same renderpass (in which case it would have
+ * written unresolved multisampled and otherwise-discarded output to temporary
+ * buffers that need to be read back). The job chain for doing a forced read and
+ * forced write (from/to temporary buffers) is run as many times as necessary.
+ *
+ * Usually used with BASE_JD_REQ_FS.
+ */
+#define BASE_JD_REQ_END_RENDERPASS ((base_jd_core_req)1 << 19)
+
+/* SW-only requirement: The atom needs to run on a limited core mask affinity.
+ *
+ * If this bit is set then the kbase_context.limited_core_mask will be applied
+ * to the affinity.
+ */
+#define BASE_JD_REQ_LIMITED_CORE_MASK ((base_jd_core_req)1 << 20)
+
+/* These requirement bits are currently unused in base_jd_core_req
+ */
+#define BASEP_JD_REQ_RESERVED                                                  \
+   (~(BASE_JD_REQ_ATOM_TYPE | BASE_JD_REQ_EXTERNAL_RESOURCES |                 \
+      BASE_JD_REQ_EVENT_ONLY_ON_FAILURE | BASEP_JD_REQ_EVENT_NEVER |           \
+      BASE_JD_REQ_EVENT_COALESCE | BASE_JD_REQ_COHERENT_GROUP |                \
+      BASE_JD_REQ_SPECIFIC_COHERENT_GROUP | BASE_JD_REQ_FS_AFBC |              \
+      BASE_JD_REQ_PERMON | BASE_JD_REQ_SKIP_CACHE_START |                      \
+      BASE_JD_REQ_SKIP_CACHE_END | BASE_JD_REQ_JOB_SLOT |                      \
+      BASE_JD_REQ_START_RENDERPASS | BASE_JD_REQ_END_RENDERPASS |              \
+      BASE_JD_REQ_LIMITED_CORE_MASK))
+
+/* Mask of all bits in base_jd_core_req that control the type of the atom.
+ *
+ * This allows dependency only atoms to have flags set
+ */
+#define BASE_JD_REQ_ATOM_TYPE                                                  \
+   (BASE_JD_REQ_FS | BASE_JD_REQ_CS | BASE_JD_REQ_T | BASE_JD_REQ_CF |         \
+    BASE_JD_REQ_V | BASE_JD_REQ_SOFT_JOB | BASE_JD_REQ_ONLY_COMPUTE)
+
+/**
+ * BASE_JD_REQ_SOFT_JOB_TYPE - Mask of all bits in base_jd_core_req that
+ * controls the type of a soft job.
+ */
+#define BASE_JD_REQ_SOFT_JOB_TYPE (BASE_JD_REQ_SOFT_JOB | 0x1f)
+
+/* Returns non-zero value if core requirements passed define a soft job or
+ * a dependency only job.
+ */
+#define BASE_JD_REQ_SOFT_JOB_OR_DEP(core_req)                                  \
+   (((core_req)&BASE_JD_REQ_SOFT_JOB) ||                                       \
+    ((core_req)&BASE_JD_REQ_ATOM_TYPE) == BASE_JD_REQ_DEP)
+
+/**
+ * enum kbase_jd_atom_state - Atom states
+ *
+ * @KBASE_JD_ATOM_STATE_UNUSED: Atom is not used.
+ * @KBASE_JD_ATOM_STATE_QUEUED: Atom is queued in JD.
+ * @KBASE_JD_ATOM_STATE_IN_JS:  Atom has been given to JS (is runnable/running).
+ * @KBASE_JD_ATOM_STATE_HW_COMPLETED: Atom has been completed, but not yet
+ *                                    handed back to job dispatcher for
+ *                                    dependency resolution.
+ * @KBASE_JD_ATOM_STATE_COMPLETED: Atom has been completed, but not yet handed
+ *                                 back to userspace.
+ */
+enum kbase_jd_atom_state {
+   KBASE_JD_ATOM_STATE_UNUSED,
+   KBASE_JD_ATOM_STATE_QUEUED,
+   KBASE_JD_ATOM_STATE_IN_JS,
+   KBASE_JD_ATOM_STATE_HW_COMPLETED,
+   KBASE_JD_ATOM_STATE_COMPLETED
+};
+
+/**
+ * typedef base_atom_id - Type big enough to store an atom number in.
+ */
+typedef __u8 base_atom_id;
+
+/**
+ * struct base_dependency - base dependency
+ *
+ * @atom_id:         An atom number
+ * @dependency_type: Dependency type
+ */
+struct base_dependency {
+   base_atom_id atom_id;
+   base_jd_dep_type dependency_type;
+};
+
+/**
+ * struct base_jd_fragment - Set of GPU fragment job chains used for rendering.
+ *
+ * @norm_read_norm_write: Job chain for full rendering.
+ *                        GPU address of a fragment job chain to render in the
+ *                        circumstance where the tiler job chain did not exceed
+ *                        its memory usage threshold and no fragment job chain
+ *                        was previously run for the same renderpass.
+ *                        It is used no more than once per renderpass.
+ * @norm_read_forced_write: Job chain for starting incremental
+ *                          rendering.
+ *                          GPU address of a fragment job chain to render in
+ *                          the circumstance where the tiler job chain exceeded
+ *                          its memory usage threshold for the first time and
+ *                          no fragment job chain was previously run for the
+ *                          same renderpass.
+ *                          Writes unresolved multisampled and normally-
+ *                          discarded output to temporary buffers that must be
+ *                          read back by a subsequent forced_read job chain
+ *                          before the renderpass is complete.
+ *                          It is used no more than once per renderpass.
+ * @forced_read_forced_write: Job chain for continuing incremental
+ *                            rendering.
+ *                            GPU address of a fragment job chain to render in
+ *                            the circumstance where the tiler job chain
+ *                            exceeded its memory usage threshold again
+ *                            and a fragment job chain was previously run for
+ *                            the same renderpass.
+ *                            Reads unresolved multisampled and
+ *                            normally-discarded output from temporary buffers
+ *                            written by a previous forced_write job chain and
+ *                            writes the same to temporary buffers again.
+ *                            It is used as many times as required until
+ *                            rendering completes.
+ * @forced_read_norm_write: Job chain for ending incremental rendering.
+ *                          GPU address of a fragment job chain to render in the
+ *                          circumstance where the tiler job chain did not
+ *                          exceed its memory usage threshold this time and a
+ *                          fragment job chain was previously run for the same
+ *                          renderpass.
+ *                          Reads unresolved multisampled and normally-discarded
+ *                          output from temporary buffers written by a previous
+ *                          forced_write job chain in order to complete a
+ *                          renderpass.
+ *                          It is used no more than once per renderpass.
+ *
+ * This structure is referenced by the main atom structure if
+ * BASE_JD_REQ_END_RENDERPASS is set in the base_jd_core_req.
+ */
+struct base_jd_fragment {
+   __u64 norm_read_norm_write;
+   __u64 norm_read_forced_write;
+   __u64 forced_read_forced_write;
+   __u64 forced_read_norm_write;
+};
+
+/**
+ * typedef base_jd_prio - Base Atom priority.
+ *
+ * Only certain priority levels are actually implemented, as specified by the
+ * BASE_JD_PRIO_<...> definitions below. It is undefined to use a priority
+ * level that is not one of those defined below.
+ *
+ * Priority levels only affect scheduling after the atoms have had dependencies
+ * resolved. For example, a low priority atom that has had its dependencies
+ * resolved might run before a higher priority atom that has not had its
+ * dependencies resolved.
+ *
+ * In general, fragment atoms do not affect non-fragment atoms with
+ * lower priorities, and vice versa. One exception is that there is only one
+ * priority value for each context. So a high-priority (e.g.) fragment atom
+ * could increase its context priority, causing its non-fragment atoms to also
+ * be scheduled sooner.
+ *
+ * The atoms are scheduled as follows with respect to their priorities:
+ * * Let atoms 'X' and 'Y' be for the same job slot who have dependencies
+ *   resolved, and atom 'X' has a higher priority than atom 'Y'
+ * * If atom 'Y' is currently running on the HW, then it is interrupted to
+ *   allow atom 'X' to run soon after
+ * * If instead neither atom 'Y' nor atom 'X' are running, then when choosing
+ *   the next atom to run, atom 'X' will always be chosen instead of atom 'Y'
+ * * Any two atoms that have the same priority could run in any order with
+ *   respect to each other. That is, there is no ordering constraint between
+ *   atoms of the same priority.
+ *
+ * The sysfs file 'js_ctx_scheduling_mode' is used to control how atoms are
+ * scheduled between contexts. The default value, 0, will cause higher-priority
+ * atoms to be scheduled first, regardless of their context. The value 1 will
+ * use a round-robin algorithm when deciding which context's atoms to schedule
+ * next, so higher-priority atoms can only preempt lower priority atoms within
+ * the same context. See KBASE_JS_SYSTEM_PRIORITY_MODE and
+ * KBASE_JS_PROCESS_LOCAL_PRIORITY_MODE for more details.
+ */
+typedef __u8 base_jd_prio;
+
+/* Medium atom priority. This is a priority higher than BASE_JD_PRIO_LOW */
+#define BASE_JD_PRIO_MEDIUM ((base_jd_prio)0)
+/* High atom priority. This is a priority higher than BASE_JD_PRIO_MEDIUM and
+ * BASE_JD_PRIO_LOW
+ */
+#define BASE_JD_PRIO_HIGH ((base_jd_prio)1)
+/* Low atom priority. */
+#define BASE_JD_PRIO_LOW ((base_jd_prio)2)
+/* Real-Time atom priority. This is a priority higher than BASE_JD_PRIO_HIGH,
+ * BASE_JD_PRIO_MEDIUM, and BASE_JD_PRIO_LOW
+ */
+#define BASE_JD_PRIO_REALTIME ((base_jd_prio)3)
+
+/* Invalid atom priority (max uint8_t value) */
+#define BASE_JD_PRIO_INVALID ((base_jd_prio)255)
+
+/* Count of the number of priority levels. This itself is not a valid
+ * base_jd_prio setting
+ */
+#define BASE_JD_NR_PRIO_LEVELS 4
+
+/**
+ * struct base_jd_atom_v2 - Node of a dependency graph used to submit a
+ *                          GPU job chain or soft-job to the kernel driver.
+ *
+ * @jc:            GPU address of a job chain or (if BASE_JD_REQ_END_RENDERPASS
+ *                 is set in the base_jd_core_req) the CPU address of a
+ *                 base_jd_fragment object.
+ * @udata:         User data.
+ * @extres_list:   List of external resources.
+ * @nr_extres:     Number of external resources or JIT allocations.
+ * @jit_id:        Zero-terminated array of IDs of just-in-time memory
+ *                 allocations written to by the atom. When the atom
+ *                 completes, the value stored at the
+ *                 &struct_base_jit_alloc_info.heap_info_gpu_addr of
+ *                 each allocation is read in order to enforce an
+ *                 overall physical memory usage limit.
+ * @pre_dep:       Pre-dependencies. One need to use SETTER function to assign
+ *                 this field; this is done in order to reduce possibility of
+ *                 improper assignment of a dependency field.
+ * @atom_number:   Unique number to identify the atom.
+ * @prio:          Atom priority. Refer to base_jd_prio for more details.
+ * @device_nr:     Core group when BASE_JD_REQ_SPECIFIC_COHERENT_GROUP
+ *                 specified.
+ * @jobslot:       Job slot to use when BASE_JD_REQ_JOB_SLOT is specified.
+ * @core_req:      Core requirements.
+ * @renderpass_id: Renderpass identifier used to associate an atom that has
+ *                 BASE_JD_REQ_START_RENDERPASS set in its core requirements
+ *                 with an atom that has BASE_JD_REQ_END_RENDERPASS set.
+ * @padding:       Unused. Must be zero.
+ *
+ * This structure has changed since UK 10.2 for which base_jd_core_req was a
+ * __u16 value.
+ *
+ * In UK 10.3 a core_req field of a __u32 type was added to the end of the
+ * structure, and the place in the structure previously occupied by __u16
+ * core_req was kept but renamed to compat_core_req.
+ *
+ * From UK 11.20 - compat_core_req is now occupied by __u8 jit_id[2].
+ * Compatibility with UK 10.x from UK 11.y is not handled because
+ * the major version increase prevents this.
+ *
+ * For UK 11.20 jit_id[2] must be initialized to zero.
+ */
+struct base_jd_atom_v2 {
+   __u64 jc;
+   struct base_jd_udata udata;
+   __u64 extres_list;
+   __u16 nr_extres;
+   __u8 jit_id[2];
+   struct base_dependency pre_dep[2];
+   base_atom_id atom_number;
+   base_jd_prio prio;
+   __u8 device_nr;
+   __u8 jobslot;
+   base_jd_core_req core_req;
+   __u8 renderpass_id;
+   __u8 padding[7];
+};
+
+/**
+ * struct base_jd_atom - Same as base_jd_atom_v2, but has an extra seq_nr
+ *                          at the beginning.
+ *
+ * @seq_nr:        Sequence number of logical grouping of atoms.
+ * @jc:            GPU address of a job chain or (if BASE_JD_REQ_END_RENDERPASS
+ *                 is set in the base_jd_core_req) the CPU address of a
+ *                 base_jd_fragment object.
+ * @udata:         User data.
+ * @extres_list:   List of external resources.
+ * @nr_extres:     Number of external resources or JIT allocations.
+ * @jit_id:        Zero-terminated array of IDs of just-in-time memory
+ *                 allocations written to by the atom. When the atom
+ *                 completes, the value stored at the
+ *                 &struct_base_jit_alloc_info.heap_info_gpu_addr of
+ *                 each allocation is read in order to enforce an
+ *                 overall physical memory usage limit.
+ * @pre_dep:       Pre-dependencies. One need to use SETTER function to assign
+ *                 this field; this is done in order to reduce possibility of
+ *                 improper assignment of a dependency field.
+ * @atom_number:   Unique number to identify the atom.
+ * @prio:          Atom priority. Refer to base_jd_prio for more details.
+ * @device_nr:     Core group when BASE_JD_REQ_SPECIFIC_COHERENT_GROUP
+ *                 specified.
+ * @jobslot:       Job slot to use when BASE_JD_REQ_JOB_SLOT is specified.
+ * @core_req:      Core requirements.
+ * @renderpass_id: Renderpass identifier used to associate an atom that has
+ *                 BASE_JD_REQ_START_RENDERPASS set in its core requirements
+ *                 with an atom that has BASE_JD_REQ_END_RENDERPASS set.
+ * @padding:       Unused. Must be zero.
+ */
+typedef struct base_jd_atom {
+   __u64 seq_nr;
+   __u64 jc;
+   struct base_jd_udata udata;
+   __u64 extres_list;
+   __u16 nr_extres;
+   __u8 jit_id[2];
+   struct base_dependency pre_dep[2];
+   base_atom_id atom_number;
+   base_jd_prio prio;
+   __u8 device_nr;
+   __u8 jobslot;
+   base_jd_core_req core_req;
+   __u8 renderpass_id;
+   __u8 padding[7];
+} base_jd_atom;
+
+/* Job chain event code bits
+ * Defines the bits used to create ::base_jd_event_code
+ */
+enum {
+   BASE_JD_SW_EVENT_KERNEL = (1u << 15), /* Kernel side event */
+   BASE_JD_SW_EVENT = (1u << 14),        /* SW defined event */
+   /* Event indicates success (SW events only) */
+   BASE_JD_SW_EVENT_SUCCESS = (1u << 13),
+   BASE_JD_SW_EVENT_JOB = (0u << 11),      /* Job related event */
+   BASE_JD_SW_EVENT_BAG = (1u << 11),      /* Bag related event */
+   BASE_JD_SW_EVENT_INFO = (2u << 11),     /* Misc/info event */
+   BASE_JD_SW_EVENT_RESERVED = (3u << 11), /* Reserved event type */
+   /* Mask to extract the type from an event code */
+   BASE_JD_SW_EVENT_TYPE_MASK = (3u << 11)
+};
+
+/**
+ * enum base_jd_event_code - Job chain event codes
+ *
+ * @BASE_JD_EVENT_RANGE_HW_NONFAULT_START: Start of hardware non-fault status
+ *                                         codes.
+ *                                         Obscurely, BASE_JD_EVENT_TERMINATED
+ *                                         indicates a real fault, because the
+ *                                         job was hard-stopped.
+ * @BASE_JD_EVENT_NOT_STARTED: Can't be seen by userspace, treated as
+ *                             'previous job done'.
+ * @BASE_JD_EVENT_STOPPED:     Can't be seen by userspace, becomes
+ *                             TERMINATED, DONE or JOB_CANCELLED.
+ * @BASE_JD_EVENT_TERMINATED:  This is actually a fault status code - the job
+ *                             was hard stopped.
+ * @BASE_JD_EVENT_ACTIVE: Can't be seen by userspace, jobs only returned on
+ *                        complete/fail/cancel.
+ * @BASE_JD_EVENT_RANGE_HW_NONFAULT_END: End of hardware non-fault status codes.
+ *                                       Obscurely, BASE_JD_EVENT_TERMINATED
+ *                                       indicates a real fault,
+ *                                       because the job was hard-stopped.
+ * @BASE_JD_EVENT_RANGE_HW_FAULT_OR_SW_ERROR_START: Start of hardware fault and
+ *                                                  software error status codes.
+ * @BASE_JD_EVENT_RANGE_HW_FAULT_OR_SW_ERROR_END: End of hardware fault and
+ *                                                software error status codes.
+ * @BASE_JD_EVENT_RANGE_SW_SUCCESS_START: Start of software success status
+ *                                        codes.
+ * @BASE_JD_EVENT_RANGE_SW_SUCCESS_END: End of software success status codes.
+ * @BASE_JD_EVENT_RANGE_KERNEL_ONLY_START: Start of kernel-only status codes.
+ *                                         Such codes are never returned to
+ *                                         user-space.
+ * @BASE_JD_EVENT_RANGE_KERNEL_ONLY_END: End of kernel-only status codes.
+ * @BASE_JD_EVENT_DONE: atom has completed successfull
+ * @BASE_JD_EVENT_JOB_CONFIG_FAULT: Atom dependencies configuration error which
+ *                                  shall result in a failed atom
+ * @BASE_JD_EVENT_JOB_POWER_FAULT:  The job could not be executed because the
+ *                                  part of the memory system required to access
+ *                                  job descriptors was not powered on
+ * @BASE_JD_EVENT_JOB_READ_FAULT:   Reading a job descriptor into the Job
+ *                                  manager failed
+ * @BASE_JD_EVENT_JOB_WRITE_FAULT:  Writing a job descriptor from the Job
+ *                                  manager failed
+ * @BASE_JD_EVENT_JOB_AFFINITY_FAULT: The job could not be executed because the
+ *                                    specified affinity mask does not intersect
+ *                                    any available cores
+ * @BASE_JD_EVENT_JOB_BUS_FAULT:    A bus access failed while executing a job
+ * @BASE_JD_EVENT_INSTR_INVALID_PC: A shader instruction with an illegal program
+ *                                  counter was executed.
+ * @BASE_JD_EVENT_INSTR_INVALID_ENC: A shader instruction with an illegal
+ *                                  encoding was executed.
+ * @BASE_JD_EVENT_INSTR_TYPE_MISMATCH: A shader instruction was executed where
+ *                                  the instruction encoding did not match the
+ *                                  instruction type encoded in the program
+ *                                  counter.
+ * @BASE_JD_EVENT_INSTR_OPERAND_FAULT: A shader instruction was executed that
+ *                                  contained invalid combinations of operands.
+ * @BASE_JD_EVENT_INSTR_TLS_FAULT:  A shader instruction was executed that tried
+ *                                  to access the thread local storage section
+ *                                  of another thread.
+ * @BASE_JD_EVENT_INSTR_ALIGN_FAULT: A shader instruction was executed that
+ *                                  tried to do an unsupported unaligned memory
+ *                                  access.
+ * @BASE_JD_EVENT_INSTR_BARRIER_FAULT: A shader instruction was executed that
+ *                                  failed to complete an instruction barrier.
+ * @BASE_JD_EVENT_DATA_INVALID_FAULT: Any data structure read as part of the job
+ *                                  contains invalid combinations of data.
+ * @BASE_JD_EVENT_TILE_RANGE_FAULT: Tile or fragment shading was asked to
+ *                                  process a tile that is entirely outside the
+ *                                  bounding box of the frame.
+ * @BASE_JD_EVENT_STATE_FAULT:      Matches ADDR_RANGE_FAULT. A virtual address
+ *                                  has been found that exceeds the virtual
+ *                                  address range.
+ * @BASE_JD_EVENT_OUT_OF_MEMORY:    The tiler ran out of memory when executing a
+ * job.
+ * @BASE_JD_EVENT_UNKNOWN:          If multiple jobs in a job chain fail, only
+ *                                  the first one the reports an error will set
+ *                                  and return full error information.
+ *                                  Subsequent failing jobs will not update the
+ *                                  error status registers, and may write an
+ *                                  error status of UNKNOWN.
+ * @BASE_JD_EVENT_DELAYED_BUS_FAULT: The GPU received a bus fault for access to
+ *                                  physical memory where the original virtual
+ *                                  address is no longer available.
+ * @BASE_JD_EVENT_SHAREABILITY_FAULT: Matches GPU_SHAREABILITY_FAULT. A cache
+ *                                  has detected that the same line has been
+ *                                  accessed as both shareable and non-shareable
+ *                                  memory from inside the GPU.
+ * @BASE_JD_EVENT_TRANSLATION_FAULT_LEVEL1: A memory access hit an invalid table
+ *                                  entry at level 1 of the translation table.
+ * @BASE_JD_EVENT_TRANSLATION_FAULT_LEVEL2: A memory access hit an invalid table
+ *                                  entry at level 2 of the translation table.
+ * @BASE_JD_EVENT_TRANSLATION_FAULT_LEVEL3: A memory access hit an invalid table
+ *                                  entry at level 3 of the translation table.
+ * @BASE_JD_EVENT_TRANSLATION_FAULT_LEVEL4: A memory access hit an invalid table
+ *                                  entry at level 4 of the translation table.
+ * @BASE_JD_EVENT_PERMISSION_FAULT: A memory access could not be allowed due to
+ *                                  the permission flags set in translation
+ *                                  table
+ * @BASE_JD_EVENT_TRANSTAB_BUS_FAULT_LEVEL1: A bus fault occurred while reading
+ *                                  level 0 of the translation tables.
+ * @BASE_JD_EVENT_TRANSTAB_BUS_FAULT_LEVEL2: A bus fault occurred while reading
+ *                                  level 1 of the translation tables.
+ * @BASE_JD_EVENT_TRANSTAB_BUS_FAULT_LEVEL3: A bus fault occurred while reading
+ *                                  level 2 of the translation tables.
+ * @BASE_JD_EVENT_TRANSTAB_BUS_FAULT_LEVEL4: A bus fault occurred while reading
+ *                                  level 3 of the translation tables.
+ * @BASE_JD_EVENT_ACCESS_FLAG:      Matches ACCESS_FLAG_0. A memory access hit a
+ *                                  translation table entry with the ACCESS_FLAG
+ *                                  bit set to zero in level 0 of the
+ *                                  page table, and the DISABLE_AF_FAULT flag
+ *                                  was not set.
+ * @BASE_JD_EVENT_MEM_GROWTH_FAILED: raised for JIT_ALLOC atoms that failed to
+ *                                   grow memory on demand
+ * @BASE_JD_EVENT_JOB_CANCELLED: raised when this atom was hard-stopped or its
+ *                               dependencies failed
+ * @BASE_JD_EVENT_JOB_INVALID: raised for many reasons, including invalid data
+ *                             in the atom which overlaps with
+ *                             BASE_JD_EVENT_JOB_CONFIG_FAULT, or if the
+ *                             platform doesn't support the feature specified in
+ *                             the atom.
+ * @BASE_JD_EVENT_DRV_TERMINATED: this is a special event generated to indicate
+ *                                to userspace that the KBase context has been
+ *                                destroyed and Base should stop listening for
+ *                                further events
+ * @BASE_JD_EVENT_REMOVED_FROM_NEXT: raised when an atom that was configured in
+ *                                   the GPU has to be retried (but it has not
+ *                                   started) due to e.g., GPU reset
+ * @BASE_JD_EVENT_END_RP_DONE: this is used for incremental rendering to signal
+ *                             the completion of a renderpass. This value
+ *                             shouldn't be returned to userspace but I haven't
+ *                             seen where it is reset back to JD_EVENT_DONE.
+ *
+ * HW and low-level SW events are represented by event codes.
+ * The status of jobs which succeeded are also represented by
+ * an event code (see @BASE_JD_EVENT_DONE).
+ * Events are usually reported as part of a &struct base_jd_event.
+ *
+ * The event codes are encoded in the following way:
+ * * 10:0  - subtype
+ * * 12:11 - type
+ * * 13    - SW success (only valid if the SW bit is set)
+ * * 14    - SW event (HW event if not set)
+ * * 15    - Kernel event (should never be seen in userspace)
+ *
+ * Events are split up into ranges as follows:
+ * * BASE_JD_EVENT_RANGE_<description>_START
+ * * BASE_JD_EVENT_RANGE_<description>_END
+ *
+ * code is in <description>'s range when:
+ * BASE_JD_EVENT_RANGE_<description>_START <= code <
+ *   BASE_JD_EVENT_RANGE_<description>_END
+ *
+ * Ranges can be asserted for adjacency by testing that the END of the previous
+ * is equal to the START of the next. This is useful for optimizing some tests
+ * for range.
+ *
+ * A limitation is that the last member of this enum must explicitly be handled
+ * (with an assert-unreachable statement) in switch statements that use
+ * variables of this type. Otherwise, the compiler warns that we have not
+ * handled that enum value.
+ */
+enum base_jd_event_code {
+   /* HW defined exceptions */
+   BASE_JD_EVENT_RANGE_HW_NONFAULT_START = 0,
+
+   /* non-fatal exceptions */
+   BASE_JD_EVENT_NOT_STARTED = 0x00,
+   BASE_JD_EVENT_DONE = 0x01,
+   BASE_JD_EVENT_STOPPED = 0x03,
+   BASE_JD_EVENT_TERMINATED = 0x04,
+   BASE_JD_EVENT_ACTIVE = 0x08,
+
+   BASE_JD_EVENT_RANGE_HW_NONFAULT_END = 0x40,
+   BASE_JD_EVENT_RANGE_HW_FAULT_OR_SW_ERROR_START = 0x40,
+
+   /* job exceptions */
+   BASE_JD_EVENT_JOB_CONFIG_FAULT = 0x40,
+   BASE_JD_EVENT_JOB_POWER_FAULT = 0x41,
+   BASE_JD_EVENT_JOB_READ_FAULT = 0x42,
+   BASE_JD_EVENT_JOB_WRITE_FAULT = 0x43,
+   BASE_JD_EVENT_JOB_AFFINITY_FAULT = 0x44,
+   BASE_JD_EVENT_JOB_BUS_FAULT = 0x48,
+   BASE_JD_EVENT_INSTR_INVALID_PC = 0x50,
+   BASE_JD_EVENT_INSTR_INVALID_ENC = 0x51,
+   BASE_JD_EVENT_INSTR_TYPE_MISMATCH = 0x52,
+   BASE_JD_EVENT_INSTR_OPERAND_FAULT = 0x53,
+   BASE_JD_EVENT_INSTR_TLS_FAULT = 0x54,
+   BASE_JD_EVENT_INSTR_BARRIER_FAULT = 0x55,
+   BASE_JD_EVENT_INSTR_ALIGN_FAULT = 0x56,
+   BASE_JD_EVENT_DATA_INVALID_FAULT = 0x58,
+   BASE_JD_EVENT_TILE_RANGE_FAULT = 0x59,
+   BASE_JD_EVENT_STATE_FAULT = 0x5A,
+   BASE_JD_EVENT_OUT_OF_MEMORY = 0x60,
+   BASE_JD_EVENT_UNKNOWN = 0x7F,
+
+   /* GPU exceptions */
+   BASE_JD_EVENT_DELAYED_BUS_FAULT = 0x80,
+   BASE_JD_EVENT_SHAREABILITY_FAULT = 0x88,
+
+   /* MMU exceptions */
+   BASE_JD_EVENT_TRANSLATION_FAULT_LEVEL1 = 0xC1,
+   BASE_JD_EVENT_TRANSLATION_FAULT_LEVEL2 = 0xC2,
+   BASE_JD_EVENT_TRANSLATION_FAULT_LEVEL3 = 0xC3,
+   BASE_JD_EVENT_TRANSLATION_FAULT_LEVEL4 = 0xC4,
+   BASE_JD_EVENT_PERMISSION_FAULT = 0xC8,
+   BASE_JD_EVENT_TRANSTAB_BUS_FAULT_LEVEL1 = 0xD1,
+   BASE_JD_EVENT_TRANSTAB_BUS_FAULT_LEVEL2 = 0xD2,
+   BASE_JD_EVENT_TRANSTAB_BUS_FAULT_LEVEL3 = 0xD3,
+   BASE_JD_EVENT_TRANSTAB_BUS_FAULT_LEVEL4 = 0xD4,
+   BASE_JD_EVENT_ACCESS_FLAG = 0xD8,
+
+   /* SW defined exceptions */
+   BASE_JD_EVENT_MEM_GROWTH_FAILED =
+      BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_JOB | 0x000,
+   BASE_JD_EVENT_JOB_CANCELLED =
+      BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_JOB | 0x002,
+   BASE_JD_EVENT_JOB_INVALID = BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_JOB | 0x003,
+
+   BASE_JD_EVENT_RANGE_HW_FAULT_OR_SW_ERROR_END =
+      BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_RESERVED | 0x3FF,
+
+   BASE_JD_EVENT_RANGE_SW_SUCCESS_START =
+      BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_SUCCESS | 0x000,
+
+   BASE_JD_EVENT_DRV_TERMINATED = BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_SUCCESS |
+                                  BASE_JD_SW_EVENT_INFO | 0x000,
+
+   BASE_JD_EVENT_RANGE_SW_SUCCESS_END = BASE_JD_SW_EVENT |
+                                        BASE_JD_SW_EVENT_SUCCESS |
+                                        BASE_JD_SW_EVENT_RESERVED | 0x3FF,
+
+   BASE_JD_EVENT_RANGE_KERNEL_ONLY_START =
+      BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_KERNEL | 0x000,
+   BASE_JD_EVENT_REMOVED_FROM_NEXT =
+      BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_KERNEL | BASE_JD_SW_EVENT_JOB | 0x000,
+   BASE_JD_EVENT_END_RP_DONE =
+      BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_KERNEL | BASE_JD_SW_EVENT_JOB | 0x001,
+
+   BASE_JD_EVENT_RANGE_KERNEL_ONLY_END = BASE_JD_SW_EVENT |
+                                         BASE_JD_SW_EVENT_KERNEL |
+                                         BASE_JD_SW_EVENT_RESERVED | 0x3FF
+};
+
+/**
+ * struct base_jd_event_v2 - Event reporting structure
+ *
+ * @event_code:  event code of type @ref base_jd_event_code.
+ * @atom_number: the atom number that has completed.
+ * @padding:     padding.
+ * @udata:       user data.
+ *
+ * This structure is used by the kernel driver to report information
+ * about GPU events. They can either be HW-specific events or low-level
+ * SW events, such as job-chain completion.
+ *
+ * The event code contains an event type field which can be extracted
+ * by ANDing with BASE_JD_SW_EVENT_TYPE_MASK.
+ */
+struct base_jd_event_v2 {
+   __u32 event_code;
+   base_atom_id atom_number;
+   __u8 padding[3];
+   struct base_jd_udata udata;
+};
+
+/**
+ * struct base_dump_cpu_gpu_counters - Structure for
+ *                                     BASE_JD_REQ_SOFT_DUMP_CPU_GPU_COUNTERS
+ *                                     jobs.
+ * @system_time:   gpu timestamp
+ * @cycle_counter: gpu cycle count
+ * @sec:           cpu time(sec)
+ * @usec:          cpu time(usec)
+ * @padding:       padding
+ *
+ * This structure is stored into the memory pointed to by the @jc field
+ * of &struct base_jd_atom.
+ *
+ * It must not occupy the same CPU cache line(s) as any neighboring data.
+ * This is to avoid cases where access to pages containing the structure
+ * is shared between cached and un-cached memory regions, which would
+ * cause memory corruption.
+ */
+
+struct base_dump_cpu_gpu_counters {
+   __u64 system_time;
+   __u64 cycle_counter;
+   __u64 sec;
+   __u32 usec;
+   __u8 padding[36];
+};
+
+/**
+ * struct mali_base_gpu_core_props - GPU core props info
+ *
+ * @product_id: Pro specific value.
+ * @version_status: Status of the GPU release. No defined values, but starts at
+ *   0 and increases by one for each release status (alpha, beta, EAC, etc.).
+ *   4 bit values (0-15).
+ * @minor_revision: Minor release number of the GPU. "P" part of an "RnPn"
+ *   release number.
+ *   8 bit values (0-255).
+ * @major_revision: Major release number of the GPU. "R" part of an "RnPn"
+ *   release number.
+ *   4 bit values (0-15).
+ * @padding: padding to align to 8-byte
+ * @gpu_freq_khz_max: The maximum GPU frequency. Reported to applications by
+ *   clGetDeviceInfo()
+ * @log2_program_counter_size: Size of the shader program counter, in bits.
+ * @texture_features: TEXTURE_FEATURES_x registers, as exposed by the GPU. This
+ *   is a bitpattern where a set bit indicates that the format is supported.
+ *   Before using a texture format, it is recommended that the corresponding
+ *   bit be checked.
+ * @gpu_available_memory_size: Theoretical maximum memory available to the GPU.
+ *   It is unlikely that a client will be able to allocate all of this memory
+ *   for their own purposes, but this at least provides an upper bound on the
+ *   memory available to the GPU.
+ *   This is required for OpenCL's clGetDeviceInfo() call when
+ *   CL_DEVICE_GLOBAL_MEM_SIZE is requested, for OpenCL GPU devices. The
+ *   client will not be expecting to allocate anywhere near this value.
+ * @num_exec_engines: The number of execution engines. Only valid for tGOX
+ *   (Bifrost) GPUs, where GPU_HAS_REG_CORE_FEATURES is defined. Otherwise,
+ *   this is always 0.
+ */
+struct mali_base_gpu_core_props {
+   __u32 product_id;
+   __u16 version_status;
+   __u16 minor_revision;
+   __u16 major_revision;
+   __u16 padding;
+   __u32 gpu_freq_khz_max;
+   __u32 log2_program_counter_size;
+   __u32 texture_features[BASE_GPU_NUM_TEXTURE_FEATURES_REGISTERS];
+   __u64 gpu_available_memory_size;
+   __u8 num_exec_engines;
+};
+
+#endif /* _UAPI_BASE_JM_KERNEL_H_ */
diff -urN mesa-23.0.0/src/panfrost/base/include/jm/mali_kbase_jm_ioctl.h mesa/src/panfrost/base/include/jm/mali_kbase_jm_ioctl.h
--- mesa-23.0.0/src/panfrost/base/include/jm/mali_kbase_jm_ioctl.h	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/base/include/jm/mali_kbase_jm_ioctl.h	2023-03-06 19:19:32.685308686 +0100
@@ -0,0 +1,228 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ *
+ * (C) COPYRIGHT 2020-2022 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU license.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ */
+
+#ifndef _UAPI_KBASE_JM_IOCTL_H_
+#define _UAPI_KBASE_JM_IOCTL_H_
+
+#include <asm-generic/ioctl.h>
+#include <linux/types.h>
+
+/*
+ * 11.1:
+ * - Add BASE_MEM_TILER_ALIGN_TOP under base_mem_alloc_flags
+ * 11.2:
+ * - KBASE_MEM_QUERY_FLAGS can return KBASE_REG_PF_GROW and KBASE_REG_PROTECTED,
+ *   which some user-side clients prior to 11.2 might fault if they received
+ *   them
+ * 11.3:
+ * - New ioctls KBASE_IOCTL_STICKY_RESOURCE_MAP and
+ *   KBASE_IOCTL_STICKY_RESOURCE_UNMAP
+ * 11.4:
+ * - New ioctl KBASE_IOCTL_MEM_FIND_GPU_START_AND_OFFSET
+ * 11.5:
+ * - New ioctl: KBASE_IOCTL_MEM_JIT_INIT (old ioctl renamed to _OLD)
+ * 11.6:
+ * - Added flags field to base_jit_alloc_info structure, which can be used to
+ *   specify pseudo chunked tiler alignment for JIT allocations.
+ * 11.7:
+ * - Removed UMP support
+ * 11.8:
+ * - Added BASE_MEM_UNCACHED_GPU under base_mem_alloc_flags
+ * 11.9:
+ * - Added BASE_MEM_PERMANENT_KERNEL_MAPPING and BASE_MEM_FLAGS_KERNEL_ONLY
+ *   under base_mem_alloc_flags
+ * 11.10:
+ * - Enabled the use of nr_extres field of base_jd_atom_v2 structure for
+ *   JIT_ALLOC and JIT_FREE type softjobs to enable multiple JIT allocations
+ *   with one softjob.
+ * 11.11:
+ * - Added BASE_MEM_GPU_VA_SAME_4GB_PAGE under base_mem_alloc_flags
+ * 11.12:
+ * - Removed ioctl: KBASE_IOCTL_GET_PROFILING_CONTROLS
+ * 11.13:
+ * - New ioctl: KBASE_IOCTL_MEM_EXEC_INIT
+ * 11.14:
+ * - Add BASE_MEM_GROUP_ID_MASK, base_mem_group_id_get, base_mem_group_id_set
+ *   under base_mem_alloc_flags
+ * 11.15:
+ * - Added BASEP_CONTEXT_MMU_GROUP_ID_MASK under base_context_create_flags.
+ * - Require KBASE_IOCTL_SET_FLAGS before BASE_MEM_MAP_TRACKING_HANDLE can be
+ *   passed to mmap().
+ * 11.16:
+ * - Extended ioctl KBASE_IOCTL_MEM_SYNC to accept imported dma-buf.
+ * - Modified (backwards compatible) ioctl KBASE_IOCTL_MEM_IMPORT behavior for
+ *   dma-buf. Now, buffers are mapped on GPU when first imported, no longer
+ *   requiring external resource or sticky resource tracking. UNLESS,
+ *   CONFIG_MALI_DMA_BUF_MAP_ON_DEMAND is enabled.
+ * 11.17:
+ * - Added BASE_JD_REQ_JOB_SLOT.
+ * - Reused padding field in base_jd_atom_v2 to pass job slot number.
+ * - New ioctl: KBASE_IOCTL_GET_CPU_GPU_TIMEINFO
+ * 11.18:
+ * - Added BASE_MEM_IMPORT_SYNC_ON_MAP_UNMAP under base_mem_alloc_flags
+ * 11.19:
+ * - Extended base_jd_atom_v2 to allow a renderpass ID to be specified.
+ * 11.20:
+ * - Added new phys_pages member to kbase_ioctl_mem_jit_init for
+ *   KBASE_IOCTL_MEM_JIT_INIT, previous variants of this renamed to use _10_2
+ *   (replacing '_OLD') and _11_5 suffixes
+ * - Replaced compat_core_req (deprecated in 10.3) with jit_id[2] in
+ *   base_jd_atom_v2. It must currently be initialized to zero.
+ * - Added heap_info_gpu_addr to base_jit_alloc_info, and
+ *   BASE_JIT_ALLOC_HEAP_INFO_IS_SIZE allowable in base_jit_alloc_info's
+ *   flags member. Previous variants of this structure are kept and given _10_2
+ *   and _11_5 suffixes.
+ * - The above changes are checked for safe values in usual builds
+ * 11.21:
+ * - v2.0 of mali_trace debugfs file, which now versions the file separately
+ * 11.22:
+ * - Added base_jd_atom (v3), which is seq_nr + base_jd_atom_v2.
+ *   KBASE_IOCTL_JOB_SUBMIT supports both in parallel.
+ * 11.23:
+ * - Modified KBASE_IOCTL_MEM_COMMIT behavior to reject requests to modify
+ *   the physical memory backing of JIT allocations. This was not supposed
+ *   to be a valid use case, but it was allowed by the previous implementation.
+ * 11.24:
+ * - Added a sysfs file 'serialize_jobs' inside a new sub-directory
+ *   'scheduling'.
+ * 11.25:
+ * - Enabled JIT pressure limit in base/kbase by default
+ * 11.26
+ * - Added kinstr_jm API
+ * 11.27
+ * - Backwards compatible extension to HWC ioctl.
+ * 11.28:
+ * - Added kernel side cache ops needed hint
+ * 11.29:
+ * - Reserve ioctl 52
+ * 11.30:
+ * - Add a new priority level BASE_JD_PRIO_REALTIME
+ * - Add ioctl 54: This controls the priority setting.
+ * 11.31:
+ * - Added BASE_JD_REQ_LIMITED_CORE_MASK.
+ * - Added ioctl 55: set_limited_core_count.
+ * 11.32:
+ * - Added new HW performance counters interface to all GPUs.
+ * 11.33:
+ * - Removed Kernel legacy HWC interface
+ * 11.34:
+ * - First release of new HW performance counters interface.
+ * 11.35:
+ * - Dummy model (no mali) backend will now clear HWC values after each sample
+ */
+#define BASE_UK_VERSION_MAJOR 11
+#define BASE_UK_VERSION_MINOR 35
+
+/**
+ * struct kbase_ioctl_version_check - Check version compatibility between
+ * kernel and userspace
+ *
+ * @major: Major version number
+ * @minor: Minor version number
+ */
+struct kbase_ioctl_version_check {
+   __u16 major;
+   __u16 minor;
+};
+
+#define KBASE_IOCTL_VERSION_CHECK                                              \
+   _IOWR(KBASE_IOCTL_TYPE, 0, struct kbase_ioctl_version_check)
+
+/**
+ * struct kbase_ioctl_job_submit - Submit jobs/atoms to the kernel
+ *
+ * @addr: Memory address of an array of struct base_jd_atom_v2 or v3
+ * @nr_atoms: Number of entries in the array
+ * @stride: sizeof(struct base_jd_atom_v2) or sizeof(struct base_jd_atom)
+ */
+struct kbase_ioctl_job_submit {
+   __u64 addr;
+   __u32 nr_atoms;
+   __u32 stride;
+};
+
+#define KBASE_IOCTL_JOB_SUBMIT                                                 \
+   _IOW(KBASE_IOCTL_TYPE, 2, struct kbase_ioctl_job_submit)
+
+#define KBASE_IOCTL_POST_TERM _IO(KBASE_IOCTL_TYPE, 4)
+
+/**
+ * struct kbase_ioctl_soft_event_update - Update the status of a soft-event
+ * @event: GPU address of the event which has been updated
+ * @new_status: The new status to set
+ * @flags: Flags for future expansion
+ */
+struct kbase_ioctl_soft_event_update {
+   __u64 event;
+   __u32 new_status;
+   __u32 flags;
+};
+
+#define KBASE_IOCTL_SOFT_EVENT_UPDATE                                          \
+   _IOW(KBASE_IOCTL_TYPE, 28, struct kbase_ioctl_soft_event_update)
+
+/**
+ * struct kbase_kinstr_jm_fd_out - Explains the compatibility information for
+ * the `struct kbase_kinstr_jm_atom_state_change` structure returned from the
+ * kernel
+ *
+ * @size:    The size of the `struct kbase_kinstr_jm_atom_state_change`
+ * @version: Represents a breaking change in the
+ *           `struct kbase_kinstr_jm_atom_state_change`
+ * @padding: Explicit padding to get the structure up to 64bits. See
+ * https://www.kernel.org/doc/Documentation/ioctl/botching-up-ioctls.rst
+ *
+ * The `struct kbase_kinstr_jm_atom_state_change` may have extra members at the
+ * end of the structure that older user space might not understand. If the
+ * `version` is the same, the structure is still compatible with newer kernels.
+ * The `size` can be used to cast the opaque memory returned from the kernel.
+ */
+struct kbase_kinstr_jm_fd_out {
+   __u16 size;
+   __u8 version;
+   __u8 padding[5];
+};
+
+/**
+ * struct kbase_kinstr_jm_fd_in - Options when creating the file descriptor
+ *
+ * @count: Number of atom states that can be stored in the kernel circular
+ *         buffer. Must be a power of two
+ * @padding: Explicit padding to get the structure up to 64bits. See
+ * https://www.kernel.org/doc/Documentation/ioctl/botching-up-ioctls.rst
+ */
+struct kbase_kinstr_jm_fd_in {
+   __u16 count;
+   __u8 padding[6];
+};
+
+union kbase_kinstr_jm_fd {
+   struct kbase_kinstr_jm_fd_in in;
+   struct kbase_kinstr_jm_fd_out out;
+};
+
+#define KBASE_IOCTL_KINSTR_JM_FD                                               \
+   _IOWR(KBASE_IOCTL_TYPE, 51, union kbase_kinstr_jm_fd)
+
+#define KBASE_IOCTL_VERSION_CHECK_RESERVED                                     \
+   _IOWR(KBASE_IOCTL_TYPE, 52, struct kbase_ioctl_version_check)
+
+#endif /* _UAPI_KBASE_JM_IOCTL_H_ */
diff -urN mesa-23.0.0/src/panfrost/base/include/mali_base_common_kernel.h mesa/src/panfrost/base/include/mali_base_common_kernel.h
--- mesa-23.0.0/src/panfrost/base/include/mali_base_common_kernel.h	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/base/include/mali_base_common_kernel.h	2023-03-06 19:19:32.697308765 +0100
@@ -0,0 +1,235 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ *
+ * (C) COPYRIGHT 2022 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU license.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ */
+
+#ifndef _UAPI_BASE_COMMON_KERNEL_H_
+#define _UAPI_BASE_COMMON_KERNEL_H_
+
+#include <linux/types.h>
+
+struct base_mem_handle {
+   struct {
+      __u64 handle;
+   } basep;
+};
+
+#define BASE_GPU_NUM_TEXTURE_FEATURES_REGISTERS 4
+
+/* Memory allocation, access/hint flags & mask.
+ *
+ * See base_mem_alloc_flags.
+ */
+
+/* IN */
+/* Read access CPU side
+ */
+#define BASE_MEM_PROT_CPU_RD ((base_mem_alloc_flags)1 << 0)
+
+/* Write access CPU side
+ */
+#define BASE_MEM_PROT_CPU_WR ((base_mem_alloc_flags)1 << 1)
+
+/* Read access GPU side
+ */
+#define BASE_MEM_PROT_GPU_RD ((base_mem_alloc_flags)1 << 2)
+
+/* Write access GPU side
+ */
+#define BASE_MEM_PROT_GPU_WR ((base_mem_alloc_flags)1 << 3)
+
+/* Execute allowed on the GPU side
+ */
+#define BASE_MEM_PROT_GPU_EX ((base_mem_alloc_flags)1 << 4)
+
+/* Will be permanently mapped in kernel space.
+ * Flag is only allowed on allocations originating from kbase.
+ */
+#define BASEP_MEM_PERMANENT_KERNEL_MAPPING ((base_mem_alloc_flags)1 << 5)
+
+/* The allocation will completely reside within the same 4GB chunk in the GPU
+ * virtual space.
+ * Since this flag is primarily required only for the TLS memory which will
+ * not be used to contain executable code and also not used for Tiler heap,
+ * it can't be used along with BASE_MEM_PROT_GPU_EX and TILER_ALIGN_TOP flags.
+ */
+#define BASE_MEM_GPU_VA_SAME_4GB_PAGE ((base_mem_alloc_flags)1 << 6)
+
+/* Userspace is not allowed to free this memory.
+ * Flag is only allowed on allocations originating from kbase.
+ */
+#define BASEP_MEM_NO_USER_FREE ((base_mem_alloc_flags)1 << 7)
+
+/* Grow backing store on GPU Page Fault
+ */
+#define BASE_MEM_GROW_ON_GPF ((base_mem_alloc_flags)1 << 9)
+
+/* Page coherence Outer shareable, if available
+ */
+#define BASE_MEM_COHERENT_SYSTEM ((base_mem_alloc_flags)1 << 10)
+
+/* Page coherence Inner shareable
+ */
+#define BASE_MEM_COHERENT_LOCAL ((base_mem_alloc_flags)1 << 11)
+
+/* IN/OUT */
+/* Should be cached on the CPU, returned if actually cached
+ */
+#define BASE_MEM_CACHED_CPU ((base_mem_alloc_flags)1 << 12)
+
+/* IN/OUT */
+/* Must have same VA on both the GPU and the CPU
+ */
+#define BASE_MEM_SAME_VA ((base_mem_alloc_flags)1 << 13)
+
+/* OUT */
+/* Must call mmap to acquire a GPU address for the allocation
+ */
+#define BASE_MEM_NEED_MMAP ((base_mem_alloc_flags)1 << 14)
+
+/* IN */
+/* Page coherence Outer shareable, required.
+ */
+#define BASE_MEM_COHERENT_SYSTEM_REQUIRED ((base_mem_alloc_flags)1 << 15)
+
+/* Protected memory
+ */
+#define BASE_MEM_PROTECTED ((base_mem_alloc_flags)1 << 16)
+
+/* Not needed physical memory
+ */
+#define BASE_MEM_DONT_NEED ((base_mem_alloc_flags)1 << 17)
+
+/* Must use shared CPU/GPU zone (SAME_VA zone) but doesn't require the
+ * addresses to be the same
+ */
+#define BASE_MEM_IMPORT_SHARED ((base_mem_alloc_flags)1 << 18)
+
+/* Should be uncached on the GPU, will work only for GPUs using AARCH64 mmu
+ * mode. Some components within the GPU might only be able to access memory
+ * that is GPU cacheable. Refer to the specific GPU implementation for more
+ * details. The 3 shareability flags will be ignored for GPU uncached memory.
+ * If used while importing USER_BUFFER type memory, then the import will fail
+ * if the memory is not aligned to GPU and CPU cache line width.
+ */
+#define BASE_MEM_UNCACHED_GPU ((base_mem_alloc_flags)1 << 21)
+
+/*
+ * Bits [22:25] for group_id (0~15).
+ *
+ * base_mem_group_id_set() should be used to pack a memory group ID into a
+ * base_mem_alloc_flags value instead of accessing the bits directly.
+ * base_mem_group_id_get() should be used to extract the memory group ID from
+ * a base_mem_alloc_flags value.
+ */
+#define BASEP_MEM_GROUP_ID_SHIFT 22
+#define BASE_MEM_GROUP_ID_MASK                                                 \
+   ((base_mem_alloc_flags)0xF << BASEP_MEM_GROUP_ID_SHIFT)
+
+/* Must do CPU cache maintenance when imported memory is mapped/unmapped
+ * on GPU. Currently applicable to dma-buf type only.
+ */
+#define BASE_MEM_IMPORT_SYNC_ON_MAP_UNMAP ((base_mem_alloc_flags)1 << 26)
+
+/* OUT */
+/* Kernel side cache sync ops required */
+#define BASE_MEM_KERNEL_SYNC ((base_mem_alloc_flags)1 << 28)
+
+/* Number of bits used as flags for base memory management
+ *
+ * Must be kept in sync with the base_mem_alloc_flags flags
+ */
+#define BASE_MEM_FLAGS_NR_BITS 30
+
+/* A mask for all output bits, excluding IN/OUT bits.
+ */
+#define BASE_MEM_FLAGS_OUTPUT_MASK BASE_MEM_NEED_MMAP
+
+/* A mask for all input bits, including IN/OUT bits.
+ */
+#define BASE_MEM_FLAGS_INPUT_MASK                                              \
+   (((1 << BASE_MEM_FLAGS_NR_BITS) - 1) & ~BASE_MEM_FLAGS_OUTPUT_MASK)
+
+/* Special base mem handles.
+ */
+#define BASEP_MEM_INVALID_HANDLE           (0ul)
+#define BASE_MEM_MMU_DUMP_HANDLE           (1ul << LOCAL_PAGE_SHIFT)
+#define BASE_MEM_TRACE_BUFFER_HANDLE       (2ul << LOCAL_PAGE_SHIFT)
+#define BASE_MEM_MAP_TRACKING_HANDLE       (3ul << LOCAL_PAGE_SHIFT)
+#define BASEP_MEM_WRITE_ALLOC_PAGES_HANDLE (4ul << LOCAL_PAGE_SHIFT)
+/* reserved handles ..-47<<PAGE_SHIFT> for future special handles */
+#define BASE_MEM_COOKIE_BASE (64ul << LOCAL_PAGE_SHIFT)
+#define BASE_MEM_FIRST_FREE_ADDRESS                                            \
+   ((BITS_PER_LONG << LOCAL_PAGE_SHIFT) + BASE_MEM_COOKIE_BASE)
+
+/* Flags to pass to ::base_context_init.
+ * Flags can be ORed together to enable multiple things.
+ *
+ * These share the same space as BASEP_CONTEXT_FLAG_*, and so must
+ * not collide with them.
+ */
+typedef __u32 base_context_create_flags;
+
+/* Flags for base context */
+
+/* No flags set */
+#define BASE_CONTEXT_CREATE_FLAG_NONE ((base_context_create_flags)0)
+
+/* Base context is embedded in a cctx object (flag used for CINSTR
+ * software counter macros)
+ */
+#define BASE_CONTEXT_CCTX_EMBEDDED ((base_context_create_flags)1 << 0)
+
+/* Base context is a 'System Monitor' context for Hardware counters.
+ *
+ * One important side effect of this is that job submission is disabled.
+ */
+#define BASE_CONTEXT_SYSTEM_MONITOR_SUBMIT_DISABLED                            \
+   ((base_context_create_flags)1 << 1)
+
+/* Bit-shift used to encode a memory group ID in base_context_create_flags
+ */
+#define BASEP_CONTEXT_MMU_GROUP_ID_SHIFT (3)
+
+/* Bitmask used to encode a memory group ID in base_context_create_flags
+ */
+#define BASEP_CONTEXT_MMU_GROUP_ID_MASK                                        \
+   ((base_context_create_flags)0xF << BASEP_CONTEXT_MMU_GROUP_ID_SHIFT)
+
+/* Bitpattern describing the base_context_create_flags that can be
+ * passed to the kernel
+ */
+#define BASEP_CONTEXT_CREATE_KERNEL_FLAGS                                      \
+   (BASE_CONTEXT_SYSTEM_MONITOR_SUBMIT_DISABLED |                              \
+    BASEP_CONTEXT_MMU_GROUP_ID_MASK)
+
+/* Flags for base tracepoint
+ */
+
+/* Enable additional tracepoints for latency measurements (TL_ATOM_READY,
+ * TL_ATOM_DONE, TL_ATOM_PRIO_CHANGE, TL_ATOM_EVENT_POST)
+ */
+#define BASE_TLSTREAM_ENABLE_LATENCY_TRACEPOINTS (1 << 0)
+
+/* Indicate that job dumping is enabled. This could affect certain timers
+ * to account for the performance impact.
+ */
+#define BASE_TLSTREAM_JOB_DUMPING_ENABLED (1 << 1)
+
+#endif /* _UAPI_BASE_COMMON_KERNEL_H_ */
diff -urN mesa-23.0.0/src/panfrost/base/include/mali_base_kernel.h mesa/src/panfrost/base/include/mali_base_kernel.h
--- mesa-23.0.0/src/panfrost/base/include/mali_base_kernel.h	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/base/include/mali_base_kernel.h	2023-03-06 19:19:32.704308811 +0100
@@ -0,0 +1,696 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ *
+ * (C) COPYRIGHT 2010-2022 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU license.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ */
+
+/*
+ * Base structures shared with the kernel.
+ */
+
+#ifndef _UAPI_BASE_KERNEL_H_
+#define _UAPI_BASE_KERNEL_H_
+
+#include <linux/types.h>
+#include "mali_base_common_kernel.h"
+
+#define BASE_MAX_COHERENT_GROUPS 16
+
+#if defined(PAGE_MASK) && defined(PAGE_SHIFT)
+#define LOCAL_PAGE_SHIFT PAGE_SHIFT
+#define LOCAL_PAGE_LSB   ~PAGE_MASK
+#else
+#ifndef OSU_CONFIG_CPU_PAGE_SIZE_LOG2
+#define OSU_CONFIG_CPU_PAGE_SIZE_LOG2 12
+#endif
+
+#if defined(OSU_CONFIG_CPU_PAGE_SIZE_LOG2)
+#define LOCAL_PAGE_SHIFT OSU_CONFIG_CPU_PAGE_SIZE_LOG2
+#define LOCAL_PAGE_LSB   ((1ul << OSU_CONFIG_CPU_PAGE_SIZE_LOG2) - 1)
+#else
+#error Failed to find page size
+#endif
+#endif
+
+/* Physical memory group ID for normal usage.
+ */
+#define BASE_MEM_GROUP_DEFAULT (0)
+
+/* Number of physical memory groups.
+ */
+#define BASE_MEM_GROUP_COUNT (16)
+
+/**
+ * typedef base_mem_alloc_flags - Memory allocation, access/hint flags.
+ *
+ * A combination of MEM_PROT/MEM_HINT flags must be passed to each allocator
+ * in order to determine the best cache policy. Some combinations are
+ * of course invalid (e.g. MEM_PROT_CPU_WR | MEM_HINT_CPU_RD),
+ * which defines a write-only region on the CPU side, which is
+ * heavily read by the CPU...
+ * Other flags are only meaningful to a particular allocator.
+ * More flags can be added to this list, as long as they don't clash
+ * (see BASE_MEM_FLAGS_NR_BITS for the number of the first free bit).
+ */
+typedef __u32 base_mem_alloc_flags;
+
+/* A mask for all the flags which are modifiable via the base_mem_set_flags
+ * interface.
+ */
+#define BASE_MEM_FLAGS_MODIFIABLE                                              \
+   (BASE_MEM_DONT_NEED | BASE_MEM_COHERENT_SYSTEM | BASE_MEM_COHERENT_LOCAL)
+
+/* A mask of all the flags that can be returned via the base_mem_get_flags()
+ * interface.
+ */
+#define BASE_MEM_FLAGS_QUERYABLE                                               \
+   (BASE_MEM_FLAGS_INPUT_MASK &                                                \
+    ~(BASE_MEM_SAME_VA | BASE_MEM_COHERENT_SYSTEM_REQUIRED |                   \
+      BASE_MEM_DONT_NEED | BASE_MEM_IMPORT_SHARED | BASE_MEM_FLAGS_RESERVED |  \
+      BASEP_MEM_FLAGS_KERNEL_ONLY))
+
+/**
+ * enum base_mem_import_type - Memory types supported by @a base_mem_import
+ *
+ * @BASE_MEM_IMPORT_TYPE_INVALID: Invalid type
+ * @BASE_MEM_IMPORT_TYPE_UMM: UMM import. Handle type is a file descriptor (int)
+ * @BASE_MEM_IMPORT_TYPE_USER_BUFFER: User buffer import. Handle is a
+ * base_mem_import_user_buffer
+ *
+ * Each type defines what the supported handle type is.
+ *
+ * If any new type is added here ARM must be contacted
+ * to allocate a numeric value for it.
+ * Do not just add a new type without synchronizing with ARM
+ * as future releases from ARM might include other new types
+ * which could clash with your custom types.
+ */
+enum base_mem_import_type {
+   BASE_MEM_IMPORT_TYPE_INVALID = 0,
+   /*
+    * Import type with value 1 is deprecated.
+    */
+   BASE_MEM_IMPORT_TYPE_UMM = 2,
+   BASE_MEM_IMPORT_TYPE_USER_BUFFER = 3
+};
+
+/**
+ * struct base_mem_import_user_buffer - Handle of an imported user buffer
+ *
+ * @ptr:	address of imported user buffer
+ * @length:	length of imported user buffer in bytes
+ *
+ * This structure is used to represent a handle of an imported user buffer.
+ */
+
+struct base_mem_import_user_buffer {
+   __u64 ptr;
+   __u64 length;
+};
+
+/* Mask to detect 4GB boundary alignment */
+#define BASE_MEM_MASK_4GB 0xfffff000UL
+/* Mask to detect 4GB boundary (in page units) alignment */
+#define BASE_MEM_PFN_MASK_4GB (BASE_MEM_MASK_4GB >> LOCAL_PAGE_SHIFT)
+
+/* Limit on the 'extension' parameter for an allocation with the
+ * BASE_MEM_TILER_ALIGN_TOP flag set
+ *
+ * This is the same as the maximum limit for a Buffer Descriptor's chunk size
+ */
+#define BASE_MEM_TILER_ALIGN_TOP_EXTENSION_MAX_PAGES_LOG2                      \
+   (21u - (LOCAL_PAGE_SHIFT))
+#define BASE_MEM_TILER_ALIGN_TOP_EXTENSION_MAX_PAGES                           \
+   (1ull << (BASE_MEM_TILER_ALIGN_TOP_EXTENSION_MAX_PAGES_LOG2))
+
+/* Bit mask of cookies used for memory allocation setup */
+#define KBASE_COOKIE_MASK ~1UL /* bit 0 is reserved */
+
+/* Maximum size allowed in a single KBASE_IOCTL_MEM_ALLOC call */
+#define KBASE_MEM_ALLOC_MAX_SIZE ((8ull << 30) >> PAGE_SHIFT) /* 8 GB */
+
+/*
+ * struct base_fence - Cross-device synchronisation fence.
+ *
+ * A fence is used to signal when the GPU has finished accessing a resource that
+ * may be shared with other devices, and also to delay work done asynchronously
+ * by the GPU until other devices have finished accessing a shared resource.
+ */
+struct base_fence {
+   struct {
+      int fd;
+      int stream_fd;
+   } basep;
+};
+
+/**
+ * struct base_mem_aliasing_info - Memory aliasing info
+ *
+ * @handle: Handle to alias, can be BASE_MEM_WRITE_ALLOC_PAGES_HANDLE
+ * @offset: Offset within the handle to start aliasing from, in pages.
+ *          Not used with BASE_MEM_WRITE_ALLOC_PAGES_HANDLE.
+ * @length: Length to alias, in pages. For BASE_MEM_WRITE_ALLOC_PAGES_HANDLE
+ *          specifies the number of times the special page is needed.
+ *
+ * Describes a memory handle to be aliased.
+ * A subset of the handle can be chosen for aliasing, given an offset and a
+ * length.
+ * A special handle BASE_MEM_WRITE_ALLOC_PAGES_HANDLE is used to represent a
+ * region where a special page is mapped with a write-alloc cache setup,
+ * typically used when the write result of the GPU isn't needed, but the GPU
+ * must write anyway.
+ *
+ * Offset and length are specified in pages.
+ * Offset must be within the size of the handle.
+ * Offset+length must not overrun the size of the handle.
+ */
+struct base_mem_aliasing_info {
+   struct base_mem_handle handle;
+   __u64 offset;
+   __u64 length;
+};
+
+/* Maximum percentage of just-in-time memory allocation trimming to perform
+ * on free.
+ */
+#define BASE_JIT_MAX_TRIM_LEVEL (100)
+
+/* Maximum number of concurrent just-in-time memory allocations.
+ */
+#define BASE_JIT_ALLOC_COUNT (255)
+
+/* base_jit_alloc_info in use for kernel driver versions 10.2 to early 11.5
+ *
+ * jit_version is 1
+ *
+ * Due to the lack of padding specified, user clients between 32 and 64-bit
+ * may have assumed a different size of the struct
+ *
+ * An array of structures was not supported
+ */
+struct base_jit_alloc_info_10_2 {
+   __u64 gpu_alloc_addr;
+   __u64 va_pages;
+   __u64 commit_pages;
+   __u64 extension;
+   __u8 id;
+};
+
+/* base_jit_alloc_info introduced by kernel driver version 11.5, and in use up
+ * to 11.19
+ *
+ * This structure had a number of modifications during and after kernel driver
+ * version 11.5, but remains size-compatible throughout its version history, and
+ * with earlier variants compatible with future variants by requiring
+ * zero-initialization to the unused space in the structure.
+ *
+ * jit_version is 2
+ *
+ * Kernel driver version history:
+ * 11.5: Initial introduction with 'usage_id' and padding[5]. All padding bytes
+ *       must be zero. Kbase minor version was not incremented, so some
+ *       versions of 11.5 do not have this change.
+ * 11.5: Added 'bin_id' and 'max_allocations', replacing 2 padding bytes (Kbase
+ *       minor version not incremented)
+ * 11.6: Added 'flags', replacing 1 padding byte
+ * 11.10: Arrays of this structure are supported
+ */
+struct base_jit_alloc_info_11_5 {
+   __u64 gpu_alloc_addr;
+   __u64 va_pages;
+   __u64 commit_pages;
+   __u64 extension;
+   __u8 id;
+   __u8 bin_id;
+   __u8 max_allocations;
+   __u8 flags;
+   __u8 padding[2];
+   __u16 usage_id;
+};
+
+/**
+ * struct base_jit_alloc_info - Structure which describes a JIT allocation
+ *                              request.
+ * @gpu_alloc_addr:             The GPU virtual address to write the JIT
+ *                              allocated GPU virtual address to.
+ * @va_pages:                   The minimum number of virtual pages required.
+ * @commit_pages:               The minimum number of physical pages which
+ *                              should back the allocation.
+ * @extension:                     Granularity of physical pages to grow the
+ *                              allocation by during a fault.
+ * @id:                         Unique ID provided by the caller, this is used
+ *                              to pair allocation and free requests.
+ *                              Zero is not a valid value.
+ * @bin_id:                     The JIT allocation bin, used in conjunction with
+ *                              @max_allocations to limit the number of each
+ *                              type of JIT allocation.
+ * @max_allocations:            The maximum number of allocations allowed within
+ *                              the bin specified by @bin_id. Should be the same
+ *                              for all allocations within the same bin.
+ * @flags:                      flags specifying the special requirements for
+ *                              the JIT allocation, see
+ *                              %BASE_JIT_ALLOC_VALID_FLAGS
+ * @padding:                    Expansion space - should be initialised to zero
+ * @usage_id:                   A hint about which allocation should be reused.
+ *                              The kernel should attempt to use a previous
+ *                              allocation with the same usage_id
+ * @heap_info_gpu_addr:         Pointer to an object in GPU memory describing
+ *                              the actual usage of the region.
+ *
+ * jit_version is 3.
+ *
+ * When modifications are made to this structure, it is still compatible with
+ * jit_version 3 when: a) the size is unchanged, and b) new members only
+ * replace the padding bytes.
+ *
+ * Previous jit_version history:
+ * jit_version == 1, refer to &base_jit_alloc_info_10_2
+ * jit_version == 2, refer to &base_jit_alloc_info_11_5
+ *
+ * Kbase version history:
+ * 11.20: added @heap_info_gpu_addr
+ */
+struct base_jit_alloc_info {
+   __u64 gpu_alloc_addr;
+   __u64 va_pages;
+   __u64 commit_pages;
+   __u64 extension;
+   __u8 id;
+   __u8 bin_id;
+   __u8 max_allocations;
+   __u8 flags;
+   __u8 padding[2];
+   __u16 usage_id;
+   __u64 heap_info_gpu_addr;
+};
+
+enum base_external_resource_access {
+   BASE_EXT_RES_ACCESS_SHARED,
+   BASE_EXT_RES_ACCESS_EXCLUSIVE
+};
+
+struct base_external_resource {
+   __u64 ext_resource;
+};
+
+/**
+ * BASE_EXT_RES_COUNT_MAX - The maximum number of external resources
+ * which can be mapped/unmapped in a single request.
+ */
+#define BASE_EXT_RES_COUNT_MAX 10
+
+/**
+ * struct base_external_resource_list - Structure which describes a list of
+ *                                      external resources.
+ * @count:                              The number of resources.
+ * @ext_res:                            Array of external resources which is
+ *                                      sized at allocation time.
+ */
+struct base_external_resource_list {
+   __u64 count;
+   struct base_external_resource ext_res[1];
+};
+
+struct base_jd_debug_copy_buffer {
+   __u64 address;
+   __u64 size;
+   struct base_external_resource extres;
+};
+
+#define GPU_MAX_JOB_SLOTS 16
+
+/**
+ * DOC: User-side Base GPU Property Queries
+ *
+ * The User-side Base GPU Property Query interface encapsulates two
+ * sub-modules:
+ *
+ * - "Dynamic GPU Properties"
+ * - "Base Platform Config GPU Properties"
+ *
+ * Base only deals with properties that vary between different GPU
+ * implementations - the Dynamic GPU properties and the Platform Config
+ * properties.
+ *
+ * For properties that are constant for the GPU Architecture, refer to the
+ * GPU module. However, we will discuss their relevance here just to
+ * provide background information.
+ *
+ * About the GPU Properties in Base and GPU modules
+ *
+ * The compile-time properties (Platform Config, GPU Compile-time
+ * properties) are exposed as pre-processor macros.
+ *
+ * Complementing the compile-time properties are the Dynamic GPU
+ * Properties, which act as a conduit for the GPU Configuration
+ * Discovery.
+ *
+ * In general, the dynamic properties are present to verify that the platform
+ * has been configured correctly with the right set of Platform Config
+ * Compile-time Properties.
+ *
+ * As a consistent guide across the entire DDK, the choice for dynamic or
+ * compile-time should consider the following, in order:
+ * 1. Can the code be written so that it doesn't need to know the
+ * implementation limits at all?
+ * 2. If you need the limits, get the information from the Dynamic Property
+ * lookup. This should be done once as you fetch the context, and then cached
+ * as part of the context data structure, so it's cheap to access.
+ * 3. If there's a clear and arguable inefficiency in using Dynamic Properties,
+ * then use a Compile-Time Property (Platform Config, or GPU Compile-time
+ * property). Examples of where this might be sensible follow:
+ *  - Part of a critical inner-loop
+ *  - Frequent re-use throughout the driver, causing significant extra load
+ * instructions or control flow that would be worthwhile optimizing out.
+ *
+ * We cannot provide an exhaustive set of examples, neither can we provide a
+ * rule for every possible situation. Use common sense, and think about: what
+ * the rest of the driver will be doing; how the compiler might represent the
+ * value if it is a compile-time constant; whether an OEM shipping multiple
+ * devices would benefit much more from a single DDK binary, instead of
+ * insignificant micro-optimizations.
+ *
+ * Dynamic GPU Properties
+ *
+ * Dynamic GPU properties are presented in two sets:
+ * 1. the commonly used properties in @ref base_gpu_props, which have been
+ * unpacked from GPU register bitfields.
+ * 2. The full set of raw, unprocessed properties in gpu_raw_gpu_props
+ * (also a member of base_gpu_props). All of these are presented in
+ * the packed form, as presented by the GPU  registers themselves.
+ *
+ * The raw properties in gpu_raw_gpu_props are necessary to
+ * allow a user of the Mali Tools (e.g. PAT) to determine "Why is this device
+ * behaving differently?". In this case, all information about the
+ * configuration is potentially useful, but it does not need to be processed
+ * by the driver. Instead, the raw registers can be processed by the Mali
+ * Tools software on the host PC.
+ *
+ * The properties returned extend the GPU Configuration Discovery
+ * registers. For example, GPU clock speed is not specified in the GPU
+ * Architecture, but is necessary for OpenCL's clGetDeviceInfo() function.
+ *
+ * The GPU properties are obtained by a call to
+ * base_get_gpu_props(). This simply returns a pointer to a const
+ * base_gpu_props structure. It is constant for the life of a base
+ * context. Multiple calls to base_get_gpu_props() to a base context
+ * return the same pointer to a constant structure. This avoids cache pollution
+ * of the common data.
+ *
+ * This pointer must not be freed, because it does not point to the start of a
+ * region allocated by the memory allocator; instead, just close the @ref
+ * base_context.
+ *
+ *
+ * Kernel Operation
+ *
+ * During Base Context Create time, user-side makes a single kernel call:
+ * - A call to fill user memory with GPU information structures
+ *
+ * The kernel-side will fill the provided the entire processed base_gpu_props
+ * structure, because this information is required in both
+ * user and kernel side; it does not make sense to decode it twice.
+ *
+ * Coherency groups must be derived from the bitmasks, but this can be done
+ * kernel side, and just once at kernel startup: Coherency groups must already
+ * be known kernel-side, to support chains that specify a 'Only Coherent Group'
+ * SW requirement, or 'Only Coherent Group with Tiler' SW requirement.
+ *
+ * Coherency Group calculation
+ *
+ * Creation of the coherent group data is done at device-driver startup, and so
+ * is one-time. This will most likely involve a loop with CLZ, shifting, and
+ * bit clearing on the L2_PRESENT mask, depending on whether the
+ * system is L2 Coherent. The number of shader cores is done by a
+ * population count, since faulty cores may be disabled during production,
+ * producing a non-contiguous mask.
+ *
+ * The memory requirements for this algorithm can be determined either by a __u64
+ * population count on the L2_PRESENT mask (a LUT helper already is
+ * required for the above), or simple assumption that there can be no more than
+ * 16 coherent groups, since core groups are typically 4 cores.
+ */
+
+/*
+ * More information is possible - but associativity and bus width are not
+ * required by upper-level apis.
+ */
+struct mali_base_gpu_l2_cache_props {
+   __u8 log2_line_size;
+   __u8 log2_cache_size;
+   __u8 num_l2_slices; /* Number of L2C slices. 1 or higher */
+   __u8 padding[5];
+};
+
+struct mali_base_gpu_tiler_props {
+   __u32 bin_size_bytes;    /* Max is 4*2^15 */
+   __u32 max_active_levels; /* Max is 2^15 */
+};
+
+/**
+ * struct mali_base_gpu_thread_props - GPU threading system details.
+ * @max_threads: Max. number of threads per core
+ * @max_workgroup_size:     Max. number of threads per workgroup
+ * @max_barrier_size:       Max. number of threads that can synchronize on a
+ *                          simple barrier
+ * @max_registers:          Total size [1..65535] of the register file available
+ *                          per core.
+ * @max_task_queue:         Max. tasks [1..255] which may be sent to a core
+ *                          before it becomes blocked.
+ * @max_thread_group_split: Max. allowed value [1..15] of the Thread Group Split
+ *                          field.
+ * @impl_tech:              0 = Not specified, 1 = Silicon, 2 = FPGA,
+ *                          3 = SW Model/Emulation
+ * @padding:                padding to align to 8-byte
+ * @tls_alloc:              Number of threads per core that TLS must be
+ *                          allocated for
+ */
+struct mali_base_gpu_thread_props {
+   __u32 max_threads;
+   __u32 max_workgroup_size;
+   __u32 max_barrier_size;
+   __u16 max_registers;
+   __u8 max_task_queue;
+   __u8 max_thread_group_split;
+   __u8 impl_tech;
+   __u8 padding[3];
+   __u32 tls_alloc;
+};
+
+/**
+ * struct mali_base_gpu_coherent_group - descriptor for a coherent group
+ * @core_mask: Core restriction mask required for the group
+ * @num_cores: Number of cores in the group
+ * @padding:   padding to align to 8-byte
+ *
+ * \c core_mask exposes all cores in that coherent group, and \c num_cores
+ * provides a cached population-count for that mask.
+ *
+ * @note Whilst all cores are exposed in the mask, not all may be available to
+ *       the application, depending on the Kernel Power policy.
+ *
+ * @note if u64s must be 8-byte aligned, then this structure has 32-bits of
+ *       wastage.
+ */
+struct mali_base_gpu_coherent_group {
+   __u64 core_mask;
+   __u16 num_cores;
+   __u16 padding[3];
+};
+
+/**
+ * struct mali_base_gpu_coherent_group_info - Coherency group information
+ * @num_groups: Number of coherent groups in the GPU.
+ * @num_core_groups: Number of core groups (coherent or not) in the GPU.
+ *                   Equivalent to the number of L2 Caches.
+ *                   The GPU Counter dumping writes 2048 bytes per core group,
+ *                   regardless of whether the core groups are coherent or not.
+ *                   Hence this member is needed to calculate how much memory
+ *                   is required for dumping.
+ *                   @note Do not use it to work out how many valid elements
+ *                         are in the group[] member. Use num_groups instead.
+ * @coherency: Coherency features of the memory, accessed by gpu_mem_features
+ *             methods
+ * @padding: padding to align to 8-byte
+ * @group: Descriptors of coherent groups
+ *
+ * Note that the sizes of the members could be reduced. However, the \c group
+ * member might be 8-byte aligned to ensure the __u64 core_mask is 8-byte
+ * aligned, thus leading to wastage if the other members sizes were reduced.
+ *
+ * The groups are sorted by core mask. The core masks are non-repeating and do
+ * not intersect.
+ */
+struct mali_base_gpu_coherent_group_info {
+   __u32 num_groups;
+   __u32 num_core_groups;
+   __u32 coherency;
+   __u32 padding;
+   struct mali_base_gpu_coherent_group group[BASE_MAX_COHERENT_GROUPS];
+};
+
+#if MALI_USE_CSF
+#include "csf/mali_base_csf_kernel.h"
+#else
+#include "jm/mali_base_jm_kernel.h"
+#endif
+
+/**
+ * struct gpu_raw_gpu_props - A complete description of the GPU's Hardware
+ *                            Configuration Discovery registers.
+ * @shader_present: Shader core present bitmap
+ * @tiler_present: Tiler core present bitmap
+ * @l2_present: Level 2 cache present bitmap
+ * @stack_present: Core stack present bitmap
+ * @l2_features: L2 features
+ * @core_features: Core features
+ * @mem_features: Mem features
+ * @mmu_features: Mmu features
+ * @as_present: Bitmap of address spaces present
+ * @js_present: Job slots present
+ * @js_features: Array of job slot features.
+ * @tiler_features: Tiler features
+ * @texture_features: TEXTURE_FEATURES_x registers, as exposed by the GPU
+ * @gpu_id: GPU and revision identifier
+ * @thread_max_threads: Maximum number of threads per core
+ * @thread_max_workgroup_size: Maximum number of threads per workgroup
+ * @thread_max_barrier_size: Maximum number of threads per barrier
+ * @thread_features: Thread features
+ * @coherency_mode: Note: This is the _selected_ coherency mode rather than the
+ *                  available modes as exposed in the coherency_features register
+ * @thread_tls_alloc: Number of threads per core that TLS must be allocated for
+ * @gpu_features: GPU features
+ *
+ * The information is presented inefficiently for access. For frequent access,
+ * the values should be better expressed in an unpacked form in the
+ * base_gpu_props structure.
+ *
+ * The raw properties in gpu_raw_gpu_props are necessary to
+ * allow a user of the Mali Tools (e.g. PAT) to determine "Why is this device
+ * behaving differently?". In this case, all information about the
+ * configuration is potentially useful, but it does not need to be processed
+ * by the driver. Instead, the raw registers can be processed by the Mali
+ * Tools software on the host PC.
+ *
+ */
+struct gpu_raw_gpu_props {
+   __u64 shader_present;
+   __u64 tiler_present;
+   __u64 l2_present;
+   __u64 stack_present;
+   __u32 l2_features;
+   __u32 core_features;
+   __u32 mem_features;
+   __u32 mmu_features;
+
+   __u32 as_present;
+
+   __u32 js_present;
+   __u32 js_features[GPU_MAX_JOB_SLOTS];
+   __u32 tiler_features;
+   __u32 texture_features[BASE_GPU_NUM_TEXTURE_FEATURES_REGISTERS];
+
+   __u32 gpu_id;
+
+   __u32 thread_max_threads;
+   __u32 thread_max_workgroup_size;
+   __u32 thread_max_barrier_size;
+   __u32 thread_features;
+
+   /*
+    * Note: This is the _selected_ coherency mode rather than the
+    * available modes as exposed in the coherency_features register.
+    */
+   __u32 coherency_mode;
+
+   __u32 thread_tls_alloc;
+   __u64 gpu_features;
+};
+
+/**
+ * struct base_gpu_props - Return structure for base_get_gpu_props().
+ * @core_props:     Core props.
+ * @l2_props:       L2 props.
+ * @unused_1:       Keep for backwards compatibility.
+ * @tiler_props:    Tiler props.
+ * @thread_props:   Thread props.
+ * @raw_props:      This member is large, likely to be 128 bytes.
+ * @coherency_info: This must be last member of the structure.
+ *
+ * NOTE: the raw_props member in this data structure contains the register
+ * values from which the value of the other members are derived. The derived
+ * members exist to allow for efficient access and/or shielding the details
+ * of the layout of the registers.
+ */
+struct base_gpu_props {
+   struct mali_base_gpu_core_props core_props;
+   struct mali_base_gpu_l2_cache_props l2_props;
+   __u64 unused_1;
+   struct mali_base_gpu_tiler_props tiler_props;
+   struct mali_base_gpu_thread_props thread_props;
+   struct gpu_raw_gpu_props raw_props;
+   struct mali_base_gpu_coherent_group_info coherency_info;
+};
+
+#define BASE_MEM_GROUP_ID_GET(flags)                                           \
+   ((flags & BASE_MEM_GROUP_ID_MASK) >> BASEP_MEM_GROUP_ID_SHIFT)
+
+#define BASE_MEM_GROUP_ID_SET(id)                                              \
+   (((base_mem_alloc_flags)((id < 0 || id >= BASE_MEM_GROUP_COUNT)             \
+                               ? BASE_MEM_GROUP_DEFAULT                        \
+                               : id)                                           \
+     << BASEP_MEM_GROUP_ID_SHIFT) &                                            \
+    BASE_MEM_GROUP_ID_MASK)
+
+#define BASE_CONTEXT_MMU_GROUP_ID_SET(group_id)                                \
+   (BASEP_CONTEXT_MMU_GROUP_ID_MASK & ((base_context_create_flags)(group_id)   \
+                                       << BASEP_CONTEXT_MMU_GROUP_ID_SHIFT))
+
+#define BASE_CONTEXT_MMU_GROUP_ID_GET(flags)                                   \
+   ((flags & BASEP_CONTEXT_MMU_GROUP_ID_MASK) >>                               \
+    BASEP_CONTEXT_MMU_GROUP_ID_SHIFT)
+
+/*
+ * A number of bit flags are defined for requesting cpu_gpu_timeinfo. These
+ * flags are also used, where applicable, for specifying which fields
+ * are valid following the request operation.
+ */
+
+/* For monotonic (counter) timefield */
+#define BASE_TIMEINFO_MONOTONIC_FLAG (1UL << 0)
+/* For system wide timestamp */
+#define BASE_TIMEINFO_TIMESTAMP_FLAG (1UL << 1)
+/* For GPU cycle counter */
+#define BASE_TIMEINFO_CYCLE_COUNTER_FLAG (1UL << 2)
+/* Specify kernel GPU register timestamp */
+#define BASE_TIMEINFO_KERNEL_SOURCE_FLAG (1UL << 30)
+/* Specify userspace cntvct_el0 timestamp source */
+#define BASE_TIMEINFO_USER_SOURCE_FLAG (1UL << 31)
+
+#define BASE_TIMEREQUEST_ALLOWED_FLAGS                                         \
+   (BASE_TIMEINFO_MONOTONIC_FLAG | BASE_TIMEINFO_TIMESTAMP_FLAG |              \
+    BASE_TIMEINFO_CYCLE_COUNTER_FLAG | BASE_TIMEINFO_KERNEL_SOURCE_FLAG |      \
+    BASE_TIMEINFO_USER_SOURCE_FLAG)
+
+/* Maximum number of source allocations allowed to create an alias allocation.
+ * This needs to be 4096 * 6 to allow cube map arrays with up to 4096 array
+ * layers, since each cube map in the array will have 6 faces.
+ */
+#define BASE_MEM_ALIAS_MAX_ENTS ((size_t)24576)
+
+#endif /* _UAPI_BASE_KERNEL_H_ */
diff -urN mesa-23.0.0/src/panfrost/base/include/mali_kbase_gpuprops.h mesa/src/panfrost/base/include/mali_kbase_gpuprops.h
--- mesa-23.0.0/src/panfrost/base/include/mali_kbase_gpuprops.h	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/base/include/mali_kbase_gpuprops.h	2023-03-06 19:19:32.705308818 +0100
@@ -0,0 +1,127 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ *
+ * (C) COPYRIGHT 2017-2022 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU license.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ */
+
+#ifndef _UAPI_KBASE_GPUPROP_H_
+#define _UAPI_KBASE_GPUPROP_H_
+
+/**********************************
+ * Definitions for GPU properties *
+ **********************************/
+#define KBASE_GPUPROP_VALUE_SIZE_U8  (0x0)
+#define KBASE_GPUPROP_VALUE_SIZE_U16 (0x1)
+#define KBASE_GPUPROP_VALUE_SIZE_U32 (0x2)
+#define KBASE_GPUPROP_VALUE_SIZE_U64 (0x3)
+
+#define KBASE_GPUPROP_PRODUCT_ID     1
+#define KBASE_GPUPROP_VERSION_STATUS 2
+#define KBASE_GPUPROP_MINOR_REVISION 3
+#define KBASE_GPUPROP_MAJOR_REVISION 4
+/* 5 previously used for GPU speed */
+#define KBASE_GPUPROP_GPU_FREQ_KHZ_MAX 6
+/* 7 previously used for minimum GPU speed */
+#define KBASE_GPUPROP_LOG2_PROGRAM_COUNTER_SIZE 8
+#define KBASE_GPUPROP_TEXTURE_FEATURES_0        9
+#define KBASE_GPUPROP_TEXTURE_FEATURES_1        10
+#define KBASE_GPUPROP_TEXTURE_FEATURES_2        11
+#define KBASE_GPUPROP_GPU_AVAILABLE_MEMORY_SIZE 12
+
+#define KBASE_GPUPROP_L2_LOG2_LINE_SIZE  13
+#define KBASE_GPUPROP_L2_LOG2_CACHE_SIZE 14
+#define KBASE_GPUPROP_L2_NUM_L2_SLICES   15
+
+#define KBASE_GPUPROP_TILER_BIN_SIZE_BYTES    16
+#define KBASE_GPUPROP_TILER_MAX_ACTIVE_LEVELS 17
+
+#define KBASE_GPUPROP_MAX_THREADS            18
+#define KBASE_GPUPROP_MAX_WORKGROUP_SIZE     19
+#define KBASE_GPUPROP_MAX_BARRIER_SIZE       20
+#define KBASE_GPUPROP_MAX_REGISTERS          21
+#define KBASE_GPUPROP_MAX_TASK_QUEUE         22
+#define KBASE_GPUPROP_MAX_THREAD_GROUP_SPLIT 23
+#define KBASE_GPUPROP_IMPL_TECH              24
+
+#define KBASE_GPUPROP_RAW_SHADER_PRESENT            25
+#define KBASE_GPUPROP_RAW_TILER_PRESENT             26
+#define KBASE_GPUPROP_RAW_L2_PRESENT                27
+#define KBASE_GPUPROP_RAW_STACK_PRESENT             28
+#define KBASE_GPUPROP_RAW_L2_FEATURES               29
+#define KBASE_GPUPROP_RAW_CORE_FEATURES             30
+#define KBASE_GPUPROP_RAW_MEM_FEATURES              31
+#define KBASE_GPUPROP_RAW_MMU_FEATURES              32
+#define KBASE_GPUPROP_RAW_AS_PRESENT                33
+#define KBASE_GPUPROP_RAW_JS_PRESENT                34
+#define KBASE_GPUPROP_RAW_JS_FEATURES_0             35
+#define KBASE_GPUPROP_RAW_JS_FEATURES_1             36
+#define KBASE_GPUPROP_RAW_JS_FEATURES_2             37
+#define KBASE_GPUPROP_RAW_JS_FEATURES_3             38
+#define KBASE_GPUPROP_RAW_JS_FEATURES_4             39
+#define KBASE_GPUPROP_RAW_JS_FEATURES_5             40
+#define KBASE_GPUPROP_RAW_JS_FEATURES_6             41
+#define KBASE_GPUPROP_RAW_JS_FEATURES_7             42
+#define KBASE_GPUPROP_RAW_JS_FEATURES_8             43
+#define KBASE_GPUPROP_RAW_JS_FEATURES_9             44
+#define KBASE_GPUPROP_RAW_JS_FEATURES_10            45
+#define KBASE_GPUPROP_RAW_JS_FEATURES_11            46
+#define KBASE_GPUPROP_RAW_JS_FEATURES_12            47
+#define KBASE_GPUPROP_RAW_JS_FEATURES_13            48
+#define KBASE_GPUPROP_RAW_JS_FEATURES_14            49
+#define KBASE_GPUPROP_RAW_JS_FEATURES_15            50
+#define KBASE_GPUPROP_RAW_TILER_FEATURES            51
+#define KBASE_GPUPROP_RAW_TEXTURE_FEATURES_0        52
+#define KBASE_GPUPROP_RAW_TEXTURE_FEATURES_1        53
+#define KBASE_GPUPROP_RAW_TEXTURE_FEATURES_2        54
+#define KBASE_GPUPROP_RAW_GPU_ID                    55
+#define KBASE_GPUPROP_RAW_THREAD_MAX_THREADS        56
+#define KBASE_GPUPROP_RAW_THREAD_MAX_WORKGROUP_SIZE 57
+#define KBASE_GPUPROP_RAW_THREAD_MAX_BARRIER_SIZE   58
+#define KBASE_GPUPROP_RAW_THREAD_FEATURES           59
+#define KBASE_GPUPROP_RAW_COHERENCY_MODE            60
+
+#define KBASE_GPUPROP_COHERENCY_NUM_GROUPS      61
+#define KBASE_GPUPROP_COHERENCY_NUM_CORE_GROUPS 62
+#define KBASE_GPUPROP_COHERENCY_COHERENCY       63
+#define KBASE_GPUPROP_COHERENCY_GROUP_0         64
+#define KBASE_GPUPROP_COHERENCY_GROUP_1         65
+#define KBASE_GPUPROP_COHERENCY_GROUP_2         66
+#define KBASE_GPUPROP_COHERENCY_GROUP_3         67
+#define KBASE_GPUPROP_COHERENCY_GROUP_4         68
+#define KBASE_GPUPROP_COHERENCY_GROUP_5         69
+#define KBASE_GPUPROP_COHERENCY_GROUP_6         70
+#define KBASE_GPUPROP_COHERENCY_GROUP_7         71
+#define KBASE_GPUPROP_COHERENCY_GROUP_8         72
+#define KBASE_GPUPROP_COHERENCY_GROUP_9         73
+#define KBASE_GPUPROP_COHERENCY_GROUP_10        74
+#define KBASE_GPUPROP_COHERENCY_GROUP_11        75
+#define KBASE_GPUPROP_COHERENCY_GROUP_12        76
+#define KBASE_GPUPROP_COHERENCY_GROUP_13        77
+#define KBASE_GPUPROP_COHERENCY_GROUP_14        78
+#define KBASE_GPUPROP_COHERENCY_GROUP_15        79
+
+#define KBASE_GPUPROP_TEXTURE_FEATURES_3     80
+#define KBASE_GPUPROP_RAW_TEXTURE_FEATURES_3 81
+
+#define KBASE_GPUPROP_NUM_EXEC_ENGINES 82
+
+#define KBASE_GPUPROP_RAW_THREAD_TLS_ALLOC 83
+#define KBASE_GPUPROP_TLS_ALLOC            84
+#define KBASE_GPUPROP_RAW_GPU_FEATURES     85
+
+#endif
diff -urN mesa-23.0.0/src/panfrost/base/include/mali_kbase_ioctl.h mesa/src/panfrost/base/include/mali_kbase_ioctl.h
--- mesa-23.0.0/src/panfrost/base/include/mali_kbase_ioctl.h	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/base/include/mali_kbase_ioctl.h	2023-03-06 19:19:32.710308851 +0100
@@ -0,0 +1,756 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ *
+ * (C) COPYRIGHT 2017-2022 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU license.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ */
+
+#ifndef _UAPI_KBASE_IOCTL_H_
+#define _UAPI_KBASE_IOCTL_H_
+
+#ifdef __cpluscplus
+extern "C" {
+#endif
+
+#include <asm-generic/ioctl.h>
+#include <linux/types.h>
+
+#if MALI_USE_CSF
+#include "csf/mali_kbase_csf_ioctl.h"
+#else
+#include "jm/mali_kbase_jm_ioctl.h"
+#endif /* MALI_USE_CSF */
+
+#define KBASE_IOCTL_TYPE 0x80
+
+/**
+ * struct kbase_ioctl_set_flags - Set kernel context creation flags
+ *
+ * @create_flags: Flags - see base_context_create_flags
+ */
+struct kbase_ioctl_set_flags {
+   __u32 create_flags;
+};
+
+#define KBASE_IOCTL_SET_FLAGS                                                  \
+   _IOW(KBASE_IOCTL_TYPE, 1, struct kbase_ioctl_set_flags)
+
+/**
+ * struct kbase_ioctl_get_gpuprops - Read GPU properties from the kernel
+ *
+ * @buffer: Pointer to the buffer to store properties into
+ * @size: Size of the buffer
+ * @flags: Flags - must be zero for now
+ *
+ * The ioctl will return the number of bytes stored into @buffer or an error
+ * on failure (e.g. @size is too small). If @size is specified as 0 then no
+ * data will be written but the return value will be the number of bytes needed
+ * for all the properties.
+ *
+ * @flags may be used in the future to request a different format for the
+ * buffer. With @flags == 0 the following format is used.
+ *
+ * The buffer will be filled with pairs of values, a __u32 key identifying the
+ * property followed by the value. The size of the value is identified using
+ * the bottom bits of the key. The value then immediately followed the key and
+ * is tightly packed (there is no padding). All keys and values are
+ * little-endian.
+ *
+ * 00 = __u8
+ * 01 = __u16
+ * 10 = __u32
+ * 11 = __u64
+ */
+struct kbase_ioctl_get_gpuprops {
+   __u64 buffer;
+   __u32 size;
+   __u32 flags;
+};
+
+#define KBASE_IOCTL_GET_GPUPROPS                                               \
+   _IOW(KBASE_IOCTL_TYPE, 3, struct kbase_ioctl_get_gpuprops)
+
+/**
+ * union kbase_ioctl_mem_alloc - Allocate memory on the GPU
+ * @in: Input parameters
+ * @in.va_pages: The number of pages of virtual address space to reserve
+ * @in.commit_pages: The number of physical pages to allocate
+ * @in.extension: The number of extra pages to allocate on each GPU fault which
+ * grows the region
+ * @in.flags: Flags
+ * @out: Output parameters
+ * @out.flags: Flags
+ * @out.gpu_va: The GPU virtual address which is allocated
+ */
+union kbase_ioctl_mem_alloc {
+   struct {
+      __u64 va_pages;
+      __u64 commit_pages;
+      __u64 extension;
+      __u64 flags;
+   } in;
+   struct {
+      __u64 flags;
+      __u64 gpu_va;
+   } out;
+};
+
+#define KBASE_IOCTL_MEM_ALLOC                                                  \
+   _IOWR(KBASE_IOCTL_TYPE, 5, union kbase_ioctl_mem_alloc)
+
+/**
+ * struct kbase_ioctl_mem_query - Query properties of a GPU memory region
+ * @in: Input parameters
+ * @in.gpu_addr: A GPU address contained within the region
+ * @in.query: The type of query
+ * @out: Output parameters
+ * @out.value: The result of the query
+ *
+ * Use a %KBASE_MEM_QUERY_xxx flag as input for @query.
+ */
+union kbase_ioctl_mem_query {
+   struct {
+      __u64 gpu_addr;
+      __u64 query;
+   } in;
+   struct {
+      __u64 value;
+   } out;
+};
+
+#define KBASE_IOCTL_MEM_QUERY                                                  \
+   _IOWR(KBASE_IOCTL_TYPE, 6, union kbase_ioctl_mem_query)
+
+#define KBASE_MEM_QUERY_COMMIT_SIZE ((__u64)1)
+#define KBASE_MEM_QUERY_VA_SIZE     ((__u64)2)
+#define KBASE_MEM_QUERY_FLAGS       ((__u64)3)
+
+/**
+ * struct kbase_ioctl_mem_free - Free a memory region
+ * @gpu_addr: Handle to the region to free
+ */
+struct kbase_ioctl_mem_free {
+   __u64 gpu_addr;
+};
+
+#define KBASE_IOCTL_MEM_FREE                                                   \
+   _IOW(KBASE_IOCTL_TYPE, 7, struct kbase_ioctl_mem_free)
+
+/**
+ * struct kbase_ioctl_hwcnt_reader_setup - Setup HWC dumper/reader
+ * @buffer_count: requested number of dumping buffers
+ * @fe_bm:        counters selection bitmask (Front end)
+ * @shader_bm:    counters selection bitmask (Shader)
+ * @tiler_bm:     counters selection bitmask (Tiler)
+ * @mmu_l2_bm:    counters selection bitmask (MMU_L2)
+ *
+ * A fd is returned from the ioctl if successful, or a negative value on error
+ */
+struct kbase_ioctl_hwcnt_reader_setup {
+   __u32 buffer_count;
+   __u32 fe_bm;
+   __u32 shader_bm;
+   __u32 tiler_bm;
+   __u32 mmu_l2_bm;
+};
+
+#define KBASE_IOCTL_HWCNT_READER_SETUP                                         \
+   _IOW(KBASE_IOCTL_TYPE, 8, struct kbase_ioctl_hwcnt_reader_setup)
+
+/**
+ * struct kbase_ioctl_hwcnt_values - Values to set dummy the dummy counters to.
+ * @data:    Counter samples for the dummy model.
+ * @size:    Size of the counter sample data.
+ * @padding: Padding.
+ */
+struct kbase_ioctl_hwcnt_values {
+   __u64 data;
+   __u32 size;
+   __u32 padding;
+};
+
+#define KBASE_IOCTL_HWCNT_SET                                                  \
+   _IOW(KBASE_IOCTL_TYPE, 32, struct kbase_ioctl_hwcnt_values)
+
+/**
+ * struct kbase_ioctl_disjoint_query - Query the disjoint counter
+ * @counter:   A counter of disjoint events in the kernel
+ */
+struct kbase_ioctl_disjoint_query {
+   __u32 counter;
+};
+
+#define KBASE_IOCTL_DISJOINT_QUERY                                             \
+   _IOR(KBASE_IOCTL_TYPE, 12, struct kbase_ioctl_disjoint_query)
+
+/**
+ * struct kbase_ioctl_get_ddk_version - Query the kernel version
+ * @version_buffer: Buffer to receive the kernel version string
+ * @size: Size of the buffer
+ * @padding: Padding
+ *
+ * The ioctl will return the number of bytes written into version_buffer
+ * (which includes a NULL byte) or a negative error code
+ *
+ * The ioctl request code has to be _IOW because the data in ioctl struct is
+ * being copied to the kernel, even though the kernel then writes out the
+ * version info to the buffer specified in the ioctl.
+ */
+struct kbase_ioctl_get_ddk_version {
+   __u64 version_buffer;
+   __u32 size;
+   __u32 padding;
+};
+
+#define KBASE_IOCTL_GET_DDK_VERSION                                            \
+   _IOW(KBASE_IOCTL_TYPE, 13, struct kbase_ioctl_get_ddk_version)
+
+/**
+ * struct kbase_ioctl_mem_jit_init_10_2 - Initialize the just-in-time memory
+ *                                        allocator (between kernel driver
+ *                                        version 10.2--11.4)
+ * @va_pages: Number of VA pages to reserve for JIT
+ *
+ * Note that depending on the VA size of the application and GPU, the value
+ * specified in @va_pages may be ignored.
+ *
+ * New code should use KBASE_IOCTL_MEM_JIT_INIT instead, this is kept for
+ * backwards compatibility.
+ */
+struct kbase_ioctl_mem_jit_init_10_2 {
+   __u64 va_pages;
+};
+
+#define KBASE_IOCTL_MEM_JIT_INIT_10_2                                          \
+   _IOW(KBASE_IOCTL_TYPE, 14, struct kbase_ioctl_mem_jit_init_10_2)
+
+/**
+ * struct kbase_ioctl_mem_jit_init_11_5 - Initialize the just-in-time memory
+ *                                        allocator (between kernel driver
+ *                                        version 11.5--11.19)
+ * @va_pages: Number of VA pages to reserve for JIT
+ * @max_allocations: Maximum number of concurrent allocations
+ * @trim_level: Level of JIT allocation trimming to perform on free (0 - 100%)
+ * @group_id: Group ID to be used for physical allocations
+ * @padding: Currently unused, must be zero
+ *
+ * Note that depending on the VA size of the application and GPU, the value
+ * specified in @va_pages may be ignored.
+ *
+ * New code should use KBASE_IOCTL_MEM_JIT_INIT instead, this is kept for
+ * backwards compatibility.
+ */
+struct kbase_ioctl_mem_jit_init_11_5 {
+   __u64 va_pages;
+   __u8 max_allocations;
+   __u8 trim_level;
+   __u8 group_id;
+   __u8 padding[5];
+};
+
+#define KBASE_IOCTL_MEM_JIT_INIT_11_5                                          \
+   _IOW(KBASE_IOCTL_TYPE, 14, struct kbase_ioctl_mem_jit_init_11_5)
+
+/**
+ * struct kbase_ioctl_mem_jit_init - Initialize the just-in-time memory
+ *                                   allocator
+ * @va_pages: Number of GPU virtual address pages to reserve for just-in-time
+ *            memory allocations
+ * @max_allocations: Maximum number of concurrent allocations
+ * @trim_level: Level of JIT allocation trimming to perform on free (0 - 100%)
+ * @group_id: Group ID to be used for physical allocations
+ * @padding: Currently unused, must be zero
+ * @phys_pages: Maximum number of physical pages to allocate just-in-time
+ *
+ * Note that depending on the VA size of the application and GPU, the value
+ * specified in @va_pages may be ignored.
+ */
+struct kbase_ioctl_mem_jit_init {
+   __u64 va_pages;
+   __u8 max_allocations;
+   __u8 trim_level;
+   __u8 group_id;
+   __u8 padding[5];
+   __u64 phys_pages;
+};
+
+#define KBASE_IOCTL_MEM_JIT_INIT                                               \
+   _IOW(KBASE_IOCTL_TYPE, 14, struct kbase_ioctl_mem_jit_init)
+
+/**
+ * struct kbase_ioctl_mem_sync - Perform cache maintenance on memory
+ *
+ * @handle: GPU memory handle (GPU VA)
+ * @user_addr: The address where it is mapped in user space
+ * @size: The number of bytes to synchronise
+ * @type: The direction to synchronise: 0 is sync to memory (clean),
+ * 1 is sync from memory (invalidate). Use the BASE_SYNCSET_OP_xxx constants.
+ * @padding: Padding to round up to a multiple of 8 bytes, must be zero
+ */
+struct kbase_ioctl_mem_sync {
+   __u64 handle;
+   __u64 user_addr;
+   __u64 size;
+   __u8 type;
+   __u8 padding[7];
+};
+
+#define KBASE_IOCTL_MEM_SYNC                                                   \
+   _IOW(KBASE_IOCTL_TYPE, 15, struct kbase_ioctl_mem_sync)
+
+/**
+ * union kbase_ioctl_mem_find_cpu_offset - Find the offset of a CPU pointer
+ *
+ * @in: Input parameters
+ * @in.gpu_addr: The GPU address of the memory region
+ * @in.cpu_addr: The CPU address to locate
+ * @in.size: A size in bytes to validate is contained within the region
+ * @out: Output parameters
+ * @out.offset: The offset from the start of the memory region to @cpu_addr
+ */
+union kbase_ioctl_mem_find_cpu_offset {
+   struct {
+      __u64 gpu_addr;
+      __u64 cpu_addr;
+      __u64 size;
+   } in;
+   struct {
+      __u64 offset;
+   } out;
+};
+
+#define KBASE_IOCTL_MEM_FIND_CPU_OFFSET                                        \
+   _IOWR(KBASE_IOCTL_TYPE, 16, union kbase_ioctl_mem_find_cpu_offset)
+
+/**
+ * struct kbase_ioctl_get_context_id - Get the kernel context ID
+ *
+ * @id: The kernel context ID
+ */
+struct kbase_ioctl_get_context_id {
+   __u32 id;
+};
+
+#define KBASE_IOCTL_GET_CONTEXT_ID                                             \
+   _IOR(KBASE_IOCTL_TYPE, 17, struct kbase_ioctl_get_context_id)
+
+/**
+ * struct kbase_ioctl_tlstream_acquire - Acquire a tlstream fd
+ *
+ * @flags: Flags
+ *
+ * The ioctl returns a file descriptor when successful
+ */
+struct kbase_ioctl_tlstream_acquire {
+   __u32 flags;
+};
+
+#define KBASE_IOCTL_TLSTREAM_ACQUIRE                                           \
+   _IOW(KBASE_IOCTL_TYPE, 18, struct kbase_ioctl_tlstream_acquire)
+
+#define KBASE_IOCTL_TLSTREAM_FLUSH _IO(KBASE_IOCTL_TYPE, 19)
+
+/**
+ * struct kbase_ioctl_mem_commit - Change the amount of memory backing a region
+ *
+ * @gpu_addr: The memory region to modify
+ * @pages:    The number of physical pages that should be present
+ *
+ * The ioctl may return on the following error codes or 0 for success:
+ *   -ENOMEM: Out of memory
+ *   -EINVAL: Invalid arguments
+ */
+struct kbase_ioctl_mem_commit {
+   __u64 gpu_addr;
+   __u64 pages;
+};
+
+#define KBASE_IOCTL_MEM_COMMIT                                                 \
+   _IOW(KBASE_IOCTL_TYPE, 20, struct kbase_ioctl_mem_commit)
+
+/**
+ * union kbase_ioctl_mem_alias - Create an alias of memory regions
+ * @in: Input parameters
+ * @in.flags: Flags, see BASE_MEM_xxx
+ * @in.stride: Bytes between start of each memory region
+ * @in.nents: The number of regions to pack together into the alias
+ * @in.aliasing_info: Pointer to an array of struct base_mem_aliasing_info
+ * @out: Output parameters
+ * @out.flags: Flags, see BASE_MEM_xxx
+ * @out.gpu_va: Address of the new alias
+ * @out.va_pages: Size of the new alias
+ */
+union kbase_ioctl_mem_alias {
+   struct {
+      __u64 flags;
+      __u64 stride;
+      __u64 nents;
+      __u64 aliasing_info;
+   } in;
+   struct {
+      __u64 flags;
+      __u64 gpu_va;
+      __u64 va_pages;
+   } out;
+};
+
+#define KBASE_IOCTL_MEM_ALIAS                                                  \
+   _IOWR(KBASE_IOCTL_TYPE, 21, union kbase_ioctl_mem_alias)
+
+/**
+ * union kbase_ioctl_mem_import - Import memory for use by the GPU
+ * @in: Input parameters
+ * @in.flags: Flags, see BASE_MEM_xxx
+ * @in.phandle: Handle to the external memory
+ * @in.type: Type of external memory, see base_mem_import_type
+ * @in.padding: Amount of extra VA pages to append to the imported buffer
+ * @out: Output parameters
+ * @out.flags: Flags, see BASE_MEM_xxx
+ * @out.gpu_va: Address of the new alias
+ * @out.va_pages: Size of the new alias
+ */
+union kbase_ioctl_mem_import {
+   struct {
+      __u64 flags;
+      __u64 phandle;
+      __u32 type;
+      __u32 padding;
+   } in;
+   struct {
+      __u64 flags;
+      __u64 gpu_va;
+      __u64 va_pages;
+   } out;
+};
+
+#define KBASE_IOCTL_MEM_IMPORT                                                 \
+   _IOWR(KBASE_IOCTL_TYPE, 22, union kbase_ioctl_mem_import)
+
+/**
+ * struct kbase_ioctl_mem_flags_change - Change the flags for a memory region
+ * @gpu_va: The GPU region to modify
+ * @flags: The new flags to set
+ * @mask: Mask of the flags to modify
+ */
+struct kbase_ioctl_mem_flags_change {
+   __u64 gpu_va;
+   __u64 flags;
+   __u64 mask;
+};
+
+#define KBASE_IOCTL_MEM_FLAGS_CHANGE                                           \
+   _IOW(KBASE_IOCTL_TYPE, 23, struct kbase_ioctl_mem_flags_change)
+
+/**
+ * struct kbase_ioctl_stream_create - Create a synchronisation stream
+ * @name: A name to identify this stream. Must be NULL-terminated.
+ *
+ * Note that this is also called a "timeline", but is named stream to avoid
+ * confusion with other uses of the word.
+ *
+ * Unused bytes in @name (after the first NULL byte) must be also be NULL bytes.
+ *
+ * The ioctl returns a file descriptor.
+ */
+struct kbase_ioctl_stream_create {
+   char name[32];
+};
+
+#define KBASE_IOCTL_STREAM_CREATE                                              \
+   _IOW(KBASE_IOCTL_TYPE, 24, struct kbase_ioctl_stream_create)
+
+/**
+ * struct kbase_ioctl_fence_validate - Validate a fd refers to a fence
+ * @fd: The file descriptor to validate
+ */
+struct kbase_ioctl_fence_validate {
+   int fd;
+};
+
+#define KBASE_IOCTL_FENCE_VALIDATE                                             \
+   _IOW(KBASE_IOCTL_TYPE, 25, struct kbase_ioctl_fence_validate)
+
+/**
+ * struct kbase_ioctl_mem_profile_add - Provide profiling information to kernel
+ * @buffer: Pointer to the information
+ * @len: Length
+ * @padding: Padding
+ *
+ * The data provided is accessible through a debugfs file
+ */
+struct kbase_ioctl_mem_profile_add {
+   __u64 buffer;
+   __u32 len;
+   __u32 padding;
+};
+
+#define KBASE_IOCTL_MEM_PROFILE_ADD                                            \
+   _IOW(KBASE_IOCTL_TYPE, 27, struct kbase_ioctl_mem_profile_add)
+
+/**
+ * struct kbase_ioctl_sticky_resource_map - Permanently map an external resource
+ * @count: Number of resources
+ * @address: Array of __u64 GPU addresses of the external resources to map
+ */
+struct kbase_ioctl_sticky_resource_map {
+   __u64 count;
+   __u64 address;
+};
+
+#define KBASE_IOCTL_STICKY_RESOURCE_MAP                                        \
+   _IOW(KBASE_IOCTL_TYPE, 29, struct kbase_ioctl_sticky_resource_map)
+
+/**
+ * struct kbase_ioctl_sticky_resource_unmap - Unmap a resource mapped which was
+ *                                          previously permanently mapped
+ * @count: Number of resources
+ * @address: Array of __u64 GPU addresses of the external resources to unmap
+ */
+struct kbase_ioctl_sticky_resource_unmap {
+   __u64 count;
+   __u64 address;
+};
+
+#define KBASE_IOCTL_STICKY_RESOURCE_UNMAP                                      \
+   _IOW(KBASE_IOCTL_TYPE, 30, struct kbase_ioctl_sticky_resource_unmap)
+
+/**
+ * union kbase_ioctl_mem_find_gpu_start_and_offset - Find the start address of
+ *                                                   the GPU memory region for
+ *                                                   the given gpu address and
+ *                                                   the offset of that address
+ *                                                   into the region
+ * @in: Input parameters
+ * @in.gpu_addr: GPU virtual address
+ * @in.size: Size in bytes within the region
+ * @out: Output parameters
+ * @out.start: Address of the beginning of the memory region enclosing @gpu_addr
+ *             for the length of @offset bytes
+ * @out.offset: The offset from the start of the memory region to @gpu_addr
+ */
+union kbase_ioctl_mem_find_gpu_start_and_offset {
+   struct {
+      __u64 gpu_addr;
+      __u64 size;
+   } in;
+   struct {
+      __u64 start;
+      __u64 offset;
+   } out;
+};
+
+#define KBASE_IOCTL_MEM_FIND_GPU_START_AND_OFFSET                              \
+   _IOWR(KBASE_IOCTL_TYPE, 31, union kbase_ioctl_mem_find_gpu_start_and_offset)
+
+#define KBASE_IOCTL_CINSTR_GWT_START _IO(KBASE_IOCTL_TYPE, 33)
+
+#define KBASE_IOCTL_CINSTR_GWT_STOP _IO(KBASE_IOCTL_TYPE, 34)
+
+/**
+ * union kbase_ioctl_cinstr_gwt_dump - Used to collect all GPU write fault
+ *                                     addresses.
+ * @in: Input parameters
+ * @in.addr_buffer: Address of buffer to hold addresses of gpu modified areas.
+ * @in.size_buffer: Address of buffer to hold size of modified areas (in pages)
+ * @in.len: Number of addresses the buffers can hold.
+ * @in.padding: padding
+ * @out: Output parameters
+ * @out.no_of_addr_collected: Number of addresses collected into addr_buffer.
+ * @out.more_data_available: Status indicating if more addresses are available.
+ * @out.padding: padding
+ *
+ * This structure is used when performing a call to dump GPU write fault
+ * addresses.
+ */
+union kbase_ioctl_cinstr_gwt_dump {
+   struct {
+      __u64 addr_buffer;
+      __u64 size_buffer;
+      __u32 len;
+      __u32 padding;
+
+   } in;
+   struct {
+      __u32 no_of_addr_collected;
+      __u8 more_data_available;
+      __u8 padding[27];
+   } out;
+};
+
+#define KBASE_IOCTL_CINSTR_GWT_DUMP                                            \
+   _IOWR(KBASE_IOCTL_TYPE, 35, union kbase_ioctl_cinstr_gwt_dump)
+
+/**
+ * struct kbase_ioctl_mem_exec_init - Initialise the EXEC_VA memory zone
+ *
+ * @va_pages: Number of VA pages to reserve for EXEC_VA
+ */
+struct kbase_ioctl_mem_exec_init {
+   __u64 va_pages;
+};
+
+#define KBASE_IOCTL_MEM_EXEC_INIT                                              \
+   _IOW(KBASE_IOCTL_TYPE, 38, struct kbase_ioctl_mem_exec_init)
+
+/**
+ * union kbase_ioctl_get_cpu_gpu_timeinfo - Request zero or more types of
+ *                                          cpu/gpu time (counter values)
+ * @in: Input parameters
+ * @in.request_flags: Bit-flags indicating the requested types.
+ * @in.paddings:      Unused, size alignment matching the out.
+ * @out: Output parameters
+ * @out.sec:           Integer field of the monotonic time, unit in seconds.
+ * @out.nsec:          Fractional sec of the monotonic time, in nano-seconds.
+ * @out.padding:       Unused, for __u64 alignment
+ * @out.timestamp:     System wide timestamp (counter) value.
+ * @out.cycle_counter: GPU cycle counter value.
+ */
+union kbase_ioctl_get_cpu_gpu_timeinfo {
+   struct {
+      __u32 request_flags;
+      __u32 paddings[7];
+   } in;
+   struct {
+      __u64 sec;
+      __u32 nsec;
+      __u32 padding;
+      __u64 timestamp;
+      __u64 cycle_counter;
+   } out;
+};
+
+#define KBASE_IOCTL_GET_CPU_GPU_TIMEINFO                                       \
+   _IOWR(KBASE_IOCTL_TYPE, 50, union kbase_ioctl_get_cpu_gpu_timeinfo)
+
+/**
+ * struct kbase_ioctl_context_priority_check - Check the max possible priority
+ * @priority: Input priority & output priority
+ */
+
+struct kbase_ioctl_context_priority_check {
+   __u8 priority;
+};
+
+#define KBASE_IOCTL_CONTEXT_PRIORITY_CHECK                                     \
+   _IOWR(KBASE_IOCTL_TYPE, 54, struct kbase_ioctl_context_priority_check)
+
+/**
+ * struct kbase_ioctl_set_limited_core_count - Set the limited core count.
+ *
+ * @max_core_count: Maximum core count
+ */
+struct kbase_ioctl_set_limited_core_count {
+   __u8 max_core_count;
+};
+
+#define KBASE_IOCTL_SET_LIMITED_CORE_COUNT                                     \
+   _IOW(KBASE_IOCTL_TYPE, 55, struct kbase_ioctl_set_limited_core_count)
+
+/**
+ * struct kbase_ioctl_kinstr_prfcnt_enum_info - Enum Performance counter
+ *                                              information
+ * @info_item_size:  Performance counter item size in bytes.
+ * @info_item_count: Performance counter item count in the info_list_ptr.
+ * @info_list_ptr:   Performance counter item list pointer which points to a
+ *                   list with info_item_count of items.
+ *
+ * On success: returns info_item_size and info_item_count if info_list_ptr is
+ * NULL, returns performance counter information if info_list_ptr is not NULL.
+ * On error: returns a negative error code.
+ */
+struct kbase_ioctl_kinstr_prfcnt_enum_info {
+   __u32 info_item_size;
+   __u32 info_item_count;
+   __u64 info_list_ptr;
+};
+
+#define KBASE_IOCTL_KINSTR_PRFCNT_ENUM_INFO                                    \
+   _IOWR(KBASE_IOCTL_TYPE, 56, struct kbase_ioctl_kinstr_prfcnt_enum_info)
+
+/**
+ * struct kbase_ioctl_kinstr_prfcnt_setup - Setup HWC dumper/reader
+ * @in: input parameters.
+ * @in.request_item_count: Number of requests in the requests array.
+ * @in.request_item_size:  Size in bytes of each request in the requests array.
+ * @in.requests_ptr:       Pointer to the requests array.
+ * @out: output parameters.
+ * @out.prfcnt_metadata_item_size: Size of each item in the metadata array for
+ *                                 each sample.
+ * @out.prfcnt_mmap_size_bytes:    Size in bytes that user-space should mmap
+ *                                 for reading performance counter samples.
+ *
+ * A fd is returned from the ioctl if successful, or a negative value on error.
+ */
+union kbase_ioctl_kinstr_prfcnt_setup {
+   struct {
+      __u32 request_item_count;
+      __u32 request_item_size;
+      __u64 requests_ptr;
+   } in;
+   struct {
+      __u32 prfcnt_metadata_item_size;
+      __u32 prfcnt_mmap_size_bytes;
+   } out;
+};
+
+#define KBASE_IOCTL_KINSTR_PRFCNT_SETUP                                        \
+   _IOWR(KBASE_IOCTL_TYPE, 57, union kbase_ioctl_kinstr_prfcnt_setup)
+
+/***************
+ * test ioctls *
+ ***************/
+#if MALI_UNIT_TEST
+/* These ioctls are purely for test purposes and are not used in the production
+ * driver, they therefore may change without notice
+ */
+
+#define KBASE_IOCTL_TEST_TYPE (KBASE_IOCTL_TYPE + 1)
+
+/**
+ * struct kbase_ioctl_tlstream_stats - Read tlstream stats for test purposes
+ * @bytes_collected: number of bytes read by user
+ * @bytes_generated: number of bytes generated by tracepoints
+ */
+struct kbase_ioctl_tlstream_stats {
+   __u32 bytes_collected;
+   __u32 bytes_generated;
+};
+
+#define KBASE_IOCTL_TLSTREAM_STATS                                             \
+   _IOR(KBASE_IOCTL_TEST_TYPE, 2, struct kbase_ioctl_tlstream_stats)
+
+#endif /* MALI_UNIT_TEST */
+
+/* Customer extension range */
+#define KBASE_IOCTL_EXTRA_TYPE (KBASE_IOCTL_TYPE + 2)
+
+/* If the integration needs extra ioctl add them there
+ * like this:
+ *
+ * struct my_ioctl_args {
+ *  ....
+ * }
+ *
+ * #define KBASE_IOCTL_MY_IOCTL \
+ *         _IOWR(KBASE_IOCTL_EXTRA_TYPE, 0, struct my_ioctl_args)
+ */
+
+#ifdef __cpluscplus
+}
+#endif
+
+#endif /* _UAPI_KBASE_IOCTL_H_ */
diff -urN mesa-23.0.0/src/panfrost/base/include/old/mali-ioctl.h mesa/src/panfrost/base/include/old/mali-ioctl.h
--- mesa-23.0.0/src/panfrost/base/include/old/mali-ioctl.h	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/base/include/old/mali-ioctl.h	2023-03-06 19:19:32.694308745 +0100
@@ -0,0 +1,744 @@
+/*
+ * © Copyright 2017-2018 The Panfrost Community
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU license.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+/**
+ * Definitions for all of the ioctls for the original open source bifrost GPU
+ * kernel driver, written by ARM.
+ */
+
+#ifndef __KBASE_IOCTL_H__
+#define __KBASE_IOCTL_H__
+
+typedef uint8_t u8;
+typedef uint16_t u16;
+typedef uint32_t u32;
+typedef uint64_t u64;
+
+typedef int32_t s32;
+typedef int64_t s64;
+
+typedef u8 mali_atom_id;
+
+/**
+ * Since these structs are passed to and from the kernel we need to make sure
+ * that we get the size of each struct to match exactly what the kernel is
+ * expecting. So, when editing this file make sure to add static asserts that
+ * check each struct's size against the arg length you see in strace.
+ */
+
+enum kbase_ioctl_mem_flags {
+   /* IN */
+   BASE_MEM_PROT_CPU_RD = (1U << 0), /**< Read access CPU side */
+   BASE_MEM_PROT_CPU_WR = (1U << 1), /**< Write access CPU side */
+   BASE_MEM_PROT_GPU_RD = (1U << 2), /**< Read access GPU side */
+   BASE_MEM_PROT_GPU_WR = (1U << 3), /**< Write access GPU side */
+   BASE_MEM_PROT_GPU_EX = (1U << 4), /**< Execute allowed on the GPU
+                                          side */
+
+   BASE_MEM_GROW_ON_GPF = (1U << 9), /**< Grow backing store on GPU
+                                          Page Fault */
+
+   BASE_MEM_COHERENT_SYSTEM = (1U << 10), /**< Page coherence Outer
+                                               shareable, if available */
+   BASE_MEM_COHERENT_LOCAL = (1U << 11),  /**< Page coherence Inner
+                                               shareable */
+   BASE_MEM_CACHED_CPU = (1U << 12),      /**< Should be cached on the
+                                               CPU */
+
+   /* IN/OUT */
+   BASE_MEM_SAME_VA = (1U << 13), /**< Must have same VA on both the GPU
+                                       and the CPU */
+   /* OUT */
+   BASE_MEM_NEED_MMAP = (1U << 14), /**< Must call mmap to acquire a GPU
+                                        address for the alloc */
+   /* IN */
+   BASE_MEM_COHERENT_SYSTEM_REQUIRED = (1U << 15), /**< Page coherence
+                                        Outer shareable, required. */
+   BASE_MEM_SECURE = (1U << 16),                   /**< Secure memory */
+   BASE_MEM_DONT_NEED = (1U << 17),                /**< Not needed physical
+                                                        memory */
+   BASE_MEM_IMPORT_SHARED = (1U << 18), /**< Must use shared CPU/GPU zone
+                                             (SAME_VA zone) but doesn't
+                                             require the addresses to
+                                             be the same */
+};
+
+#define KBASE_IOCTL_MEM_FLAGS_IN_MASK                                          \
+   (BASE_MEM_PROT_CPU_RD | BASE_MEM_PROT_CPU_WR | BASE_MEM_PROT_GPU_RD |       \
+    BASE_MEM_PROT_GPU_WR | BASE_MEM_PROT_GPU_EX | BASE_MEM_GROW_ON_GPF |       \
+    BASE_MEM_COHERENT_SYSTEM | BASE_MEM_COHERENT_LOCAL | BASE_MEM_CACHED_CPU | \
+    BASE_MEM_COHERENT_SYSTEM_REQUIRED | BASE_MEM_SECURE | BASE_MEM_DONT_NEED | \
+    BASE_MEM_IMPORT_SHARED)
+#define BASE_MEM_MAP_TRACKING_HANDLE (3ull << 12)
+
+enum kbase_ioctl_coherency_mode {
+   COHERENCY_ACE_LITE = 0,
+   COHERENCY_ACE = 1,
+   COHERENCY_NONE = 31
+};
+
+/*
+ * Mali Atom priority
+ *
+ * Only certain priority levels are actually implemented, as specified by the
+ * BASE_JD_PRIO_<...> definitions below. It is undefined to use a priority
+ * level that is not one of those defined below.
+ *
+ * Priority levels only affect scheduling between atoms of the same type within
+ * a mali context, and only after the atoms have had dependencies resolved.
+ * Fragment atoms does not affect non-frament atoms with lower priorities, and
+ * the other way around. For example, a low priority atom that has had its
+ * dependencies resolved might run before a higher priority atom that has not
+ * had its dependencies resolved.
+ *
+ * The scheduling between mali contexts/processes and between atoms from
+ * different mali contexts/processes is unaffected by atom priority.
+ *
+ * The atoms are scheduled as follows with respect to their priorities:
+ * - Let atoms 'X' and 'Y' be for the same job slot who have dependencies
+ *   resolved, and atom 'X' has a higher priority than atom 'Y'
+ * - If atom 'Y' is currently running on the HW, then it is interrupted to
+ *   allow atom 'X' to run soon after
+ * - If instead neither atom 'Y' nor atom 'X' are running, then when choosing
+ *   the next atom to run, atom 'X' will always be chosen instead of atom 'Y'
+ * - Any two atoms that have the same priority could run in any order with
+ *   respect to each other. That is, there is no ordering constraint between
+ *   atoms of the same priority.
+ */
+typedef u8 mali_jd_prio;
+#define BASE_JD_PRIO_MEDIUM ((mali_jd_prio)0)
+#define BASE_JD_PRIO_HIGH   ((mali_jd_prio)1)
+#define BASE_JD_PRIO_LOW    ((mali_jd_prio)2)
+
+/**
+ * @brief Job dependency type.
+ *
+ * A flags field will be inserted into the atom structure to specify whether a
+ * dependency is a data or ordering dependency (by putting it before/after
+ * 'core_req' in the structure it should be possible to add without changing
+ * the structure size).  When the flag is set for a particular dependency to
+ * signal that it is an ordering only dependency then errors will not be
+ * propagated.
+ */
+typedef u8 mali_jd_dep_type;
+#define BASE_JD_DEP_TYPE_INVALID (0)       /**< Invalid dependency */
+#define BASE_JD_DEP_TYPE_DATA    (1U << 0) /**< Data dependency */
+#define BASE_JD_DEP_TYPE_ORDER   (1U << 1) /**< Order dependency */
+
+/**
+ * @brief Job chain hardware requirements.
+ *
+ * A job chain must specify what GPU features it needs to allow the
+ * driver to schedule the job correctly.  By not specifying the
+ * correct settings can/will cause an early job termination.  Multiple
+ * values can be ORed together to specify multiple requirements.
+ * Special case is ::BASE_JD_REQ_DEP, which is used to express complex
+ * dependencies, and that doesn't execute anything on the hardware.
+ */
+typedef u32 mali_jd_core_req;
+
+/* Requirements that come from the HW */
+
+/**
+ * No requirement, dependency only
+ */
+#define BASE_JD_REQ_DEP ((mali_jd_core_req)0)
+
+/**
+ * Requires fragment shaders
+ */
+#define BASE_JD_REQ_FS ((mali_jd_core_req)1 << 0)
+
+/**
+ * Requires compute shaders
+ * This covers any of the following Midgard Job types:
+ * - Vertex Shader Job
+ * - Geometry Shader Job
+ * - An actual Compute Shader Job
+ *
+ * Compare this with @ref BASE_JD_REQ_ONLY_COMPUTE, which specifies that the
+ * job is specifically just the "Compute Shader" job type, and not the "Vertex
+ * Shader" nor the "Geometry Shader" job type.
+ */
+#define BASE_JD_REQ_CS ((mali_jd_core_req)1 << 1)
+#define BASE_JD_REQ_T  ((mali_jd_core_req)1 << 2) /**< Requires tiling */
+#define BASE_JD_REQ_CF ((mali_jd_core_req)1 << 3) /**< Requires cache flushes */
+#define BASE_JD_REQ_V                                                          \
+   ((mali_jd_core_req)1 << 4) /**< Requires value writeback */
+
+/* SW-only requirements - the HW does not expose these as part of the job slot
+ * capabilities */
+
+/* Requires fragment job with AFBC encoding */
+#define BASE_JD_REQ_FS_AFBC ((mali_jd_core_req)1 << 13)
+
+/**
+ * SW-only requirement: coalesce completion events.
+ * If this bit is set then completion of this atom will not cause an event to
+ * be sent to userspace, whether successful or not; completion events will be
+ * deferred until an atom completes which does not have this bit set.
+ *
+ * This bit may not be used in combination with BASE_JD_REQ_EXTERNAL_RESOURCES.
+ */
+#define BASE_JD_REQ_EVENT_COALESCE ((mali_jd_core_req)1 << 5)
+
+/**
+ * SW Only requirement: the job chain requires a coherent core group. We don't
+ * mind which coherent core group is used.
+ */
+#define BASE_JD_REQ_COHERENT_GROUP ((mali_jd_core_req)1 << 6)
+
+/**
+ * SW Only requirement: The performance counters should be enabled only when
+ * they are needed, to reduce power consumption.
+ */
+
+#define BASE_JD_REQ_PERMON ((mali_jd_core_req)1 << 7)
+
+/**
+ * SW Only requirement: External resources are referenced by this atom.  When
+ * external resources are referenced no syncsets can be bundled with the atom
+ * but should instead be part of a NULL jobs inserted into the dependency
+ * tree.  The first pre_dep object must be configured for the external
+ * resouces to use, the second pre_dep object can be used to create other
+ * dependencies.
+ *
+ * This bit may not be used in combination with BASE_JD_REQ_EVENT_COALESCE.
+ */
+#define BASE_JD_REQ_EXTERNAL_RESOURCES ((mali_jd_core_req)1 << 8)
+
+/**
+ * SW Only requirement: Software defined job. Jobs with this bit set will not
+ * be submitted to the hardware but will cause some action to happen within
+ * the driver
+ */
+#define BASE_JD_REQ_SOFT_JOB ((mali_jd_core_req)1 << 9)
+
+#define BASE_JD_REQ_SOFT_DUMP_CPU_GPU_TIME (BASE_JD_REQ_SOFT_JOB | 0x1)
+#define BASE_JD_REQ_SOFT_FENCE_TRIGGER     (BASE_JD_REQ_SOFT_JOB | 0x2)
+#define BASE_JD_REQ_SOFT_FENCE_WAIT        (BASE_JD_REQ_SOFT_JOB | 0x3)
+
+/**
+ * SW Only requirement : Replay job.
+ *
+ * If the preceding job fails, the replay job will cause the jobs specified in
+ * the list of mali_jd_replay_payload pointed to by the jc pointer to be
+ * replayed.
+ *
+ * A replay job will only cause jobs to be replayed up to MALIP_JD_REPLAY_LIMIT
+ * times. If a job fails more than MALIP_JD_REPLAY_LIMIT times then the replay
+ * job is failed, as well as any following dependencies.
+ *
+ * The replayed jobs will require a number of atom IDs. If there are not enough
+ * free atom IDs then the replay job will fail.
+ *
+ * If the preceding job does not fail, then the replay job is returned as
+ * completed.
+ *
+ * The replayed jobs will never be returned to userspace. The preceding failed
+ * job will be returned to userspace as failed; the status of this job should
+ * be ignored. Completion should be determined by the status of the replay soft
+ * job.
+ *
+ * In order for the jobs to be replayed, the job headers will have to be
+ * modified. The Status field will be reset to NOT_STARTED. If the Job Type
+ * field indicates a Vertex Shader Job then it will be changed to Null Job.
+ *
+ * The replayed jobs have the following assumptions :
+ *
+ * - No external resources. Any required external resources will be held by the
+ *   replay atom.
+ * - Pre-dependencies are created based on job order.
+ * - Atom numbers are automatically assigned.
+ * - device_nr is set to 0. This is not relevant as
+ *   BASE_JD_REQ_SPECIFIC_COHERENT_GROUP should not be set.
+ * - Priority is inherited from the replay job.
+ */
+#define BASE_JD_REQ_SOFT_REPLAY (BASE_JD_REQ_SOFT_JOB | 0x4)
+/**
+ * SW only requirement: event wait/trigger job.
+ *
+ * - BASE_JD_REQ_SOFT_EVENT_WAIT: this job will block until the event is set.
+ * - BASE_JD_REQ_SOFT_EVENT_SET: this job sets the event, thus unblocks the
+ *   other waiting jobs. It completes immediately.
+ * - BASE_JD_REQ_SOFT_EVENT_RESET: this job resets the event, making it
+ *   possible for other jobs to wait upon. It completes immediately.
+ */
+#define BASE_JD_REQ_SOFT_EVENT_WAIT  (BASE_JD_REQ_SOFT_JOB | 0x5)
+#define BASE_JD_REQ_SOFT_EVENT_SET   (BASE_JD_REQ_SOFT_JOB | 0x6)
+#define BASE_JD_REQ_SOFT_EVENT_RESET (BASE_JD_REQ_SOFT_JOB | 0x7)
+
+#define BASE_JD_REQ_SOFT_DEBUG_COPY (BASE_JD_REQ_SOFT_JOB | 0x8)
+
+/**
+ * SW only requirement: Just In Time allocation
+ *
+ * This job requests a JIT allocation based on the request in the
+ * @base_jit_alloc_info structure which is passed via the jc element of
+ * the atom.
+ *
+ * It should be noted that the id entry in @base_jit_alloc_info must not
+ * be reused until it has been released via @BASE_JD_REQ_SOFT_JIT_FREE.
+ *
+ * Should this soft job fail it is expected that a @BASE_JD_REQ_SOFT_JIT_FREE
+ * soft job to free the JIT allocation is still made.
+ *
+ * The job will complete immediately.
+ */
+#define BASE_JD_REQ_SOFT_JIT_ALLOC (BASE_JD_REQ_SOFT_JOB | 0x9)
+/**
+ * SW only requirement: Just In Time free
+ *
+ * This job requests a JIT allocation created by @BASE_JD_REQ_SOFT_JIT_ALLOC
+ * to be freed. The ID of the JIT allocation is passed via the jc element of
+ * the atom.
+ *
+ * The job will complete immediately.
+ */
+#define BASE_JD_REQ_SOFT_JIT_FREE (BASE_JD_REQ_SOFT_JOB | 0xa)
+
+/**
+ * SW only requirement: Map external resource
+ *
+ * This job requests external resource(s) are mapped once the dependencies
+ * of the job have been satisfied. The list of external resources are
+ * passed via the jc element of the atom which is a pointer to a
+ * @base_external_resource_list.
+ */
+#define BASE_JD_REQ_SOFT_EXT_RES_MAP (BASE_JD_REQ_SOFT_JOB | 0xb)
+/**
+ * SW only requirement: Unmap external resource
+ *
+ * This job requests external resource(s) are unmapped once the dependencies
+ * of the job has been satisfied. The list of external resources are
+ * passed via the jc element of the atom which is a pointer to a
+ * @base_external_resource_list.
+ */
+#define BASE_JD_REQ_SOFT_EXT_RES_UNMAP (BASE_JD_REQ_SOFT_JOB | 0xc)
+
+/**
+ * HW Requirement: Requires Compute shaders (but not Vertex or Geometry Shaders)
+ *
+ * This indicates that the Job Chain contains Midgard Jobs of the 'Compute
+ * Shaders' type.
+ *
+ * In contrast to @ref BASE_JD_REQ_CS, this does \b not indicate that the Job
+ * Chain contains 'Geometry Shader' or 'Vertex Shader' jobs.
+ */
+#define BASE_JD_REQ_ONLY_COMPUTE ((mali_jd_core_req)1 << 10)
+
+/**
+ * HW Requirement: Use the mali_jd_atom::device_nr field to specify a
+ * particular core group
+ *
+ * If both @ref BASE_JD_REQ_COHERENT_GROUP and this flag are set, this flag
+ * takes priority
+ *
+ * This is only guaranteed to work for @ref BASE_JD_REQ_ONLY_COMPUTE atoms.
+ *
+ * If the core availability policy is keeping the required core group turned
+ * off, then the job will fail with a @ref BASE_JD_EVENT_PM_EVENT error code.
+ */
+#define BASE_JD_REQ_SPECIFIC_COHERENT_GROUP ((mali_jd_core_req)1 << 11)
+
+/**
+ * SW Flag: If this bit is set then the successful completion of this atom
+ * will not cause an event to be sent to userspace
+ */
+#define BASE_JD_REQ_EVENT_ONLY_ON_FAILURE ((mali_jd_core_req)1 << 12)
+
+/**
+ * SW Flag: If this bit is set then completion of this atom will not cause an
+ * event to be sent to userspace, whether successful or not.
+ */
+#define BASE_JD_REQ_EVENT_NEVER ((mali_jd_core_req)1 << 14)
+
+/**
+ * SW Flag: Skip GPU cache clean and invalidation before starting a GPU job.
+ *
+ * If this bit is set then the GPU's cache will not be cleaned and invalidated
+ * until a GPU job starts which does not have this bit set or a job completes
+ * which does not have the @ref BASE_JD_REQ_SKIP_CACHE_END bit set. Do not use
+ * if the CPU may have written to memory addressed by the job since the last job
+ * without this bit set was submitted.
+ */
+#define BASE_JD_REQ_SKIP_CACHE_START ((mali_jd_core_req)1 << 15)
+
+/**
+ * SW Flag: Skip GPU cache clean and invalidation after a GPU job completes.
+ *
+ * If this bit is set then the GPU's cache will not be cleaned and invalidated
+ * until a GPU job completes which does not have this bit set or a job starts
+ * which does not have the @ref BASE_JD_REQ_SKIP_CACHE_START bti set. Do not
+ * use if the CPU may read from or partially overwrite memory addressed by the
+ * job before the next job without this bit set completes.
+ */
+#define BASE_JD_REQ_SKIP_CACHE_END ((mali_jd_core_req)1 << 16)
+
+/**
+ * These requirement bits are currently unused in mali_jd_core_req
+ */
+#define MALIP_JD_REQ_RESERVED                                                  \
+   (~(BASE_JD_REQ_ATOM_TYPE | BASE_JD_REQ_EXTERNAL_RESOURCES |                 \
+      BASE_JD_REQ_EVENT_ONLY_ON_FAILURE | MALIP_JD_REQ_EVENT_NEVER |           \
+      BASE_JD_REQ_EVENT_COALESCE | BASE_JD_REQ_COHERENT_GROUP |                \
+      BASE_JD_REQ_SPECIFIC_COHERENT_GROUP | BASE_JD_REQ_FS_AFBC |              \
+      BASE_JD_REQ_PERMON | BASE_JD_REQ_SKIP_CACHE_START |                      \
+      BASE_JD_REQ_SKIP_CACHE_END))
+
+/**
+ * Mask of all bits in mali_jd_core_req that control the type of the atom.
+ *
+ * This allows dependency only atoms to have flags set
+ */
+#define BASE_JD_REQ_ATOM_TYPE                                                  \
+   (BASE_JD_REQ_FS | BASE_JD_REQ_CS | BASE_JD_REQ_T | BASE_JD_REQ_CF |         \
+    BASE_JD_REQ_V | BASE_JD_REQ_SOFT_JOB | BASE_JD_REQ_ONLY_COMPUTE)
+
+/**
+ * Mask of all bits in mali_jd_core_req that control the type of a soft job.
+ */
+#define BASE_JD_REQ_SOFT_JOB_TYPE (BASE_JD_REQ_SOFT_JOB | 0x1f)
+
+/*
+ * Returns non-zero value if core requirements passed define a soft job or
+ * a dependency only job.
+ */
+#define BASE_JD_REQ_SOFT_JOB_OR_DEP(core_req)                                  \
+   ((core_req & BASE_JD_REQ_SOFT_JOB) ||                                       \
+    (core_req & BASE_JD_REQ_ATOM_TYPE) == BASE_JD_REQ_DEP)
+
+/**
+ * @brief The payload for a replay job. This must be in GPU memory.
+ */
+struct mali_jd_replay_payload {
+   /**
+    * Pointer to the first entry in the mali_jd_replay_jc list.  These
+    * will be replayed in @b reverse order (so that extra ones can be added
+    * to the head in future soft jobs without affecting this soft job)
+    */
+   u64 tiler_jc_list;
+
+   /**
+    * Pointer to the fragment job chain.
+    */
+   u64 fragment_jc;
+
+   /**
+    * Pointer to the tiler heap free FBD field to be modified.
+    */
+   u64 tiler_heap_free;
+
+   /**
+    * Hierarchy mask for the replayed fragment jobs. May be zero.
+    */
+   u16 fragment_hierarchy_mask;
+
+   /**
+    * Hierarchy mask for the replayed tiler jobs. May be zero.
+    */
+   u16 tiler_hierarchy_mask;
+
+   /**
+    * Default weight to be used for hierarchy levels not in the original
+    * mask.
+    */
+   u32 hierarchy_default_weight;
+
+   /**
+    * Core requirements for the tiler job chain
+    */
+   mali_jd_core_req tiler_core_req;
+
+   /**
+    * Core requirements for the fragment job chain
+    */
+   mali_jd_core_req fragment_core_req;
+};
+
+/**
+ * @brief An entry in the linked list of job chains to be replayed. This must
+ *        be in GPU memory.
+ */
+struct mali_jd_replay_jc {
+   /**
+    * Pointer to next entry in the list. A setting of NULL indicates the
+    * end of the list.
+    */
+   u64 next;
+
+   /**
+    * Pointer to the job chain.
+    */
+   u64 jc;
+};
+
+typedef u64 mali_ptr;
+
+#define MALI_PTR_FMT       "0x%" PRIx64
+#define MALI_SHORT_PTR_FMT "0x%" PRIxPTR
+
+#ifdef __LP64__
+#define PAD_CPU_PTR(p) p
+#else
+#define PAD_CPU_PTR(p)                                                         \
+   p;                                                                          \
+   u32:                                                                        \
+   32;
+#endif
+
+/* FIXME: Again, they don't specify any of these as packed structs. However,
+ * looking at these structs I'm worried that there is already spots where the
+ * compiler is potentially sticking in padding...
+ * Going to try something a little crazy, and just hope that our compiler
+ * happens to add the same kind of offsets since we can't really compare sizes
+ */
+
+/*
+ * Blob provided by the driver to store callback driver, not actually modified
+ * by the driver itself
+ */
+struct mali_jd_udata {
+   u64 blob[2];
+};
+
+struct mali_jd_dependency {
+   mali_atom_id atom_id;             /**< An atom number */
+   mali_jd_dep_type dependency_type; /**< Dependency type */
+};
+
+#define MALI_EXT_RES_MAX 10
+
+/* The original header never explicitly defines any values for these. In C,
+ * this -should- expand to SHARED == 0 and EXCLUSIVE == 1, so the only flag we
+ * actually need to decode here is EXCLUSIVE
+ */
+enum mali_external_resource_access {
+   MALI_EXT_RES_ACCESS_SHARED,
+   MALI_EXT_RES_ACCESS_EXCLUSIVE,
+};
+
+/* An aligned address to the resource | mali_external_resource_access */
+typedef u64 mali_external_resource;
+
+struct base_jd_atom_v2 {
+   mali_ptr jc;                          /**< job-chain GPU address */
+   struct mali_jd_udata udata;           /**< user data */
+   u64 extres_list;                      /**< list of external resources */
+   u16 nr_extres;                        /**< nr of external resources */
+   u16 compat_core_req;                  /**< core requirements which
+                                           correspond to the legacy support
+                                           for UK 10.2 */
+   struct mali_jd_dependency pre_dep[2]; /**< pre-dependencies, one need to
+                                        use SETTER function to assign
+                                        this field, this is done in
+                                        order to reduce possibility of
+                                        improper assigment of a
+                                        dependency field */
+   mali_atom_id atom_number;             /**< unique number to identify the
+                                           atom */
+   mali_jd_prio prio;                    /**< Atom priority. Refer to @ref
+                                           mali_jd_prio for more details */
+   u8 device_nr;                         /**< coregroup when
+                                           BASE_JD_REQ_SPECIFIC_COHERENT_GROUP
+                                           specified */
+   u8 : 8;
+   mali_jd_core_req core_req; /**< core requirements */
+} __attribute__((packed));
+
+/**
+ * enum mali_error - Mali error codes shared with userspace
+ *
+ * This is subset of those common Mali errors that can be returned to userspace.
+ * Values of matching user and kernel space enumerators MUST be the same.
+ * MALI_ERROR_NONE is guaranteed to be 0.
+ *
+ * @MALI_ERROR_NONE: Success
+ * @MALI_ERROR_OUT_OF_GPU_MEMORY: Not used in the kernel driver
+ * @MALI_ERROR_OUT_OF_MEMORY: Memory allocation failure
+ * @MALI_ERROR_FUNCTION_FAILED: Generic error code
+ */
+enum mali_error {
+   MALI_ERROR_NONE = 0,
+   MALI_ERROR_OUT_OF_GPU_MEMORY,
+   MALI_ERROR_OUT_OF_MEMORY,
+   MALI_ERROR_FUNCTION_FAILED,
+};
+
+/**
+ * Header used by all ioctls
+ */
+union kbase_ioctl_header {
+#ifdef dvalin
+   u32 pad[0];
+#else
+   /* [in] The ID of the UK function being called */
+   u32 id : 32;
+   /* [out] The return value of the UK function that was called */
+   enum mali_error rc : 32;
+
+   u64 : 64;
+#endif
+} __attribute__((packed));
+
+struct kbase_ioctl_get_version {
+   union kbase_ioctl_header header;
+   u16 major; /* [out] */
+   u16 minor; /* [out] */
+   u32 : 32;
+} __attribute__((packed));
+
+struct mali_mem_import_user_buffer {
+   u64 ptr;
+   u64 length;
+};
+
+union kbase_ioctl_mem_import {
+   struct {
+      union kbase_ioctl_header header;
+      u64 phandle;
+      enum {
+         BASE_MEM_IMPORT_TYPE_INVALID = 0,
+         BASE_MEM_IMPORT_TYPE_UMP = 1,
+         BASE_MEM_IMPORT_TYPE_UMM = 2,
+         BASE_MEM_IMPORT_TYPE_USER_BUFFER = 3,
+      } type : 32;
+      u32    : 32;
+      u64 flags;
+   } in;
+   struct {
+      union kbase_ioctl_header header;
+      u64 pad[2];
+      u64 flags;
+      u64 gpu_va;
+      u64 va_pages;
+   } out;
+} __attribute__((packed));
+
+struct kbase_ioctl_mem_commit {
+   union kbase_ioctl_header header;
+   /* [in] */
+   mali_ptr gpu_addr;
+   u64 pages;
+   /* [out] */
+   u32 result_subcode;
+   u32 : 32;
+} __attribute__((packed));
+
+enum kbase_ioctl_mem_query_type {
+   BASE_MEM_QUERY_COMMIT_SIZE = 1,
+   BASE_MEM_QUERY_VA_SIZE = 2,
+   BASE_MEM_QUERY_FLAGS = 3
+};
+
+struct kbase_ioctl_mem_query {
+   union kbase_ioctl_header header;
+   /* [in] */
+   mali_ptr gpu_addr;
+   enum kbase_ioctl_mem_query_type query : 32;
+   u32                                   : 32;
+   /* [out] */
+   u64 value;
+} __attribute__((packed));
+
+struct kbase_ioctl_mem_free {
+   union kbase_ioctl_header header;
+   mali_ptr gpu_addr; /* [in] */
+} __attribute__((packed));
+/* FIXME: Size unconfirmed (haven't seen in a trace yet) */
+
+struct kbase_ioctl_mem_flags_change {
+   union kbase_ioctl_header header;
+   /* [in] */
+   mali_ptr gpu_va;
+   u64 flags;
+   u64 mask;
+} __attribute__((packed));
+/* FIXME: Size unconfirmed (haven't seen in a trace yet) */
+
+struct kbase_ioctl_mem_alias {
+   union kbase_ioctl_header header;
+   /* [in/out] */
+   u64 flags;
+   /* [in] */
+   u64 stride;
+   u64 nents;
+   u64 ai;
+   /* [out] */
+   mali_ptr gpu_va;
+   u64 va_pages;
+} __attribute__((packed));
+
+struct kbase_ioctl_mem_sync {
+   union kbase_ioctl_header header;
+   mali_ptr handle;
+   u64 user_addr;
+   u64 size;
+   enum {
+      MALI_SYNC_TO_DEVICE = 1,
+      MALI_SYNC_TO_CPU = 2,
+   } type : 8;
+   u64    : 56;
+} __attribute__((packed));
+
+struct kbase_ioctl_set_flags {
+   union kbase_ioctl_header header;
+   u32 create_flags; /* [in] */
+   u32 : 32;
+} __attribute__((packed));
+
+struct kbase_ioctl_stream_create {
+   union kbase_ioctl_header header;
+   /* [in] */
+   char name[32];
+   /* [out] */
+   s32 fd;
+   u32 : 32;
+} __attribute__((packed));
+
+struct kbase_ioctl_job_submit {
+   union kbase_ioctl_header header;
+   /* [in] */
+   u64 addr;
+   u32 nr_atoms;
+   u32 stride;
+} __attribute__((packed));
+
+struct kbase_ioctl_get_context_id {
+   union kbase_ioctl_header header;
+   /* [out] */
+   s64 id;
+} __attribute__((packed));
+
+#undef PAD_CPU_PTR
+
+enum base_jd_event_code {
+   BASE_JD_EVENT_DONE = 1,
+};
+
+struct base_jd_event_v2 {
+   enum base_jd_event_code event_code;
+   mali_atom_id atom_number;
+   struct mali_jd_udata udata;
+};
+
+/* Defined in mali-props.h */
+struct kbase_ioctl_gpu_props_reg_dump;
+
+/* For ioctl's we haven't written decoding stuff for yet */
+typedef struct {
+   union kbase_ioctl_header header;
+} __ioctl_placeholder;
+
+#endif /* __KBASE_IOCTL_H__ */
diff -urN mesa-23.0.0/src/panfrost/base/include/old/mali-ioctl-midgard.h mesa/src/panfrost/base/include/old/mali-ioctl-midgard.h
--- mesa-23.0.0/src/panfrost/base/include/old/mali-ioctl-midgard.h	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/base/include/old/mali-ioctl-midgard.h	2023-03-06 19:19:32.687308699 +0100
@@ -0,0 +1,92 @@
+/*
+ * © Copyright 2017-2018 The Panfrost Community
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU license.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+#ifndef __KBASE_IOCTL_MIDGARD_H__
+#define __KBASE_IOCTL_MIDGARD_H__
+
+#include "mali-ioctl.h"
+
+#define KBASE_IOCTL_TYPE_BASE 0x80
+#define KBASE_IOCTL_TYPE_MAX  0x82
+
+union kbase_ioctl_mem_alloc {
+   struct {
+      union kbase_ioctl_header header;
+      u64 va_pages;
+      u64 commit_pages;
+      u64 extension;
+      u64 flags;
+   } in;
+   struct {
+      union kbase_ioctl_header header;
+      u64 pad[3];
+      u64 flags;
+      mali_ptr gpu_va;
+      u16 va_alignment;
+   } out;
+   u64 pad[7];
+} __attribute__((packed));
+
+#define KBASE_IOCTL_TYPE_COUNT                                                 \
+   (KBASE_IOCTL_TYPE_MAX - KBASE_IOCTL_TYPE_BASE + 1)
+
+#define KBASE_IOCTL_GET_VERSION (_IOWR(0x80, 0, struct kbase_ioctl_get_version))
+#define KBASE_IOCTL_MEM_ALLOC   (_IOWR(0x82, 0, union kbase_ioctl_mem_alloc))
+#define KBASE_IOCTL_MEM_IMPORT  (_IOWR(0x82, 1, union kbase_ioctl_mem_import))
+#define KBASE_IOCTL_MEM_COMMIT  (_IOWR(0x82, 2, struct kbase_ioctl_mem_commit))
+#define KBASE_IOCTL_MEM_QUERY   (_IOWR(0x82, 3, struct kbase_ioctl_mem_query))
+#define KBASE_IOCTL_MEM_FREE    (_IOWR(0x82, 4, struct kbase_ioctl_mem_free))
+#define KBASE_IOCTL_MEM_FLAGS_CHANGE                                           \
+   (_IOWR(0x82, 5, struct kbase_ioctl_mem_flags_change))
+#define KBASE_IOCTL_MEM_ALIAS   (_IOWR(0x82, 6, struct kbase_ioctl_mem_alias))
+#define KBASE_IOCTL_MEM_SYNC    (_IOWR(0x82, 8, struct kbase_ioctl_mem_sync))
+#define KBASE_IOCTL_POST_TERM   (_IOWR(0x82, 9, __ioctl_placeholder))
+#define KBASE_IOCTL_HWCNT_SETUP (_IOWR(0x82, 10, __ioctl_placeholder))
+#define KBASE_IOCTL_HWCNT_DUMP  (_IOWR(0x82, 11, __ioctl_placeholder))
+#define KBASE_IOCTL_HWCNT_CLEAR (_IOWR(0x82, 12, __ioctl_placeholder))
+#define KBASE_IOCTL_GPU_PROPS_REG_DUMP                                         \
+   (_IOWR(0x82, 14, struct kbase_ioctl_gpu_props_reg_dump))
+#define KBASE_IOCTL_FIND_CPU_OFFSET (_IOWR(0x82, 15, __ioctl_placeholder))
+#define KBASE_IOCTL_GET_VERSION_NEW                                            \
+   (_IOWR(0x82, 16, struct kbase_ioctl_get_version))
+#define KBASE_IOCTL_SET_FLAGS        (_IOWR(0x82, 18, struct kbase_ioctl_set_flags))
+#define KBASE_IOCTL_SET_TEST_DATA    (_IOWR(0x82, 19, __ioctl_placeholder))
+#define KBASE_IOCTL_INJECT_ERROR     (_IOWR(0x82, 20, __ioctl_placeholder))
+#define KBASE_IOCTL_MODEL_CONTROL    (_IOWR(0x82, 21, __ioctl_placeholder))
+#define KBASE_IOCTL_KEEP_GPU_POWERED (_IOWR(0x82, 22, __ioctl_placeholder))
+#define KBASE_IOCTL_FENCE_VALIDATE   (_IOWR(0x82, 23, __ioctl_placeholder))
+#define KBASE_IOCTL_STREAM_CREATE                                              \
+   (_IOWR(0x82, 24, struct kbase_ioctl_stream_create))
+#define KBASE_IOCTL_GET_PROFILING_CONTROLS                                     \
+   (_IOWR(0x82, 25, __ioctl_placeholder))
+#define KBASE_IOCTL_SET_PROFILING_CONTROLS                                     \
+   (_IOWR(0x82, 26, __ioctl_placeholder))
+#define KBASE_IOCTL_DEBUGFS_MEM_PROFILE_ADD                                    \
+   (_IOWR(0x82, 27, __ioctl_placeholder))
+#define KBASE_IOCTL_JOB_SUBMIT     (_IOWR(0x82, 28, struct kbase_ioctl_job_submit))
+#define KBASE_IOCTL_DISJOINT_QUERY (_IOWR(0x82, 29, __ioctl_placeholder))
+#define KBASE_IOCTL_GET_CONTEXT_ID                                             \
+   (_IOWR(0x82, 31, struct kbase_ioctl_get_context_id))
+#define KBASE_IOCTL_TLSTREAM_ACQUIRE_V10_4                                     \
+   (_IOWR(0x82, 32, __ioctl_placeholder))
+#define KBASE_IOCTL_TLSTREAM_TEST      (_IOWR(0x82, 33, __ioctl_placeholder))
+#define KBASE_IOCTL_TLSTREAM_STATS     (_IOWR(0x82, 34, __ioctl_placeholder))
+#define KBASE_IOCTL_TLSTREAM_FLUSH     (_IOWR(0x82, 35, __ioctl_placeholder))
+#define KBASE_IOCTL_HWCNT_READER_SETUP (_IOWR(0x82, 36, __ioctl_placeholder))
+#define KBASE_IOCTL_SET_PRFCNT_VALUES  (_IOWR(0x82, 37, __ioctl_placeholder))
+#define KBASE_IOCTL_SOFT_EVENT_UPDATE  (_IOWR(0x82, 38, __ioctl_placeholder))
+#define KBASE_IOCTL_MEM_JIT_INIT       (_IOWR(0x82, 39, __ioctl_placeholder))
+#define KBASE_IOCTL_TLSTREAM_ACQUIRE   (_IOWR(0x82, 40, __ioctl_placeholder))
+
+#endif /* __KBASE_IOCTL_MIDGARD_H__ */
diff -urN mesa-23.0.0/src/panfrost/base/include/old/mali-props.h mesa/src/panfrost/base/include/old/mali-props.h
--- mesa-23.0.0/src/panfrost/base/include/old/mali-props.h	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/base/include/old/mali-props.h	2023-03-06 19:19:32.695308752 +0100
@@ -0,0 +1,262 @@
+/*
+ * © Copyright 2017-2018 The Panfrost Community
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU license.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+#ifndef __MALI_PROPS_H__
+#define __MALI_PROPS_H__
+
+#include "mali-ioctl.h"
+
+#define MALI_GPU_NUM_TEXTURE_FEATURES_REGISTERS 3
+#define MALI_GPU_MAX_JOB_SLOTS                  16
+#define MALI_MAX_COHERENT_GROUPS                16
+
+/* Capabilities of a job slot as reported by JS_FEATURES registers */
+
+#define JS_FEATURE_NULL_JOB        (1u << 1)
+#define JS_FEATURE_SET_VALUE_JOB   (1u << 2)
+#define JS_FEATURE_CACHE_FLUSH_JOB (1u << 3)
+#define JS_FEATURE_COMPUTE_JOB     (1u << 4)
+#define JS_FEATURE_VERTEX_JOB      (1u << 5)
+#define JS_FEATURE_GEOMETRY_JOB    (1u << 6)
+#define JS_FEATURE_TILER_JOB       (1u << 7)
+#define JS_FEATURE_FUSED_JOB       (1u << 8)
+#define JS_FEATURE_FRAGMENT_JOB    (1u << 9)
+
+struct mali_gpu_core_props {
+   /**
+    * Product specific value.
+    */
+   u32 product_id;
+
+   /**
+    * Status of the GPU release.
+    * No defined values, but starts at 0 and increases by one for each
+    * release status (alpha, beta, EAC, etc.).
+    * 4 bit values (0-15).
+    */
+   u16 version_status;
+
+   /**
+    * Minor release number of the GPU. "P" part of an "RnPn" release
+    * number.
+    * 8 bit values (0-255).
+    */
+   u16 minor_revision;
+
+   /**
+    * Major release number of the GPU. "R" part of an "RnPn" release
+    * number.
+    * 4 bit values (0-15).
+    */
+   u16 major_revision;
+
+   u16 : 16;
+
+   /**
+    * @usecase GPU clock speed is not specified in the Midgard
+    * Architecture, but is <b>necessary for OpenCL's clGetDeviceInfo()
+    * function</b>.
+    */
+   u32 gpu_speed_mhz;
+
+   /**
+    * @usecase GPU clock max/min speed is required for computing
+    * best/worst case in tasks as job scheduling ant irq_throttling. (It
+    * is not specified in the Midgard Architecture).
+    */
+   u32 gpu_freq_khz_max;
+   u32 gpu_freq_khz_min;
+
+   /**
+    * Size of the shader program counter, in bits.
+    */
+   u32 log2_program_counter_size;
+
+   /**
+    * TEXTURE_FEATURES_x registers, as exposed by the GPU. This is a
+    * bitpattern where a set bit indicates that the format is supported.
+    *
+    * Before using a texture format, it is recommended that the
+    * corresponding bit be checked.
+    */
+   u32 texture_features[MALI_GPU_NUM_TEXTURE_FEATURES_REGISTERS];
+
+   /**
+    * Theoretical maximum memory available to the GPU. It is unlikely
+    * that a client will be able to allocate all of this memory for their
+    * own purposes, but this at least provides an upper bound on the
+    * memory available to the GPU.
+    *
+    * This is required for OpenCL's clGetDeviceInfo() call when
+    * CL_DEVICE_GLOBAL_MEM_SIZE is requested, for OpenCL GPU devices. The
+    * client will not be expecting to allocate anywhere near this value.
+    */
+   u64 gpu_available_memory_size;
+};
+
+struct mali_gpu_l2_cache_props {
+   u8 log2_line_size;
+   u8 log2_cache_size;
+   u8 num_l2_slices; /* Number of L2C slices. 1 or higher */
+   u64 : 40;
+};
+
+struct mali_gpu_tiler_props {
+   u32 bin_size_bytes;    /* Max is 4*2^15 */
+   u32 max_active_levels; /* Max is 2^15 */
+};
+
+struct mali_gpu_thread_props {
+   u32 max_threads;           /* Max. number of threads per core */
+   u32 max_workgroup_size;    /* Max. number of threads per workgroup */
+   u32 max_barrier_size;      /* Max. number of threads that can
+                                 synchronize on a simple barrier */
+   u16 max_registers;         /* Total size [1..65535] of the register
+                                 file available per core. */
+   u8 max_task_queue;         /* Max. tasks [1..255] which may be sent
+                                 to a core before it becomes blocked. */
+   u8 max_thread_group_split; /* Max. allowed value [1..15] of the
+                                 Thread Group Split field. */
+   enum {
+      MALI_GPU_IMPLEMENTATION_UNKNOWN = 0,
+      MALI_GPU_IMPLEMENTATION_SILICON = 1,
+      MALI_GPU_IMPLEMENTATION_FPGA = 2,
+      MALI_GPU_IMPLEMENTATION_SW = 3,
+   } impl_tech : 8;
+   u64         : 56;
+};
+
+/**
+ * @brief descriptor for a coherent group
+ *
+ * \c core_mask exposes all cores in that coherent group, and \c num_cores
+ * provides a cached population-count for that mask.
+ *
+ * @note Whilst all cores are exposed in the mask, not all may be available to
+ * the application, depending on the Kernel Power policy.
+ *
+ * @note if u64s must be 8-byte aligned, then this structure has 32-bits of
+ * wastage.
+ */
+struct mali_ioctl_gpu_coherent_group {
+   u64 core_mask; /**< Core restriction mask required for the
+                    group */
+   u16 num_cores; /**< Number of cores in the group */
+   u64 : 48;
+};
+
+/**
+ * @brief Coherency group information
+ *
+ * Note that the sizes of the members could be reduced. However, the \c group
+ * member might be 8-byte aligned to ensure the u64 core_mask is 8-byte
+ * aligned, thus leading to wastage if the other members sizes were reduced.
+ *
+ * The groups are sorted by core mask. The core masks are non-repeating and do
+ * not intersect.
+ */
+struct mali_gpu_coherent_group_info {
+   u32 num_groups;
+
+   /**
+    * Number of core groups (coherent or not) in the GPU. Equivalent to
+    * the number of L2 Caches.
+    *
+    * The GPU Counter dumping writes 2048 bytes per core group,
+    * regardless of whether the core groups are coherent or not. Hence
+    * this member is needed to calculate how much memory is required for
+    * dumping.
+    *
+    * @note Do not use it to work out how many valid elements are in the
+    * group[] member. Use num_groups instead.
+    */
+   u32 num_core_groups;
+
+   /**
+    * Coherency features of the memory, accessed by @ref gpu_mem_features
+    * methods
+    */
+   u32 coherency;
+
+   u32 : 32;
+
+   /**
+    * Descriptors of coherent groups
+    */
+   struct mali_ioctl_gpu_coherent_group group[MALI_MAX_COHERENT_GROUPS];
+};
+
+/**
+ * A complete description of the GPU's Hardware Configuration Discovery
+ * registers.
+ *
+ * The information is presented inefficiently for access. For frequent access,
+ * the values should be better expressed in an unpacked form in the
+ * base_gpu_props structure.
+ *
+ * @usecase The raw properties in @ref gpu_raw_gpu_props are necessary to
+ * allow a user of the Mali Tools (e.g. PAT) to determine "Why is this device
+ * behaving differently?". In this case, all information about the
+ * configuration is potentially useful, but it <b>does not need to be processed
+ * by the driver</b>. Instead, the raw registers can be processed by the Mali
+ * Tools software on the host PC.
+ *
+ */
+struct mali_gpu_raw_props {
+   u64 shader_present;
+   u64 tiler_present;
+   u64 l2_present;
+   u64 stack_present;
+
+   u32 l2_features;
+   u32 suspend_size; /* API 8.2+ */
+   u32 mem_features;
+   u32 mmu_features;
+
+   u32 as_present;
+
+   u32 js_present;
+   u32 js_features[MALI_GPU_MAX_JOB_SLOTS];
+   u32 tiler_features;
+   u32 texture_features[3];
+
+   u32 gpu_id;
+
+   u32 thread_max_threads;
+   u32 thread_max_workgroup_size;
+   u32 thread_max_barrier_size;
+   u32 thread_features;
+
+   /*
+    * Note: This is the _selected_ coherency mode rather than the
+    * available modes as exposed in the coherency_features register.
+    */
+   u32 coherency_mode;
+};
+
+struct kbase_ioctl_gpu_props_reg_dump {
+   union kbase_ioctl_header header;
+   struct mali_gpu_core_props core;
+   struct mali_gpu_l2_cache_props l2;
+   u64 : 64;
+   struct mali_gpu_tiler_props tiler;
+   struct mali_gpu_thread_props thread;
+
+   struct mali_gpu_raw_props raw;
+
+   /** This must be last member of the structure */
+   struct mali_gpu_coherent_group_info coherency_info;
+} __attribute__((packed));
+
+#endif
diff -urN mesa-23.0.0/src/panfrost/base/meson.build mesa/src/panfrost/base/meson.build
--- mesa-23.0.0/src/panfrost/base/meson.build	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/base/meson.build	2023-03-06 17:54:35.835494947 +0100
@@ -0,0 +1,55 @@
+# Copyright © 2018 Rob Clark
+# Copyright © 2019 Collabora
+# Copyright © 2022 Icecream95
+
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+# SOFTWARE.
+
+libpanfrost_base_versions = ['0', '1', '2', '258']
+libpanfrost_base_per_arch = []
+
+foreach ver : libpanfrost_base_versions
+  libpanfrost_base_per_arch += static_library(
+    'pan-base-v' + ver,
+    'pan_vX_base.c',
+    include_directories : [
+      inc_src, inc_include, inc_gallium, inc_mesa, inc_gallium_aux,
+      include_directories('include'),
+    ],
+    c_args : ['-DPAN_BASE_VER=' + ver],
+    gnu_symbol_visibility : 'hidden',
+    dependencies: [dep_valgrind],
+)
+endforeach
+
+libpanfrost_base = static_library(
+  'panfrost_base',
+  'pan_base.c',
+  include_directories : [
+    inc_include, inc_src, inc_mapi, inc_mesa, inc_gallium, inc_gallium_aux, inc_panfrost_hw,
+    include_directories('include'),
+  ],
+  gnu_symbol_visibility : 'hidden',
+  build_by_default : false,
+  link_with: [libpanfrost_base_per_arch],
+)
+
+libpanfrost_base_dep = declare_dependency(
+  link_with: [libpanfrost_base_per_arch, libpanfrost_base],
+  include_directories: [include_directories('.')],
+)
diff -urN mesa-23.0.0/src/panfrost/base/pan_base.c mesa/src/panfrost/base/pan_base.c
--- mesa-23.0.0/src/panfrost/base/pan_base.c	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/base/pan_base.c	2023-03-06 19:19:32.713308871 +0100
@@ -0,0 +1,300 @@
+/*
+ * Copyright (C) 2022 Icecream95
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <errno.h>
+#include <fcntl.h>
+#include <inttypes.h>
+#include <poll.h>
+#include <pthread.h>
+#include <stdbool.h>
+#include <stddef.h>
+#include <stdint.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <unistd.h>
+#include <sys/ioctl.h>
+#include <sys/mman.h>
+
+#include "util/macros.h"
+#include "pan_base.h"
+
+#include "mali_kbase_ioctl.h"
+
+bool
+kbase_open(kbase k, int fd, unsigned cs_queue_count, bool verbose)
+{
+   *k = (struct kbase_){0};
+   k->fd = fd;
+   k->cs_queue_count = cs_queue_count;
+   k->page_size = sysconf(_SC_PAGE_SIZE);
+   k->verbose = verbose;
+
+   if (k->fd == -1)
+      return kbase_open_csf_noop(k);
+
+   struct kbase_ioctl_version_check ver = {0};
+
+   if (ioctl(k->fd, KBASE_IOCTL_VERSION_CHECK_RESERVED, &ver) == 0) {
+      return kbase_open_csf(k);
+   } else if (ioctl(k->fd, KBASE_IOCTL_VERSION_CHECK, &ver) == 0) {
+      if (ver.major == 3)
+         return kbase_open_old(k);
+      else
+         return kbase_open_new(k);
+   }
+
+   return false;
+}
+
+/* If fd != -1, ownership is passed in */
+int
+kbase_alloc_gem_handle_locked(kbase k, base_va va, int fd)
+{
+   kbase_handle h = {.va = va, .fd = fd};
+
+   unsigned size = util_dynarray_num_elements(&k->gem_handles, kbase_handle);
+
+   kbase_handle *handles = util_dynarray_begin(&k->gem_handles);
+
+   for (unsigned i = 0; i < size; ++i) {
+      if (handles[i].fd == -2) {
+         handles[i] = h;
+         return i;
+      }
+   }
+
+   util_dynarray_append(&k->gem_handles, kbase_handle, h);
+
+   return size;
+}
+
+int
+kbase_alloc_gem_handle(kbase k, base_va va, int fd)
+{
+   pthread_mutex_lock(&k->handle_lock);
+
+   int ret = kbase_alloc_gem_handle_locked(k, va, fd);
+
+   pthread_mutex_unlock(&k->handle_lock);
+
+   return ret;
+}
+
+void
+kbase_free_gem_handle(kbase k, int handle)
+{
+   pthread_mutex_lock(&k->handle_lock);
+
+   unsigned size = util_dynarray_num_elements(&k->gem_handles, kbase_handle);
+
+   int fd;
+
+   if (handle >= size) {
+      pthread_mutex_unlock(&k->handle_lock);
+      return;
+   }
+
+   if (handle + 1 < size) {
+      kbase_handle *ptr =
+         util_dynarray_element(&k->gem_handles, kbase_handle, handle);
+      fd = ptr->fd;
+      ptr->fd = -2;
+   } else {
+      fd = (util_dynarray_pop(&k->gem_handles, kbase_handle)).fd;
+   }
+
+   if (fd != -1)
+      close(fd);
+
+   pthread_mutex_unlock(&k->handle_lock);
+}
+
+kbase_handle
+kbase_gem_handle_get(kbase k, int handle)
+{
+   kbase_handle h = {.fd = -1};
+
+   pthread_mutex_lock(&k->handle_lock);
+
+   unsigned size = util_dynarray_num_elements(&k->gem_handles, kbase_handle);
+
+   if (handle < size)
+      h = *util_dynarray_element(&k->gem_handles, kbase_handle, handle);
+
+   pthread_mutex_unlock(&k->handle_lock);
+
+   return h;
+}
+
+int
+kbase_wait_bo(kbase k, int handle, int64_t timeout_ns, bool wait_readers)
+{
+   struct kbase_wait_ctx wait = kbase_wait_init(k, timeout_ns);
+
+   while (kbase_wait_for_event(&wait)) {
+      pthread_mutex_lock(&k->handle_lock);
+      if (handle >= util_dynarray_num_elements(&k->gem_handles, kbase_handle)) {
+         pthread_mutex_unlock(&k->handle_lock);
+         kbase_wait_fini(wait);
+         errno = EINVAL;
+         return -1;
+      }
+      kbase_handle *ptr =
+         util_dynarray_element(&k->gem_handles, kbase_handle, handle);
+      if (!ptr->use_count) {
+         pthread_mutex_unlock(&k->handle_lock);
+         kbase_wait_fini(wait);
+         return 0;
+      }
+      pthread_mutex_unlock(&k->handle_lock);
+   }
+
+   kbase_wait_fini(wait);
+   errno = ETIMEDOUT;
+   return -1;
+}
+
+static void
+adjust_time(struct timespec *tp, int64_t ns)
+{
+   ns += tp->tv_nsec;
+   tp->tv_nsec = ns % 1000000000;
+   tp->tv_sec += ns / 1000000000;
+}
+
+static int64_t
+ns_until(struct timespec tp)
+{
+   struct timespec now;
+   clock_gettime(CLOCK_MONOTONIC, &now);
+
+   int64_t sec = (tp.tv_sec - now.tv_sec) * 1000000000;
+   int64_t ns = tp.tv_nsec - now.tv_nsec;
+
+   /* Clamp the value to zero to avoid errors from ppoll */
+   return MAX2(sec + ns, 0);
+}
+
+static void
+kbase_wait_signal(kbase k)
+{
+   /* We must acquire the event condition lock, otherwise another
+    * thread could be between the trylock and the cond_wait, and
+    * not notice the broadcast. */
+   pthread_mutex_lock(&k->event_cnd_lock);
+   pthread_cond_broadcast(&k->event_cnd);
+   pthread_mutex_unlock(&k->event_cnd_lock);
+}
+
+struct kbase_wait_ctx
+kbase_wait_init(kbase k, int64_t timeout_ns)
+{
+   struct timespec tp;
+   clock_gettime(CLOCK_MONOTONIC, &tp);
+
+   adjust_time(&tp, timeout_ns);
+
+   return (struct kbase_wait_ctx){
+      .k = k,
+      .until = tp,
+   };
+}
+
+bool
+kbase_wait_for_event(struct kbase_wait_ctx *ctx)
+{
+   kbase k = ctx->k;
+
+   /* Return instantly the first time so that a check outside the
+    * wait_for_Event loop is not required */
+   if (!ctx->has_cnd_lock) {
+      pthread_mutex_lock(&k->event_cnd_lock);
+      ctx->has_cnd_lock = true;
+      return true;
+   }
+
+   if (!ctx->has_lock) {
+      if (pthread_mutex_trylock(&k->event_read_lock) == 0) {
+         ctx->has_lock = true;
+         pthread_mutex_unlock(&k->event_cnd_lock);
+      } else {
+         int ret = pthread_cond_timedwait(&k->event_cnd, &k->event_cnd_lock,
+                                          &ctx->until);
+         return ret != ETIMEDOUT;
+      }
+   }
+
+   bool event = k->poll_event(k, ns_until(ctx->until));
+   k->handle_events(k);
+   kbase_wait_signal(k);
+   return event;
+}
+
+void
+kbase_wait_fini(struct kbase_wait_ctx ctx)
+{
+   kbase k = ctx.k;
+
+   if (ctx.has_lock) {
+      pthread_mutex_unlock(&k->event_read_lock);
+      kbase_wait_signal(k);
+   } else if (ctx.has_cnd_lock) {
+      pthread_mutex_unlock(&k->event_cnd_lock);
+   }
+}
+
+void
+kbase_ensure_handle_events(kbase k)
+{
+   /* If we don't manage to take the lock, then events have recently/will
+    * soon be handled, there is no need to do anything. */
+   if (pthread_mutex_trylock(&k->event_read_lock) == 0) {
+      k->handle_events(k);
+      pthread_mutex_unlock(&k->event_read_lock);
+      kbase_wait_signal(k);
+   }
+}
+
+bool
+kbase_poll_fd_until(int fd, bool wait_shared, struct timespec tp)
+{
+   struct pollfd pfd = {
+      .fd = fd,
+      .events = wait_shared ? POLLOUT : POLLIN,
+   };
+
+   uint64_t timeout = ns_until(tp);
+
+   struct timespec t = {
+      .tv_sec = timeout / 1000000000,
+      .tv_nsec = timeout % 1000000000,
+   };
+
+   int ret = ppoll(&pfd, 1, &t, NULL);
+
+   if (ret == -1 && errno != EINTR)
+      perror("kbase_poll_fd_until");
+
+   return ret != 0;
+}
diff -urN mesa-23.0.0/src/panfrost/base/pan_base.h mesa/src/panfrost/base/pan_base.h
--- mesa-23.0.0/src/panfrost/base/pan_base.h	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/base/pan_base.h	2023-03-06 19:19:32.716308891 +0100
@@ -0,0 +1,232 @@
+/*
+ * Copyright (C) 2022 Icecream95 <ixn@disroot.org>
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/* Library for interfacing with kbase */
+#ifndef PAN_BASE_H
+#define PAN_BASE_H
+
+#include "util/list.h"
+#include "util/u_dynarray.h"
+
+#define PAN_EVENT_SIZE 16
+
+typedef uint64_t base_va;
+struct base_ptr {
+   void *cpu;
+   base_va gpu;
+};
+
+struct kbase_syncobj;
+
+/* The job is done when the queue seqnum > seqnum */
+struct kbase_sync_link {
+   struct kbase_sync_link *next; /* must be first */
+   uint64_t seqnum;
+   void (*callback)(void *);
+   void *data;
+};
+
+struct kbase_event_slot {
+   struct kbase_sync_link *syncobjs;
+   struct kbase_sync_link **back;
+   uint64_t last_submit;
+   uint64_t last;
+};
+
+struct kbase_context {
+   uint8_t csg_handle;
+   uint8_t kcpu_queue;
+   bool kcpu_init; // TODO: Always create a queue?
+   uint32_t csg_uid;
+   unsigned num_csi;
+
+   unsigned tiler_heap_chunk_size;
+   base_va tiler_heap_va;
+   base_va tiler_heap_header;
+};
+
+struct kbase_cs {
+   struct kbase_context *ctx;
+   void *user_io;
+   base_va va;
+   unsigned size;
+   unsigned event_mem_offset;
+   unsigned csi;
+
+   uint64_t last_insert;
+
+   // TODO: This is only here because it's convenient for emit_csf_queue
+   uint32_t *latest_flush;
+};
+
+#define KBASE_SLOT_COUNT 2
+
+typedef struct {
+   base_va va;
+   int fd;
+   uint8_t use_count;
+   /* For emulating implicit sync. TODO make this work on v10 */
+   uint8_t last_access[KBASE_SLOT_COUNT];
+} kbase_handle;
+
+struct kbase_;
+typedef struct kbase_ *kbase;
+
+struct kbase_ {
+   unsigned setup_state;
+   bool verbose;
+
+   int fd;
+   unsigned api;
+   unsigned page_size;
+   // TODO: Actually we may want to try to pack multiple contexts / queue
+   // "sets" into a single group...
+   unsigned cs_queue_count;
+
+   /* Must not hold handle_lock while acquiring event_read_lock */
+   pthread_mutex_t handle_lock;
+   pthread_mutex_t event_read_lock;
+   pthread_mutex_t event_cnd_lock;
+   pthread_cond_t event_cnd;
+   /* TODO: Per-context/queue locks? */
+   pthread_mutex_t queue_lock;
+
+   struct list_head syncobjs;
+
+   unsigned gpuprops_size;
+   void *gpuprops;
+
+   void *tracking_region;
+   void *csf_user_reg;
+   struct base_ptr event_mem;
+   struct base_ptr kcpu_event_mem;
+   // TODO: dynamically size
+   struct kbase_event_slot event_slots[256];
+   // TODO: USe a bitset?
+   unsigned event_slot_usage;
+
+   uint8_t atom_number;
+
+   struct util_dynarray gem_handles;
+   struct util_dynarray atom_bos[256];
+   uint64_t job_seq;
+
+   void (*close)(kbase k);
+
+   bool (*get_pan_gpuprop)(kbase k, unsigned name, uint64_t *value);
+   bool (*get_mali_gpuprop)(kbase k, unsigned name, uint64_t *value);
+
+   struct base_ptr (*alloc)(kbase k, size_t size, unsigned pan_flags,
+                            unsigned mali_flags);
+   void (*free)(kbase k, base_va va);
+
+   int (*import_dmabuf)(kbase k, int fd);
+   void *(*mmap_import)(kbase k, base_va va, size_t size);
+
+   void (*cache_clean)(void *ptr, size_t size);
+   void (*cache_invalidate)(void *ptr, size_t size);
+
+   /* Returns false on timeout */
+   bool (*poll_event)(kbase k, int64_t timeout_ns);
+   bool (*handle_events)(kbase k);
+
+   /* <= v9 GPUs */
+   int (*submit)(kbase k, uint64_t va, unsigned req, struct kbase_syncobj *o,
+                 int32_t *handles, unsigned num_handles);
+
+   /* >= v10 GPUs */
+   struct kbase_context *(*context_create)(kbase k);
+   void (*context_destroy)(kbase k, struct kbase_context *ctx);
+   bool (*context_recreate)(kbase k, struct kbase_context *ctx);
+
+   // TODO: Pass in a priority?
+   struct kbase_cs (*cs_bind)(kbase k, struct kbase_context *ctx, base_va va,
+                              unsigned size);
+   void (*cs_term)(kbase k, struct kbase_cs *cs);
+   void (*cs_rebind)(kbase k, struct kbase_cs *cs);
+
+   bool (*cs_submit)(kbase k, struct kbase_cs *cs, uint64_t insert_offset,
+                     struct kbase_syncobj *o, uint64_t seqnum);
+   bool (*cs_wait)(kbase k, struct kbase_cs *cs, uint64_t extract_offset,
+                   struct kbase_syncobj *o);
+
+   int (*kcpu_fence_export)(kbase k, struct kbase_context *ctx);
+   bool (*kcpu_fence_import)(kbase k, struct kbase_context *ctx, int fd);
+
+   bool (*kcpu_cqs_set)(kbase k, struct kbase_context *ctx, base_va addr,
+                        uint64_t value);
+   bool (*kcpu_cqs_wait)(kbase k, struct kbase_context *ctx, base_va addr,
+                         uint64_t value);
+
+   /* syncobj functions */
+   struct kbase_syncobj *(*syncobj_create)(kbase k);
+   void (*syncobj_destroy)(kbase k, struct kbase_syncobj *o);
+   struct kbase_syncobj *(*syncobj_dup)(kbase k, struct kbase_syncobj *o);
+   /* TODO: timeout? (and for cs_wait) */
+   bool (*syncobj_wait)(kbase k, struct kbase_syncobj *o);
+
+   /* Returns false if there are no active queues */
+   bool (*callback_all_queues)(kbase k, int32_t *count,
+                               void (*callback)(void *), void *data);
+
+   void (*mem_sync)(kbase k, base_va gpu, void *cpu, size_t size,
+                    bool invalidate);
+};
+
+bool kbase_open(kbase k, int fd, unsigned cs_queue_count, bool verbose);
+
+/* Called from kbase_open */
+bool kbase_open_old(kbase k);
+bool kbase_open_new(kbase k);
+bool kbase_open_csf(kbase k);
+bool kbase_open_csf_noop(kbase k);
+
+/* BO management */
+int kbase_alloc_gem_handle(kbase k, base_va va, int fd);
+int kbase_alloc_gem_handle_locked(kbase k, base_va va, int fd);
+void kbase_free_gem_handle(kbase k, int handle);
+kbase_handle kbase_gem_handle_get(kbase k, int handle);
+int kbase_wait_bo(kbase k, int handle, int64_t timeout_ns, bool wait_readers);
+
+/* Event waiting */
+struct kbase_wait_ctx {
+   kbase k;
+   struct timespec until;
+   bool has_lock;
+   bool has_cnd_lock;
+};
+
+struct kbase_wait_ctx kbase_wait_init(kbase k, int64_t timeout_ns);
+/* Returns false on timeout, kbase_wait_fini must still be called */
+bool kbase_wait_for_event(struct kbase_wait_ctx *ctx);
+void kbase_wait_fini(struct kbase_wait_ctx ctx);
+
+void kbase_ensure_handle_events(kbase k);
+
+bool kbase_poll_fd_until(int fd, bool wait_shared, struct timespec tp);
+
+/* Must not conflict with PANFROST_BO_* flags */
+#define MALI_BO_CACHED_CPU   (1 << 16)
+#define MALI_BO_UNCACHED_GPU (1 << 17)
+
+#endif
diff -urN mesa-23.0.0/src/panfrost/base/pan_base_noop.h mesa/src/panfrost/base/pan_base_noop.h
--- mesa-23.0.0/src/panfrost/base/pan_base_noop.h	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/base/pan_base_noop.h	2023-03-06 19:19:32.718308904 +0100
@@ -0,0 +1,151 @@
+/*
+ * Copyright (C) 2022 Icecream95 <ixn@disroot.org>
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef PAN_BASE_NOOP_H
+#define PAN_BASE_NOOP_H
+
+/* For Mali-G610 as used in RK3588 */
+#define PROP(name, value) ((name << 2) | 2), value
+static const uint32_t gpu_props[] = {
+   PROP(KBASE_GPUPROP_RAW_GPU_ID, 0xa8670000),
+   PROP(KBASE_GPUPROP_PRODUCT_ID, 0xa867),
+   PROP(KBASE_GPUPROP_RAW_SHADER_PRESENT, 0x50005),
+   PROP(KBASE_GPUPROP_RAW_TEXTURE_FEATURES_0, 0xc1ffff9e),
+   PROP(KBASE_GPUPROP_TLS_ALLOC, 0x800),
+   PROP(KBASE_GPUPROP_RAW_TILER_FEATURES, 0x809),
+};
+#undef PROP
+
+#define NOOP_COOKIE_ALLOC     0x41000
+#define NOOP_COOKIE_USER_IO   0x42000
+#define NOOP_COOKIE_MEM_ALLOC 0x43000
+
+static int
+kbase_ioctl(int fd, unsigned long request, ...)
+{
+   int ret = 0;
+
+   va_list args;
+
+   va_start(args, request);
+   void *ptr = va_arg(args, void *);
+   va_end(args);
+
+   switch (request) {
+   case KBASE_IOCTL_GET_GPUPROPS: {
+      struct kbase_ioctl_get_gpuprops *props = ptr;
+
+      if (props->size)
+         memcpy((void *)(uintptr_t)props->buffer, gpu_props,
+                MIN2(props->size, sizeof(gpu_props)));
+
+      ret = sizeof(gpu_props);
+      break;
+   }
+
+   case KBASE_IOCTL_MEM_ALLOC: {
+      union kbase_ioctl_mem_alloc *alloc = ptr;
+
+      alloc->out.gpu_va = NOOP_COOKIE_ALLOC;
+      alloc->out.flags = BASE_MEM_SAME_VA;
+      break;
+   }
+
+   case KBASE_IOCTL_CS_QUEUE_GROUP_CREATE_1_6: {
+      union kbase_ioctl_cs_queue_group_create_1_6 *create = ptr;
+
+      // TODO: Don't return duplicates?
+      create->out.group_handle = 0;
+      create->out.group_uid = 1;
+      break;
+   }
+
+   case KBASE_IOCTL_CS_TILER_HEAP_INIT: {
+      union kbase_ioctl_cs_tiler_heap_init *init = ptr;
+
+      /* The values don't really matter, the CPU has no business in accessing
+       * these. */
+      init->out.gpu_heap_va = 0x60000;
+      init->out.first_chunk_va = 0x61000;
+      break;
+   }
+
+   case KBASE_IOCTL_CS_QUEUE_BIND: {
+      union kbase_ioctl_cs_queue_bind *bind = ptr;
+      bind->out.mmap_handle = NOOP_COOKIE_USER_IO;
+      break;
+   }
+
+   case KBASE_IOCTL_MEM_IMPORT: {
+      union kbase_ioctl_mem_import *import = ptr;
+
+      if (import->in.type != BASE_MEM_IMPORT_TYPE_UMM) {
+         ret = -1;
+         errno = EINVAL;
+         break;
+      }
+
+      int *fd = (int *)(uintptr_t)import->in.phandle;
+
+      off_t size = lseek(*fd, 0, SEEK_END);
+
+      import->out.flags = BASE_MEM_NEED_MMAP;
+      import->out.gpu_va = NOOP_COOKIE_MEM_ALLOC;
+      import->out.va_pages = DIV_ROUND_UP(size, 4096);
+   }
+
+   case KBASE_IOCTL_SET_FLAGS:
+   case KBASE_IOCTL_MEM_EXEC_INIT:
+   case KBASE_IOCTL_MEM_JIT_INIT:
+   case KBASE_IOCTL_CS_QUEUE_REGISTER:
+   case KBASE_IOCTL_CS_QUEUE_KICK:
+   case KBASE_IOCTL_CS_TILER_HEAP_TERM:
+   case KBASE_IOCTL_CS_QUEUE_GROUP_TERMINATE:
+   case KBASE_IOCTL_MEM_SYNC:
+      break;
+
+   default:
+      ret = -1;
+      errno = ENOSYS;
+   }
+
+   return ret;
+}
+
+static void *
+kbase_mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset)
+{
+   switch (offset) {
+   case BASE_MEM_MAP_TRACKING_HANDLE:
+   case BASEP_MEM_CSF_USER_REG_PAGE_HANDLE:
+   case NOOP_COOKIE_ALLOC:
+   case NOOP_COOKIE_USER_IO:
+   case NOOP_COOKIE_MEM_ALLOC:
+      return mmap(NULL, length, prot, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
+
+   default:
+      errno = ENOSYS;
+      return MAP_FAILED;
+   }
+}
+#endif
diff -urN mesa-23.0.0/src/panfrost/base/pan_cache.h mesa/src/panfrost/base/pan_cache.h
--- mesa-23.0.0/src/panfrost/base/pan_cache.h	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/base/pan_cache.h	2023-03-06 19:19:32.719308911 +0100
@@ -0,0 +1,97 @@
+/*
+ * Copyright (C) 2022 Icecream95 <ixn@disroot.org>
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef PAN_CACHE_H
+#define PAN_CACHE_H
+
+#ifdef __aarch64__
+
+static void
+cache_clean(volatile void *addr)
+{
+   __asm__ volatile("dc cvac, %0" ::"r"(addr) : "memory");
+}
+
+static void
+cache_invalidate(volatile void *addr)
+{
+   __asm__ volatile("dc civac, %0" ::"r"(addr) : "memory");
+}
+
+typedef void (*cacheline_op)(volatile void *addr);
+
+#define CACHELINE_SIZE 64
+
+static void
+cacheline_op_range(volatile void *start, size_t length, cacheline_op op)
+{
+   volatile void *ptr =
+      (volatile void *)((uintptr_t)start & ~((uintptr_t)CACHELINE_SIZE - 1));
+   volatile void *end =
+      (volatile void *)ALIGN_POT((uintptr_t)start + length, CACHELINE_SIZE);
+   for (; ptr < end; ptr += CACHELINE_SIZE)
+      op(ptr);
+}
+
+static void
+cache_clean_range(volatile void *start, size_t length)
+{
+   /* TODO: Do an invalidate at the start of the range? */
+   cacheline_op_range(start, length, cache_clean);
+}
+
+static void
+cache_invalidate_range(volatile void *start, size_t length)
+{
+   cacheline_op_range(start, length, cache_invalidate);
+}
+
+#endif /* __aarch64__ */
+
+/* The #ifdef covers both 32-bit and 64-bit ARM */
+#ifdef __ARM_ARCH
+static void
+cache_barrier(void)
+{
+   __asm__ volatile("dsb sy" ::: "memory");
+}
+
+static void
+memory_barrier(void)
+{
+   __asm__ volatile("dmb sy" ::: "memory");
+}
+#else
+
+/* TODO: How to do cache barriers when emulated? */
+static void
+cache_barrier(void)
+{
+}
+
+static void
+memory_barrier(void)
+{
+}
+#endif
+#endif
diff -urN mesa-23.0.0/src/panfrost/base/pan_vX_base.c mesa/src/panfrost/base/pan_vX_base.c
--- mesa-23.0.0/src/panfrost/base/pan_vX_base.c	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/base/pan_vX_base.c	2023-03-06 19:19:32.760309181 +0100
@@ -0,0 +1,1805 @@
+/*
+ * Copyright (C) 2022 Icecream95
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <errno.h>
+#include <fcntl.h>
+#include <inttypes.h>
+#include <poll.h>
+#include <pthread.h>
+#include <stdarg.h>
+#include <stdbool.h>
+#include <stddef.h>
+#include <stdint.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <unistd.h>
+#include <sys/ioctl.h>
+#include <sys/mman.h>
+
+#ifdef HAVE_VALGRIND
+#include <valgrind.h>
+#else
+#define RUNNING_ON_VALGRIND 0
+#endif
+
+#include "util/list.h"
+#include "util/macros.h"
+#include "util/os_file.h"
+#include "util/u_atomic.h"
+
+#include "pan_base.h"
+#include "pan_cache.h"
+
+#include "drm-uapi/panfrost_drm.h"
+
+#define PAN_BASE_API (PAN_BASE_VER & 0xff)
+#if (PAN_BASE_VER & 0x100) == 0x100
+#define PAN_BASE_NOOP
+#endif
+
+#if PAN_BASE_API >= 2
+#include "csf/mali_gpu_csf_registers.h"
+
+#define MALI_USE_CSF 1
+#endif
+
+#include "mali_kbase_gpuprops.h"
+
+#ifndef PAN_BASE_NOOP
+#define kbase_mmap mmap
+#endif
+
+#if PAN_BASE_API >= 1
+#include "mali_base_kernel.h"
+#include "mali_kbase_ioctl.h"
+
+#ifdef PAN_BASE_NOOP
+#include "pan_base_noop.h"
+#else
+#define kbase_ioctl ioctl
+#endif
+#else
+
+#include "old/mali-ioctl-midgard.h"
+#include "old/mali-ioctl.h"
+#include "old/mali-props.h"
+#endif
+
+#define LOG(fmt, ...)                                                          \
+   do {                                                                        \
+      if (k->verbose) {                                                        \
+         struct timespec tp;                                                   \
+         clock_gettime(CLOCK_MONOTONIC_RAW, &tp);                              \
+         printf("%" PRIu64 ".%09li\t" fmt, (uint64_t)tp.tv_sec,                \
+                tp.tv_nsec __VA_OPT__(, ) __VA_ARGS__);                        \
+      }                                                                        \
+   } while (0)
+
+#if PAN_BASE_API == 0
+static int
+kbase_ioctl(int fd, unsigned long request, ...)
+{
+   int ioc_size = _IOC_SIZE(request);
+
+   assert(ioc_size);
+
+   va_list args;
+
+   va_start(args, request);
+   int *ptr = va_arg(args, void *);
+   va_end(args);
+
+   *ptr = (_IOC_TYPE(request) - 0x80) * 256 + _IOC_NR(request);
+
+   int ret = ioctl(fd, request, ptr);
+   if (ret)
+      return ret;
+
+   int r = *ptr;
+   switch (r) {
+   case MALI_ERROR_OUT_OF_GPU_MEMORY:
+      errno = ENOSPC;
+      return -1;
+   case MALI_ERROR_OUT_OF_MEMORY:
+      errno = ENOMEM;
+      return -1;
+   case MALI_ERROR_FUNCTION_FAILED:
+      errno = EINVAL;
+      return -1;
+   default:
+      return 0;
+   }
+}
+#endif
+
+#if PAN_BASE_API >= 1
+static bool
+kbase_get_mali_gpuprop(kbase k, unsigned name, uint64_t *value)
+{
+   int i = 0;
+   uint64_t x = 0;
+   while (i < k->gpuprops_size) {
+      x = 0;
+      memcpy(&x, k->gpuprops + i, 4);
+      i += 4;
+
+      int size = 1 << (x & 3);
+      int this_name = x >> 2;
+
+      x = 0;
+      memcpy(&x, k->gpuprops + i, size);
+      i += size;
+
+      if (this_name == name) {
+         *value = x;
+         return true;
+      }
+   }
+
+   return false;
+}
+#else
+static bool
+kbase_get_mali_gpuprop(kbase k, unsigned name, uint64_t *value)
+{
+   struct kbase_ioctl_gpu_props_reg_dump *props = k->gpuprops;
+
+   switch (name) {
+   case KBASE_GPUPROP_PRODUCT_ID:
+      *value = props->core.product_id;
+      return true;
+   case KBASE_GPUPROP_RAW_SHADER_PRESENT:
+      *value = props->raw.shader_present;
+      return true;
+   case KBASE_GPUPROP_RAW_TEXTURE_FEATURES_0:
+      *value = props->raw.texture_features[0];
+      return true;
+   case KBASE_GPUPROP_RAW_TILER_FEATURES:
+      *value = props->raw.tiler_features;
+      return true;
+   case KBASE_GPUPROP_RAW_GPU_ID:
+      *value = props->raw.gpu_id;
+      return true;
+   default:
+      return false;
+   }
+}
+#endif
+
+static bool
+alloc_handles(kbase k)
+{
+   util_dynarray_init(&k->gem_handles, NULL);
+   return true;
+}
+
+static bool
+free_handles(kbase k)
+{
+   util_dynarray_fini(&k->gem_handles);
+   return true;
+}
+
+static bool
+set_flags(kbase k)
+{
+   struct kbase_ioctl_set_flags flags = {.create_flags = 0};
+
+   int ret = kbase_ioctl(k->fd, KBASE_IOCTL_SET_FLAGS, &flags);
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_SET_FLAGS)");
+      return false;
+   }
+   return true;
+}
+
+static bool
+mmap_tracking(kbase k)
+{
+   k->tracking_region = kbase_mmap(NULL, k->page_size, PROT_NONE, MAP_SHARED,
+                                   k->fd, BASE_MEM_MAP_TRACKING_HANDLE);
+
+   if (k->tracking_region == MAP_FAILED) {
+      perror("mmap(BASE_MEM_MAP_TRACKING_HANDLE)");
+      k->tracking_region = NULL;
+      return false;
+   }
+   return true;
+}
+
+static bool
+munmap_tracking(kbase k)
+{
+   if (k->tracking_region)
+      return munmap(k->tracking_region, k->page_size) == 0;
+   return true;
+}
+
+#if PAN_BASE_API >= 1
+static bool
+get_gpuprops(kbase k)
+{
+   struct kbase_ioctl_get_gpuprops props = {0};
+
+   int ret = kbase_ioctl(k->fd, KBASE_IOCTL_GET_GPUPROPS, &props);
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_GET_GPUPROPS(0))");
+      return false;
+   } else if (!ret) {
+      fprintf(stderr, "GET_GPUPROPS returned zero size\n");
+      return false;
+   }
+
+   k->gpuprops_size = ret;
+   k->gpuprops = calloc(k->gpuprops_size, 1);
+
+   props.size = k->gpuprops_size;
+   props.buffer = (uint64_t)(uintptr_t)k->gpuprops;
+
+   ret = kbase_ioctl(k->fd, KBASE_IOCTL_GET_GPUPROPS, &props);
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_GET_GPUPROPS(size))");
+      return false;
+   }
+
+   return true;
+}
+#else
+static bool
+get_gpuprops(kbase k)
+{
+   k->gpuprops = calloc(1, sizeof(struct kbase_ioctl_gpu_props_reg_dump));
+
+   int ret = kbase_ioctl(k->fd, KBASE_IOCTL_GPU_PROPS_REG_DUMP, k->gpuprops);
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_GPU_PROPS_REG_DUMP)");
+      return false;
+   }
+
+   return true;
+}
+#endif
+
+static bool
+free_gpuprops(kbase k)
+{
+   free(k->gpuprops);
+   return true;
+}
+
+#if PAN_BASE_API >= 2
+static bool
+mmap_user_reg(kbase k)
+{
+   k->csf_user_reg = kbase_mmap(NULL, k->page_size, PROT_READ, MAP_SHARED,
+                                k->fd, BASEP_MEM_CSF_USER_REG_PAGE_HANDLE);
+
+   if (k->csf_user_reg == MAP_FAILED) {
+      perror("mmap(BASEP_MEM_CSF_USER_REG_PAGE_HANDLE)");
+      k->csf_user_reg = NULL;
+      return false;
+   }
+   return true;
+}
+
+static bool
+munmap_user_reg(kbase k)
+{
+   if (k->csf_user_reg)
+      return munmap(k->csf_user_reg, k->page_size) == 0;
+   return true;
+}
+#endif
+
+#if PAN_BASE_API >= 1
+static bool
+init_mem_exec(kbase k)
+{
+   struct kbase_ioctl_mem_exec_init init = {
+      .va_pages = 0x100000,
+   };
+
+   int ret = kbase_ioctl(k->fd, KBASE_IOCTL_MEM_EXEC_INIT, &init);
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_MEM_EXEC_INIT)");
+      return false;
+   }
+   return true;
+}
+
+static bool
+init_mem_jit(kbase k)
+{
+   struct kbase_ioctl_mem_jit_init init = {
+      .va_pages = 1 << 25,
+      .max_allocations = 255,
+      .phys_pages = 1 << 25,
+   };
+
+   int ret = kbase_ioctl(k->fd, KBASE_IOCTL_MEM_JIT_INIT, &init);
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_MEM_JIT_INIT)");
+      return false;
+   }
+   return true;
+}
+#endif
+
+#if PAN_BASE_API >= 2
+static struct base_ptr kbase_alloc(kbase k, size_t size, unsigned pan_flags,
+                                   unsigned mali_flags);
+
+static bool
+alloc_event_mem(kbase k)
+{
+   k->event_mem = kbase_alloc(k, k->page_size * 2, PANFROST_BO_NOEXEC,
+                              BASE_MEM_PROT_CPU_RD | BASE_MEM_PROT_CPU_WR |
+                                 BASE_MEM_PROT_GPU_RD | BASE_MEM_PROT_GPU_WR |
+                                 BASE_MEM_SAME_VA | BASE_MEM_CSF_EVENT);
+   k->kcpu_event_mem = (struct base_ptr){
+      .cpu = k->event_mem.cpu + k->page_size,
+      .gpu = k->event_mem.gpu + k->page_size,
+   };
+   return k->event_mem.cpu;
+}
+
+static bool
+free_event_mem(kbase k)
+{
+   if (k->event_mem.cpu)
+      return munmap(k->event_mem.cpu, k->page_size * 2) == 0;
+   return true;
+}
+#endif
+
+#if PAN_BASE_API >= 2
+static bool
+cs_group_create(kbase k, struct kbase_context *c)
+{
+   /* TODO: What about compute-only contexts? */
+   union kbase_ioctl_cs_queue_group_create_1_6 create = {
+      .in = {
+         /* Mali *still* only supports a single tiler unit */
+         .tiler_mask = 1,
+         .fragment_mask = ~0ULL,
+         .compute_mask = ~0ULL,
+
+         .cs_min = k->cs_queue_count,
+
+         .priority = 1,
+         .tiler_max = 1,
+         .fragment_max = 64,
+         .compute_max = 64,
+      }};
+
+   int ret = kbase_ioctl(k->fd, KBASE_IOCTL_CS_QUEUE_GROUP_CREATE_1_6, &create);
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_CS_QUEUE_GROUP_CREATE_1_6)");
+      return false;
+   }
+
+   c->csg_handle = create.out.group_handle;
+   c->csg_uid = create.out.group_uid;
+
+   /* Should be at least 1 */
+   assert(c->csg_uid);
+
+   return true;
+}
+
+static bool
+cs_group_term(kbase k, struct kbase_context *c)
+{
+   if (!c->csg_uid)
+      return true;
+
+   struct kbase_ioctl_cs_queue_group_term term = {.group_handle =
+                                                     c->csg_handle};
+
+   int ret = kbase_ioctl(k->fd, KBASE_IOCTL_CS_QUEUE_GROUP_TERMINATE, &term);
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_CS_QUEUE_GROUP_TERMINATE)");
+      return false;
+   }
+   return true;
+}
+#endif
+
+#if PAN_BASE_API >= 2
+static bool
+tiler_heap_create(kbase k, struct kbase_context *c)
+{
+   c->tiler_heap_chunk_size = 1 << 21; /* 2 MB */
+
+   union kbase_ioctl_cs_tiler_heap_init init = {
+      .in = {
+         .chunk_size = c->tiler_heap_chunk_size,
+         .initial_chunks = 5,
+         .max_chunks = 200,
+         .target_in_flight = 65535,
+      }};
+
+   int ret = kbase_ioctl(k->fd, KBASE_IOCTL_CS_TILER_HEAP_INIT, &init);
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_CS_TILER_HEAP_INIT)");
+      return false;
+   }
+
+   c->tiler_heap_va = init.out.gpu_heap_va;
+   c->tiler_heap_header = init.out.first_chunk_va;
+
+   return true;
+}
+
+static bool
+tiler_heap_term(kbase k, struct kbase_context *c)
+{
+   if (!c->tiler_heap_va)
+      return true;
+
+   struct kbase_ioctl_cs_tiler_heap_term term = {.gpu_heap_va =
+                                                    c->tiler_heap_va};
+
+   int ret = kbase_ioctl(k->fd, KBASE_IOCTL_CS_TILER_HEAP_TERM, &term);
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_CS_TILER_HEAP_TERM)");
+      return false;
+   }
+   return true;
+}
+#endif
+
+typedef bool (*kbase_func)(kbase k);
+
+struct kbase_op {
+   kbase_func part;
+   kbase_func cleanup;
+   const char *label;
+};
+
+static struct kbase_op kbase_main[] = {
+   {alloc_handles, free_handles, "Allocate handle array"},
+#if PAN_BASE_API >= 1
+   {set_flags, NULL, "Set flags"},
+#endif
+   {mmap_tracking, munmap_tracking, "Map tracking handle"},
+#if PAN_BASE_API == 0
+   {set_flags, NULL, "Set flags"},
+#endif
+   {get_gpuprops, free_gpuprops, "Get GPU properties"},
+#if PAN_BASE_API >= 2
+   {mmap_user_reg, munmap_user_reg, "Map user register page"},
+#endif
+#if PAN_BASE_API >= 1
+   {init_mem_exec, NULL, "Initialise EXEC_VA zone"},
+   {init_mem_jit, NULL, "Initialise JIT allocator"},
+#endif
+#if PAN_BASE_API >= 2
+   {alloc_event_mem, free_event_mem, "Allocate event memory"},
+#endif
+};
+
+static void
+kbase_close(kbase k)
+{
+   while (k->setup_state) {
+      unsigned i = k->setup_state - 1;
+      if (kbase_main[i].cleanup)
+         kbase_main[i].cleanup(k);
+      --k->setup_state;
+   }
+
+   pthread_mutex_destroy(&k->handle_lock);
+   pthread_mutex_destroy(&k->event_read_lock);
+   pthread_mutex_destroy(&k->event_cnd_lock);
+   pthread_mutex_destroy(&k->queue_lock);
+   pthread_cond_destroy(&k->event_cnd);
+
+   close(k->fd);
+}
+
+static bool
+kbase_get_pan_gpuprop(kbase k, unsigned name, uint64_t *value)
+{
+   unsigned conv[] = {
+      [DRM_PANFROST_PARAM_GPU_PROD_ID] = KBASE_GPUPROP_PRODUCT_ID,
+      [DRM_PANFROST_PARAM_SHADER_PRESENT] = KBASE_GPUPROP_RAW_SHADER_PRESENT,
+      [DRM_PANFROST_PARAM_TEXTURE_FEATURES0] =
+         KBASE_GPUPROP_RAW_TEXTURE_FEATURES_0,
+      [DRM_PANFROST_PARAM_THREAD_TLS_ALLOC] = KBASE_GPUPROP_TLS_ALLOC,
+      [DRM_PANFROST_PARAM_TILER_FEATURES] = KBASE_GPUPROP_RAW_TILER_FEATURES,
+   };
+
+   if (name < ARRAY_SIZE(conv) && conv[name])
+      return kbase_get_mali_gpuprop(k, conv[name], value);
+
+   switch (name) {
+   case DRM_PANFROST_PARAM_AFBC_FEATURES:
+      *value = 0;
+      return true;
+   case DRM_PANFROST_PARAM_GPU_REVISION: {
+      if (!kbase_get_mali_gpuprop(k, KBASE_GPUPROP_RAW_GPU_ID, value))
+         return false;
+      *value &= 0xffff;
+      return true;
+   }
+   default:
+      return false;
+   }
+}
+
+static void
+kbase_free(kbase k, base_va va)
+{
+   struct kbase_ioctl_mem_free f = {.gpu_addr = va};
+
+   int ret = kbase_ioctl(k->fd, KBASE_IOCTL_MEM_FREE, &f);
+
+   if (ret == -1)
+      perror("ioctl(KBASE_IOCTL_MEM_FREE)");
+}
+
+static struct base_ptr
+kbase_alloc(kbase k, size_t size, unsigned pan_flags, unsigned mali_flags)
+{
+   struct base_ptr r = {0};
+
+   unsigned pages = DIV_ROUND_UP(size, k->page_size);
+
+   union kbase_ioctl_mem_alloc a = {.in = {
+                                       .va_pages = pages,
+                                       .commit_pages = pages,
+                                    }};
+
+   size_t alloc_size = size;
+   unsigned flags = mali_flags;
+   bool exec_align = false;
+
+   if (!flags) {
+      flags = BASE_MEM_PROT_CPU_RD | BASE_MEM_PROT_CPU_WR |
+              BASE_MEM_PROT_GPU_RD | BASE_MEM_PROT_GPU_WR | BASE_MEM_SAME_VA;
+
+      /* Add COHERENT_LOCAL to keep GPU cores coherent with each
+       * other. */
+      if (PAN_BASE_API >= 1)
+         flags |= BASE_MEM_COHERENT_LOCAL;
+   }
+
+   if (pan_flags & PANFROST_BO_HEAP) {
+      size_t align_size = 2 * 1024 * 1024 / k->page_size; /* 2 MB */
+
+      a.in.va_pages = ALIGN_POT(a.in.va_pages, align_size);
+      a.in.commit_pages = 0;
+      a.in.extension = align_size;
+      flags |= BASE_MEM_GROW_ON_GPF;
+   }
+
+#if PAN_BASE_API >= 1
+   if (pan_flags & MALI_BO_CACHED_CPU)
+      flags |= BASE_MEM_CACHED_CPU;
+#endif
+
+#if PAN_BASE_API >= 2
+   if (pan_flags & MALI_BO_UNCACHED_GPU)
+      flags |= BASE_MEM_UNCACHED_GPU;
+#endif
+
+   if (!(pan_flags & PANFROST_BO_NOEXEC)) {
+      /* Using SAME_VA for executable BOs would make it too likely
+       * for a blend shader to end up on the wrong side of a 4 GB
+       * boundary. */
+      flags |= BASE_MEM_PROT_GPU_EX;
+      flags &= ~(BASE_MEM_PROT_GPU_WR | BASE_MEM_SAME_VA);
+
+      if (PAN_BASE_API == 0) {
+         /* Assume 4K pages */
+         a.in.va_pages = 0x1000; /* Align shader BOs to 16 MB */
+         size = 1 << 26;         /* Four times the alignment */
+         exec_align = true;
+      }
+   }
+
+   a.in.flags = flags;
+
+   int ret = kbase_ioctl(k->fd, KBASE_IOCTL_MEM_ALLOC, &a);
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_MEM_ALLOC)");
+      return r;
+   }
+
+   // TODO: Is this always true, even in the face of multithreading?
+   if (PAN_BASE_API == 0)
+      a.out.gpu_va = 0x41000;
+
+   if ((flags & BASE_MEM_SAME_VA) &&
+       !((a.out.flags & BASE_MEM_SAME_VA) && a.out.gpu_va < 0x80000)) {
+
+      fprintf(stderr, "Flags: 0x%" PRIx64 ", VA: 0x%" PRIx64 "\n",
+              (uint64_t)a.out.flags, (uint64_t)a.out.gpu_va);
+      errno = EINVAL;
+      return r;
+   }
+
+   void *ptr = kbase_mmap(NULL, size, PROT_READ | PROT_WRITE, MAP_SHARED, k->fd,
+                          a.out.gpu_va);
+
+   if (ptr == MAP_FAILED) {
+      perror("mmap(GPU BO)");
+      kbase_free(k, a.out.gpu_va);
+      return r;
+   }
+
+   uint64_t gpu_va =
+      (a.out.flags & BASE_MEM_SAME_VA) ? (uintptr_t)ptr : a.out.gpu_va;
+
+   if (exec_align) {
+      gpu_va = ALIGN_POT(gpu_va, 1 << 24);
+
+      ptr = kbase_mmap(NULL, alloc_size, PROT_READ | PROT_WRITE, MAP_SHARED,
+                       k->fd, gpu_va);
+
+      if (ptr == MAP_FAILED) {
+         perror("mmap(GPU EXEC BO)");
+         kbase_free(k, gpu_va);
+         return r;
+      }
+   }
+
+   r.cpu = ptr;
+   r.gpu = gpu_va;
+
+   return r;
+}
+
+static int
+kbase_import_dmabuf(kbase k, int fd)
+{
+   int ret;
+
+   pthread_mutex_lock(&k->handle_lock);
+
+   unsigned size = util_dynarray_num_elements(&k->gem_handles, kbase_handle);
+
+   kbase_handle *handles = util_dynarray_begin(&k->gem_handles);
+
+   for (unsigned i = 0; i < size; ++i) {
+      kbase_handle h = handles[i];
+
+      if (h.fd < 0)
+         continue;
+
+      ret = os_same_file_description(h.fd, fd);
+
+      if (ret == 0) {
+         pthread_mutex_unlock(&k->handle_lock);
+         return i;
+      } else if (ret < 0) {
+         printf("error in os_same_file_description(%i, %i)\n", h.fd, fd);
+      }
+   }
+
+   int dup = os_dupfd_cloexec(fd);
+
+   union kbase_ioctl_mem_import import = {
+      .in = {
+         .phandle = (uintptr_t)&dup,
+         .type = BASE_MEM_IMPORT_TYPE_UMM,
+         /* Usage flags: CPU/GPU reads/writes */
+         .flags = 0xf,
+      }};
+
+   ret = kbase_ioctl(k->fd, KBASE_IOCTL_MEM_IMPORT, &import);
+
+   int handle;
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_MEM_IMPORT)");
+      handle = -1;
+   } else if (import.out.flags & BASE_MEM_NEED_MMAP) {
+      uint64_t va = (uintptr_t)kbase_mmap(
+         NULL, import.out.va_pages * k->page_size, PROT_READ | PROT_WRITE,
+         MAP_SHARED, k->fd, import.out.gpu_va);
+
+      if (va == (uintptr_t)MAP_FAILED) {
+         perror("mmap(IMPORTED BO)");
+         handle = -1;
+      } else {
+         handle = kbase_alloc_gem_handle_locked(k, va, dup);
+      }
+   } else {
+      handle = kbase_alloc_gem_handle_locked(k, import.out.gpu_va, dup);
+   }
+
+   pthread_mutex_unlock(&k->handle_lock);
+
+   return handle;
+}
+
+static void *
+kbase_mmap_import(kbase k, base_va va, size_t size)
+{
+   return kbase_mmap(NULL, size, PROT_READ | PROT_WRITE, MAP_SHARED, k->fd, va);
+}
+
+struct kbase_fence {
+   struct list_head link;
+
+   unsigned slot;
+   uint64_t value;
+};
+
+struct kbase_syncobj {
+   struct list_head link;
+
+   struct list_head fences;
+};
+
+static struct kbase_syncobj *
+kbase_syncobj_create(kbase k)
+{
+   struct kbase_syncobj *o = calloc(1, sizeof(*o));
+   list_inithead(&o->fences);
+   pthread_mutex_lock(&k->queue_lock);
+   list_add(&o->link, &k->syncobjs);
+   pthread_mutex_unlock(&k->queue_lock);
+   return o;
+}
+
+static void
+kbase_syncobj_destroy(kbase k, struct kbase_syncobj *o)
+{
+   pthread_mutex_lock(&k->queue_lock);
+   list_del(&o->link);
+   pthread_mutex_unlock(&k->queue_lock);
+
+   list_for_each_entry_safe(struct kbase_fence, fence, &o->fences, link) {
+      list_del(&fence->link);
+      free(fence);
+   }
+
+   free(o);
+}
+
+static void
+kbase_syncobj_add_fence(struct kbase_syncobj *o, unsigned slot, uint64_t value)
+{
+   struct kbase_fence *fence = calloc(1, sizeof(*fence));
+
+   fence->slot = slot;
+   fence->value = value;
+
+   list_add(&fence->link, &o->fences);
+}
+
+static void
+kbase_syncobj_update_fence(struct kbase_syncobj *o, unsigned slot,
+                           uint64_t value)
+{
+   list_for_each_entry(struct kbase_fence, fence, &o->fences, link) {
+      if (fence->slot == slot) {
+         if (value > fence->value)
+            fence->value = value;
+
+         return;
+      }
+   }
+
+   kbase_syncobj_add_fence(o, slot, value);
+}
+
+static struct kbase_syncobj *
+kbase_syncobj_dup(kbase k, struct kbase_syncobj *o)
+{
+   struct kbase_syncobj *dup = kbase_syncobj_create(k);
+
+   pthread_mutex_lock(&k->queue_lock);
+
+   list_for_each_entry(struct kbase_fence, fence, &o->fences, link)
+      kbase_syncobj_add_fence(dup, fence->slot, fence->value);
+
+   pthread_mutex_unlock(&k->queue_lock);
+
+   return dup;
+}
+
+static void
+kbase_syncobj_update(kbase k, struct kbase_syncobj *o)
+{
+   list_for_each_entry_safe(struct kbase_fence, fence, &o->fences, link) {
+      uint64_t value = k->event_slots[fence->slot].last;
+
+      if (value > fence->value) {
+         LOG("syncobj %p slot %u value %" PRIu64 " vs %" PRIu64 "\n", o,
+             fence->slot, fence->value, value);
+
+         list_del(&fence->link);
+         free(fence);
+      }
+   }
+}
+
+static bool
+kbase_syncobj_wait(kbase k, struct kbase_syncobj *o)
+{
+   if (list_is_empty(&o->fences)) {
+      LOG("syncobj has no fences\n");
+      return true;
+   }
+
+   struct kbase_wait_ctx wait = kbase_wait_init(k, 1 * 1000000000LL);
+
+   while (kbase_wait_for_event(&wait)) {
+      kbase_syncobj_update(k, o);
+
+      if (list_is_empty(&o->fences)) {
+         kbase_wait_fini(wait);
+         return true;
+      }
+   }
+
+   kbase_wait_fini(wait);
+
+   fprintf(stderr, "syncobj %p wait timeout\n", o);
+   return false;
+}
+
+static bool
+kbase_poll_event(kbase k, int64_t timeout_ns)
+{
+   struct pollfd pfd = {
+      .fd = k->fd,
+      .events = POLLIN,
+   };
+
+   struct timespec t = {
+      .tv_sec = timeout_ns / 1000000000,
+      .tv_nsec = timeout_ns % 1000000000,
+   };
+
+   int ret = ppoll(&pfd, 1, &t, NULL);
+
+   if (ret == -1 && errno != EINTR)
+      perror("poll(mali fd)");
+
+   LOG("poll returned %i\n", pfd.revents);
+
+   return ret != 0;
+}
+
+#if PAN_BASE_API < 2
+static bool
+kbase_handle_events(kbase k)
+{
+   struct base_jd_event_v2 event;
+   bool ret = true;
+
+   for (;;) {
+      int ret = read(k->fd, &event, sizeof(event));
+
+      if (ret == -1) {
+         if (errno == EAGAIN) {
+            return true;
+         } else {
+            perror("read(mali fd)");
+            return false;
+         }
+      }
+
+      if (event.event_code != BASE_JD_EVENT_DONE) {
+         fprintf(stderr, "Atom %i reported event 0x%x!\n", event.atom_number,
+                 event.event_code);
+         ret = false;
+      }
+
+      pthread_mutex_lock(&k->handle_lock);
+
+      k->event_slots[event.atom_number].last = event.udata.blob[0];
+
+      unsigned size = util_dynarray_num_elements(&k->gem_handles, kbase_handle);
+      kbase_handle *handle_data = util_dynarray_begin(&k->gem_handles);
+
+      struct util_dynarray *handles = k->atom_bos + event.atom_number;
+
+      util_dynarray_foreach(handles, int32_t, h) {
+         if (*h >= size)
+            continue;
+         assert(handle_data[*h].use_count);
+         --handle_data[*h].use_count;
+      }
+      util_dynarray_fini(handles);
+
+      pthread_mutex_unlock(&k->handle_lock);
+   }
+
+   return ret;
+}
+
+#else
+
+static bool
+kbase_read_event(kbase k)
+{
+   struct base_csf_notification event;
+   int ret = read(k->fd, &event, sizeof(event));
+
+   if (ret == -1) {
+      if (errno == EAGAIN) {
+         return true;
+      } else {
+         perror("read(mali_fd)");
+         return false;
+      }
+   }
+
+   if (ret != sizeof(event)) {
+      fprintf(stderr, "read(mali_fd) returned %i, expected %i!\n", ret,
+              (int)sizeof(event));
+      return false;
+   }
+
+   switch (event.type) {
+   case BASE_CSF_NOTIFICATION_EVENT:
+      LOG("Notification event!\n");
+      return true;
+
+   case BASE_CSF_NOTIFICATION_GPU_QUEUE_GROUP_ERROR:
+      break;
+
+   case BASE_CSF_NOTIFICATION_CPU_QUEUE_DUMP:
+      fprintf(stderr, "No event from mali_fd!\n");
+      return true;
+
+   default:
+      fprintf(stderr, "Unknown event type!\n");
+      return true;
+   }
+
+   struct base_gpu_queue_group_error e = event.payload.csg_error.error;
+
+   switch (e.error_type) {
+   case BASE_GPU_QUEUE_GROUP_ERROR_FATAL: {
+      // See CS_FATAL_EXCEPTION_* in mali_gpu_csf_registers.h
+      fprintf(stderr,
+              "Queue group error: status 0x%x "
+              "sideband 0x%" PRIx64 "\n",
+              e.payload.fatal_group.status,
+              (uint64_t)e.payload.fatal_group.sideband);
+      break;
+   }
+   case BASE_GPU_QUEUE_GROUP_QUEUE_ERROR_FATAL: {
+      unsigned queue = e.payload.fatal_queue.csi_index;
+
+      // See CS_FATAL_EXCEPTION_* in mali_gpu_csf_registers.h
+      fprintf(stderr,
+              "Queue %i error: status 0x%x "
+              "sideband 0x%" PRIx64 "\n",
+              queue, e.payload.fatal_queue.status,
+              (uint64_t)e.payload.fatal_queue.sideband);
+
+      /* TODO: Decode the instruct that it got stuck at */
+
+      break;
+   }
+
+   case BASE_GPU_QUEUE_GROUP_ERROR_TIMEOUT:
+      fprintf(stderr, "Command stream timeout!\n");
+      break;
+   case BASE_GPU_QUEUE_GROUP_ERROR_TILER_HEAP_OOM:
+      fprintf(stderr, "Command stream OOM!\n");
+      break;
+   default:
+      fprintf(stderr, "Unknown error type!\n");
+   }
+
+   return false;
+}
+
+static void
+kbase_update_queue_callbacks(kbase k, struct kbase_event_slot *slot,
+                             uint64_t seqnum)
+{
+   struct kbase_sync_link **list = &slot->syncobjs;
+   struct kbase_sync_link **back = slot->back;
+
+   while (*list) {
+      struct kbase_sync_link *link = *list;
+
+      LOG("seq %" PRIu64 " %" PRIu64 "\n", seqnum, link->seqnum);
+
+      /* Items in the list should be in order, there is no need to
+       * check any more if we can't process this link yet. */
+      if (seqnum <= link->seqnum)
+         break;
+
+      LOG("done, calling %p(%p)\n", link->callback, link->data);
+      link->callback(link->data);
+      *list = link->next;
+      if (&link->next == back)
+         slot->back = list;
+      free(link);
+   }
+}
+
+static bool
+kbase_handle_events(kbase k)
+{
+#ifdef PAN_BASE_NOOP
+   return true;
+#endif
+
+   /* This will clear the event count, so there's no need to do it in a
+    * loop. */
+   bool ret = kbase_read_event(k);
+
+   uint64_t *event_mem = k->event_mem.cpu;
+
+   pthread_mutex_lock(&k->queue_lock);
+
+   for (unsigned i = 0; i < k->event_slot_usage; ++i) {
+      uint64_t seqnum = event_mem[i * 2];
+      uint64_t cmp = k->event_slots[i].last;
+
+      LOG("MAIN SEQ %" PRIu64 " > %" PRIu64 "?\n", seqnum, cmp);
+
+      if (seqnum < cmp) {
+         if (false)
+            fprintf(stderr,
+                    "seqnum at offset %i went backward "
+                    "from %" PRIu64 " to %" PRIu64 "!\n",
+                    i, cmp, seqnum);
+      } else /*if (seqnum > cmp)*/ {
+         kbase_update_queue_callbacks(k, &k->event_slots[i], seqnum);
+      }
+
+      /* TODO: Atomic operations? */
+      k->event_slots[i].last = seqnum;
+   }
+
+   pthread_mutex_unlock(&k->queue_lock);
+
+   return ret;
+}
+
+#endif
+
+#if PAN_BASE_API < 2
+static uint8_t
+kbase_latest_slot(uint8_t a, uint8_t b, uint8_t newest)
+{
+   /* If a == 4 and newest == 5, a will become 255 */
+   a -= newest;
+   b -= newest;
+   a = MAX2(a, b);
+   a += newest;
+   return a;
+}
+
+static int
+kbase_submit(kbase k, uint64_t va, unsigned req, struct kbase_syncobj *o,
+             int32_t *handles, unsigned num_handles)
+{
+   struct util_dynarray buf;
+   util_dynarray_init(&buf, NULL);
+
+   memcpy(util_dynarray_resize(&buf, int32_t, num_handles), handles,
+          num_handles * sizeof(int32_t));
+
+   pthread_mutex_lock(&k->handle_lock);
+
+   unsigned slot = (req & PANFROST_JD_REQ_FS) ? 0 : 1;
+   unsigned dep_slots[KBASE_SLOT_COUNT];
+
+   uint8_t nr = k->atom_number++;
+
+   struct base_jd_atom_v2 atom = {
+      .jc = va,
+      .atom_number = nr,
+      .udata.blob[0] = k->job_seq++,
+   };
+
+   for (unsigned i = 0; i < KBASE_SLOT_COUNT; ++i)
+      dep_slots[i] = nr;
+
+   /* Make sure that we haven't taken an atom that's already in use. */
+   assert(!k->atom_bos[nr].data);
+   k->atom_bos[atom.atom_number] = buf;
+
+   unsigned handle_buf_size =
+      util_dynarray_num_elements(&k->gem_handles, kbase_handle);
+   kbase_handle *handle_buf = util_dynarray_begin(&k->gem_handles);
+
+   struct util_dynarray extres;
+   util_dynarray_init(&extres, NULL);
+
+   /* Mark the BOs as in use */
+   for (unsigned i = 0; i < num_handles; ++i) {
+      int32_t h = handles[i];
+      assert(h < handle_buf_size);
+      assert(handle_buf[h].use_count < 255);
+
+      /* Implicit sync */
+      if (handle_buf[h].use_count)
+         for (unsigned s = 0; s < KBASE_SLOT_COUNT; ++s)
+            dep_slots[s] = kbase_latest_slot(dep_slots[s],
+                                             handle_buf[h].last_access[s], nr);
+
+      handle_buf[h].last_access[slot] = nr;
+      ++handle_buf[h].use_count;
+
+      if (handle_buf[h].fd != -1)
+         util_dynarray_append(&extres, base_va, handle_buf[h].va);
+   }
+
+   pthread_mutex_unlock(&k->handle_lock);
+
+   /* TODO: Better work out the difference between handle_lock and
+    * queue_lock. */
+   if (o) {
+      pthread_mutex_lock(&k->queue_lock);
+      kbase_syncobj_update_fence(o, nr, atom.udata.blob[0]);
+      pthread_mutex_unlock(&k->queue_lock);
+   }
+
+   assert(KBASE_SLOT_COUNT == 2);
+   if (dep_slots[0] != nr) {
+      atom.pre_dep[0].atom_id = dep_slots[0];
+      /* TODO: Use data dependencies?  */
+      atom.pre_dep[0].dependency_type = BASE_JD_DEP_TYPE_ORDER;
+   }
+   if (dep_slots[1] != nr) {
+      atom.pre_dep[1].atom_id = dep_slots[1];
+      atom.pre_dep[1].dependency_type = BASE_JD_DEP_TYPE_ORDER;
+   }
+
+   if (extres.size) {
+      atom.core_req |= BASE_JD_REQ_EXTERNAL_RESOURCES;
+      atom.nr_extres = util_dynarray_num_elements(&extres, base_va);
+      atom.extres_list = (uintptr_t)util_dynarray_begin(&extres);
+   }
+
+   if (req & PANFROST_JD_REQ_FS)
+      atom.core_req |= BASE_JD_REQ_FS;
+   else
+      atom.core_req |= BASE_JD_REQ_CS | BASE_JD_REQ_T;
+
+   struct kbase_ioctl_job_submit submit = {
+      .nr_atoms = 1,
+      .stride = sizeof(atom),
+      .addr = (uintptr_t)&atom,
+   };
+
+   int ret = kbase_ioctl(k->fd, KBASE_IOCTL_JOB_SUBMIT, &submit);
+
+   util_dynarray_fini(&extres);
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_JOB_SUBMIT)");
+      return -1;
+   }
+
+   return atom.atom_number;
+}
+
+#else
+static struct kbase_context *
+kbase_context_create(kbase k)
+{
+   struct kbase_context *c = calloc(1, sizeof(*c));
+
+   if (!cs_group_create(k, c)) {
+      free(c);
+      return NULL;
+   }
+
+   if (!tiler_heap_create(k, c)) {
+      cs_group_term(k, c);
+      free(c);
+      return NULL;
+   }
+
+   return c;
+}
+
+static void kbase_kcpu_queue_destroy(kbase k, struct kbase_context *ctx);
+
+static void
+kbase_context_destroy(kbase k, struct kbase_context *ctx)
+{
+   kbase_kcpu_queue_destroy(k, ctx);
+   tiler_heap_term(k, ctx);
+   cs_group_term(k, ctx);
+   free(ctx);
+}
+
+static bool
+kbase_context_recreate(kbase k, struct kbase_context *ctx)
+{
+   kbase_kcpu_queue_destroy(k, ctx);
+   tiler_heap_term(k, ctx);
+   cs_group_term(k, ctx);
+
+   if (!cs_group_create(k, ctx)) {
+      free(ctx);
+      return false;
+   }
+
+   if (!tiler_heap_create(k, ctx)) {
+      free(ctx);
+      return false;
+   }
+
+   return true;
+}
+
+static struct kbase_cs
+kbase_cs_bind_noevent(kbase k, struct kbase_context *ctx, base_va va,
+                      unsigned size, unsigned csi)
+{
+   struct kbase_cs cs = {
+      .ctx = ctx,
+      .va = va,
+      .size = size,
+      .csi = csi,
+      .latest_flush = (uint32_t *)k->csf_user_reg,
+   };
+
+   struct kbase_ioctl_cs_queue_register reg = {
+      .buffer_gpu_addr = va,
+      .buffer_size = size,
+      .priority = 1,
+   };
+
+   int ret = kbase_ioctl(k->fd, KBASE_IOCTL_CS_QUEUE_REGISTER, &reg);
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_CS_QUEUE_REGISTER)");
+      return cs;
+   }
+
+   union kbase_ioctl_cs_queue_bind bind = {.in = {
+                                              .buffer_gpu_addr = va,
+                                              .group_handle = ctx->csg_handle,
+                                              .csi_index = csi,
+                                           }};
+
+   ret = kbase_ioctl(k->fd, KBASE_IOCTL_CS_QUEUE_BIND, &bind);
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_CS_QUEUE_BIND)");
+      // hack
+      cs.user_io = (void *)1;
+      return cs;
+   }
+
+   cs.user_io = kbase_mmap(NULL, k->page_size * BASEP_QUEUE_NR_MMAP_USER_PAGES,
+                           PROT_READ | PROT_WRITE, MAP_SHARED, k->fd,
+                           bind.out.mmap_handle);
+
+   if (cs.user_io == MAP_FAILED) {
+      perror("mmap(CS USER IO)");
+      cs.user_io = NULL;
+   }
+
+   return cs;
+}
+
+static struct kbase_cs
+kbase_cs_bind(kbase k, struct kbase_context *ctx, base_va va, unsigned size)
+{
+   struct kbase_cs cs = kbase_cs_bind_noevent(k, ctx, va, size, ctx->num_csi++);
+
+   // TODO: Fix this problem properly
+   if (k->event_slot_usage >= 256) {
+      fprintf(stderr, "error: Too many contexts created!\n");
+
+      /* *very* dangerous, but might just work */
+      --k->event_slot_usage;
+   }
+
+   // TODO: This is a misnomer... it isn't a byte offset
+   cs.event_mem_offset = k->event_slot_usage++;
+   k->event_slots[cs.event_mem_offset].back =
+      &k->event_slots[cs.event_mem_offset].syncobjs;
+
+   uint64_t *event_data =
+      k->event_mem.cpu + cs.event_mem_offset * PAN_EVENT_SIZE;
+
+   /* We use the "Higher" wait condition, so initialise to 1 to allow
+    * waiting before writing... */
+   event_data[0] = 1;
+   /* And reset the error field to 0, to avoid INHERITing faults */
+   event_data[1] = 0;
+
+   /* Just a zero-init is fine... reads and writes are always paired */
+   uint64_t *kcpu_data =
+      k->kcpu_event_mem.cpu + cs.event_mem_offset * PAN_EVENT_SIZE;
+   kcpu_data[0] = 0;
+   kcpu_data[1] = 0;
+
+   /* To match the event data */
+   k->event_slots[cs.event_mem_offset].last = 1;
+   k->event_slots[cs.event_mem_offset].last_submit = 1;
+
+   return cs;
+}
+
+static void
+kbase_cs_term(kbase k, struct kbase_cs *cs)
+{
+   if (cs->user_io) {
+      LOG("unmapping %p user_io %p\n", cs, cs->user_io);
+      munmap(cs->user_io, k->page_size * BASEP_QUEUE_NR_MMAP_USER_PAGES);
+   }
+
+   struct kbase_ioctl_cs_queue_terminate term = {
+      .buffer_gpu_addr = cs->va,
+   };
+
+   kbase_ioctl(k->fd, KBASE_IOCTL_CS_QUEUE_TERMINATE, &term);
+
+   pthread_mutex_lock(&k->queue_lock);
+   kbase_update_queue_callbacks(k, &k->event_slots[cs->event_mem_offset],
+                                ~0ULL);
+
+   k->event_slots[cs->event_mem_offset].last = ~0ULL;
+
+   /* Make sure that no syncobjs are referencing this CS */
+   list_for_each_entry(struct kbase_syncobj, o, &k->syncobjs, link)
+      kbase_syncobj_update(k, o);
+
+   k->event_slots[cs->event_mem_offset].last = 0;
+   pthread_mutex_unlock(&k->queue_lock);
+}
+
+static void
+kbase_cs_rebind(kbase k, struct kbase_cs *cs)
+{
+   struct kbase_cs new;
+   new = kbase_cs_bind_noevent(k, cs->ctx, cs->va, cs->size, cs->csi);
+
+   cs->user_io = new.user_io;
+   LOG("remapping %p user_io %p\n", cs, cs->user_io);
+
+   fprintf(stderr, "bound csi %i again\n", cs->csi);
+}
+
+static bool
+kbase_cs_kick(kbase k, struct kbase_cs *cs)
+{
+   struct kbase_ioctl_cs_queue_kick kick = {
+      .buffer_gpu_addr = cs->va,
+   };
+
+   int ret = kbase_ioctl(k->fd, KBASE_IOCTL_CS_QUEUE_KICK, &kick);
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_CS_QUEUE_KICK)");
+      return false;
+   }
+
+   return true;
+}
+
+#define CS_RING_DOORBELL(cs) *((uint32_t *)(cs->user_io)) = 1
+
+#define CS_READ_REGISTER(cs, r) *((uint64_t *)(cs->user_io + 4096 * 2 + r))
+
+#define CS_WRITE_REGISTER(cs, r, v) *((uint64_t *)(cs->user_io + 4096 + r)) = v
+
+static bool
+kbase_cs_submit(kbase k, struct kbase_cs *cs, uint64_t insert_offset,
+                struct kbase_syncobj *o, uint64_t seqnum)
+{
+   LOG("submit %p, seq %" PRIu64 ", insert %" PRIu64 " -> %" PRIu64 "\n", cs,
+       seqnum, cs->last_insert, insert_offset);
+
+   if (!cs->user_io)
+      return false;
+
+   if (insert_offset == cs->last_insert)
+      return true;
+
+#ifndef PAN_BASE_NOOP
+   struct kbase_event_slot *slot = &k->event_slots[cs->event_mem_offset];
+
+   pthread_mutex_lock(&k->queue_lock);
+   slot->last_submit = seqnum + 1;
+
+   if (o)
+      kbase_syncobj_update_fence(o, cs->event_mem_offset, seqnum);
+   pthread_mutex_unlock(&k->queue_lock);
+#endif
+
+   memory_barrier();
+
+   bool active = CS_READ_REGISTER(cs, CS_ACTIVE);
+   LOG("active is %i\n", active);
+
+   CS_WRITE_REGISTER(cs, CS_INSERT, insert_offset);
+   cs->last_insert = insert_offset;
+
+   if (false /*active*/) {
+      memory_barrier();
+      CS_RING_DOORBELL(cs);
+      memory_barrier();
+
+      active = CS_READ_REGISTER(cs, CS_ACTIVE);
+      LOG("active is now %i\n", active);
+   } else {
+      kbase_cs_kick(k, cs);
+   }
+
+   return true;
+}
+
+static bool
+kbase_cs_wait(kbase k, struct kbase_cs *cs, uint64_t extract_offset,
+              struct kbase_syncobj *o)
+{
+   if (!cs->user_io)
+      return false;
+
+   if (kbase_syncobj_wait(k, o))
+      return true;
+
+   uint64_t e = CS_READ_REGISTER(cs, CS_EXTRACT);
+   unsigned a = CS_READ_REGISTER(cs, CS_ACTIVE);
+
+   fprintf(stderr,
+           "CSI %i CS_EXTRACT (%" PRIu64 ") != %" PRIu64 ", "
+           "CS_ACTIVE (%i)\n",
+           cs->csi, e, extract_offset, a);
+
+   fprintf(stderr, "fences:\n");
+   list_for_each_entry(struct kbase_fence, fence, &o->fences, link) {
+      fprintf(stderr, " slot %i: seqnum %" PRIu64 "\n", fence->slot,
+              fence->value);
+   }
+
+   return false;
+}
+
+static bool
+kbase_kcpu_queue_create(kbase k, struct kbase_context *ctx)
+{
+#ifdef PAN_BASE_NOOP
+   return false;
+#endif
+
+   if (ctx->kcpu_init)
+      return true;
+
+   struct kbase_ioctl_kcpu_queue_new create = {0};
+
+   int ret;
+   ret = ioctl(k->fd, KBASE_IOCTL_KCPU_QUEUE_CREATE, &create);
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_KCPU_QUEUE_CREATE)");
+      return false;
+   }
+
+   ctx->kcpu_queue = create.id;
+   ctx->kcpu_init = true;
+   return true;
+}
+
+static void
+kbase_kcpu_queue_destroy(kbase k, struct kbase_context *ctx)
+{
+   if (!ctx->kcpu_init)
+      return;
+
+   struct kbase_ioctl_kcpu_queue_delete destroy = {
+      .id = ctx->kcpu_queue,
+   };
+
+   int ret;
+   ret = ioctl(k->fd, KBASE_IOCTL_KCPU_QUEUE_DELETE, &destroy);
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_KCPU_QUEUE_DELETE)");
+   }
+
+   ctx->kcpu_init = false;
+}
+
+static bool
+kbase_kcpu_command(kbase k, struct kbase_context *ctx,
+                   struct base_kcpu_command *cmd)
+{
+   int err;
+   bool ret = true;
+
+   if (!kbase_kcpu_queue_create(k, ctx))
+      return false;
+
+   struct kbase_ioctl_kcpu_queue_enqueue enqueue = {
+      .addr = (uintptr_t)cmd,
+      .nr_commands = 1,
+      .id = ctx->kcpu_queue,
+   };
+
+   err = kbase_ioctl(k->fd, KBASE_IOCTL_KCPU_QUEUE_ENQUEUE, &enqueue);
+   if (err != -1)
+      return ret;
+
+   /* If the enqueue failed, probably we hit the limit of enqueued
+    * commands (256), wait a bit and try again.
+    */
+
+   struct kbase_wait_ctx wait = kbase_wait_init(k, 1000000000);
+   while (kbase_wait_for_event(&wait)) {
+      err = kbase_ioctl(k->fd, KBASE_IOCTL_KCPU_QUEUE_ENQUEUE, &enqueue);
+      if (err != -1)
+         break;
+
+      if (errno != EBUSY) {
+         ret = false;
+         perror("ioctl(KBASE_IOCTL_KCPU_QUEUE_ENQUEUE");
+         break;
+      }
+   }
+   kbase_wait_fini(wait);
+
+   return ret;
+}
+
+static int
+kbase_kcpu_fence_export(kbase k, struct kbase_context *ctx)
+{
+   struct base_fence fence = {
+      .basep.fd = -1,
+   };
+
+   struct base_kcpu_command fence_cmd = {
+      .type = BASE_KCPU_COMMAND_TYPE_FENCE_SIGNAL,
+      .info.fence.fence = (uintptr_t)&fence,
+   };
+
+   return kbase_kcpu_command(k, ctx, &fence_cmd) ? fence.basep.fd : -1;
+}
+
+static bool
+kbase_kcpu_fence_import(kbase k, struct kbase_context *ctx, int fd)
+{
+   struct base_kcpu_command fence_cmd = {
+      .type = BASE_KCPU_COMMAND_TYPE_FENCE_WAIT,
+      .info.fence.fence = (uintptr_t) &
+                          (struct base_fence){
+                             .basep.fd = fd,
+                          },
+   };
+
+   return kbase_kcpu_command(k, ctx, &fence_cmd);
+}
+
+static bool
+kbase_kcpu_cqs_set(kbase k, struct kbase_context *ctx, base_va addr,
+                   uint64_t value)
+{
+   struct base_kcpu_command set_cmd = {
+      .type = BASE_KCPU_COMMAND_TYPE_CQS_SET_OPERATION,
+      .info.cqs_set_operation =
+         {
+            .objs = (uintptr_t) &
+                    (struct base_cqs_set_operation_info){
+                       .addr = addr,
+                       .val = value,
+                       .operation = BASEP_CQS_SET_OPERATION_SET,
+                       .data_type = BASEP_CQS_DATA_TYPE_U64,
+                    },
+            .nr_objs = 1,
+         },
+   };
+
+   return kbase_kcpu_command(k, ctx, &set_cmd);
+}
+
+static bool
+kbase_kcpu_cqs_wait(kbase k, struct kbase_context *ctx, base_va addr,
+                    uint64_t value)
+{
+   struct base_kcpu_command wait_cmd = {
+      .type = BASE_KCPU_COMMAND_TYPE_CQS_WAIT_OPERATION,
+      .info.cqs_wait_operation =
+         {
+            .objs = (uintptr_t) &
+                    (struct base_cqs_wait_operation_info){
+                       .addr = addr,
+                       .val = value,
+                       .operation = BASEP_CQS_WAIT_OPERATION_GT,
+                       .data_type = BASEP_CQS_DATA_TYPE_U64,
+                    },
+            .nr_objs = 1,
+            .inherit_err_flags = 0,
+         },
+   };
+
+   return kbase_kcpu_command(k, ctx, &wait_cmd);
+}
+#endif
+
+// TODO: Only define for CSF kbases?
+static bool
+kbase_callback_all_queues(kbase k, int32_t *count, void (*callback)(void *),
+                          void *data)
+{
+   pthread_mutex_lock(&k->queue_lock);
+
+   int32_t queue_count = 0;
+
+   for (unsigned i = 0; i < k->event_slot_usage; ++i) {
+      struct kbase_event_slot *slot = &k->event_slots[i];
+
+      /* There is no need to do anything for idle slots */
+      if (slot->last == slot->last_submit)
+         continue;
+
+      struct kbase_sync_link *link = malloc(sizeof(*link));
+      *link = (struct kbase_sync_link){
+         .next = NULL,
+         .seqnum = slot->last_submit,
+         .callback = callback,
+         .data = data,
+      };
+
+      // TODO: Put insertion code into its own function
+      struct kbase_sync_link **list = slot->back;
+      slot->back = &link->next;
+      assert(!*list);
+      *list = link;
+
+      ++queue_count;
+   }
+
+   p_atomic_add(count, queue_count);
+
+   pthread_mutex_unlock(&k->queue_lock);
+
+   return queue_count != 0;
+}
+
+static void
+kbase_mem_sync(kbase k, base_va gpu, void *cpu, size_t size, bool invalidate)
+{
+#ifdef __aarch64__
+   /* Valgrind replaces the operations with DC CVAU, which is not enough
+    * for CPU<->GPU coherency. The ioctl can be used instead. */
+   if (!RUNNING_ON_VALGRIND) {
+      /* I don't that memory barriers are needed here... having the
+       * DMB SY before submit should be enough. TODO what about
+       * dma-bufs? */
+      if (invalidate)
+         cache_invalidate_range(cpu, size);
+      else
+         cache_clean_range(cpu, size);
+      return;
+   }
+#endif
+
+   struct kbase_ioctl_mem_sync sync = {
+      .handle = gpu,
+      .user_addr = (uintptr_t)cpu,
+      .size = size,
+      .type = invalidate + (PAN_BASE_API == 0 ? 0 : 1),
+   };
+
+   int ret;
+   ret = kbase_ioctl(k->fd, KBASE_IOCTL_MEM_SYNC, &sync);
+   if (ret == -1)
+      perror("ioctl(KBASE_IOCTL_MEM_SYNC)");
+}
+
+bool
+#if defined(PAN_BASE_NOOP)
+kbase_open_csf_noop
+#elif PAN_BASE_API == 0
+kbase_open_old
+#elif PAN_BASE_API == 1
+kbase_open_new
+#elif PAN_BASE_API == 2
+kbase_open_csf
+#endif
+   (kbase k)
+{
+   k->api = PAN_BASE_API;
+
+   pthread_mutex_init(&k->handle_lock, NULL);
+   pthread_mutex_init(&k->event_read_lock, NULL);
+   pthread_mutex_init(&k->event_cnd_lock, NULL);
+   pthread_mutex_init(&k->queue_lock, NULL);
+
+   pthread_condattr_t attr;
+   pthread_condattr_init(&attr);
+   pthread_condattr_setclock(&attr, CLOCK_MONOTONIC);
+   pthread_cond_init(&k->event_cnd, &attr);
+   pthread_condattr_destroy(&attr);
+
+   list_inithead(&k->syncobjs);
+
+   /* For later APIs, we've already checked the version in pan_base.c */
+#if PAN_BASE_API == 0
+   struct kbase_ioctl_get_version ver = {0};
+   kbase_ioctl(k->fd, KBASE_IOCTL_GET_VERSION, &ver);
+#endif
+
+   k->close = kbase_close;
+
+   k->get_pan_gpuprop = kbase_get_pan_gpuprop;
+   k->get_mali_gpuprop = kbase_get_mali_gpuprop;
+
+   k->alloc = kbase_alloc;
+   k->free = kbase_free;
+   k->import_dmabuf = kbase_import_dmabuf;
+   k->mmap_import = kbase_mmap_import;
+
+   k->poll_event = kbase_poll_event;
+   k->handle_events = kbase_handle_events;
+
+#if PAN_BASE_API < 2
+   k->submit = kbase_submit;
+#else
+   k->context_create = kbase_context_create;
+   k->context_destroy = kbase_context_destroy;
+   k->context_recreate = kbase_context_recreate;
+
+   k->cs_bind = kbase_cs_bind;
+   k->cs_term = kbase_cs_term;
+   k->cs_rebind = kbase_cs_rebind;
+   k->cs_submit = kbase_cs_submit;
+   k->cs_wait = kbase_cs_wait;
+
+   k->kcpu_fence_export = kbase_kcpu_fence_export;
+   k->kcpu_fence_import = kbase_kcpu_fence_import;
+   k->kcpu_cqs_set = kbase_kcpu_cqs_set;
+   k->kcpu_cqs_wait = kbase_kcpu_cqs_wait;
+#endif
+
+   k->syncobj_create = kbase_syncobj_create;
+   k->syncobj_destroy = kbase_syncobj_destroy;
+   k->syncobj_dup = kbase_syncobj_dup;
+   k->syncobj_wait = kbase_syncobj_wait;
+
+   k->callback_all_queues = kbase_callback_all_queues;
+
+   k->mem_sync = kbase_mem_sync;
+
+   for (unsigned i = 0; i < ARRAY_SIZE(kbase_main); ++i) {
+      ++k->setup_state;
+      if (!kbase_main[i].part(k)) {
+         k->close(k);
+         return false;
+      }
+   }
+   return true;
+}
diff -urN mesa-23.0.0/src/panfrost/ci/deqp-panfrost-g610.toml mesa/src/panfrost/ci/deqp-panfrost-g610.toml
--- mesa-23.0.0/src/panfrost/ci/deqp-panfrost-g610.toml	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/ci/deqp-panfrost-g610.toml	2023-03-06 17:54:35.839494973 +0100
@@ -0,0 +1,11 @@
+# Basic test set
+[[deqp]]
+deqp = "/deqp/modules/gles2/deqp-gles2"
+caselists = ["/deqp/mustpass/gles2-master.txt"]
+deqp_args = [
+    "--deqp-surface-width=256", "--deqp-surface-height=256",
+    "--deqp-surface-type=pbuffer", "--deqp-visibility=hidden",
+    "--deqp-gl-config-name=rgba8888d24s8ms0",
+]
+version_check = "GL ES 3.1.*git"
+renderer_check = "Mali-G610"
diff -urN mesa-23.0.0/src/panfrost/ci/panfrost-g52-skips.txt mesa/src/panfrost/ci/panfrost-g52-skips.txt
--- mesa-23.0.0/src/panfrost/ci/panfrost-g52-skips.txt	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/ci/panfrost-g52-skips.txt	2023-03-06 17:54:35.839494973 +0100
@@ -6,6 +6,14 @@
 # kernel driver
 dEQP-GLES31.functional.draw_indirect.compute_interop.large.*
 
+# We lack a dependency between the vertex job filling the indirect draw
+# buffers and the indirect draw compute job reading from these buffers,
+# leading to unreliable results (the tests pass if the vertex job is
+# done before the compute job starts, and fail otherwise). Let's disable
+# those tests until we sort it out.
+KHR-GLES31.core.draw_indirect.advanced-twoPass-transformFeedback-arrays
+KHR-GLES31.core.draw_indirect.advanced-twoPass-transformFeedback-elements
+
 # fail seen here and in others
 # https://gitlab.freedesktop.org/mesa/mesa/-/jobs/19776551
 dEQP-GLES31.functional.ssbo.layout.random.all_shared_buffer.36
diff -urN mesa-23.0.0/src/panfrost/compiler/bifrost_compile.c mesa/src/panfrost/compiler/bifrost_compile.c
--- mesa-23.0.0/src/panfrost/compiler/bifrost_compile.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/compiler/bifrost_compile.c	2023-03-06 19:19:32.819309571 +0100
@@ -409,6 +406,22 @@
                   nir_dest_bit_size(instr->dest));
 }
 
+static bi_index bi_load_sysval(bi_builder *b, int sysval,
+                               unsigned nr_components, unsigned offset);
+
+static bi_index
+bi_vertex_id_offset(bi_builder *b, bool offset)
+{
+   bi_index vtx = bi_vertex_id(b);
+
+   if (!offset)
+      return vtx;
+
+   bi_index first = bi_load_sysval(b, PAN_SYSVAL_VERTEX_INSTANCE_OFFSETS, 1, 0);
+
+   return bi_iadd_u32(b, vtx, first, false);
+}
+
 static void
 bi_emit_load_attr(bi_builder *b, nir_intrinsic_instr *instr)
 {
@@ -425,8 +438,15 @@
       (component == 0) ? bi_dest_index(&instr->dest) : bi_temp(b->shader);
    bi_instr *I;
 
+   /* The attribute offset field was removed from the compute job payload
+    * in v10. */
+   bool needs_offset = b->shader->arch >= 10 &&
+                       b->shader->nir->info.has_transform_feedback_varyings;
+
+   bi_index vertex_id = bi_vertex_id_offset(b, needs_offset);
+
    if (immediate) {
-      I = bi_ld_attr_imm_to(b, dest, bi_vertex_id(b), bi_instance_id(b), regfmt,
+      I = bi_ld_attr_imm_to(b, dest, vertex_id, bi_instance_id(b), regfmt,
                             vecsize, imm_index);
    } else {
       bi_index idx = bi_src_index(&instr->src[0]);
@@ -436,8 +456,8 @@
       else if (base != 0)
          idx = bi_iadd_u32(b, idx, bi_imm_u32(base), false);
 
-      I = bi_ld_attr_to(b, dest, bi_vertex_id(b), bi_instance_id(b), idx,
-                        regfmt, vecsize);
+      I = bi_ld_attr_to(b, dest, vertex_id, bi_instance_id(b), idx, regfmt,
+                        vecsize);
    }
 
    if (b->shader->arch >= 9)
@@ -1853,15 +1873,7 @@
     * and lower here if needed.
     */
    case nir_intrinsic_load_vertex_id:
-      if (b->shader->malloc_idvs) {
-         bi_mov_i32_to(b, dst, bi_vertex_id(b));
-      } else {
-         bi_index first =
-            bi_load_sysval(b, PAN_SYSVAL_VERTEX_INSTANCE_OFFSETS, 1, 0);
-
-         bi_iadd_u32_to(b, dst, bi_vertex_id(b), first, false);
-      }
-
+      bi_mov_i32_to(b, dst, bi_vertex_id_offset(b, !b->shader->malloc_idvs));
       break;
 
    /* We only use in our transform feedback lowering */
@@ -2856,7 +2868,7 @@
       break;
 
    case nir_op_i2i16:
-      assert(src_sz == 8 || src_sz == 32);
+      assert(src_sz == 32 || src_sz == 16 || src_sz == 8);
 
       if (src_sz == 8)
          bi_v2s8_to_v2s16_to(b, dst, s0);
@@ -2865,7 +2877,7 @@
       break;
 
    case nir_op_u2u16:
-      assert(src_sz == 8 || src_sz == 32);
+      assert(src_sz == 32 || src_sz == 16 || src_sz == 8);
 
       if (src_sz == 8)
          bi_v2u8_to_v2u16_to(b, dst, s0);
@@ -3754,11 +3777,8 @@
    image_src = bi_lshift_or_i32(b, sampler, image_src, bi_imm_u8(0));
    image_src = bi_lshift_or_i32(b, texture, image_src, bi_imm_u8(16));
 
-   /* Only write the components that we actually read */
-   unsigned mask = nir_ssa_def_components_read(&instr->dest.ssa);
-   unsigned comps_per_reg = nir_dest_bit_size(instr->dest) == 16 ? 2 : 1;
-   unsigned res_size = DIV_ROUND_UP(util_bitcount(mask), comps_per_reg);
-
+   unsigned mask = BI_WRITE_MASK_RGBA;
+   unsigned res_size = nir_dest_bit_size(instr->dest) == 16 ? 2 : 4;
    enum bi_register_format regfmt = bi_reg_fmt_for_nir(instr->dest_type);
    enum bi_dimension dim = valhall_tex_dimension(instr->sampler_dim);
    bi_index dest = bi_temp(b->shader);
@@ -3785,32 +3805,11 @@
       unreachable("Unhandled Valhall texture op");
    }
 
-   /* The hardware will write only what we read, and it will into
-    * contiguous registers without gaps (different from Bifrost). NIR
-    * expects the gaps, so fill in the holes (they'll be copypropped and
-    * DCE'd away later).
-    */
-   bi_index unpacked[4] = {bi_null(), bi_null(), bi_null(), bi_null()};
-
-   bi_emit_cached_split_i32(b, dest, res_size);
-
-   /* Index into the packed component array */
-   unsigned j = 0;
-   unsigned comps[4] = {0};
-   unsigned nr_components = nir_dest_num_components(instr->dest);
-
-   for (unsigned i = 0; i < nr_components; ++i) {
-      if (mask & BITFIELD_BIT(i)) {
-         unpacked[i] = dest;
-         comps[i] = j++;
-      } else {
-         unpacked[i] = bi_zero();
-      }
-   }
-
-   bi_make_vec_to(b, bi_dest_index(&instr->dest), unpacked, comps,
-                  nir_dest_num_components(instr->dest),
-                  nir_dest_bit_size(instr->dest));
+   bi_index w[4] = {bi_null(), bi_null(), bi_null(), bi_null()};
+   bi_emit_split_i32(b, w, dest, res_size);
+   bi_emit_collect_to(
+      b, bi_dest_index(&instr->dest), w,
+      DIV_ROUND_UP(nir_dest_num_components(instr->dest) * res_size, 4));
 }
 
 /* Simple textures ops correspond to NIR tex or txl with LOD = 0 on 2D/cube
@@ -4953,6 +4952,8 @@
 
    bi_validate(ctx, "NIR -> BIR");
 
+   _mesa_hash_table_u64_destroy(ctx->allocated_vec);
+
    /* If the shader doesn't write any colour or depth outputs, it may
     * still need an ATEST at the very end! */
    bool need_dummy_atest = (ctx->stage == MESA_SHADER_FRAGMENT) &&
diff -urN mesa-23.0.0/src/panfrost/compiler/bi_lower_swizzle.c mesa/src/panfrost/compiler/bi_lower_swizzle.c
--- mesa-23.0.0/src/panfrost/compiler/bi_lower_swizzle.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/compiler/bi_lower_swizzle.c	2023-03-06 19:19:32.855309808 +0100
@@ -133,10 +133,8 @@
       bi_index dest = ins->dest[0];
       bi_index tmp = bi_temp(ctx);
 
-      bi_index swizzled_src = bi_replace_index(ins->src[0], tmp);
-      ins->src[0].swizzle = BI_SWIZZLE_H01;
       ins->dest[0] = tmp;
-      bi_swz_v2i16_to(&b, dest, swizzled_src);
+      bi_swz_v2i16_to(&b, dest, bi_replace_index(ins->src[0], tmp));
       return;
    }
 
diff -urN mesa-23.0.0/src/panfrost/compiler/cmdline.c mesa/src/panfrost/compiler/cmdline.c
--- mesa-23.0.0/src/panfrost/compiler/cmdline.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/compiler/cmdline.c	2023-03-06 19:19:32.901310112 +0100
@@ -26,13 +26,191 @@
 
 #include <getopt.h>
 #include <string.h>
-#include "bifrost/disassemble.h"
-#include "util/macros.h"
 #include "valhall/disassemble.h"
+#include "compiler.h"
+#include "bifrost/disassemble.h"
+
+#include "compiler/glsl/gl_nir.h"
+#include "compiler/glsl/glsl_to_nir.h"
+#include "compiler/glsl/standalone.h"
+#include "compiler/nir_types.h"
+#include "main/mtypes.h"
+#include "util/u_dynarray.h"
+#include "bifrost_compile.h"
 
 unsigned gpu_id = 0x7212;
 int verbose = 0;
 
+static gl_shader_stage
+filename_to_stage(const char *stage)
+{
+   const char *ext = strrchr(stage, '.');
+
+   if (ext == NULL) {
+      fprintf(stderr, "No extension found in %s\n", stage);
+      exit(1);
+   }
+
+   if (!strcmp(ext, ".cs") || !strcmp(ext, ".comp"))
+      return MESA_SHADER_COMPUTE;
+   else if (!strcmp(ext, ".vs") || !strcmp(ext, ".vert"))
+      return MESA_SHADER_VERTEX;
+   else if (!strcmp(ext, ".fs") || !strcmp(ext, ".frag"))
+      return MESA_SHADER_FRAGMENT;
+   else {
+      fprintf(stderr, "Invalid extension %s\n", ext);
+      exit(1);
+   }
+
+   unreachable("Should've returned or bailed");
+}
+
+static int
+st_packed_uniforms_type_size(const struct glsl_type *type, bool bindless)
+{
+   return glsl_count_dword_slots(type, bindless);
+}
+
+static int
+glsl_type_size(const struct glsl_type *type, bool bindless)
+{
+   return glsl_count_attribute_slots(type, false);
+}
+
+static void
+insert_sorted(struct exec_list *var_list, nir_variable *new_var)
+{
+   nir_foreach_variable_in_list(var, var_list) {
+      if (var->data.location > new_var->data.location) {
+         exec_node_insert_node_before(&var->node, &new_var->node);
+         return;
+      }
+   }
+   exec_list_push_tail(var_list, &new_var->node);
+}
+
+static void
+sort_varyings(nir_shader *nir, nir_variable_mode mode)
+{
+   struct exec_list new_list;
+   exec_list_make_empty(&new_list);
+   nir_foreach_variable_with_modes_safe(var, nir, mode) {
+      exec_node_remove(&var->node);
+      insert_sorted(&new_list, var);
+   }
+   exec_list_append(&nir->variables, &new_list);
+}
+
+static void
+fixup_varying_slots(nir_shader *nir, nir_variable_mode mode)
+{
+   nir_foreach_variable_with_modes(var, nir, mode) {
+      if (var->data.location >= VARYING_SLOT_VAR0) {
+         var->data.location += 9;
+      } else if ((var->data.location >= VARYING_SLOT_TEX0) &&
+                 (var->data.location <= VARYING_SLOT_TEX7)) {
+         var->data.location += VARYING_SLOT_VAR0 - VARYING_SLOT_TEX0;
+      }
+   }
+}
+
+static void
+compile_shader(int stages, char **files)
+{
+   struct gl_shader_program *prog;
+   nir_shader *nir[MESA_SHADER_COMPUTE + 1];
+   unsigned shader_types[MESA_SHADER_COMPUTE + 1];
+
+   if (stages > MESA_SHADER_COMPUTE) {
+      fprintf(stderr, "Too many stages");
+      exit(1);
+   }
+
+   for (unsigned i = 0; i < stages; ++i)
+      shader_types[i] = filename_to_stage(files[i]);
+
+   struct standalone_options options = {.glsl_version = 460,
+                                        .do_link = true,
+                                        .lower_precision = true};
+
+   static struct gl_context local_ctx;
+
+   prog = standalone_compile_shader(&options, stages, files, &local_ctx);
+
+   for (unsigned i = 0; i < stages; ++i) {
+      gl_shader_stage stage = shader_types[i];
+      prog->_LinkedShaders[stage]->Program->info.stage = stage;
+   }
+
+   struct util_dynarray binary;
+
+   util_dynarray_init(&binary, NULL);
+
+   for (unsigned i = 0; i < stages; ++i) {
+      nir[i] = glsl_to_nir(&local_ctx.Const, prog, shader_types[i],
+                           &bifrost_nir_options);
+
+      if (shader_types[i] == MESA_SHADER_VERTEX) {
+         nir_assign_var_locations(nir[i], nir_var_shader_in,
+                                  &nir[i]->num_inputs, glsl_type_size);
+         sort_varyings(nir[i], nir_var_shader_out);
+         nir_assign_var_locations(nir[i], nir_var_shader_out,
+                                  &nir[i]->num_outputs, glsl_type_size);
+         fixup_varying_slots(nir[i], nir_var_shader_out);
+      } else if (shader_types[i] == MESA_SHADER_FRAGMENT) {
+         sort_varyings(nir[i], nir_var_shader_in);
+         nir_assign_var_locations(nir[i], nir_var_shader_in,
+                                  &nir[i]->num_inputs, glsl_type_size);
+         fixup_varying_slots(nir[i], nir_var_shader_in);
+         nir_assign_var_locations(nir[i], nir_var_shader_out,
+                                  &nir[i]->num_outputs, glsl_type_size);
+      }
+
+      nir_assign_var_locations(nir[i], nir_var_uniform, &nir[i]->num_uniforms,
+                               glsl_type_size);
+
+      NIR_PASS_V(nir[i], nir_lower_global_vars_to_local);
+      NIR_PASS_V(nir[i], nir_lower_io_to_temporaries,
+                 nir_shader_get_entrypoint(nir[i]), true, i == 0);
+      NIR_PASS_V(nir[i], nir_opt_copy_prop_vars);
+      NIR_PASS_V(nir[i], nir_opt_combine_stores, nir_var_all);
+
+      NIR_PASS_V(nir[i], nir_lower_system_values);
+      NIR_PASS_V(nir[i], gl_nir_lower_samplers, prog);
+      NIR_PASS_V(nir[i], nir_split_var_copies);
+      NIR_PASS_V(nir[i], nir_lower_var_copies);
+
+      NIR_PASS_V(nir[i], nir_lower_io, nir_var_uniform,
+                 st_packed_uniforms_type_size, (nir_lower_io_options)0);
+      NIR_PASS_V(nir[i], nir_lower_uniforms_to_ubo, true, false);
+
+      /* before buffers and vars_to_ssa */
+      NIR_PASS_V(nir[i], gl_nir_lower_images, true);
+
+      NIR_PASS_V(nir[i], gl_nir_lower_buffers, prog);
+      NIR_PASS_V(nir[i], nir_opt_constant_folding);
+
+      struct panfrost_compile_inputs inputs = {
+         .gpu_id = gpu_id,
+         .fixed_sysval_ubo = -1,
+      };
+      struct pan_shader_info info = {0};
+
+      util_dynarray_clear(&binary);
+      bifrost_compile_shader_nir(nir[i], &inputs, &binary, &info);
+
+      char *fn = NULL;
+      asprintf(&fn, "shader_%u.bin", i);
+      assert(fn != NULL);
+      FILE *fp = fopen(fn, "wb");
+      fwrite(binary.data, 1, binary.size, fp);
+      fclose(fp);
+      free(fn);
+   }
+
+   util_dynarray_fini(&binary);
+}
+
 #define BI_FOURCC(ch0, ch1, ch2, ch3)                                          \
    ((uint32_t)(ch0) | (uint32_t)(ch1) << 8 | (uint32_t)(ch2) << 16 |           \
     (uint32_t)(ch3) << 24)
@@ -145,6 +321,14 @@
       }
    }
 
-   disassemble(argv[optind + 1]);
+   if (strcmp(argv[optind], "compile") == 0)
+      compile_shader(argc - optind - 1, &argv[optind + 1]);
+   else if (strcmp(argv[optind], "disasm") == 0)
+      disassemble(argv[optind + 1]);
+   else {
+      fprintf(stderr, "Unknown command. Valid: compile/disasm\n");
+      return 1;
+   }
+
    return 0;
 }
diff -urN mesa-23.0.0/src/panfrost/compiler/valhall/ISA.xml mesa/src/panfrost/compiler/valhall/ISA.xml
--- mesa-23.0.0/src/panfrost/compiler/valhall/ISA.xml	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/compiler/valhall/ISA.xml	2023-03-06 19:15:05.805535852 +0100
@@ -33,7 +33,7 @@
     <constant desc="Zero">0x00000000</constant>
     <constant desc="All ones; integer $-1$">0xFFFFFFFF</constant>
     <constant desc="Maximum integer; floating-point NaN">0x7FFFFFFF</constant>
-    <constant desc="Integers $(-2, -3, -4, -6)$">0xFAFCFDFE</constant>
+    <constant desc="Integers $(-2, -3, -4, -5)$">0xFAFCFDFE</constant>
     <constant desc="16-bit integer $2^8$">0x01000000</constant>
     <constant desc="Multiples of 16 $(0, 32, 0, 128)$">0x80002000</constant>
     <constant desc="Multiples of 16 $(48, 80, 96, 112)$">0x70605030</constant>
@@ -322,7 +322,7 @@
     <value desc="Zero-extend to 16-bit, low-half">h0</value>
     <value desc="Zero-extend to 16-bit, high-half">h1</value>
     <value desc="Zero-extend to 32-bit">w0</value>
-    <value desc="Zero-extend to 64-bit">d0</value>
+    <value desc="Zero-extend to 32-bit">d0</value>
   </enum>
 
   <enum name="Load lane (16-bit)">
@@ -2061,7 +2061,7 @@
       as the `mux` modifier is evaluated on the mask. If true, `A` is chosen,
       else `B` is chosen. The `bit` modifier acts bitwise, equivalent to
       `bitselect()` in OpenCL, so `MUX.i32.bit A, B, mask` calculates
-      `(A &amp; mask) | (B &amp; ~mask)`.
+      `(A &amp; mask) | (A &amp; ~mask)`.
     </desc>
     <mod name="mux" start="32" size="2"/>
     <src>A</src>
@@ -2075,7 +2075,7 @@
       as the `mux` modifier is evaluated on the mask. If true, `A` is chosen,
       else `B` is chosen. The `bit` modifier acts bitwise, equivalent to
       `bitselect()` in OpenCL, so `MUX.i32.bit A, B, mask` calculates
-      `(A &amp; mask) | (B &amp; ~mask)`.
+      `(A &amp; mask) | (A &amp; ~mask)`.
     </desc>
     <mod name="mux" start="32" size="2"/>
     <src swizzle="true">A</src>
diff -urN mesa-23.0.0/src/panfrost/compiler/valhall/va_pack.c mesa/src/panfrost/compiler/valhall/va_pack.c
--- mesa-23.0.0/src/panfrost/compiler/valhall/va_pack.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/compiler/valhall/va_pack.c	2023-03-06 19:19:32.949310429 +0100
@@ -933,7 +933,7 @@
          hex |= ((uint64_t)I->fetch_component) << 14;
       }
 
-      hex |= (I->write_mask << 22);
+      hex |= (VA_WRITE_MASK_RGBA << 22);
       hex |= ((uint64_t)va_pack_register_type(I)) << 26;
       hex |= ((uint64_t)I->dimension) << 28;
 
diff -urN mesa-23.0.0/src/panfrost/csf_test/interpret.py mesa/src/panfrost/csf_test/interpret.py
--- mesa-23.0.0/src/panfrost/csf_test/interpret.py	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/csf_test/interpret.py	2023-03-06 17:54:35.839494973 +0100
@@ -0,0 +1,1820 @@
+#!/usr/bin/env python3
+
+import os
+import re
+import struct
+import subprocess
+import sys
+
+try:
+    py_path = os.path.dirname(os.path.realpath(__file__)) + "/../bifrost/valhall"
+except:
+    py_path = "../bifrost/valhall"
+
+if py_path not in sys.path:
+    sys.path.insert(0, py_path)
+
+import asm
+import struct
+
+def ff(val):
+    return struct.unpack("=f", struct.pack("=I", val))[0]
+
+def ii(val):
+    return struct.unpack("=I", struct.pack("=f", val))[0]
+
+shaders = {
+    "atomic": """
+IADD_IMM.i32.reconverge r0, 0x0, #0x0
+NOP.wait0
+ICMP_OR.u32.ge.m1 r1, r0, u2, 0x0
+BRANCHZ.eq.reconverge ^r1.h0, offset:1
+BRANCHZ.eq 0x0, offset:3
+ATOM1_RETURN.i32.slot0.ainc @r1, u0, offset:0x0
+IADD_IMM.i32 r0, ^r0, #0x1
+BRANCHZ.eq.reconverge 0x0, offset:-7
+NOP.end
+""",
+    "rmw": """
+IADD_IMM.i32.reconverge r0, 0x0, #0x0
+ICMP_OR.u32.ge.m1 r1, r0, u2, 0x0
+BRANCHZ.eq.reconverge r1.h0, offset:1
+BRANCHZ.eq 0x0, offset:6
+NOP.wait1
+LOAD.i32.unsigned.slot0.wait0 @r1, u0, offset:0
+IADD_IMM.i32 r1, ^r1, #0x1
+STORE.i32.slot1 @r1, u0, offset:0
+IADD_IMM.i32 r0, ^r0, #0x1
+BRANCHZ.eq.reconverge 0x0, offset:-9
+NOP.end
+""",
+    "global_invocation": """
+IADD_IMM.i32 r0, ^r60, #0x1
+STORE.i32.slot0.end @r0, u0, offset:0
+""",
+    "invoc_offset": """
+LSHIFT_OR.i32 r0, ^r60, 0x3020100.b22, 0x0
+IADD.s32 r0, u0, ^r0
+ICMP_OR.u32.lt.i1 r1, r0, u0, 0x0
+IADD.s32 r1, ^r1, u1
+MOV.i32 r2, u2
+STORE.i32.slot0.end @r2, ^r0, offset:0
+""",
+    "invoc_rmw": """
+LSHIFT_OR.i32 r0, ^r60, 0x3020100.b22, 0x0
+IADD.s32 r0, u0, ^r0
+ICMP_OR.u32.lt.i1 r1, r0, u0, 0x0
+IADD.s32 r1, ^r1, u1
+LOAD.i32.unsigned.slot0.wait0 @r2, r0, offset:0
+IADD.s32 r2, ^r2, u2
+STORE.i32.slot1.end @r2, ^r0, offset:0
+""",
+
+    "preframe": """
+U16_TO_U32.discard r0, r59.h00
+U16_TO_U32 r1, ^r59.h10
+IADD_IMM.i32 r2, 0x0, #0x1
+IADD_IMM.i32 r3, 0x0, #0x0
+TEX_FETCH.slot0.skip.f.32.2d.wait @r4:r5:r6:r7, @r0:r1, ^r2
+FADD.f32 r4, ^r4, 0x40490FDB
+FADD.f32 r5, ^r5, 0x40490FDB
+BLEND.slot0.v4.f32.end @r4:r5:r6:r7, blend_descriptor_0.w0, r60, target:0x0
+""",
+
+
+    "position": """
+LEA_BUF_IMM.slot0.wait0 @r4:r5, r59, table:0xD, index:0x0
+#BRANCHZI.absolute 0x1000000, ^r4
+# position of 16384
+IADD_IMM.i32 r2, 0x0, #0x0e
+# position of 16
+IADD_IMM.i32 r2, 0x0, #0x04
+LSHIFT_OR.i32 r0, 0x03020100.b1, r2, 0x0
+LSHIFT_AND.i32 r0, r60, r2, ^r0
+IADD_IMM.i32 r1, 0x0, #0x01
+RSHIFT_AND.i32 r1, r60, 0x03020100.b11, ^r1
+LSHIFT_OR.i32 r1, ^r1, ^r2, 0x0
+S32_TO_F32 r0, ^r0
+S32_TO_F32 r1, ^r1
+
+RSHIFT_OR.i32 r2, ^r60, 0x03020100.b22, 0x0
+S32_TO_F32 r2, ^r2
+FADD.f32 r0, ^r0, r2.neg
+#FADD.f32 r1, ^r1, ^r2
+S32_TO_F32 r2, ^r60
+#MOV.i32 r1, 0x0
+
+FADD.f32 r0, ^r0, 0x40490FDB
+FADD.f32 r1, ^r1, 0x40490FDB
+#FMA.f32 r2, ^r2, 0x3DCCCCCD, 0x0
+MOV.i32 r2, 0x3DCCCCCD
+MOV.i32 r3, 0x0
+
+#STORE.i128.slot0 @r0:r1:r2:r3, thread_local_pointer, offset:0
+
+IADD_IMM.i32 r8, 0x0, #0x00004000
+STORE.i16.istream.slot0 @r8, r4, offset:64
+
+STORE.i128.istream.slot0 @r0:r1:r2:r3, r4, offset:0
+STORE.i128.slot0.end @r0:r1:r2:r3, ^r4, offset:0x7000
+""",
+
+    "fragment": """
+ATOM1_RETURN.i32.slot0.ainc.wait0 @r0, u0, offset:0
+IADD_IMM.i32 r1, 0x0, #0x1ff
+LSHIFT_AND.i32 r0, ^r0, 0x0, ^r1
+SHADDX.u64 r2, u2, ^r0.w0, shift:0x2
+STORE.i32.slot0.wait0 @r59, ^r2, offset:0
+
+IADD_IMM.i32 r4, 0x0, #0x3f100000
+IADD_IMM.i32 r5, 0x0, #0x3f400000
+IADD_IMM.i32 r6, 0x0, #0x3f300000
+IADD_IMM.i32 r7, 0x0, #0x32cccccd
+BLEND.slot0.v4.f32.end @r4:r5:r6:r7, blend_descriptor_0.w0, r60, target:0x0
+""",
+
+}
+
+flg = 0xf
+#flg = 0x20000f # Uncached!
+
+HEAP_SIZE = 1024 * 1024
+
+memory = {
+    "ev": (8192, 0x8200f),
+    "x": 1024 * 1024,
+    "y": 4096,
+    "ls_alloc": 4096,
+    "occlusion": 4096,
+
+    "ssbo": 4096,
+    "tls": 4096,
+
+    #"plane_0": (256 * 256 * 32, 0x380f), # 2 MB
+    "plane_0": (256 * 256 * 32, 0x280f), # 2 MB
+
+    "idk": HEAP_SIZE,
+    "heap": HEAP_SIZE,
+}
+
+w = 0xffffffff
+
+# Words are 32-bit, apart from address references
+descriptors = {
+    "shader": [0x118, 1 << 12, "invoc_rmw"],
+    "ls": [3, 31, "ls_alloc"],
+    "fau": [("ssbo", 0), ("ssbo", 16)],
+    "fau2": [("ev", 8 + (0 << 34)), 7, 0],
+
+    "tiler_heap": [
+        0x029, 1 << 21, #HEAP_SIZE,
+        0x1000, 0x60, 0x1040, 0x60, 0x1000 + (1 << 21), 0x60
+        #"heap", ("heap", 64), ("heap", HEAP_SIZE),
+    ],
+
+} | {
+    x: [
+        0, 0,
+        # Hierarchy mask,
+        # Single-sampled
+        # Last provoking vertex
+        0x6 | (0 << 18),
+        0x00ff00ff,
+        # Layer
+        0, 0,
+        "tiler_heap",
+        ("idk", 0x10),
+        #("tiler_heap", -0xfff0),
+        # "Weights"
+    ] + ([0] * (32 - 10)) + [
+        # "State"
+        0,
+        31,
+        0,
+        0x10000000,
+    ] for x in ("tiler_ctx", "tiler_ctx2", "tiler_ctx3")
+} | {
+
+    "thread_storage": [
+        1, 31,
+        "tls",
+        0, 0,
+    ],
+
+    # Preload r59/r60
+    "preframe_shader": [0x128, 3 << 11, "preframe"],
+    "position_shader": [0x138, 3 << 11, "position"],
+    "fragment_shader": [0x128, 3 << 11, "fragment"],
+
+    "idvs_zs": [
+        0x70077, # Depth/stencil type, Always for stencil tests
+        0, 0, # Stencil state
+        0, # unk
+        # Depth source minimum, write disabled
+        # [0, 1] Depth clamp
+        # Depth function: Always
+        (1 << 23) | (7 << 29),
+        0, # Depth units
+        0, # Depth factor
+        0, # Depth bias clamp
+    ],
+
+    "preframe_zs": [
+        0x70077, # Depth/stencil type, Always for stencil tests
+        0, 0, # Stencil state
+        0, # unk
+        # Depth source minimum, write disabled
+        # [0, 1] Depth clamp
+        # Depth function: Always
+        (1 << 23) | (7 << 29),
+        0, # Depth units
+        0, # Depth factor
+        0, # Depth bias clamp
+    ],
+
+    "idvs_blend": [
+        # Load dest, enable
+        1 | (1 << 9),
+        # RGB/Alpha: Src + Zero * Src
+        # All channels
+        ((2 | (2 << 4) | (1 << 8)) * 0x1001) | (0xf << 28),
+        # Fixed function blending, four components
+        2 | (3 << 3),
+        # RGBA8 TB pixel format / F32 register format
+        0 | (237 << 12) | (0 << 22) | (1 << 24),
+    ],
+
+    "preframe_blend": [
+        # Load dest, enable
+        1 | (1 << 9),
+        # RGB/Alpha: Src + Zero * Src
+        # All channels
+        ((2 | (2 << 4) | (1 << 8)) * 0x1001) | (0xf << 28),
+        # Fixed function blending, four components
+        2 | (3 << 3),
+        # RGBA8 TB pixel format / F32 register format
+        0 | (237 << 12) | (0 << 22) | (1 << 24),
+    ],
+
+    "preframe_surface": [
+        # Plane descriptor, generic, tiled, RAW32 clump format
+        10 | (1 << 4) | (1 << 8) | (2 << 24),
+        256 * 256 * 4,
+        "plane_0",
+        0,
+        0, 0,
+        0, # was 15,
+    ],
+
+    "preframe_table": [
+        # Texture descriptor, 2D, format
+        2 | (2 << 4) | (187 << (10 + 12)),
+        # Width, height
+        255 | (255 << 16),
+        # Swizzle, interleave
+        1672 | (1 << 12),
+        0,
+        "preframe_surface",
+        0, 0,
+
+        # Sampler descriptor, clamp to edge
+        1 | (9 << 8) | (9 << 12) | (9 << 16),
+        0, 0, 0, 0, 0, 0, 0,
+    ],
+
+    "preframe_resources": [
+        ("preframe_table", (1 << (32 + 24))), 0x40, 0,
+    ],
+
+    "dcds": [
+        # Clean fragment write, primitive barrier
+        (1 << 9) | (1 << 10),
+        # Sample mask of 0xffff, RT mask of 1
+        0x1ffff,
+        0, 0, # vertex array
+        0, 0, # unk
+        0, 0x3f800000, # min/max depth
+        0, 0, # unk
+        "preframe_zs", # depth/stencil
+        ("preframe_blend", 1), # blend (count == 1)
+        0, 0, # occlusion
+
+        # Shader environment:
+        0, # Attribute offset
+        2, # FAU count
+        0, 0, 0, 0, 0, 0, # unk
+        ("preframe_resources", 1), # Resources
+        "preframe_shader", # Shader
+        0, 0, # Thread storage
+        "fau", # FAU
+    ],
+
+    "framebuffer": [
+        1, 0, # Pre/post, downscale, layer index
+        0x10000, 0, # Argument
+        "ls_alloc", # Sample locations
+        "dcds", # DCDs
+        0x00ff00ff, # width / height
+        0, 0x00ff00ff, # bound min/max
+        # 32x32 tile size
+        # 4096 byte buffer allocation (maybe?)
+        (10 << 9) | (4 << 24),
+        0, # Disable S, ZS/CRC, Empty Tile, CRC
+        0, # Z Clear
+        "tiler_ctx", # Tiler
+
+        # Framebuffer padding
+        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+
+        # Render target
+        # R8G8B8A8 internal format
+        (1 << 26),
+        # Write Enable
+        # R8G8B8A8 colour format
+        # Linear block format
+        # 0123 swizzle
+        # Clean pixel write enable
+        1 | (19 << 3) | (1 << 8) | (0o3210 << 16) | (1 << 31),
+
+        # AFBC overlay
+        # No YTR, no split, no wide, no reverse, no front, no alpha
+        # RGBA8 compression mode
+        0 | (10 << 10),
+        0, 0, 0, 0, 0,
+
+        # RT Buffer
+        "plane_0",
+        256 * 4 * 16, # Row stride (for tiling)
+        0x400, # Surface stride / Body offset
+
+        # RT Clear
+        0x2e234589, 0, 0, 0,
+    ],
+
+    "index_buffer": [
+        0, 1, 2,
+        0, 2, 1,
+        1, 0, 2,
+        1, 2, 0,
+        2, 0, 1,
+        2, 1, 0,
+
+        #63, 64, 65,
+        1, 2, 3,
+        4, 5, 6,
+        12, 13, 14,
+        0, 1, 2,
+        4, 5, 6,
+        8, 9, 10,
+        3, 4, 5,
+    ],
+
+    "point_index": [x * 4 for x in range(32)] + [
+        0, 64, 440, 0,
+    ],
+
+    "position_data": [
+        ii(10.0), ii(10.0), ii(1.0), ii(1.0),
+    ],
+}
+
+# TODO: Use mako? Or just change the syntax for "LDM/STM"
+# and use f-strings again?
+
+cmds = """
+!cs 0
+resources fragment
+
+@ Bound min
+mov w2a, i16:0,0
+@ Bound max
+mov w2b, i16:255,255
+mov x28, $framebuffer+1
+
+slot 2
+
+fragment
+
+mov w4a, #0x0
+UNK 02 24, #0x4a0000ff0211
+wait 1
+
+mov x50, $ev
+evstr w5f, [x50], unk 0xfd, irq
+
+!raw sleep 20
+!memset plane_0 0 0 262144
+!raw sleep 200
+!dump plane_0 0 12
+!heatmap plane_0 0 262144 gran 4096 len 32768 stride 32768
+"""
+
+altcmds = """
+!cs 0
+
+@ Some time is required for the change to become active
+@ Just submitting a second job appears to be enough
+resources compute fragment tiler idvs
+mov x48, #0x6000000000
+heapctx x48
+
+!cs 0
+
+slot 3
+wait 3
+heapinc vt_start
+
+@ Base vertex count
+mov w24, 0
+@ Instance count
+mov w22, 1
+
+@ Vertex attribute stride
+mov x30, 0
+
+@ Primitive
+mov w38, 0x430000
+@@ Draw
+@ Pixel kill etc.
+@   Enable occlusion query
+@mov w39, 0xc000
+mov w39, 0
+@ Unk...
+mov w26, 0x1000
+@ Sample mask / render target mask
+mov w3a, 0x1ffff
+@ Min/max Z
+mov w2c, float:0
+mov w2d, float:1.0
+@ Depth/stencil
+mov x34, $idvs_zs
+@ Blend
+mov x32, $idvs_blend+1
+@ Occlusion
+mov x2e, $occlusion
+
+@ Primitive size
+mov x3c, float:3.75
+@ Fragment shader environment
+mov x14, $fragment_shader
+@ FAU count == 2
+movp x0c, $fau+0x0200000000000000
+
+@ Position shader environment
+mov x10, $position_shader
+
+mov x18, $thread_storage
+
+@ is this right?! "Vertex attribute stride" apparently?
+@  that was for pure tiler jobs, for idvs it messes up points/lines
+@  for some reason
+@mov x30, $position_data
+
+@ Tiler
+mov x28, $tiler_ctx
+
+@ Scissor min
+mov w2a, i16:0,0
+@ Scissor max
+mov w2b, i16:255,255
+
+mov w21, 18
+mov w27, 4096
+mov x36, $index_buffer
+
+idvs 0x4002, mode triangles, index uint32
+
+mov w21, 1 @36
+mov w27, 4096
+mov x36, $point_index
+
+@idvs 0x4a42, mode points, index uint32
+
+mov w21, 400000
+mov w21, 18
+@idvs 0x4a42, mode triangles, index none
+
+@idvs 0x4a42, mode points, index none
+@idvs 0x4a42, mode line-loop, index none
+
+flush_tiler
+wait 3
+heapinc vt_end
+
+mov x50, $ev
+evstr w5f, [x50], unk 0xfd, irq
+
+UNK 00 24, #0x5f0000000233
+wait all
+
+!dump64 tiler_heap 0 4096
+@!dump idk 0 1048576
+@!dump position_data 0 4096
+
+!cs 0
+
+UNK 00 24, #0x5f0000000233
+wait all
+
+slot 4
+wait 4
+heapinc vt_start
+
+mov x28, $tiler_ctx2
+idvs 0x4002, mode triangles, index none
+flush_tiler
+wait 4
+heapinc vt_end
+
+UNK 00 24, #0x5f0000000233
+wait all
+
+mov x50, $ev
+evstr w5f, [x50], unk 0xfd, irq
+
+!dump64 tiler_heap 0 4096
+
+!cs 0
+
+mov x50, $ev
+
+@ Bound min
+mov w2a, i16:0,0
+@ Bound max
+mov w2b, i16:255,255
+mov x28, $framebuffer+1
+@ Tile enable map
+mov x2c, $x
+mov x2e, 64
+
+mov w40, 1
+str w40, [x2c]
+@str w40, [x2c, 128]
+
+@ Use tile enable map
+@fragment tem 1
+
+fragment
+
+@ Does this actually do anytihng?
+mov x48, $tiler_ctx
+ldr x4a, [x48, 40]
+ldr x4c, [x48, 48]
+wait 0,4
+UNK 02 0b, 0x4a4c00100001
+
+mov x48, $tiler_ctx2
+ldr x4a, [x48, 40]
+ldr x4c, [x48, 48]
+wait 0,4
+UNK 02 0b, 0x4a4c00100001
+
+UNK 02 24, #0x5f0000f80211
+@UNK 00 24, #0x5f0000000233
+wait 1
+
+mov x54, $plane_0
+ldr x56, [x54]
+wait 0
+
+mov x52, $y
+str x56, [x52]
+
+evstr w5f, [x50], unk 0xfd, irq
+
+!raw td
+!fdump heap 0 1048576
+!tiler heap 0 1048576
+
+
+@!dump rt_buffer 0 4096
+!dump y 0 4096
+@!dump plane_0 0 524288
+@!heatmap plane_0 0 524288 gran 0x80 len 0x200 stride 0x4000
+!heatmap plane_0 0 8192 gran 0x04 len 0x20 stride 0x400
+!dump occlusion 0 4096
+@!dump ssbo 0 4096
+
+!dump64 tiler_heap 0 4096
+!dump tiler_ctx 0 4096
+!dump tiler_ctx2 0 4096
+
+@!fdump heap 0 1048576
+
+!cs 0
+
+slot 3
+wait 3
+heapinc vt_start
+
+mov x28, $tiler_ctx3
+mov w2c, float:0
+mov w2d, float:1.0
+mov x2e, $occlusion
+
+idvs 0x4002, mode triangles, index none
+flush_tiler
+wait 3
+heapinc vt_end
+
+UNK 00 24, #0x5f0000000233
+wait all
+
+mov x50, $ev
+evstr w5f, [x50], unk 0xfd, irq
+
+!dump64 tiler_heap 0 4096
+!dump tiler_ctx 0 4096
+!raw td
+
+"""
+
+docopy = """
+ldr {w00-w0f}, [x52]
+ldr {w10-w1f}, [x52, 64]
+ldr {w20-w2f}, [x52, 128]
+ldr {w30-w3f}, [x52, 192]
+add x52, x52, 256
+
+loop:
+wait 0
+
+str {w00-w0f}, [x54]
+ldr {w00-w0f}, [x52]
+str {w10-w1f}, [x54, 64]
+ldr {w10-w1f}, [x52, 64]
+str {w20-w2f}, [x54, 128]
+ldr {w20-w2f}, [x52, 128]
+str {w30-w3f}, [x54, 192]
+ldr {w30-w3f}, [x52, 192]
+
+add x54, x54, 256
+add x52, x52, 256
+add x50, x50, -256
+
+b.ne w50, loop
+b.ne w51, loop
+"""
+
+oldcmds = f"""
+!cs 0
+
+mov x50, 0x8000000
+
+mov x52, $from
+mov x54, $to
+mov x56, $x
+mov x58, $ev
+mov x5a, $y
+
+str cycles, [x56]
+{docopy}
+str cycles, [x56, 8]
+
+UNK 00 24, #0x5f0000000233
+evstr w5f, [x58], unk 0xfd, irq
+
+!cs 1
+
+mov x50, 0x8000000
+
+mov x52, $from
+mov x54, $to
+mov x56, $x
+mov x58, $ev
+mov x5a, $y
+
+add x52, x52, 0x8000000
+add x54, x54, 0x8000000
+add x56, x56, 32
+
+nop
+nop
+
+str cycles, [x56]
+{docopy}
+str cycles, [x56, 8]
+
+UNK 00 24, #0x5f0000000233
+evstr w5f, [x58], unk 0xfd, irq
+
+!delta x 0 4096
+"""
+
+oldcmds = """
+!cs 0
+endpt compute
+!cs 0
+
+@ Workgroup size 1x1x1, merging allowed
+mov w21, 0x80000000
+
+@ Workgroup count 1x1x1
+mov w25, 1
+mov w26, 1
+mov w27, 1
+
+@ Offset 0,0,0
+mov w22, 0
+mov w23, 0
+mov w24, 0
+
+@ TODO: offset x/y/z
+
+@ Resources
+mov x06, 0
+
+@ Shader
+mov x16, $shader
+
+@ Local storage
+mov x1e, $ls
+
+@ FAU
+movp x0e, $fau+0x0200000000000000
+
+slot 2
+wait 2
+
+UNK 0400000000008200
+
+mov x58, $fau
+ldr x56, [x58]
+wait 0
+
+@mov w4a, 0
+
+@slot 6
+@mov x54, $x
+@UNK 02 24, #0x4a0000f80211
+@ldr x52, [x56]
+@wait 0,1
+@str x52, [x54]
+
+mov w40, 60
+1: add w40, w40, -1
+
+@mov w4a, #0x0
+@UNK 02 24, #0x4a0000f80211
+@wait 1
+
+@mov w54, #0
+@UNK 00 24, #0x540000000233
+@wait all
+
+slot 2
+wait 2
+
+add w22, w22, 1
+@UNK 0400ff0000008200
+
+@b.ne w40, 1b
+
+!dump x 0 4096
+!dump y 0 4096
+!dump ev 0 4096
+"""
+
+oldcmds = """
+!cs 0
+
+mov x48, $x
+
+mov w21, 0x80000000
+mov w25, 1
+mov w26, 1
+mov w27, 1
+
+movp x0e, $fau+0x0200000000000000
+
+@ Write FAUs
+@add x0e, x48, 64
+@mov x50, $ev
+@str x50, [x0e]
+@mov x30, 10
+@str x30, [x0e, 8]
+@add w0f, w0f, 0x02000000
+
+@ Write shader descriptor
+@add x16, x48, 128
+@mov x30, 0x118
+@str x30, [x16]
+@mov x30, $compute
+@str x30, [x16, 8]
+
+wait 0
+
+add x1e, x48, 192
+
+mov x30, $y
+@regdump x30
+@mov x30, 0
+
+resources compute
+slot 2
+mov w54, #0xffffe0
+UNK 00 24, #0x540000000233
+
+wait all
+
+mov x54, 0
+mov w56, 0
+mov w5d, 1
+
+slot 2
+wait 2
+wait 2
+regdump x30
+UNK 0400ff0000008200
+add x30, x30, 0x200
+regdump x30
+slot 2
+wait 2
+
+mov w40, 1000
+1: add w40, w40, -1
+str cycles, [x50, 32]
+b.ne w40, 1b
+
+wait 0
+wait all
+
+@ 6 / 10 / 14
+mov w40, 1
+1: add w40, w40, -1
+UNK 0400ff0000000200
+b.ne w40, 1b
+
+mov w40, 1000
+1: add w40, w40, -1
+str cycles, [x50, 32]
+b.ne w40, 1b
+
+mov w42, 200
+mov w40, 100
+1: add w40, w40, -1
+@wait all
+@UNK 0400ff0000008001 @ compute
+
+@UNK 0400ff0000000001
+@UNK 2501504200000004 @ evadd
+@UNK 3 24, #0x4a0000000211
+
+@wait all
+b.ne w40, 1b
+
+@UNK 2601504200000004
+
+str cycles, [x50, 40]
+str cycles, [x50, 48]
+UNK 02 24, #0x4a0000000211
+wait 0
+
+add x5c, x50, 64
+evadd w5e, [x5c], unk 0xfd
+evadd w5e, [x5c], unk 0xfd, irq, unk0
+
+!dump x 0 4096
+!dump y 0 4096
+!delta ev 0 4096
+"""
+
+altcmds = """
+!cs 0
+!alloc x 4096
+!alloc ev 4096 0x8200f
+!alloc ev2 4096 0x8200f
+
+mov x10, $x
+UNK 00 30, #0x100000000000
+add x12, x10, 256
+str cycles, [x12]
+mov x5a, $ev2
+mov x48, 0
+mov w4a, 0
+slot 3
+wait 3
+UNK 00 31, 0
+mov x48, $ev
+mov w4a, 0x4321
+add x46, x48, 64
+mov w42, 0
+
+str cycles, [x12, 8]
+UNK 01 26, 0x484a00000005
+str cycles, [x12, 16]
+UNK 01 26, 0x484a00000005
+str cycles, [x12, 24]
+
+nop
+
+mov w10, 10000
+1:
+UNK 01 26, 0x484a00000005
+add w10, w10, -1
+b.ne w10, 1b
+str cycles, [x12, 32]
+
+mov w10, 10000
+1:
+UNK 01 26, 0x484a00000005
+@UNK 02 24, #0x420000000211
+add w10, w10, -1
+b.ne w10, 1b
+str cycles, [x12, 40]
+
+ldr x16, [x48, 0]
+wait 0
+str x16, [x48, 16]
+
+UNK 00 31, 0x100000000
+
+mov w4a, #0x0
+UNK 02 24, #0x4a0000000211
+
+mov w5e, 1
+add x5c, x5a, 0x100
+UNK 01 25, 0x5c5e00f80001
+
+!delta x 0 4096
+!dump ev 0 4096
+!dump ev2 0 4096
+"""
+
+altcmds = """
+!cs 0
+!alloc x 4096
+!alloc ev 4096 0x8200f
+
+iter vertex
+slot 2
+
+mov x40, $x
+mov w10, 1
+mov x48, 0
+mov w4a, 0
+call w4a, x48
+  nop
+  nop
+  nop
+  mov x20, $.
+@  movp x22, 0x0126000011223344
+  movp x22, 0x1600000060000001
+  str x22, [x20, 56]
+  1: nop
+  b 1b
+  nop
+  add x40, x40, #256
+  regdump x40
+
+mov x5a, #0x5ff7fd6000
+mov x48, $ev
+mov x40, #0x5ff7fd6000
+mov w54, #0x1
+UNK 00 24, #0x540000000233
+wait 0
+slot 6
+@UNK 00 31, #0x0
+UNK 00 09, #0x0
+wait 6
+@UNK 00 31, #0x100000000
+mov x4a, x40
+UNK 01 26, 0x484a00040001
+
+!dump x 0 4096
+@!dump ev 0 4096
+@!delta x 0 4096
+"""
+
+cycletest = """
+mov w10, 10
+1:
+str cycles, [x5c]
+add x5c, x5c, 8
+add w10, w10, -1
+mov w11, 100000
+
+inner:
+add w11, w11, -1
+b.ne w11, inner
+
+b.ne w10, 1b
+"""
+
+def get_cmds(cmd):
+    return cmds.replace("{cmd}", str(cmd))
+
+def assemble_shader(text):
+    lines = text.strip().split("\n")
+    lines = [l for l in lines if len(l) > 0 and l[0] not in "#@"]
+    return [asm.parse_asm(ln) for ln in lines]
+
+class Buffer:
+    id = 0
+
+    def __init__(self):
+        self.id = Buffer.id
+        Buffer.id += 1
+
+def resolve_rel(to, branch):
+    return (to - branch) // 8 - 1
+
+def to_int16(value):
+    assert(value < 36768)
+    assert(value >= -32768)
+    return value & 0xffff
+
+class Level(Buffer):
+    def __init__(self, indent):
+        super().__init__()
+
+        self.indent = indent
+        self.buffer = []
+        self.call_addr_offset = None
+        self.call_len_offset = None
+
+        self.labels = {}
+        self.label_refs = []
+        # Numeric labels can be reused, so have to be handled specially.
+        self.num_labels = {}
+        self.num_refs = {}
+
+    def offset(self):
+        return len(self.buffer) * 8
+
+    def __repr__(self):
+        buf = " ".join(hex(x) for x in self.buffer)
+        return f"buffer {self.id} {self.offset()} 0x200f {buf}"
+
+    def buffer_add_value(self, offset, value):
+        self.buffer[offset // 8] += value
+
+    def process_relocs(self, refs, to=None):
+        for ref, offset, type_ in refs:
+            assert(type_ == "rel")
+
+            if to is None:
+                goto = self.labels[ref]
+            else:
+                goto = to
+
+            value = to_int16(resolve_rel(goto, offset))
+            self.buffer_add_value(offset, value)
+
+    def finish(self):
+        self.process_relocs(self.label_refs)
+
+class Alloc(Buffer):
+    def __init__(self, size, flags=0x280f):
+        super().__init__()
+
+        self.size = size
+        self.flags = flags
+        self.buffer = []
+
+    def __repr__(self):
+        buf = " ".join(hex(x) for x in self.buffer)
+        return f"buffer {self.id} {self.size} {hex(self.flags)} {buf}"
+
+def fmt_reloc(r, name="reloc"):
+    dst, offset, src, src_offset = r
+    return f"{name} {dst}+{offset} {src}+{src_offset}"
+
+def fmt_exe(e):
+    return " ".join(str(x) for x in e)
+
+class Context:
+    def __init__(self):
+        self.levels = []
+        self.l = None
+
+        self.allocs = {}
+        self.completed = []
+        self.reloc = []
+        self.reloc_split = []
+
+        self.exe = []
+        self.last_exe = None
+
+        self.is_call = False
+
+    def set_l(self):
+        if len(self.levels):
+            self.l = self.levels[-1]
+
+    def pop_until(self, indent):
+        while self.l.indent != indent:
+            l = self.levels.pop()
+            self.completed.append(l)
+
+            self.set_l()
+            if not len(self.levels):
+                return
+
+            buf_len = l.offset()
+
+            r = self.l
+            self.reloc.append((r.id, r.call_addr_offset * 8, l.id, 0))
+            r.buffer[r.call_len_offset] = (
+                (r.buffer[r.call_len_offset] & (0xffff << 48)) +
+                buf_len)
+            r.buffer[r.call_addr_offset] &= (0xffff << 48)
+
+            r.call_addr_offset = None
+            r.call_len_offset = None
+
+    def flush_exe(self):
+        ind = self.levels[0].indent
+
+        self.pop_until(ind)
+        if len(self.levels[0].buffer):
+            l = self.levels.pop()
+            l.finish()
+            self.completed.append(l)
+
+            self.levels.append(Level(ind))
+            self.set_l()
+
+        if not len(self.exe):
+            return
+
+        if self.last_exe is None:
+            print("# Trying to add multiple CSs to an exe line, becoming confused")
+            return
+
+        if len(self.completed):
+            p = self.completed[-1]
+            assert(p.indent == ind)
+
+            self.exe[self.last_exe] += [p.id, p.offset()]
+
+        self.last_exe = None
+
+    def add_shaders(self, shaders):
+        for sh in shaders:
+            qwords = assemble_shader(shaders[sh])
+            sh = sh.lower()
+
+            a = Alloc(len(qwords) * 8, flags=0x2017)
+            a.buffer = qwords
+            self.allocs[sh] = a
+
+    def add_memory(self, memory):
+        for m in memory:
+            f = memory[m]
+            if isinstance(f, int):
+                size, flags = f, 0x280f
+            else:
+                size, flags = f
+            self.allocs[m] = Alloc(size, flags)
+
+    def add_descriptors(self, descriptors):
+        for d in descriptors:
+            words = descriptors[d]
+            a = Alloc(0)
+
+            buf = []
+            for w in words:
+                if isinstance(w, int):
+                    buf.append(w)
+                else:
+                    if isinstance(w, str):
+                        alloc, offset = w, 0
+                    else:
+                        alloc, offset = w
+                    ref = self.allocs[alloc]
+                    self.reloc.append((a.id, len(buf) * 4,
+                                       ref.id, offset))
+                    buf.append(0)
+                    buf.append(0)
+
+            it = iter(buf)
+            a.buffer = [x | (y << 32) for x, y in zip(it, it)]
+            a.size = len(a.buffer) * 8
+            self.allocs[d] = a
+
+    def interpret(self, text):
+        text = text.split("\n")
+
+        old_indent = None
+
+        for orig_line in text:
+            #print(orig_line, file=sys.stderr)
+
+            line = orig_line.split("@")[0].expandtabs().rstrip().lower()
+            if not line:
+                continue
+
+            indent = len(line) - len(line.lstrip())
+            line = line.lstrip()
+
+            if old_indent is None:
+                self.levels.append(Level(indent))
+            elif indent != old_indent:
+                if indent > old_indent:
+                    assert(self.is_call)
+
+                    self.levels.append(Level(indent))
+                else:
+                    self.pop_until(indent)
+
+            self.set_l()
+
+            old_indent = indent
+            self.is_call = False
+
+            given_code = None
+
+            # TODO: Check against this to test the disassembler?
+            if re.match(r"[0-9a-f]{16} ", line):
+                given_code = int(line[:16], 16)
+                line = line[16:].lstrip()
+
+            s = [x.strip(",") for x in line.split()]
+
+            if s[0].endswith(":") or (len(s) == 1 and is_num(s[0])):
+                label = s[0]
+                if s[0].endswith(":"):
+                    label = label[:-1]
+
+                if is_num(label):
+                    label = int(label)
+                    if label in self.l.num_refs:
+                        self.l.process_relocs(self.l.num_refs[label], self.l.offset())
+                        del self.l.num_refs[label]
+                    self.l.num_labels[label] = self.l.offset()
+                else:
+                    if label in self.l.labels:
+                        print("Label reuse is not supported for non-numeric labels")
+                    self.l.labels[label] = self.l.offset()
+
+                s = s[1:]
+                if not len(s):
+                    continue
+
+            for i in range(len(s)):
+                if s[i].startswith("$"):
+                    name, *offset = s[i][1:].split("+")
+                    if name == ".":
+                        buf = self.l
+                    else:
+                        buf = self.allocs[name]
+                    if len(offset):
+                        assert(len(offset) == 1)
+                        offset = int(offset[0], 0)
+                    else:
+                        offset = 0
+
+                    if s[0] == "movp":
+                        rels = self.reloc_split
+                    else:
+                        rels = self.reloc
+
+                    rels.append((self.l.id, self.l.offset(),
+                                 buf.id, offset))
+                    s[i] = "#0x0"
+
+            def is_num(str):
+                return re.fullmatch(r"[0-9]+", str)
+
+            def hx(word):
+                return int(word, 16)
+
+            def reg(word):
+                return hx(word[1:])
+
+            def val(word):
+                if word.startswith("float:"):
+                    return ii(float(word.split(":")[1]))
+                elif word.startswith("i16:"):
+                    lo, hi = word.split(":")[1].split(",")
+                    lo, hi = val(lo), val(hi)
+                    assert(lo < (1 << 16))
+                    assert(hi < (1 << 16))
+                    return (lo & 0xffff) | (hi << 16)
+
+                value = int(word.strip("#"), 0)
+                assert(value < (1 << 48))
+                return value
+
+            sk = True
+
+            if s[0] == "!cs":
+                assert(len(s) == 2)
+                self.flush_exe()
+                self.last_exe = len(self.exe)
+                self.exe.append(["exe", int(s[1])])
+                continue
+            elif s[0] == "!parallel":
+                assert(len(s) == 2)
+                self.flush_exe()
+                self.last_exe = len(self.exe) - 1
+                self.exe[-1] += [int(s[1])]
+                continue
+            elif s[0] == "!alloc":
+                assert(len(s) == 3 or len(s) == 4)
+                alloc_id = s[1]
+                size = int(s[2])
+                flags = val(s[3]) if len(s) == 4 else 0x280f
+                self.allocs[alloc_id] = Alloc(size, flags)
+                continue
+            elif s[0] in ("!dump", "!dump64", "!fdump", "!delta", "!tiler"):
+                assert(len(s) == 4)
+                alloc_id = s[1]
+                offset = val(s[2])
+                size = val(s[3])
+                mode = {
+                    "!dump": "hex",
+                    "!dump64": "hex64",
+                    "!fdump": "filehex",
+                    "!delta": "delta",
+                    "!tiler": "tiler",
+                }[s[0]]
+                self.exe.append(("dump", self.allocs[alloc_id].id,
+                                 offset, size, mode))
+                continue
+            elif s[0] == "!heatmap":
+                assert(len(s) == 10)
+                assert(s[4] == "gran")
+                assert(s[6] == "len")
+                assert(s[8] == "stride")
+                alloc_id = s[1]
+                offset = val(s[2])
+                size = val(s[3])
+                granularity = val(s[5])
+                length = val(s[7])
+                stride = val(s[9])
+                mode = "heatmap"
+                self.exe.append(("heatmap", self.allocs[alloc_id].id,
+                                 offset, size, granularity, length, stride))
+                continue
+            elif s[0] == "!memset":
+                assert(len(s) == 5)
+                alloc_id = s[1]
+                offset = val(s[2])
+                value = val(s[3])
+                size = val(s[4])
+                self.exe.append(("memset", self.allocs[alloc_id].id,
+                                 offset, value, size))
+                continue
+            elif s[0] == "!raw":
+                self.exe.append(s[1:])
+                continue
+            elif s[0] == "movp":
+                assert(len(s) == 3)
+                assert(s[1][0] == "x")
+                addr = reg(s[1])
+                # Can't use val() as that has a max of 48 bits
+                value = int(s[2].strip("#"), 0)
+
+                self.l.buffer.append((2 << 56) | (addr << 48) | (value & 0xffffffff))
+                self.l.buffer.append((2 << 56) | ((addr + 1) << 48)
+                                       | ((value >> 32) & 0xffffffff))
+                continue
+            elif s[0] == "regdump":
+                assert(len(s) == 2)
+                assert(s[1][0] == "x")
+                dest = reg(s[1])
+
+                # Number of registers to write per instruction
+                regs = 16
+
+                cmd = 21
+                value = (dest << 40) | (((1 << regs) - 1) << 16)
+
+                for i in range(0, 0x60, regs):
+                    code = (cmd << 56) | (i << 48) | value | (i << 2)
+                    self.l.buffer.append(code)
+
+                del cmd, value
+                continue
+
+            elif s[0] == "unk":
+                if len(s) == 2:
+                    h = hx(s[1])
+                    cmd = h >> 56
+                    addr = (h >> 48) & 0xff
+                    value = h & 0xffffffffffff
+                else:
+                    assert(len(s) == 4)
+                    cmd = hx(s[2])
+                    addr = hx(s[1])
+                    value = val(s[3])
+            elif s[0] == "nop":
+                if len(s) == 1:
+                    addr = 0
+                    value = 0
+                    cmd = 0
+                else:
+                    assert(len(s) == 3)
+                    addr = hx(s[1])
+                    value = val(s[2])
+                    cmd = 0
+            elif s[0] == "mov" and s[2][0] in "xw":
+                # This is actually an addition command
+                assert(len(s) == 3)
+                assert(s[1][0] == s[2][0])
+                cmd = { "x": 17, "w": 16 }[s[1][0]]
+                addr = reg(s[1])
+                value = reg(s[2]) << 40
+            elif s[0] == "mov":
+                assert(len(s) == 3)
+                cmd = { "x": 1, "w": 2 }[s[1][0]]
+                addr = reg(s[1])
+                value = val(s[2])
+            elif s[0] == "add":
+                assert(len(s) == 4)
+                assert(s[1][0] == s[2][0])
+                assert(s[1][0] in "wx")
+                cmd = 16 if s[1][0] == "w" else 17
+                addr = reg(s[1])
+                value = (reg(s[2]) << 40) | (val(s[3]) & 0xffffffff)
+            elif s[0] == "resources":
+                assert(len(s) >= 2)
+                types = ["compute", "fragment", "tiler", "idvs"]
+                cmd = 34
+                addr = 0
+                value = 0
+                for t in s[1:]:
+                    if t in types:
+                        value |= 1 << types.index(t)
+                    else:
+                        value |= int(t, 0)
+            elif s[0] == "fragment":
+                cmd = 7
+                addr = 0
+                value = 0
+                if len(s) != 1:
+                    arg_map = {
+                        "tem": {"0": 0, "1": 1},
+                        "render": {
+                            "z_order": 0,
+                            "horizontal": 0x10,
+                            "vertical": 0x20,
+                            "reverse_horizontal": 0x50,
+                            "reverse_vertical": 0x60,
+                        },
+                        "unk": {"0": 0, "1": 1 << 32},
+                    }
+                    for arg, val in zip(s[1::2], s[2::2]):
+                        value |= arg_map[arg][val]
+            elif s[0] == "wait":
+                assert(len(s) == 2)
+                cmd = 3
+                addr = 0
+                if s[1] == "all":
+                    value = 255
+                else:
+                    value = sum(1 << int(x) for x in s[1].split(","))
+                value <<= 16
+            elif s[0] == "slot":
+                assert(len(s) == 2)
+                cmd = 23
+                addr = 0
+                value = int(s[1], 0)
+            elif s[0] == "add":
+                # TODO: unk variant
+                assert(len(s) == 4)
+                assert(s[1][0] == "x")
+                assert(s[2][0] == "x")
+                cmd = 17
+                addr = reg(s[1])
+                v = val(s[3])
+                assert(v < (1 << 32))
+                assert(v >= (-1 << 31))
+                value = (reg(s[2]) << 40) | (v & 0xffffffff)
+            elif s[0] == "idvs":
+                assert(len(s) == 6)
+                unk = val(s[1])
+                assert(s[2] == "mode")
+                modes = {
+                    "none": 0,
+                    "points": 1,
+                    "lines": 2,
+                    "line-strip": 4,
+                    "line-loop": 6,
+                    "triangles": 8,
+                    "triangle-strip": 10,
+                    "triangle-fan": 12,
+                    "polygon": 13,
+                    "quads": 14,
+                }
+                if s[3] in modes:
+                    mode = modes[s[3]]
+                else:
+                    mode = int(s[3])
+                assert(s[4] == "index")
+                itypes = {
+                    "none": 0,
+                    "uint8": 1,
+                    "uint16": 2,
+                    "uint32": 3,
+                }
+                if s[5] in itypes:
+                    index = itypes[s[5]]
+                else:
+                    index = int(s[5])
+
+                cmd = 6
+                addr = 0
+                value = (unk << 32) | (index << 8) | mode
+            elif s[0] == "flush_tiler":
+                assert(len(s) == 1)
+                cmd = 9
+                addr = 0
+                value = 0
+            elif s[0] == "str" and s[1] in ("cycles", "timestamp"):
+                assert(len(s) == 3 or len(s) == 4)
+                assert(s[2][0] == "[")
+                assert(s[-1][-1] == "]")
+                s = [x.strip("[]") for x in s]
+                assert(s[2][0] == "x")
+
+                type_ = 1 if s[1] == "cycles" else 0
+                dest = reg(s[2])
+                if len(s) == 4:
+                    offset = val(s[3])
+                else:
+                    offset = 0
+
+                cmd = 40
+                addr = 0
+                value = (dest << 40) | (type_ << 32) | to_int16(offset)
+            elif s[0] in ("ldr", "str"):
+                reglist = s[1]
+                if reglist[0] == "{":
+                    end = [x[-1] for x in s].index("}")
+                    reglist = s[1:end + 1]
+                    s = s[:1] + s[end:]
+
+                assert(len(s) == 3 or len(s) == 4)
+                assert(s[2][0] == "[")
+                assert(s[-1][-1] == "]")
+                s = [x.strip("[]") for x in s]
+                assert(s[2][0] == "x")
+
+                if isinstance(reglist, str):
+                    assert(reglist[0] in "xw")
+                    src = reg(reglist)
+                    mask = 3 if reglist[0] == "x" else 1
+                else:
+                    src = None
+                    mask = 0
+
+                    for r in ",".join(reglist).strip("{}").split(","):
+                        r = r.split("-")
+                        assert(len(r) in (1, 2))
+                        regno = [reg(x) for x in r]
+
+                        if src is None:
+                            src = regno[0]
+
+                        if len(r) == 1:
+                            assert(r[0][0] in "xw")
+                            new = 3 if r[0][0] == "x" else 1
+                            new = (new << regno[0]) >> src
+                        else:
+                            assert(regno[1] > regno[0])
+                            new = ((2 << regno[1]) - (1 << regno[0])) >> src
+
+                        assert(new < (1 << 16))
+                        assert(mask & new == 0)
+                        mask |= new
+
+                # Name is correct for str, but inverted for ldr
+                # (The same holds for src above)
+                dest = reg(s[2])
+                if len(s) == 4:
+                    offset = val(s[3])
+                else:
+                    offset = 0
+
+                cmd = 20 if s[0] == "ldr" else 21
+                addr = src
+                value = (dest << 40) | (mask << 16) | to_int16(offset)
+            elif s[0] == "b" or s[0].startswith("b."):
+                # For unconditional jumps, use w00 as a source register if it
+                # is not specified
+                if s[0] == "b" and (len(s) == 2 or
+                                    (len(s) == 3 and
+                                     s[1] in ("back", "skip"))):
+                    s = [s[0], "w00", *s[1:]]
+
+                assert(len(s) == 3 or (len(s) == 4 and s[2] in ("back", "skip")))
+                assert(s[1][0] == "w")
+
+                ops = {
+                    "b.le": 0, "b.gt": 1,
+                    "b.eq": 2, "b.ne": 3,
+                    "b.lt": 4, "b.ge": 5,
+                    "b": 6, "b.al": 6,
+                }
+
+                src = reg(s[1])
+                if len(s) == 4:
+                    offset = val(s[3])
+                    if s[2] == "back":
+                        offset = -1 - offset
+                else:
+                    label = s[2]
+                    if re.fullmatch(r"[0-9]+b", label):
+                        label = int(label[:-1])
+                        assert(label in self.l.num_labels)
+                        offset = resolve_rel(self.l.num_labels[label],
+                                             self.l.offset())
+                    elif re.fullmatch(r"[0-9]+f", label):
+                        label = int(label[:-1])
+                        if label not in self.l.num_refs:
+                            self.l.num_refs[label] = []
+                        self.l.num_refs[label].append((label, self.l.offset(), "rel"))
+                        offset = 0
+                    else:
+                        assert(not re.fullmatch(r"[0-9]+", label))
+                        self.l.label_refs.append((label, self.l.offset(), "rel"))
+                        offset = 0
+
+                cmd = 22
+                addr = 0
+                value = (src << 40) | (ops[s[0]] << 28) | to_int16(offset)
+
+            elif s[0] in ("evadd", "evstr"):
+                assert(len(s) in range(5, 8))
+                assert(s[1][0] in "wx")
+                assert(s[2].startswith("[x"))
+                assert(s[2][-1] == "]")
+                assert(s[3] == "unk")
+                s = [x.strip("[]()") for x in s]
+
+                val = reg(s[1])
+                dst = reg(s[2])
+                mask = hx(s[4])
+                irq = "irq" not in s
+                unk0 = "unk0" in s
+
+                if s[1][0] == "w":
+                    cmd = 37 if s[0] == "evadd" else 38
+                else:
+                    cmd = 51 if s[0] == "evadd" else 52
+                addr = 1
+                value = ((dst << 40) | (val << 32) | (mask << 16) |
+                         (irq << 2) | unk0)
+            elif s[0].split(".")[0] == "evwait":
+                for mod in s[0].split(".")[1:]:
+                    assert(mod in {"lo", "hi", "inherit", "no_error"})
+                assert(len(s) == 3)
+                assert(s[1][0] in "wx")
+                assert(s[2][0] == "[")
+                assert(s[-1][-1] == "]")
+                s = [x.strip("[]()") for x in s]
+                src = reg(s[2])
+                val = reg(s[1])
+                cond = 1 if ".hi" in s[0] else 0
+                error = 1 if ".no_error" in s[0] else 0
+
+                cmd = 53 if s[1][0] == "x" else 39
+                addr = 0
+                value = (src << 40) | (val << 32) | (cond << 28) | error
+            elif s[0] in ("call", "tailcall"):
+                ss = [x for x in s if x.find('(') == -1 and x.find(')') == -1]
+                assert(len(ss) == 3)
+                assert(ss[1][0] == "w")
+                assert(ss[2][0] == "x")
+                cmd = { "call": 32, "tailcall": 33 }[s[0]]
+                addr = 0
+                num = reg(ss[1])
+                target = reg(ss[2])
+                value = (num << 32) | (target << 40)
+
+                l = self.l
+
+                cur = len(l.buffer)
+                for ofs in range(cur - 2, cur):
+                    if l.buffer[ofs] >> 48 == 0x100 + target:
+                        l.call_addr_offset = ofs
+                    if l.buffer[ofs] >> 48 == 0x200 + num:
+                        l.call_len_offset = ofs
+                assert(l.call_addr_offset is not None)
+                assert(l.call_len_offset is not None)
+
+                self.is_call = True
+            elif s[0] == "heapctx":
+                assert(len(s) == 2)
+                assert(s[1][0] == "x")
+                cmd = 48
+                addr = 0
+                value = reg(s[1]) << 40
+            elif s[0] == "heapinc":
+                assert(len(s) == 2)
+                modes = {
+                    "vt_start": 0,
+                    "vt_end": 1,
+                    "frag_end": 3,
+                }
+                if s[1] in modes:
+                    mode = modes[s[1]]
+                else:
+                    mode = int(s[1])
+                cmd = 49
+                addr = 0
+                value = mode << 32
+            else:
+                print("Unknown command:", orig_line, file=sys.stderr)
+                # TODO remove
+                cmd = 0
+                addr = 0
+                value = 0
+                sk = False
+                pass
+
+            code = (cmd << 56) | (addr << 48) | value
+
+            if given_code and code != given_code:
+                print(f"Mismatch! {hex(code)} != {hex(given_code)}, {orig_line}")
+
+            self.l.buffer.append(code)
+
+            del cmd, addr, value
+
+            if False and not sk:
+                print(orig_line, file=sys.stderr)
+                print(indent, s, hex(code) if sk else "", file=sys.stderr)
+
+        self.pop_until(self.levels[0].indent)
+        self.flush_exe()
+
+    def __repr__(self):
+        r = []
+        r += [str(self.allocs[x]) for x in self.allocs]
+        r += [str(x) for x in self.completed]
+        r += [fmt_reloc(x) for x in self.reloc]
+        r += [fmt_reloc(x, name="relsplit") for x in self.reloc_split]
+        r += [fmt_exe(x) for x in self.exe]
+        return "\n".join(r)
+
+def interpret(text):
+    c = Context()
+    c.add_shaders(shaders)
+    c.add_memory(memory)
+    c.add_descriptors(descriptors)
+    c.interpret(text)
+    #print(str(c))
+    return str(c)
+
+def run(text, capture=False):
+    if capture:
+        cap = {"stdout": subprocess.PIPE, "stderr": subprocess.STDOUT}
+    else:
+        cap = {}
+
+    i = interpret(text) + "\n"
+
+    with open("/tmp/csf.cmds", "w") as f:
+        f.write(i)
+
+    # TODO: Keep seperate or merge stdout/stderr?
+    ret = subprocess.run(["csf_test", "/dev/stdin"],
+                         input=i, text=True, **cap)
+    if ret.stderr is None:
+        ret.stderr = ""
+    if ret.stdout is None:
+        ret.stdout = ""
+    return ret.stderr + ret.stdout
+
+def rebuild():
+    try:
+        p = subprocess.run(["rebuild-mesa"])
+        if p.returncode != 0:
+            return False
+    except FileNotFoundError:
+        pass
+    return True
+
+def go(text):
+    #print(interpret(text))
+    #return
+
+    if not rebuild():
+        return
+
+    print(run(text))
+    #subprocess.run("ls /tmp/fdump.????? | tail -n2 | xargs diff -U3 -s",
+    #               shell=True)
+
+os.environ["CSF_QUIET"] = "1"
+
+go(get_cmds(""))
+
+#for c in range(1, 64):
+#    val = c
+#    ret = run(get_cmds(ii(val)))
+#    print(str(val) + '\t' + [x for x in ret.split("\n") if x.startswith("0FFF10")][0])
+
+#rebuild()
+#for c in range(256):
+#    print(c, end=":")
+#    sys.stdout.flush()
+#    cmd = f"UNK 00 {hex(c)[2:]} 0x00000000"
+#    run(get_cmds(cmd))
+
+#interpret(cmds)
+#go(cmds)
diff -urN mesa-23.0.0/src/panfrost/csf_test/mali_base_csf_kernel.h mesa/src/panfrost/csf_test/mali_base_csf_kernel.h
--- mesa-23.0.0/src/panfrost/csf_test/mali_base_csf_kernel.h	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/csf_test/mali_base_csf_kernel.h	2023-03-06 19:19:32.634308349 +0100
@@ -0,0 +1,719 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ *
+ * (C) COPYRIGHT 2020-2021 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU license.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ */
+
+#ifndef _UAPI_BASE_CSF_KERNEL_H_
+#define _UAPI_BASE_CSF_KERNEL_H_
+
+#include <linux/types.h>
+
+/* Memory allocation, access/hint flags.
+ *
+ * See base_mem_alloc_flags.
+ */
+
+/* IN */
+/* Read access CPU side
+ */
+#define BASE_MEM_PROT_CPU_RD ((base_mem_alloc_flags)1 << 0)
+
+/* Write access CPU side
+ */
+#define BASE_MEM_PROT_CPU_WR ((base_mem_alloc_flags)1 << 1)
+
+/* Read access GPU side
+ */
+#define BASE_MEM_PROT_GPU_RD ((base_mem_alloc_flags)1 << 2)
+
+/* Write access GPU side
+ */
+#define BASE_MEM_PROT_GPU_WR ((base_mem_alloc_flags)1 << 3)
+
+/* Execute allowed on the GPU side
+ */
+#define BASE_MEM_PROT_GPU_EX ((base_mem_alloc_flags)1 << 4)
+
+/* Will be permanently mapped in kernel space.
+ * Flag is only allowed on allocations originating from kbase.
+ */
+#define BASEP_MEM_PERMANENT_KERNEL_MAPPING ((base_mem_alloc_flags)1 << 5)
+
+/* The allocation will completely reside within the same 4GB chunk in the GPU
+ * virtual space.
+ * Since this flag is primarily required only for the TLS memory which will
+ * not be used to contain executable code and also not used for Tiler heap,
+ * it can't be used along with BASE_MEM_PROT_GPU_EX and TILER_ALIGN_TOP flags.
+ */
+#define BASE_MEM_GPU_VA_SAME_4GB_PAGE ((base_mem_alloc_flags)1 << 6)
+
+/* Userspace is not allowed to free this memory.
+ * Flag is only allowed on allocations originating from kbase.
+ */
+#define BASEP_MEM_NO_USER_FREE ((base_mem_alloc_flags)1 << 7)
+
+#define BASE_MEM_RESERVED_BIT_8 ((base_mem_alloc_flags)1 << 8)
+
+/* Grow backing store on GPU Page Fault
+ */
+#define BASE_MEM_GROW_ON_GPF ((base_mem_alloc_flags)1 << 9)
+
+/* Page coherence Outer shareable, if available
+ */
+#define BASE_MEM_COHERENT_SYSTEM ((base_mem_alloc_flags)1 << 10)
+
+/* Page coherence Inner shareable
+ */
+#define BASE_MEM_COHERENT_LOCAL ((base_mem_alloc_flags)1 << 11)
+
+/* IN/OUT */
+/* Should be cached on the CPU, returned if actually cached
+ */
+#define BASE_MEM_CACHED_CPU ((base_mem_alloc_flags)1 << 12)
+
+/* IN/OUT */
+/* Must have same VA on both the GPU and the CPU
+ */
+#define BASE_MEM_SAME_VA ((base_mem_alloc_flags)1 << 13)
+
+/* OUT */
+/* Must call mmap to acquire a GPU address for the alloc
+ */
+#define BASE_MEM_NEED_MMAP ((base_mem_alloc_flags)1 << 14)
+
+/* IN */
+/* Page coherence Outer shareable, required.
+ */
+#define BASE_MEM_COHERENT_SYSTEM_REQUIRED ((base_mem_alloc_flags)1 << 15)
+
+/* Protected memory
+ */
+#define BASE_MEM_PROTECTED ((base_mem_alloc_flags)1 << 16)
+
+/* Not needed physical memory
+ */
+#define BASE_MEM_DONT_NEED ((base_mem_alloc_flags)1 << 17)
+
+/* Must use shared CPU/GPU zone (SAME_VA zone) but doesn't require the
+ * addresses to be the same
+ */
+#define BASE_MEM_IMPORT_SHARED ((base_mem_alloc_flags)1 << 18)
+
+/* CSF event memory
+ *
+ * If Outer shareable coherence is not specified or not available, then on
+ * allocation kbase will automatically use the uncached GPU mapping.
+ * There is no need for the client to specify BASE_MEM_UNCACHED_GPU
+ * themselves when allocating memory with the BASE_MEM_CSF_EVENT flag.
+ *
+ * This memory requires a permanent mapping
+ *
+ * See also kbase_reg_needs_kernel_mapping()
+ */
+#define BASE_MEM_CSF_EVENT ((base_mem_alloc_flags)1 << 19)
+
+#define BASE_MEM_RESERVED_BIT_20 ((base_mem_alloc_flags)1 << 20)
+
+/* Should be uncached on the GPU, will work only for GPUs using AARCH64 mmu
+ * mode. Some components within the GPU might only be able to access memory
+ * that is GPU cacheable. Refer to the specific GPU implementation for more
+ * details. The 3 shareability flags will be ignored for GPU uncached memory.
+ * If used while importing USER_BUFFER type memory, then the import will fail
+ * if the memory is not aligned to GPU and CPU cache line width.
+ */
+#define BASE_MEM_UNCACHED_GPU ((base_mem_alloc_flags)1 << 21)
+
+/*
+ * Bits [22:25] for group_id (0~15).
+ *
+ * base_mem_group_id_set() should be used to pack a memory group ID into a
+ * base_mem_alloc_flags value instead of accessing the bits directly.
+ * base_mem_group_id_get() should be used to extract the memory group ID from
+ * a base_mem_alloc_flags value.
+ */
+#define BASEP_MEM_GROUP_ID_SHIFT 22
+#define BASE_MEM_GROUP_ID_MASK                                                 \
+   ((base_mem_alloc_flags)0xF << BASEP_MEM_GROUP_ID_SHIFT)
+
+/* Must do CPU cache maintenance when imported memory is mapped/unmapped
+ * on GPU. Currently applicable to dma-buf type only.
+ */
+#define BASE_MEM_IMPORT_SYNC_ON_MAP_UNMAP ((base_mem_alloc_flags)1 << 26)
+
+/* OUT */
+/* Kernel side cache sync ops required */
+#define BASE_MEM_KERNEL_SYNC ((base_mem_alloc_flags)1 << 28)
+
+/* Number of bits used as flags for base memory management
+ *
+ * Must be kept in sync with the base_mem_alloc_flags flags
+ */
+#define BASE_MEM_FLAGS_NR_BITS 29
+
+/* A mask of all the flags which are only valid for allocations within kbase,
+ * and may not be passed from user space.
+ */
+#define BASEP_MEM_FLAGS_KERNEL_ONLY                                            \
+   (BASEP_MEM_PERMANENT_KERNEL_MAPPING | BASEP_MEM_NO_USER_FREE)
+
+/* A mask for all output bits, excluding IN/OUT bits.
+ */
+#define BASE_MEM_FLAGS_OUTPUT_MASK BASE_MEM_NEED_MMAP
+
+/* A mask for all input bits, including IN/OUT bits.
+ */
+#define BASE_MEM_FLAGS_INPUT_MASK                                              \
+   (((1 << BASE_MEM_FLAGS_NR_BITS) - 1) & ~BASE_MEM_FLAGS_OUTPUT_MASK)
+
+/* A mask of all currently reserved flags
+ */
+#define BASE_MEM_FLAGS_RESERVED                                                \
+   BASE_MEM_RESERVED_BIT_8 | BASE_MEM_RESERVED_BIT_20
+
+#define BASEP_MEM_INVALID_HANDLE           (0ul)
+#define BASE_MEM_MMU_DUMP_HANDLE           (1ul << LOCAL_PAGE_SHIFT)
+#define BASE_MEM_TRACE_BUFFER_HANDLE       (2ul << LOCAL_PAGE_SHIFT)
+#define BASE_MEM_MAP_TRACKING_HANDLE       (3ul << LOCAL_PAGE_SHIFT)
+#define BASEP_MEM_WRITE_ALLOC_PAGES_HANDLE (4ul << LOCAL_PAGE_SHIFT)
+/* reserved handles ..-47<<PAGE_SHIFT> for future special handles */
+#define BASEP_MEM_CSF_USER_REG_PAGE_HANDLE (47ul << LOCAL_PAGE_SHIFT)
+#define BASEP_MEM_CSF_USER_IO_PAGES_HANDLE (48ul << LOCAL_PAGE_SHIFT)
+#define BASE_MEM_COOKIE_BASE               (64ul << LOCAL_PAGE_SHIFT)
+#define BASE_MEM_FIRST_FREE_ADDRESS                                            \
+   ((BITS_PER_LONG << LOCAL_PAGE_SHIFT) + BASE_MEM_COOKIE_BASE)
+
+#define KBASE_CSF_NUM_USER_IO_PAGES_HANDLE                                     \
+   ((BASE_MEM_COOKIE_BASE - BASEP_MEM_CSF_USER_IO_PAGES_HANDLE) >>             \
+    LOCAL_PAGE_SHIFT)
+
+/**
+ * Valid set of just-in-time memory allocation flags
+ */
+#define BASE_JIT_ALLOC_VALID_FLAGS ((__u8)0)
+
+/* Flags to pass to ::base_context_init.
+ * Flags can be ORed together to enable multiple things.
+ *
+ * These share the same space as BASEP_CONTEXT_FLAG_*, and so must
+ * not collide with them.
+ */
+typedef __u32 base_context_create_flags;
+
+/* No flags set */
+#define BASE_CONTEXT_CREATE_FLAG_NONE ((base_context_create_flags)0)
+
+/* Base context is embedded in a cctx object (flag used for CINSTR
+ * software counter macros)
+ */
+#define BASE_CONTEXT_CCTX_EMBEDDED ((base_context_create_flags)1 << 0)
+
+/* Base context is a 'System Monitor' context for Hardware counters.
+ *
+ * One important side effect of this is that job submission is disabled.
+ */
+#define BASE_CONTEXT_SYSTEM_MONITOR_SUBMIT_DISABLED                            \
+   ((base_context_create_flags)1 << 1)
+
+/* Base context creates a CSF event notification thread.
+ *
+ * The creation of a CSF event notification thread is conditional but
+ * mandatory for the handling of CSF events.
+ */
+#define BASE_CONTEXT_CSF_EVENT_THREAD ((base_context_create_flags)1 << 2)
+
+/* Bit-shift used to encode a memory group ID in base_context_create_flags
+ */
+#define BASEP_CONTEXT_MMU_GROUP_ID_SHIFT (3)
+
+/* Bitmask used to encode a memory group ID in base_context_create_flags
+ */
+#define BASEP_CONTEXT_MMU_GROUP_ID_MASK                                        \
+   ((base_context_create_flags)0xF << BASEP_CONTEXT_MMU_GROUP_ID_SHIFT)
+
+/* Bitpattern describing the base_context_create_flags that can be
+ * passed to the kernel
+ */
+#define BASEP_CONTEXT_CREATE_KERNEL_FLAGS                                      \
+   (BASE_CONTEXT_SYSTEM_MONITOR_SUBMIT_DISABLED |                              \
+    BASEP_CONTEXT_MMU_GROUP_ID_MASK)
+
+/* Bitpattern describing the ::base_context_create_flags that can be
+ * passed to base_context_init()
+ */
+#define BASEP_CONTEXT_CREATE_ALLOWED_FLAGS                                     \
+   (BASE_CONTEXT_CCTX_EMBEDDED | BASE_CONTEXT_CSF_EVENT_THREAD |               \
+    BASEP_CONTEXT_CREATE_KERNEL_FLAGS)
+
+/* Enable additional tracepoints for latency measurements (TL_ATOM_READY,
+ * TL_ATOM_DONE, TL_ATOM_PRIO_CHANGE, TL_ATOM_EVENT_POST)
+ */
+#define BASE_TLSTREAM_ENABLE_LATENCY_TRACEPOINTS (1 << 0)
+
+/* Indicate that job dumping is enabled. This could affect certain timers
+ * to account for the performance impact.
+ */
+#define BASE_TLSTREAM_JOB_DUMPING_ENABLED (1 << 1)
+
+/* Enable KBase tracepoints for CSF builds */
+#define BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS (1 << 2)
+
+/* Enable additional CSF Firmware side tracepoints */
+#define BASE_TLSTREAM_ENABLE_CSFFW_TRACEPOINTS (1 << 3)
+
+#define BASE_TLSTREAM_FLAGS_MASK                                               \
+   (BASE_TLSTREAM_ENABLE_LATENCY_TRACEPOINTS |                                 \
+    BASE_TLSTREAM_JOB_DUMPING_ENABLED | BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS | \
+    BASE_TLSTREAM_ENABLE_CSFFW_TRACEPOINTS)
+
+/* Number of pages mapped into the process address space for a bound GPU
+ * command queue. A pair of input/output pages and a Hw doorbell page
+ * are mapped to enable direct submission of commands to Hw.
+ */
+#define BASEP_QUEUE_NR_MMAP_USER_PAGES ((size_t)3)
+
+#define BASE_QUEUE_MAX_PRIORITY (15U)
+
+/* CQS Sync object is an array of __u32 event_mem[2], error field index is 1 */
+#define BASEP_EVENT_VAL_INDEX (0U)
+#define BASEP_EVENT_ERR_INDEX (1U)
+
+/* The upper limit for number of objects that could be waited/set per command.
+ * This limit is now enforced as internally the error inherit inputs are
+ * converted to 32-bit flags in a __u32 variable occupying a previously padding
+ * field.
+ */
+#define BASEP_KCPU_CQS_MAX_NUM_OBJS ((size_t)32)
+
+/**
+ * enum base_kcpu_command_type - Kernel CPU queue command type.
+ * @BASE_KCPU_COMMAND_TYPE_FENCE_SIGNAL:       fence_signal,
+ * @BASE_KCPU_COMMAND_TYPE_FENCE_WAIT:         fence_wait,
+ * @BASE_KCPU_COMMAND_TYPE_CQS_WAIT:           cqs_wait,
+ * @BASE_KCPU_COMMAND_TYPE_CQS_SET:            cqs_set,
+ * @BASE_KCPU_COMMAND_TYPE_CQS_WAIT_OPERATION: cqs_wait_operation,
+ * @BASE_KCPU_COMMAND_TYPE_CQS_SET_OPERATION:  cqs_set_operation,
+ * @BASE_KCPU_COMMAND_TYPE_MAP_IMPORT:         map_import,
+ * @BASE_KCPU_COMMAND_TYPE_UNMAP_IMPORT:       unmap_import,
+ * @BASE_KCPU_COMMAND_TYPE_UNMAP_IMPORT_FORCE: unmap_import_force,
+ * @BASE_KCPU_COMMAND_TYPE_JIT_ALLOC:          jit_alloc,
+ * @BASE_KCPU_COMMAND_TYPE_JIT_FREE:           jit_free,
+ * @BASE_KCPU_COMMAND_TYPE_GROUP_SUSPEND:      group_suspend,
+ * @BASE_KCPU_COMMAND_TYPE_ERROR_BARRIER:      error_barrier,
+ */
+enum base_kcpu_command_type {
+   BASE_KCPU_COMMAND_TYPE_FENCE_SIGNAL,
+   BASE_KCPU_COMMAND_TYPE_FENCE_WAIT,
+   BASE_KCPU_COMMAND_TYPE_CQS_WAIT,
+   BASE_KCPU_COMMAND_TYPE_CQS_SET,
+   BASE_KCPU_COMMAND_TYPE_CQS_WAIT_OPERATION,
+   BASE_KCPU_COMMAND_TYPE_CQS_SET_OPERATION,
+   BASE_KCPU_COMMAND_TYPE_MAP_IMPORT,
+   BASE_KCPU_COMMAND_TYPE_UNMAP_IMPORT,
+   BASE_KCPU_COMMAND_TYPE_UNMAP_IMPORT_FORCE,
+   BASE_KCPU_COMMAND_TYPE_JIT_ALLOC,
+   BASE_KCPU_COMMAND_TYPE_JIT_FREE,
+   BASE_KCPU_COMMAND_TYPE_GROUP_SUSPEND,
+   BASE_KCPU_COMMAND_TYPE_ERROR_BARRIER
+};
+
+/**
+ * enum base_queue_group_priority - Priority of a GPU Command Queue Group.
+ * @BASE_QUEUE_GROUP_PRIORITY_HIGH:     GPU Command Queue Group is of high
+ *                                      priority.
+ * @BASE_QUEUE_GROUP_PRIORITY_MEDIUM:   GPU Command Queue Group is of medium
+ *                                      priority.
+ * @BASE_QUEUE_GROUP_PRIORITY_LOW:      GPU Command Queue Group is of low
+ *                                      priority.
+ * @BASE_QUEUE_GROUP_PRIORITY_REALTIME: GPU Command Queue Group is of real-time
+ *                                      priority.
+ * @BASE_QUEUE_GROUP_PRIORITY_COUNT:    Number of GPU Command Queue Group
+ *                                      priority levels.
+ *
+ * Currently this is in order of highest to lowest, but if new levels are added
+ * then those new levels may be out of order to preserve the ABI compatibility
+ * with previous releases. At that point, ensure assignment to
+ * the 'priority' member in &kbase_queue_group is updated to ensure it remains
+ * a linear ordering.
+ *
+ * There should be no gaps in the enum, otherwise use of
+ * BASE_QUEUE_GROUP_PRIORITY_COUNT in kbase must be updated.
+ */
+enum base_queue_group_priority {
+   BASE_QUEUE_GROUP_PRIORITY_HIGH = 0,
+   BASE_QUEUE_GROUP_PRIORITY_MEDIUM,
+   BASE_QUEUE_GROUP_PRIORITY_LOW,
+   BASE_QUEUE_GROUP_PRIORITY_REALTIME,
+   BASE_QUEUE_GROUP_PRIORITY_COUNT
+};
+
+struct base_kcpu_command_fence_info {
+   __u64 fence;
+};
+
+struct base_cqs_wait_info {
+   __u64 addr;
+   __u32 val;
+   __u32 padding;
+};
+
+struct base_kcpu_command_cqs_wait_info {
+   __u64 objs;
+   __u32 nr_objs;
+   __u32 inherit_err_flags;
+};
+
+struct base_cqs_set {
+   __u64 addr;
+};
+
+struct base_kcpu_command_cqs_set_info {
+   __u64 objs;
+   __u32 nr_objs;
+   __u32 padding;
+};
+
+/**
+ * typedef basep_cqs_data_type - Enumeration of CQS Data Types
+ *
+ * @BASEP_CQS_DATA_TYPE_U32: The Data Type of a CQS Object's value
+ *                           is an unsigned 32-bit integer
+ * @BASEP_CQS_DATA_TYPE_U64: The Data Type of a CQS Object's value
+ *                           is an unsigned 64-bit integer
+ */
+typedef enum PACKED {
+   BASEP_CQS_DATA_TYPE_U32 = 0,
+   BASEP_CQS_DATA_TYPE_U64 = 1,
+} basep_cqs_data_type;
+
+/**
+ * typedef basep_cqs_wait_operation_op - Enumeration of CQS Object Wait
+ *                                Operation conditions
+ *
+ * @BASEP_CQS_WAIT_OPERATION_LE: CQS Wait Operation indicating that a
+ *                                wait will be satisfied when a CQS Object's
+ *                                value is Less than or Equal to
+ *                                the Wait Operation value
+ * @BASEP_CQS_WAIT_OPERATION_GT: CQS Wait Operation indicating that a
+ *                                wait will be satisfied when a CQS Object's
+ *                                value is Greater than the Wait Operation value
+ */
+typedef enum {
+   BASEP_CQS_WAIT_OPERATION_LE = 0,
+   BASEP_CQS_WAIT_OPERATION_GT = 1,
+} basep_cqs_wait_operation_op;
+
+struct base_cqs_wait_operation_info {
+   __u64 addr;
+   __u64 val;
+   __u8 operation;
+   __u8 data_type;
+   __u8 padding[6];
+};
+
+/**
+ * struct base_kcpu_command_cqs_wait_operation_info - structure which contains
+ *information about the Timeline CQS wait objects
+ *
+ * @objs:              An array of Timeline CQS waits.
+ * @nr_objs:           Number of Timeline CQS waits in the array.
+ * @inherit_err_flags: Bit-pattern for the CQSs in the array who's error field
+ *                     to be served as the source for importing into the
+ *                     queue's error-state.
+ */
+struct base_kcpu_command_cqs_wait_operation_info {
+   __u64 objs;
+   __u32 nr_objs;
+   __u32 inherit_err_flags;
+};
+
+/**
+ * typedef basep_cqs_set_operation_op - Enumeration of CQS Set Operations
+ *
+ * @BASEP_CQS_SET_OPERATION_ADD: CQS Set operation for adding a value
+ *                                to a synchronization object
+ * @BASEP_CQS_SET_OPERATION_SET: CQS Set operation for setting the value
+ *                                of a synchronization object
+ */
+typedef enum {
+   BASEP_CQS_SET_OPERATION_ADD = 0,
+   BASEP_CQS_SET_OPERATION_SET = 1,
+} basep_cqs_set_operation_op;
+
+struct base_cqs_set_operation_info {
+   __u64 addr;
+   __u64 val;
+   __u8 operation;
+   __u8 data_type;
+   __u8 padding[6];
+};
+
+/**
+ * struct base_kcpu_command_cqs_set_operation_info - structure which contains
+ *information about the Timeline CQS set objects
+ *
+ * @objs:    An array of Timeline CQS sets.
+ * @nr_objs: Number of Timeline CQS sets in the array.
+ * @padding: Structure padding, unused bytes.
+ */
+struct base_kcpu_command_cqs_set_operation_info {
+   __u64 objs;
+   __u32 nr_objs;
+   __u32 padding;
+};
+
+/**
+ * struct base_kcpu_command_import_info - structure which contains information
+ *		about the imported buffer.
+ *
+ * @handle:	Address of imported user buffer.
+ */
+struct base_kcpu_command_import_info {
+   __u64 handle;
+};
+
+/**
+ * struct base_kcpu_command_jit_alloc_info - structure which contains
+ *		information about jit memory allocation.
+ *
+ * @info:	An array of elements of the
+ *		struct base_jit_alloc_info type.
+ * @count:	The number of elements in the info array.
+ * @padding:	Padding to a multiple of 64 bits.
+ */
+struct base_kcpu_command_jit_alloc_info {
+   __u64 info;
+   __u8 count;
+   __u8 padding[7];
+};
+
+/**
+ * struct base_kcpu_command_jit_free_info - structure which contains
+ *		information about jit memory which is to be freed.
+ *
+ * @ids:	An array containing the JIT IDs to free.
+ * @count:	The number of elements in the ids array.
+ * @padding:	Padding to a multiple of 64 bits.
+ */
+struct base_kcpu_command_jit_free_info {
+   __u64 ids;
+   __u8 count;
+   __u8 padding[7];
+};
+
+/**
+ * struct base_kcpu_command_group_suspend_info - structure which contains
+ *		suspend buffer data captured for a suspended queue group.
+ *
+ * @buffer:		Pointer to an array of elements of the type char.
+ * @size:		Number of elements in the @buffer array.
+ * @group_handle:	Handle to the mapping of CSG.
+ * @padding:		padding to a multiple of 64 bits.
+ */
+struct base_kcpu_command_group_suspend_info {
+   __u64 buffer;
+   __u32 size;
+   __u8 group_handle;
+   __u8 padding[3];
+};
+
+/**
+ * struct base_kcpu_command - kcpu command.
+ * @type:	type of the kcpu command, one enum base_kcpu_command_type
+ * @padding:	padding to a multiple of 64 bits
+ * @info:	structure which contains information about the kcpu command;
+ *		actual type is determined by @p type
+ * @info.fence:            Fence
+ * @info.cqs_wait:         CQS wait
+ * @info.cqs_set:          CQS set
+ * @info.import:           import
+ * @info.jit_alloc:        jit allocation
+ * @info.jit_free:         jit deallocation
+ * @info.suspend_buf_copy: suspend buffer copy
+ * @info.sample_time:      sample time
+ * @info.padding:          padding
+ */
+struct base_kcpu_command {
+   __u8 type;
+   __u8 padding[sizeof(__u64) - sizeof(__u8)];
+   union {
+      struct base_kcpu_command_fence_info fence;
+      struct base_kcpu_command_cqs_wait_info cqs_wait;
+      struct base_kcpu_command_cqs_set_info cqs_set;
+      struct base_kcpu_command_cqs_wait_operation_info cqs_wait_operation;
+      struct base_kcpu_command_cqs_set_operation_info cqs_set_operation;
+      struct base_kcpu_command_import_info import;
+      struct base_kcpu_command_jit_alloc_info jit_alloc;
+      struct base_kcpu_command_jit_free_info jit_free;
+      struct base_kcpu_command_group_suspend_info suspend_buf_copy;
+      __u64 padding[2]; /* No sub-struct should be larger */
+   } info;
+};
+
+/**
+ * struct basep_cs_stream_control - CSI capabilities.
+ *
+ * @features: Features of this stream
+ * @padding:  Padding to a multiple of 64 bits.
+ */
+struct basep_cs_stream_control {
+   __u32 features;
+   __u32 padding;
+};
+
+/**
+ * struct basep_cs_group_control - CSG interface capabilities.
+ *
+ * @features:     Features of this group
+ * @stream_num:   Number of streams in this group
+ * @suspend_size: Size in bytes of the suspend buffer for this group
+ * @padding:      Padding to a multiple of 64 bits.
+ */
+struct basep_cs_group_control {
+   __u32 features;
+   __u32 stream_num;
+   __u32 suspend_size;
+   __u32 padding;
+};
+
+/**
+ * struct base_gpu_queue_group_error_fatal_payload - Unrecoverable fault
+ *        error information associated with GPU command queue group.
+ *
+ * @sideband:     Additional information of the unrecoverable fault.
+ * @status:       Unrecoverable fault information.
+ *                This consists of exception type (least significant byte) and
+ *                data (remaining bytes). One example of exception type is
+ *                CS_INVALID_INSTRUCTION (0x49).
+ * @padding:      Padding to make multiple of 64bits
+ */
+struct base_gpu_queue_group_error_fatal_payload {
+   __u64 sideband;
+   __u32 status;
+   __u32 padding;
+};
+
+/**
+ * struct base_gpu_queue_error_fatal_payload - Unrecoverable fault
+ *        error information related to GPU command queue.
+ *
+ * @sideband:     Additional information about this unrecoverable fault.
+ * @status:       Unrecoverable fault information.
+ *                This consists of exception type (least significant byte) and
+ *                data (remaining bytes). One example of exception type is
+ *                CS_INVALID_INSTRUCTION (0x49).
+ * @csi_index:    Index of the CSF interface the queue is bound to.
+ * @padding:      Padding to make multiple of 64bits
+ */
+struct base_gpu_queue_error_fatal_payload {
+   __u64 sideband;
+   __u32 status;
+   __u8 csi_index;
+   __u8 padding[3];
+};
+
+/**
+ * enum base_gpu_queue_group_error_type - GPU Fatal error type.
+ *
+ * @BASE_GPU_QUEUE_GROUP_ERROR_FATAL:       Fatal error associated with GPU
+ *                                          command queue group.
+ * @BASE_GPU_QUEUE_GROUP_QUEUE_ERROR_FATAL: Fatal error associated with GPU
+ *                                          command queue.
+ * @BASE_GPU_QUEUE_GROUP_ERROR_TIMEOUT:     Fatal error associated with
+ *                                          progress timeout.
+ * @BASE_GPU_QUEUE_GROUP_ERROR_TILER_HEAP_OOM: Fatal error due to running out
+ *                                             of tiler heap memory.
+ * @BASE_GPU_QUEUE_GROUP_ERROR_FATAL_COUNT: The number of fatal error types
+ *
+ * This type is used for &struct_base_gpu_queue_group_error.error_type.
+ */
+enum base_gpu_queue_group_error_type {
+   BASE_GPU_QUEUE_GROUP_ERROR_FATAL = 0,
+   BASE_GPU_QUEUE_GROUP_QUEUE_ERROR_FATAL,
+   BASE_GPU_QUEUE_GROUP_ERROR_TIMEOUT,
+   BASE_GPU_QUEUE_GROUP_ERROR_TILER_HEAP_OOM,
+   BASE_GPU_QUEUE_GROUP_ERROR_FATAL_COUNT
+};
+
+/**
+ * struct base_gpu_queue_group_error - Unrecoverable fault information
+ * @error_type:          Error type of @base_gpu_queue_group_error_type
+ *                       indicating which field in union payload is filled
+ * @padding:             Unused bytes for 64bit boundary
+ * @payload:             Input Payload
+ * @payload.fatal_group: Unrecoverable fault error associated with
+ *                       GPU command queue group
+ * @payload.fatal_queue: Unrecoverable fault error associated with command queue
+ */
+struct base_gpu_queue_group_error {
+   __u8 error_type;
+   __u8 padding[7];
+   union {
+      struct base_gpu_queue_group_error_fatal_payload fatal_group;
+      struct base_gpu_queue_error_fatal_payload fatal_queue;
+   } payload;
+};
+
+/**
+ * enum base_csf_notification_type - Notification type
+ *
+ * @BASE_CSF_NOTIFICATION_EVENT:                 Notification with kernel event
+ * @BASE_CSF_NOTIFICATION_GPU_QUEUE_GROUP_ERROR: Notification with GPU fatal
+ *                                               error
+ * @BASE_CSF_NOTIFICATION_CPU_QUEUE_DUMP:        Notification with dumping cpu
+ *                                               queue
+ * @BASE_CSF_NOTIFICATION_COUNT:                 The number of notification type
+ *
+ * This type is used for &struct_base_csf_notification.type.
+ */
+enum base_csf_notification_type {
+   BASE_CSF_NOTIFICATION_EVENT = 0,
+   BASE_CSF_NOTIFICATION_GPU_QUEUE_GROUP_ERROR,
+   BASE_CSF_NOTIFICATION_CPU_QUEUE_DUMP,
+   BASE_CSF_NOTIFICATION_COUNT
+};
+
+/**
+ * struct base_csf_notification - Event or error notification
+ *
+ * @type:                      Notification type of @base_csf_notification_type
+ * @padding:                   Padding for 64bit boundary
+ * @payload:                   Input Payload
+ * @payload.align:             To fit the struct into a 64-byte cache line
+ * @payload.csg_error:         CSG error
+ * @payload.csg_error.handle:  Handle of GPU command queue group associated with
+ *                             fatal error
+ * @payload.csg_error.padding: Padding
+ * @payload.csg_error.error:   Unrecoverable fault error
+ *
+ */
+struct base_csf_notification {
+   __u8 type;
+   __u8 padding[7];
+   union {
+      struct {
+         __u8 handle;
+         __u8 padding[7];
+         struct base_gpu_queue_group_error error;
+      } csg_error;
+
+      __u8 align[56];
+   } payload;
+};
+
+#endif /* _UAPI_BASE_CSF_KERNEL_H_ */
diff -urN mesa-23.0.0/src/panfrost/csf_test/mali_base_kernel.h mesa/src/panfrost/csf_test/mali_base_kernel.h
--- mesa-23.0.0/src/panfrost/csf_test/mali_base_kernel.h	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/csf_test/mali_base_kernel.h	2023-03-06 19:19:32.641308396 +0100
@@ -0,0 +1,741 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ *
+ * (C) COPYRIGHT 2010-2021 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU license.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ */
+
+/*
+ * Base structures shared with the kernel.
+ */
+
+#ifndef _UAPI_BASE_KERNEL_H_
+#define _UAPI_BASE_KERNEL_H_
+
+#include <linux/types.h>
+
+struct base_mem_handle {
+   struct {
+      __u64 handle;
+   } basep;
+};
+
+#define BASE_GPU_NUM_TEXTURE_FEATURES_REGISTERS 4
+
+#define BASE_MAX_COHERENT_GROUPS 16
+
+#if defined(PAGE_MASK) && defined(PAGE_SHIFT)
+#define LOCAL_PAGE_SHIFT PAGE_SHIFT
+#define LOCAL_PAGE_LSB   ~PAGE_MASK
+#else
+#ifndef OSU_CONFIG_CPU_PAGE_SIZE_LOG2
+#define OSU_CONFIG_CPU_PAGE_SIZE_LOG2 12
+#endif
+
+#if defined(OSU_CONFIG_CPU_PAGE_SIZE_LOG2)
+#define LOCAL_PAGE_SHIFT OSU_CONFIG_CPU_PAGE_SIZE_LOG2
+#define LOCAL_PAGE_LSB   ((1ul << OSU_CONFIG_CPU_PAGE_SIZE_LOG2) - 1)
+#else
+#error Failed to find page size
+#endif
+#endif
+
+/* Physical memory group ID for normal usage.
+ */
+#define BASE_MEM_GROUP_DEFAULT (0)
+
+/* Number of physical memory groups.
+ */
+#define BASE_MEM_GROUP_COUNT (16)
+
+/**
+ * typedef base_mem_alloc_flags - Memory allocation, access/hint flags.
+ *
+ * A combination of MEM_PROT/MEM_HINT flags must be passed to each allocator
+ * in order to determine the best cache policy. Some combinations are
+ * of course invalid (e.g. MEM_PROT_CPU_WR | MEM_HINT_CPU_RD),
+ * which defines a write-only region on the CPU side, which is
+ * heavily read by the CPU...
+ * Other flags are only meaningful to a particular allocator.
+ * More flags can be added to this list, as long as they don't clash
+ * (see BASE_MEM_FLAGS_NR_BITS for the number of the first free bit).
+ */
+typedef __u32 base_mem_alloc_flags;
+
+/* A mask for all the flags which are modifiable via the base_mem_set_flags
+ * interface.
+ */
+#define BASE_MEM_FLAGS_MODIFIABLE                                              \
+   (BASE_MEM_DONT_NEED | BASE_MEM_COHERENT_SYSTEM | BASE_MEM_COHERENT_LOCAL)
+
+/* A mask of all the flags that can be returned via the base_mem_get_flags()
+ * interface.
+ */
+#define BASE_MEM_FLAGS_QUERYABLE                                               \
+   (BASE_MEM_FLAGS_INPUT_MASK &                                                \
+    ~(BASE_MEM_SAME_VA | BASE_MEM_COHERENT_SYSTEM_REQUIRED |                   \
+      BASE_MEM_DONT_NEED | BASE_MEM_IMPORT_SHARED | BASE_MEM_FLAGS_RESERVED |  \
+      BASEP_MEM_FLAGS_KERNEL_ONLY))
+
+/**
+ * enum base_mem_import_type - Memory types supported by @a base_mem_import
+ *
+ * @BASE_MEM_IMPORT_TYPE_INVALID: Invalid type
+ * @BASE_MEM_IMPORT_TYPE_UMM: UMM import. Handle type is a file descriptor (int)
+ * @BASE_MEM_IMPORT_TYPE_USER_BUFFER: User buffer import. Handle is a
+ * base_mem_import_user_buffer
+ *
+ * Each type defines what the supported handle type is.
+ *
+ * If any new type is added here ARM must be contacted
+ * to allocate a numeric value for it.
+ * Do not just add a new type without synchronizing with ARM
+ * as future releases from ARM might include other new types
+ * which could clash with your custom types.
+ */
+enum base_mem_import_type {
+   BASE_MEM_IMPORT_TYPE_INVALID = 0,
+   /*
+    * Import type with value 1 is deprecated.
+    */
+   BASE_MEM_IMPORT_TYPE_UMM = 2,
+   BASE_MEM_IMPORT_TYPE_USER_BUFFER = 3
+};
+
+/**
+ * struct base_mem_import_user_buffer - Handle of an imported user buffer
+ *
+ * @ptr:	address of imported user buffer
+ * @length:	length of imported user buffer in bytes
+ *
+ * This structure is used to represent a handle of an imported user buffer.
+ */
+
+struct base_mem_import_user_buffer {
+   __u64 ptr;
+   __u64 length;
+};
+
+/* Mask to detect 4GB boundary alignment */
+#define BASE_MEM_MASK_4GB 0xfffff000UL
+/* Mask to detect 4GB boundary (in page units) alignment */
+#define BASE_MEM_PFN_MASK_4GB (BASE_MEM_MASK_4GB >> LOCAL_PAGE_SHIFT)
+
+/* Limit on the 'extension' parameter for an allocation with the
+ * BASE_MEM_TILER_ALIGN_TOP flag set
+ *
+ * This is the same as the maximum limit for a Buffer Descriptor's chunk size
+ */
+#define BASE_MEM_TILER_ALIGN_TOP_EXTENSION_MAX_PAGES_LOG2                      \
+   (21u - (LOCAL_PAGE_SHIFT))
+#define BASE_MEM_TILER_ALIGN_TOP_EXTENSION_MAX_PAGES                           \
+   (1ull << (BASE_MEM_TILER_ALIGN_TOP_EXTENSION_MAX_PAGES_LOG2))
+
+/* Bit mask of cookies used for for memory allocation setup */
+#define KBASE_COOKIE_MASK ~1UL /* bit 0 is reserved */
+
+/* Maximum size allowed in a single KBASE_IOCTL_MEM_ALLOC call */
+#define KBASE_MEM_ALLOC_MAX_SIZE ((8ull << 30) >> PAGE_SHIFT) /* 8 GB */
+
+/*
+ * struct base_fence - Cross-device synchronisation fence.
+ *
+ * A fence is used to signal when the GPU has finished accessing a resource that
+ * may be shared with other devices, and also to delay work done asynchronously
+ * by the GPU until other devices have finished accessing a shared resource.
+ */
+struct base_fence {
+   struct {
+      int fd;
+      int stream_fd;
+   } basep;
+};
+
+/**
+ * struct base_mem_aliasing_info - Memory aliasing info
+ *
+ * Describes a memory handle to be aliased.
+ * A subset of the handle can be chosen for aliasing, given an offset and a
+ * length.
+ * A special handle BASE_MEM_WRITE_ALLOC_PAGES_HANDLE is used to represent a
+ * region where a special page is mapped with a write-alloc cache setup,
+ * typically used when the write result of the GPU isn't needed, but the GPU
+ * must write anyway.
+ *
+ * Offset and length are specified in pages.
+ * Offset must be within the size of the handle.
+ * Offset+length must not overrun the size of the handle.
+ *
+ * @handle: Handle to alias, can be BASE_MEM_WRITE_ALLOC_PAGES_HANDLE
+ * @offset: Offset within the handle to start aliasing from, in pages.
+ *          Not used with BASE_MEM_WRITE_ALLOC_PAGES_HANDLE.
+ * @length: Length to alias, in pages. For BASE_MEM_WRITE_ALLOC_PAGES_HANDLE
+ *          specifies the number of times the special page is needed.
+ */
+struct base_mem_aliasing_info {
+   struct base_mem_handle handle;
+   __u64 offset;
+   __u64 length;
+};
+
+/* Maximum percentage of just-in-time memory allocation trimming to perform
+ * on free.
+ */
+#define BASE_JIT_MAX_TRIM_LEVEL (100)
+
+/* Maximum number of concurrent just-in-time memory allocations.
+ */
+#define BASE_JIT_ALLOC_COUNT (255)
+
+/* base_jit_alloc_info in use for kernel driver versions 10.2 to early 11.5
+ *
+ * jit_version is 1
+ *
+ * Due to the lack of padding specified, user clients between 32 and 64-bit
+ * may have assumed a different size of the struct
+ *
+ * An array of structures was not supported
+ */
+struct base_jit_alloc_info_10_2 {
+   __u64 gpu_alloc_addr;
+   __u64 va_pages;
+   __u64 commit_pages;
+   __u64 extension;
+   __u8 id;
+};
+
+/* base_jit_alloc_info introduced by kernel driver version 11.5, and in use up
+ * to 11.19
+ *
+ * This structure had a number of modifications during and after kernel driver
+ * version 11.5, but remains size-compatible throughout its version history, and
+ * with earlier variants compatible with future variants by requiring
+ * zero-initialization to the unused space in the structure.
+ *
+ * jit_version is 2
+ *
+ * Kernel driver version history:
+ * 11.5: Initial introduction with 'usage_id' and padding[5]. All padding bytes
+ *       must be zero. Kbase minor version was not incremented, so some
+ *       versions of 11.5 do not have this change.
+ * 11.5: Added 'bin_id' and 'max_allocations', replacing 2 padding bytes (Kbase
+ *       minor version not incremented)
+ * 11.6: Added 'flags', replacing 1 padding byte
+ * 11.10: Arrays of this structure are supported
+ */
+struct base_jit_alloc_info_11_5 {
+   __u64 gpu_alloc_addr;
+   __u64 va_pages;
+   __u64 commit_pages;
+   __u64 extension;
+   __u8 id;
+   __u8 bin_id;
+   __u8 max_allocations;
+   __u8 flags;
+   __u8 padding[2];
+   __u16 usage_id;
+};
+
+/**
+ * struct base_jit_alloc_info - Structure which describes a JIT allocation
+ *                              request.
+ * @gpu_alloc_addr:             The GPU virtual address to write the JIT
+ *                              allocated GPU virtual address to.
+ * @va_pages:                   The minimum number of virtual pages required.
+ * @commit_pages:               The minimum number of physical pages which
+ *                              should back the allocation.
+ * @extension:                     Granularity of physical pages to grow the
+ *                              allocation by during a fault.
+ * @id:                         Unique ID provided by the caller, this is used
+ *                              to pair allocation and free requests.
+ *                              Zero is not a valid value.
+ * @bin_id:                     The JIT allocation bin, used in conjunction with
+ *                              @max_allocations to limit the number of each
+ *                              type of JIT allocation.
+ * @max_allocations:            The maximum number of allocations allowed within
+ *                              the bin specified by @bin_id. Should be the same
+ *                              for all allocations within the same bin.
+ * @flags:                      flags specifying the special requirements for
+ *                              the JIT allocation, see
+ *                              %BASE_JIT_ALLOC_VALID_FLAGS
+ * @padding:                    Expansion space - should be initialised to zero
+ * @usage_id:                   A hint about which allocation should be reused.
+ *                              The kernel should attempt to use a previous
+ *                              allocation with the same usage_id
+ * @heap_info_gpu_addr:         Pointer to an object in GPU memory describing
+ *                              the actual usage of the region.
+ *
+ * jit_version is 3.
+ *
+ * When modifications are made to this structure, it is still compatible with
+ * jit_version 3 when: a) the size is unchanged, and b) new members only
+ * replace the padding bytes.
+ *
+ * Previous jit_version history:
+ * jit_version == 1, refer to &base_jit_alloc_info_10_2
+ * jit_version == 2, refer to &base_jit_alloc_info_11_5
+ *
+ * Kbase version history:
+ * 11.20: added @heap_info_gpu_addr
+ */
+struct base_jit_alloc_info {
+   __u64 gpu_alloc_addr;
+   __u64 va_pages;
+   __u64 commit_pages;
+   __u64 extension;
+   __u8 id;
+   __u8 bin_id;
+   __u8 max_allocations;
+   __u8 flags;
+   __u8 padding[2];
+   __u16 usage_id;
+   __u64 heap_info_gpu_addr;
+};
+
+enum base_external_resource_access {
+   BASE_EXT_RES_ACCESS_SHARED,
+   BASE_EXT_RES_ACCESS_EXCLUSIVE
+};
+
+struct base_external_resource {
+   __u64 ext_resource;
+};
+
+/**
+ * The maximum number of external resources which can be mapped/unmapped
+ * in a single request.
+ */
+#define BASE_EXT_RES_COUNT_MAX 10
+
+/**
+ * struct base_external_resource_list - Structure which describes a list of
+ *                                      external resources.
+ * @count:                              The number of resources.
+ * @ext_res:                            Array of external resources which is
+ *                                      sized at allocation time.
+ */
+struct base_external_resource_list {
+   __u64 count;
+   struct base_external_resource ext_res[1];
+};
+
+struct base_jd_debug_copy_buffer {
+   __u64 address;
+   __u64 size;
+   struct base_external_resource extres;
+};
+
+#define GPU_MAX_JOB_SLOTS 16
+
+/**
+ * User-side Base GPU Property Queries
+ *
+ * The User-side Base GPU Property Query interface encapsulates two
+ * sub-modules:
+ *
+ * - "Dynamic GPU Properties"
+ * - "Base Platform Config GPU Properties"
+ *
+ * Base only deals with properties that vary between different GPU
+ * implementations - the Dynamic GPU properties and the Platform Config
+ * properties.
+ *
+ * For properties that are constant for the GPU Architecture, refer to the
+ * GPU module. However, we will discuss their relevance here just to
+ * provide background information.
+ *
+ * About the GPU Properties in Base and GPU modules
+ *
+ * The compile-time properties (Platform Config, GPU Compile-time
+ * properties) are exposed as pre-processor macros.
+ *
+ * Complementing the compile-time properties are the Dynamic GPU
+ * Properties, which act as a conduit for the GPU Configuration
+ * Discovery.
+ *
+ * In general, the dynamic properties are present to verify that the platform
+ * has been configured correctly with the right set of Platform Config
+ * Compile-time Properties.
+ *
+ * As a consistent guide across the entire DDK, the choice for dynamic or
+ * compile-time should consider the following, in order:
+ * 1. Can the code be written so that it doesn't need to know the
+ * implementation limits at all?
+ * 2. If you need the limits, get the information from the Dynamic Property
+ * lookup. This should be done once as you fetch the context, and then cached
+ * as part of the context data structure, so it's cheap to access.
+ * 3. If there's a clear and arguable inefficiency in using Dynamic Properties,
+ * then use a Compile-Time Property (Platform Config, or GPU Compile-time
+ * property). Examples of where this might be sensible follow:
+ *  - Part of a critical inner-loop
+ *  - Frequent re-use throughout the driver, causing significant extra load
+ * instructions or control flow that would be worthwhile optimizing out.
+ *
+ * We cannot provide an exhaustive set of examples, neither can we provide a
+ * rule for every possible situation. Use common sense, and think about: what
+ * the rest of the driver will be doing; how the compiler might represent the
+ * value if it is a compile-time constant; whether an OEM shipping multiple
+ * devices would benefit much more from a single DDK binary, instead of
+ * insignificant micro-optimizations.
+ *
+ * Dynamic GPU Properties
+ *
+ * Dynamic GPU properties are presented in two sets:
+ * 1. the commonly used properties in @ref base_gpu_props, which have been
+ * unpacked from GPU register bitfields.
+ * 2. The full set of raw, unprocessed properties in gpu_raw_gpu_props
+ * (also a member of base_gpu_props). All of these are presented in
+ * the packed form, as presented by the GPU  registers themselves.
+ *
+ * The raw properties in gpu_raw_gpu_props are necessary to
+ * allow a user of the Mali Tools (e.g. PAT) to determine "Why is this device
+ * behaving differently?". In this case, all information about the
+ * configuration is potentially useful, but it does not need to be processed
+ * by the driver. Instead, the raw registers can be processed by the Mali
+ * Tools software on the host PC.
+ *
+ * The properties returned extend the GPU Configuration Discovery
+ * registers. For example, GPU clock speed is not specified in the GPU
+ * Architecture, but is necessary for OpenCL's clGetDeviceInfo() function.
+ *
+ * The GPU properties are obtained by a call to
+ * base_get_gpu_props(). This simply returns a pointer to a const
+ * base_gpu_props structure. It is constant for the life of a base
+ * context. Multiple calls to base_get_gpu_props() to a base context
+ * return the same pointer to a constant structure. This avoids cache pollution
+ * of the common data.
+ *
+ * This pointer must not be freed, because it does not point to the start of a
+ * region allocated by the memory allocator; instead, just close the @ref
+ * base_context.
+ *
+ *
+ * Kernel Operation
+ *
+ * During Base Context Create time, user-side makes a single kernel call:
+ * - A call to fill user memory with GPU information structures
+ *
+ * The kernel-side will fill the provided the entire processed base_gpu_props
+ * structure, because this information is required in both
+ * user and kernel side; it does not make sense to decode it twice.
+ *
+ * Coherency groups must be derived from the bitmasks, but this can be done
+ * kernel side, and just once at kernel startup: Coherency groups must already
+ * be known kernel-side, to support chains that specify a 'Only Coherent Group'
+ * SW requirement, or 'Only Coherent Group with Tiler' SW requirement.
+ *
+ * Coherency Group calculation
+ *
+ * Creation of the coherent group data is done at device-driver startup, and so
+ * is one-time. This will most likely involve a loop with CLZ, shifting, and
+ * bit clearing on the L2_PRESENT mask, depending on whether the
+ * system is L2 Coherent. The number of shader cores is done by a
+ * population count, since faulty cores may be disabled during production,
+ * producing a non-contiguous mask.
+ *
+ * The memory requirements for this algorithm can be determined either by a __u64
+ * population count on the L2_PRESENT mask (a LUT helper already is
+ * required for the above), or simple assumption that there can be no more than
+ * 16 coherent groups, since core groups are typically 4 cores.
+ */
+
+#define BASE_GPU_NUM_TEXTURE_FEATURES_REGISTERS 4
+
+#define BASE_MAX_COHERENT_GROUPS 16
+/**
+ * struct mali_base_gpu_core_props - GPU core props info
+ * @product_id: Pro specific value.
+ * @version_status: Status of the GPU release. No defined values, but starts at
+ * 	0 and increases by one for each release status (alpha, beta, EAC, etc.).
+ * 	4 bit values (0-15).
+ * @minor_revision: Minor release number of the GPU. "P" part of an "RnPn"
+ * 	release number.
+ * 	8 bit values (0-255).
+ * @major_revision: Major release number of the GPU. "R" part of an "RnPn"
+ * 	release number.
+ * 	4 bit values (0-15).
+ * @padding: padding to allign to 8-byte
+ * @gpu_freq_khz_max: The maximum GPU frequency. Reported to applications by
+ * 	clGetDeviceInfo()
+ * @log2_program_counter_size: Size of the shader program counter, in bits.
+ * @texture_features: TEXTURE_FEATURES_x registers, as exposed by the GPU. This
+ * 	is a bitpattern where a set bit indicates that the format is supported.
+ * 	Before using a texture format, it is recommended that the corresponding
+ * 	bit be checked.
+ * @gpu_available_memory_size: Theoretical maximum memory available to the GPU.
+ * 	It is unlikely that a client will be able to allocate all of this memory
+ * 	for their own purposes, but this at least provides an upper bound on the
+ * 	memory available to the GPU.
+ * 	This is required for OpenCL's clGetDeviceInfo() call when
+ * 	CL_DEVICE_GLOBAL_MEM_SIZE is requested, for OpenCL GPU devices. The
+ * 	client will not be expecting to allocate anywhere near this value.
+ * @num_exec_engines: The number of execution engines.
+ */
+struct mali_base_gpu_core_props {
+   __u32 product_id;
+   __u16 version_status;
+   __u16 minor_revision;
+   __u16 major_revision;
+   __u16 padding;
+   __u32 gpu_freq_khz_max;
+   __u32 log2_program_counter_size;
+   __u32 texture_features[BASE_GPU_NUM_TEXTURE_FEATURES_REGISTERS];
+   __u64 gpu_available_memory_size;
+   __u8 num_exec_engines;
+};
+
+/*
+ * More information is possible - but associativity and bus width are not
+ * required by upper-level apis.
+ */
+struct mali_base_gpu_l2_cache_props {
+   __u8 log2_line_size;
+   __u8 log2_cache_size;
+   __u8 num_l2_slices; /* Number of L2C slices. 1 or higher */
+   __u8 padding[5];
+};
+
+struct mali_base_gpu_tiler_props {
+   __u32 bin_size_bytes;    /* Max is 4*2^15 */
+   __u32 max_active_levels; /* Max is 2^15 */
+};
+
+/**
+ * struct mali_base_gpu_thread_props - GPU threading system details.
+ * @max_threads: Max. number of threads per core
+ * @max_workgroup_size:     Max. number of threads per workgroup
+ * @max_barrier_size:       Max. number of threads that can synchronize on a
+ *                          simple barrier
+ * @max_registers:          Total size [1..65535] of the register file available
+ *                          per core.
+ * @max_task_queue:         Max. tasks [1..255] which may be sent to a core
+ *                          before it becomes blocked.
+ * @max_thread_group_split: Max. allowed value [1..15] of the Thread Group Split
+ *                          field.
+ * @impl_tech:              0 = Not specified, 1 = Silicon, 2 = FPGA,
+ *                          3 = SW Model/Emulation
+ * @padding:                padding to allign to 8-byte
+ * @tls_alloc:              Number of threads per core that TLS must be
+ *                          allocated for
+ */
+struct mali_base_gpu_thread_props {
+   __u32 max_threads;
+   __u32 max_workgroup_size;
+   __u32 max_barrier_size;
+   __u16 max_registers;
+   __u8 max_task_queue;
+   __u8 max_thread_group_split;
+   __u8 impl_tech;
+   __u8 padding[3];
+   __u32 tls_alloc;
+};
+
+/**
+ * struct mali_base_gpu_coherent_group - descriptor for a coherent group
+ * @core_mask: Core restriction mask required for the group
+ * @num_cores: Number of cores in the group
+ * @padding:   padding to allign to 8-byte
+ *
+ * \c core_mask exposes all cores in that coherent group, and \c num_cores
+ * 	provides a cached population-count for that mask.
+ *
+ * @note Whilst all cores are exposed in the mask, not all may be available to
+ * 	the application, depending on the Kernel Power policy.
+ *
+ * @note if u64s must be 8-byte aligned, then this structure has 32-bits of
+ * 	wastage.
+ */
+struct mali_base_gpu_coherent_group {
+   __u64 core_mask;
+   __u16 num_cores;
+   __u16 padding[3];
+};
+
+/**
+ * struct mali_base_gpu_coherent_group_info - Coherency group information
+ * @num_groups: Number of coherent groups in the GPU.
+ * @num_core_groups: Number of core groups (coherent or not) in the GPU.
+ * 	Equivalent to the number of L2 Caches.
+ * 	  The GPU Counter dumping writes 2048 bytes per core group, regardless
+ * 	of whether the core groups are coherent or not. Hence this member is
+ * 	needed to calculate how much memory is required for dumping.
+ * 	  @note Do not use it to work out how many valid elements are in the
+ * 	group[] member. Use num_groups instead.
+ * @coherency: Coherency features of the memory, accessed by gpu_mem_features
+ * 	methods
+ * @padding: padding to allign to 8-byte
+ * @group: Descriptors of coherent groups
+ *
+ * Note that the sizes of the members could be reduced. However, the \c group
+ * member might be 8-byte aligned to ensure the __u64 core_mask is 8-byte
+ * aligned, thus leading to wastage if the other members sizes were reduced.
+ *
+ * The groups are sorted by core mask. The core masks are non-repeating and do
+ * not intersect.
+ */
+struct mali_base_gpu_coherent_group_info {
+   __u32 num_groups;
+   __u32 num_core_groups;
+   __u32 coherency;
+   __u32 padding;
+   struct mali_base_gpu_coherent_group group[BASE_MAX_COHERENT_GROUPS];
+};
+
+/**
+ * struct gpu_raw_gpu_props - A complete description of the GPU's Hardware
+ *                            Configuration Discovery registers.
+ * @shader_present: Shader core present bitmap
+ * @tiler_present: Tiler core present bitmap
+ * @l2_present: Level 2 cache present bitmap
+ * @stack_present: Core stack present bitmap
+ * @l2_features: L2 features
+ * @core_features: Core features
+ * @mem_features: Mem features
+ * @mmu_features: Mmu features
+ * @as_present: Bitmap of address spaces present
+ * @js_present: Job slots present
+ * @js_features: Array of job slot features.
+ * @tiler_features: Tiler features
+ * @texture_features: TEXTURE_FEATURES_x registers, as exposed by the GPU
+ * @gpu_id: GPU and revision identifier
+ * @thread_max_threads: Maximum number of threads per core
+ * @thread_max_workgroup_size: Maximum number of threads per workgroup
+ * @thread_max_barrier_size: Maximum number of threads per barrier
+ * @thread_features: Thread features
+ * @coherency_mode: Note: This is the _selected_ coherency mode rather than the
+ *                  available modes as exposed in the coherency_features register
+ * @thread_tls_alloc: Number of threads per core that TLS must be allocated for
+ * @gpu_features: GPU features
+ *
+ * The information is presented inefficiently for access. For frequent access,
+ * the values should be better expressed in an unpacked form in the
+ * base_gpu_props structure.
+ *
+ * The raw properties in gpu_raw_gpu_props are necessary to
+ * allow a user of the Mali Tools (e.g. PAT) to determine "Why is this device
+ * behaving differently?". In this case, all information about the
+ * configuration is potentially useful, but it does not need to be processed
+ * by the driver. Instead, the raw registers can be processed by the Mali
+ * Tools software on the host PC.
+ *
+ */
+struct gpu_raw_gpu_props {
+   __u64 shader_present;
+   __u64 tiler_present;
+   __u64 l2_present;
+   __u64 stack_present;
+   __u32 l2_features;
+   __u32 core_features;
+   __u32 mem_features;
+   __u32 mmu_features;
+
+   __u32 as_present;
+
+   __u32 js_present;
+   __u32 js_features[GPU_MAX_JOB_SLOTS];
+   __u32 tiler_features;
+   __u32 texture_features[BASE_GPU_NUM_TEXTURE_FEATURES_REGISTERS];
+
+   __u32 gpu_id;
+
+   __u32 thread_max_threads;
+   __u32 thread_max_workgroup_size;
+   __u32 thread_max_barrier_size;
+   __u32 thread_features;
+
+   /*
+    * Note: This is the _selected_ coherency mode rather than the
+    * available modes as exposed in the coherency_features register.
+    */
+   __u32 coherency_mode;
+
+   __u32 thread_tls_alloc;
+   __u64 gpu_features;
+};
+
+/**
+ * struct base_gpu_props - Return structure for base_get_gpu_props().
+ * @core_props:     Core props.
+ * @l2_props:       L2 props.
+ * @unused_1:       Keep for backwards compatibility.
+ * @tiler_props:    Tiler props.
+ * @thread_props:   Thread props.
+ * @raw_props:      This member is large, likely to be 128 bytes.
+ * @coherency_info: This must be last member of the structure.
+ *
+ * NOTE: the raw_props member in this data structure contains the register
+ * values from which the value of the other members are derived. The derived
+ * members exist to allow for efficient access and/or shielding the details
+ * of the layout of the registers.
+ */
+struct base_gpu_props {
+   struct mali_base_gpu_core_props core_props;
+   struct mali_base_gpu_l2_cache_props l2_props;
+   __u64 unused_1;
+   struct mali_base_gpu_tiler_props tiler_props;
+   struct mali_base_gpu_thread_props thread_props;
+   struct gpu_raw_gpu_props raw_props;
+   struct mali_base_gpu_coherent_group_info coherency_info;
+};
+
+#define BASE_MEM_GROUP_ID_GET(flags)                                           \
+   ((flags & BASE_MEM_GROUP_ID_MASK) >> BASEP_MEM_GROUP_ID_SHIFT)
+
+#define BASE_MEM_GROUP_ID_SET(id)                                              \
+   (((base_mem_alloc_flags)((id < 0 || id >= BASE_MEM_GROUP_COUNT)             \
+                               ? BASE_MEM_GROUP_DEFAULT                        \
+                               : id)                                           \
+     << BASEP_MEM_GROUP_ID_SHIFT) &                                            \
+    BASE_MEM_GROUP_ID_MASK)
+
+#define BASE_CONTEXT_MMU_GROUP_ID_SET(group_id)                                \
+   (BASEP_CONTEXT_MMU_GROUP_ID_MASK & ((base_context_create_flags)(group_id)   \
+                                       << BASEP_CONTEXT_MMU_GROUP_ID_SHIFT))
+
+#define BASE_CONTEXT_MMU_GROUP_ID_GET(flags)                                   \
+   ((flags & BASEP_CONTEXT_MMU_GROUP_ID_MASK) >>                               \
+    BASEP_CONTEXT_MMU_GROUP_ID_SHIFT)
+
+/*
+ * A number of bit flags are defined for requesting cpu_gpu_timeinfo. These
+ * flags are also used, where applicable, for specifying which fields
+ * are valid following the request operation.
+ */
+
+/* For monotonic (counter) timefield */
+#define BASE_TIMEINFO_MONOTONIC_FLAG (1UL << 0)
+/* For system wide timestamp */
+#define BASE_TIMEINFO_TIMESTAMP_FLAG (1UL << 1)
+/* For GPU cycle counter */
+#define BASE_TIMEINFO_CYCLE_COUNTER_FLAG (1UL << 2)
+/* Specify kernel GPU register timestamp */
+#define BASE_TIMEINFO_KERNEL_SOURCE_FLAG (1UL << 30)
+/* Specify userspace cntvct_el0 timestamp source */
+#define BASE_TIMEINFO_USER_SOURCE_FLAG (1UL << 31)
+
+#define BASE_TIMEREQUEST_ALLOWED_FLAGS                                         \
+   (BASE_TIMEINFO_MONOTONIC_FLAG | BASE_TIMEINFO_TIMESTAMP_FLAG |              \
+    BASE_TIMEINFO_CYCLE_COUNTER_FLAG | BASE_TIMEINFO_KERNEL_SOURCE_FLAG |      \
+    BASE_TIMEINFO_USER_SOURCE_FLAG)
+
+/* Maximum number of source allocations allowed to create an alias allocation.
+ * This needs to be 4096 * 6 to allow cube map arrays with up to 4096 array
+ * layers, since each cube map in the array will have 6 faces.
+ */
+#define BASE_MEM_ALIAS_MAX_ENTS ((size_t)24576)
+
+#endif /* _UAPI_BASE_KERNEL_H_ */
diff -urN mesa-23.0.0/src/panfrost/csf_test/mali_gpu_csf_registers.h mesa/src/panfrost/csf_test/mali_gpu_csf_registers.h
--- mesa-23.0.0/src/panfrost/csf_test/mali_gpu_csf_registers.h	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/csf_test/mali_gpu_csf_registers.h	2023-03-06 19:19:32.642308402 +0100
@@ -0,0 +1,47 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ *
+ * (C) COPYRIGHT 2018-2021 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU license.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ */
+
+/*
+ * This header was originally autogenerated, but it is now ok (and
+ * expected) to have to add to it.
+ */
+
+#ifndef _UAPI_GPU_CSF_REGISTERS_H_
+#define _UAPI_GPU_CSF_REGISTERS_H_
+
+/* Only user block defines are included. HI words have been removed */
+
+/* CS_USER_INPUT_BLOCK register offsets */
+#define CS_INSERT                                                              \
+   0x0000 /* () Current insert offset for ring buffer, low word */
+#define CS_EXTRACT_INIT                                                        \
+   0x0008 /* () Initial extract offset for ring buffer, low word */
+
+/* CS_USER_OUTPUT_BLOCK register offsets */
+#define CS_EXTRACT                                                             \
+   0x0000 /* () Current extract offset for ring buffer, low word */
+#define CS_ACTIVE 0x0008 /* () Initial extract offset when the CS is started */
+
+/* USER register offsets */
+#define LATEST_FLUSH                                                           \
+   0x0000 /* () Flush ID of latest clean-and-invalidate operation */
+
+#endif
diff -urN mesa-23.0.0/src/panfrost/csf_test/mali_kbase_csf_ioctl.h mesa/src/panfrost/csf_test/mali_kbase_csf_ioctl.h
--- mesa-23.0.0/src/panfrost/csf_test/mali_kbase_csf_ioctl.h	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/csf_test/mali_kbase_csf_ioctl.h	2023-03-06 19:19:32.644308415 +0100
@@ -0,0 +1,480 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ *
+ * (C) COPYRIGHT 2020-2021 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU license.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ */
+
+#ifndef _UAPI_KBASE_CSF_IOCTL_H_
+#define _UAPI_KBASE_CSF_IOCTL_H_
+
+#include <asm-generic/ioctl.h>
+#include <linux/types.h>
+
+/*
+ * 1.0:
+ * - CSF IOCTL header separated from JM
+ * 1.1:
+ * - Add a new priority level BASE_QUEUE_GROUP_PRIORITY_REALTIME
+ * - Add ioctl 54: This controls the priority setting.
+ * 1.2:
+ * - Add new CSF GPU_FEATURES register into the property structure
+ *   returned by KBASE_IOCTL_GET_GPUPROPS
+ * 1.3:
+ * - Add __u32 group_uid member to
+ *   &struct_kbase_ioctl_cs_queue_group_create.out
+ * 1.4:
+ * - Replace padding in kbase_ioctl_cs_get_glb_iface with
+ *   instr_features member of same size
+ * 1.5:
+ * - Add ioctl 40: kbase_ioctl_cs_queue_register_ex, this is a new
+ *   queue registration call with extended format for supporting CS
+ *   trace configurations with CSF trace_command.
+ * 1.6:
+ * - Added new HW performance counters interface to all GPUs.
+ * 1.7:
+ * - Added reserved field to QUEUE_GROUP_CREATE ioctl for future use
+ * 1.8:
+ * - Removed Kernel legacy HWC interface
+ */
+
+#define BASE_UK_VERSION_MAJOR 1
+#define BASE_UK_VERSION_MINOR 8
+
+/**
+ * struct kbase_ioctl_version_check - Check version compatibility between
+ * kernel and userspace
+ *
+ * @major: Major version number
+ * @minor: Minor version number
+ */
+struct kbase_ioctl_version_check {
+   __u16 major;
+   __u16 minor;
+};
+
+#define KBASE_IOCTL_VERSION_CHECK_RESERVED                                     \
+   _IOWR(KBASE_IOCTL_TYPE, 0, struct kbase_ioctl_version_check)
+
+/**
+ * struct kbase_ioctl_cs_queue_register - Register a GPU command queue with the
+ *                                        base back-end
+ *
+ * @buffer_gpu_addr: GPU address of the buffer backing the queue
+ * @buffer_size: Size of the buffer in bytes
+ * @priority: Priority of the queue within a group when run within a process
+ * @padding: Currently unused, must be zero
+ *
+ * @Note: There is an identical sub-section in kbase_ioctl_cs_queue_register_ex.
+ *        Any change of this struct should also be mirrored to the latter.
+ */
+struct kbase_ioctl_cs_queue_register {
+   __u64 buffer_gpu_addr;
+   __u32 buffer_size;
+   __u8 priority;
+   __u8 padding[3];
+};
+
+#define KBASE_IOCTL_CS_QUEUE_REGISTER                                          \
+   _IOW(KBASE_IOCTL_TYPE, 36, struct kbase_ioctl_cs_queue_register)
+
+/**
+ * struct kbase_ioctl_cs_queue_kick - Kick the GPU command queue group scheduler
+ *                                    to notify that a queue has been updated
+ *
+ * @buffer_gpu_addr: GPU address of the buffer backing the queue
+ */
+struct kbase_ioctl_cs_queue_kick {
+   __u64 buffer_gpu_addr;
+};
+
+#define KBASE_IOCTL_CS_QUEUE_KICK                                              \
+   _IOW(KBASE_IOCTL_TYPE, 37, struct kbase_ioctl_cs_queue_kick)
+
+/**
+ * union kbase_ioctl_cs_queue_bind - Bind a GPU command queue to a group
+ *
+ * @in:                 Input parameters
+ * @in.buffer_gpu_addr: GPU address of the buffer backing the queue
+ * @in.group_handle:    Handle of the group to which the queue should be bound
+ * @in.csi_index:       Index of the CSF interface the queue should be bound to
+ * @in.padding:         Currently unused, must be zero
+ * @out:                Output parameters
+ * @out.mmap_handle:    Handle to be used for creating the mapping of CS
+ *                      input/output pages
+ */
+union kbase_ioctl_cs_queue_bind {
+   struct {
+      __u64 buffer_gpu_addr;
+      __u8 group_handle;
+      __u8 csi_index;
+      __u8 padding[6];
+   } in;
+   struct {
+      __u64 mmap_handle;
+   } out;
+};
+
+#define KBASE_IOCTL_CS_QUEUE_BIND                                              \
+   _IOWR(KBASE_IOCTL_TYPE, 39, union kbase_ioctl_cs_queue_bind)
+
+/**
+ * struct kbase_ioctl_cs_queue_register_ex - Register a GPU command queue with
+ * the base back-end in extended format, involving trace buffer configuration
+ *
+ * @buffer_gpu_addr: GPU address of the buffer backing the queue
+ * @buffer_size: Size of the buffer in bytes
+ * @priority: Priority of the queue within a group when run within a process
+ * @padding: Currently unused, must be zero
+ * @ex_offset_var_addr: GPU address of the trace buffer write offset variable
+ * @ex_buffer_base: Trace buffer GPU base address for the queue
+ * @ex_buffer_size: Size of the trace buffer in bytes
+ * @ex_event_size: Trace event write size, in log2 designation
+ * @ex_event_state: Trace event states configuration
+ * @ex_padding: Currently unused, must be zero
+ *
+ * @Note: There is an identical sub-section at the start of this struct to that
+ *        of @ref kbase_ioctl_cs_queue_register. Any change of this sub-section
+ *        must also be mirrored to the latter. Following the said sub-section,
+ *        the remaining fields forms the extension, marked with ex_*.
+ */
+struct kbase_ioctl_cs_queue_register_ex {
+   __u64 buffer_gpu_addr;
+   __u32 buffer_size;
+   __u8 priority;
+   __u8 padding[3];
+   __u64 ex_offset_var_addr;
+   __u64 ex_buffer_base;
+   __u32 ex_buffer_size;
+   __u8 ex_event_size;
+   __u8 ex_event_state;
+   __u8 ex_padding[2];
+};
+
+#define KBASE_IOCTL_CS_QUEUE_REGISTER_EX                                       \
+   _IOW(KBASE_IOCTL_TYPE, 40, struct kbase_ioctl_cs_queue_register_ex)
+
+/**
+ * struct kbase_ioctl_cs_queue_terminate - Terminate a GPU command queue
+ *
+ * @buffer_gpu_addr: GPU address of the buffer backing the queue
+ */
+struct kbase_ioctl_cs_queue_terminate {
+   __u64 buffer_gpu_addr;
+};
+
+#define KBASE_IOCTL_CS_QUEUE_TERMINATE                                         \
+   _IOW(KBASE_IOCTL_TYPE, 41, struct kbase_ioctl_cs_queue_terminate)
+
+/**
+ * union kbase_ioctl_cs_queue_group_create_1_6 - Create a GPU command queue
+ *                                               group
+ * @in:               Input parameters
+ * @in.tiler_mask:    Mask of tiler endpoints the group is allowed to use.
+ * @in.fragment_mask: Mask of fragment endpoints the group is allowed to use.
+ * @in.compute_mask:  Mask of compute endpoints the group is allowed to use.
+ * @in.cs_min:        Minimum number of CSs required.
+ * @in.priority:      Queue group's priority within a process.
+ * @in.tiler_max:     Maximum number of tiler endpoints the group is allowed
+ *                    to use.
+ * @in.fragment_max:  Maximum number of fragment endpoints the group is
+ *                    allowed to use.
+ * @in.compute_max:   Maximum number of compute endpoints the group is allowed
+ *                    to use.
+ * @in.padding:       Currently unused, must be zero
+ * @out:              Output parameters
+ * @out.group_handle: Handle of a newly created queue group.
+ * @out.padding:      Currently unused, must be zero
+ * @out.group_uid:    UID of the queue group available to base.
+ */
+union kbase_ioctl_cs_queue_group_create_1_6 {
+   struct {
+      __u64 tiler_mask;
+      __u64 fragment_mask;
+      __u64 compute_mask;
+      __u8 cs_min;
+      __u8 priority;
+      __u8 tiler_max;
+      __u8 fragment_max;
+      __u8 compute_max;
+      __u8 padding[3];
+
+   } in;
+   struct {
+      __u8 group_handle;
+      __u8 padding[3];
+      __u32 group_uid;
+   } out;
+};
+
+#define KBASE_IOCTL_CS_QUEUE_GROUP_CREATE_1_6                                  \
+   _IOWR(KBASE_IOCTL_TYPE, 42, union kbase_ioctl_cs_queue_group_create_1_6)
+
+/**
+ * union kbase_ioctl_cs_queue_group_create - Create a GPU command queue group
+ * @in:               Input parameters
+ * @in.tiler_mask:    Mask of tiler endpoints the group is allowed to use.
+ * @in.fragment_mask: Mask of fragment endpoints the group is allowed to use.
+ * @in.compute_mask:  Mask of compute endpoints the group is allowed to use.
+ * @in.cs_min:        Minimum number of CSs required.
+ * @in.priority:      Queue group's priority within a process.
+ * @in.tiler_max:     Maximum number of tiler endpoints the group is allowed
+ *                    to use.
+ * @in.fragment_max:  Maximum number of fragment endpoints the group is
+ *                    allowed to use.
+ * @in.compute_max:   Maximum number of compute endpoints the group is allowed
+ *                    to use.
+ * @in.padding:       Currently unused, must be zero
+ * @out:              Output parameters
+ * @out.group_handle: Handle of a newly created queue group.
+ * @out.padding:      Currently unused, must be zero
+ * @out.group_uid:    UID of the queue group available to base.
+ */
+union kbase_ioctl_cs_queue_group_create {
+   struct {
+      __u64 tiler_mask;
+      __u64 fragment_mask;
+      __u64 compute_mask;
+      __u8 cs_min;
+      __u8 priority;
+      __u8 tiler_max;
+      __u8 fragment_max;
+      __u8 compute_max;
+      __u8 padding[3];
+      __u64 reserved;
+   } in;
+   struct {
+      __u8 group_handle;
+      __u8 padding[3];
+      __u32 group_uid;
+   } out;
+};
+
+#define KBASE_IOCTL_CS_QUEUE_GROUP_CREATE                                      \
+   _IOWR(KBASE_IOCTL_TYPE, 58, union kbase_ioctl_cs_queue_group_create)
+
+/**
+ * struct kbase_ioctl_cs_queue_group_term - Terminate a GPU command queue group
+ *
+ * @group_handle: Handle of the queue group to be terminated
+ * @padding: Padding to round up to a multiple of 8 bytes, must be zero
+ */
+struct kbase_ioctl_cs_queue_group_term {
+   __u8 group_handle;
+   __u8 padding[7];
+};
+
+#define KBASE_IOCTL_CS_QUEUE_GROUP_TERMINATE                                   \
+   _IOW(KBASE_IOCTL_TYPE, 43, struct kbase_ioctl_cs_queue_group_term)
+
+#define KBASE_IOCTL_CS_EVENT_SIGNAL _IO(KBASE_IOCTL_TYPE, 44)
+
+typedef __u8 base_kcpu_queue_id; /* We support up to 256 active KCPU queues */
+
+/**
+ * struct kbase_ioctl_kcpu_queue_new - Create a KCPU command queue
+ *
+ * @id: ID of the new command queue returned by the kernel
+ * @padding: Padding to round up to a multiple of 8 bytes, must be zero
+ */
+struct kbase_ioctl_kcpu_queue_new {
+   base_kcpu_queue_id id;
+   __u8 padding[7];
+};
+
+#define KBASE_IOCTL_KCPU_QUEUE_CREATE                                          \
+   _IOR(KBASE_IOCTL_TYPE, 45, struct kbase_ioctl_kcpu_queue_new)
+
+/**
+ * struct kbase_ioctl_kcpu_queue_delete - Destroy a KCPU command queue
+ *
+ * @id: ID of the command queue to be destroyed
+ * @padding: Padding to round up to a multiple of 8 bytes, must be zero
+ */
+struct kbase_ioctl_kcpu_queue_delete {
+   base_kcpu_queue_id id;
+   __u8 padding[7];
+};
+
+#define KBASE_IOCTL_KCPU_QUEUE_DELETE                                          \
+   _IOW(KBASE_IOCTL_TYPE, 46, struct kbase_ioctl_kcpu_queue_delete)
+
+/**
+ * struct kbase_ioctl_kcpu_queue_enqueue - Enqueue commands into the KCPU queue
+ *
+ * @addr: Memory address of an array of struct base_kcpu_queue_command
+ * @nr_commands: Number of commands in the array
+ * @id: kcpu queue identifier, returned by KBASE_IOCTL_KCPU_QUEUE_CREATE ioctl
+ * @padding: Padding to round up to a multiple of 8 bytes, must be zero
+ */
+struct kbase_ioctl_kcpu_queue_enqueue {
+   __u64 addr;
+   __u32 nr_commands;
+   base_kcpu_queue_id id;
+   __u8 padding[3];
+};
+
+#define KBASE_IOCTL_KCPU_QUEUE_ENQUEUE                                         \
+   _IOW(KBASE_IOCTL_TYPE, 47, struct kbase_ioctl_kcpu_queue_enqueue)
+
+/**
+ * union kbase_ioctl_cs_tiler_heap_init - Initialize chunked tiler memory heap
+ * @in:                Input parameters
+ * @in.chunk_size:     Size of each chunk.
+ * @in.initial_chunks: Initial number of chunks that heap will be created with.
+ * @in.max_chunks:     Maximum number of chunks that the heap is allowed to use.
+ * @in.target_in_flight: Number of render-passes that the driver should attempt
+ * to keep in flight for which allocation of new chunks is allowed.
+ * @in.group_id:       Group ID to be used for physical allocations.
+ * @in.padding:        Padding
+ * @out:               Output parameters
+ * @out.gpu_heap_va:   GPU VA (virtual address) of Heap context that was set up
+ *                     for the heap.
+ * @out.first_chunk_va: GPU VA of the first chunk allocated for the heap,
+ *                     actually points to the header of heap chunk and not to
+ *                     the low address of free memory in the chunk.
+ */
+union kbase_ioctl_cs_tiler_heap_init {
+   struct {
+      __u32 chunk_size;
+      __u32 initial_chunks;
+      __u32 max_chunks;
+      __u16 target_in_flight;
+      __u8 group_id;
+      __u8 padding;
+   } in;
+   struct {
+      __u64 gpu_heap_va;
+      __u64 first_chunk_va;
+   } out;
+};
+
+#define KBASE_IOCTL_CS_TILER_HEAP_INIT                                         \
+   _IOWR(KBASE_IOCTL_TYPE, 48, union kbase_ioctl_cs_tiler_heap_init)
+
+/**
+ * struct kbase_ioctl_cs_tiler_heap_term - Terminate a chunked tiler heap
+ *                                         instance
+ *
+ * @gpu_heap_va: GPU VA of Heap context that was set up for the heap.
+ */
+struct kbase_ioctl_cs_tiler_heap_term {
+   __u64 gpu_heap_va;
+};
+
+#define KBASE_IOCTL_CS_TILER_HEAP_TERM                                         \
+   _IOW(KBASE_IOCTL_TYPE, 49, struct kbase_ioctl_cs_tiler_heap_term)
+
+/**
+ * union kbase_ioctl_cs_get_glb_iface - Request the global control block
+ *                                        of CSF interface capabilities
+ *
+ * @in:                    Input parameters
+ * @in.max_group_num:      The maximum number of groups to be read. Can be 0, in
+ *                         which case groups_ptr is unused.
+ * @in.max_total_stream    _num: The maximum number of CSs to be read. Can be 0,
+ * in which case streams_ptr is unused.
+ * @in.groups_ptr:         Pointer where to store all the group data
+ * (sequentially).
+ * @in.streams_ptr:        Pointer where to store all the CS data
+ * (sequentially).
+ * @out:                   Output parameters
+ * @out.glb_version:       Global interface version.
+ * @out.features:          Bit mask of features (e.g. whether certain types of
+ * job can be suspended).
+ * @out.group_num:         Number of CSGs supported.
+ * @out.prfcnt_size:       Size of CSF performance counters, in bytes. Bits
+ * 31:16 hold the size of firmware performance counter data and 15:0 hold the
+ * size of hardware performance counter data.
+ * @out.total_stream_num:  Total number of CSs, summed across all groups.
+ * @out.instr_features:    Instrumentation features. Bits 7:4 hold the maximum
+ *                         size of events. Bits 3:0 hold the offset update rate.
+ *                         (csf >= 1.1.0)
+ *
+ */
+union kbase_ioctl_cs_get_glb_iface {
+   struct {
+      __u32 max_group_num;
+      __u32 max_total_stream_num;
+      __u64 groups_ptr;
+      __u64 streams_ptr;
+   } in;
+   struct {
+      __u32 glb_version;
+      __u32 features;
+      __u32 group_num;
+      __u32 prfcnt_size;
+      __u32 total_stream_num;
+      __u32 instr_features;
+   } out;
+};
+
+#define KBASE_IOCTL_CS_GET_GLB_IFACE                                           \
+   _IOWR(KBASE_IOCTL_TYPE, 51, union kbase_ioctl_cs_get_glb_iface)
+
+struct kbase_ioctl_cs_cpu_queue_info {
+   __u64 buffer;
+   __u64 size;
+};
+
+#define KBASE_IOCTL_VERSION_CHECK                                              \
+   _IOWR(KBASE_IOCTL_TYPE, 52, struct kbase_ioctl_version_check)
+
+#define KBASE_IOCTL_CS_CPU_QUEUE_DUMP                                          \
+   _IOW(KBASE_IOCTL_TYPE, 53, struct kbase_ioctl_cs_cpu_queue_info)
+
+/***************
+ * test ioctls *
+ ***************/
+#if MALI_UNIT_TEST
+/* These ioctls are purely for test purposes and are not used in the production
+ * driver, they therefore may change without notice
+ */
+
+/**
+ * struct kbase_ioctl_cs_event_memory_write - Write an event memory address
+ * @cpu_addr: Memory address to write
+ * @value: Value to write
+ * @padding: Currently unused, must be zero
+ */
+struct kbase_ioctl_cs_event_memory_write {
+   __u64 cpu_addr;
+   __u8 value;
+   __u8 padding[7];
+};
+
+/**
+ * union kbase_ioctl_cs_event_memory_read - Read an event memory address
+ * @in: Input parameters
+ * @in.cpu_addr: Memory address to read
+ * @out: Output parameters
+ * @out.value: Value read
+ * @out.padding: Currently unused, must be zero
+ */
+union kbase_ioctl_cs_event_memory_read {
+   struct {
+      __u64 cpu_addr;
+   } in;
+   struct {
+      __u8 value;
+      __u8 padding[7];
+   } out;
+};
+
+#endif /* MALI_UNIT_TEST */
+
+#endif /* _UAPI_KBASE_CSF_IOCTL_H_ */
diff -urN mesa-23.0.0/src/panfrost/csf_test/mali_kbase_ioctl.h mesa/src/panfrost/csf_test/mali_kbase_ioctl.h
--- mesa-23.0.0/src/panfrost/csf_test/mali_kbase_ioctl.h	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/csf_test/mali_kbase_ioctl.h	2023-03-06 19:19:32.649308448 +0100
@@ -0,0 +1,850 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ *
+ * (C) COPYRIGHT 2017-2021 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU license.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ */
+
+#ifndef _UAPI_KBASE_IOCTL_H_
+#define _UAPI_KBASE_IOCTL_H_
+
+#ifdef __cpluscplus
+extern "C" {
+#endif
+
+#include <asm-generic/ioctl.h>
+#include <linux/types.h>
+
+#define KBASE_IOCTL_TYPE 0x80
+
+/**
+ * struct kbase_ioctl_set_flags - Set kernel context creation flags
+ *
+ * @create_flags: Flags - see base_context_create_flags
+ */
+struct kbase_ioctl_set_flags {
+   __u32 create_flags;
+};
+
+#define KBASE_IOCTL_SET_FLAGS                                                  \
+   _IOW(KBASE_IOCTL_TYPE, 1, struct kbase_ioctl_set_flags)
+
+/**
+ * struct kbase_ioctl_get_gpuprops - Read GPU properties from the kernel
+ *
+ * @buffer: Pointer to the buffer to store properties into
+ * @size: Size of the buffer
+ * @flags: Flags - must be zero for now
+ *
+ * The ioctl will return the number of bytes stored into @buffer or an error
+ * on failure (e.g. @size is too small). If @size is specified as 0 then no
+ * data will be written but the return value will be the number of bytes needed
+ * for all the properties.
+ *
+ * @flags may be used in the future to request a different format for the
+ * buffer. With @flags == 0 the following format is used.
+ *
+ * The buffer will be filled with pairs of values, a __u32 key identifying the
+ * property followed by the value. The size of the value is identified using
+ * the bottom bits of the key. The value then immediately followed the key and
+ * is tightly packed (there is no padding). All keys and values are
+ * little-endian.
+ *
+ * 00 = __u8
+ * 01 = __u16
+ * 10 = __u32
+ * 11 = __u64
+ */
+struct kbase_ioctl_get_gpuprops {
+   __u64 buffer;
+   __u32 size;
+   __u32 flags;
+};
+
+#define KBASE_IOCTL_GET_GPUPROPS                                               \
+   _IOW(KBASE_IOCTL_TYPE, 3, struct kbase_ioctl_get_gpuprops)
+
+/**
+ * union kbase_ioctl_mem_alloc - Allocate memory on the GPU
+ * @in: Input parameters
+ * @in.va_pages: The number of pages of virtual address space to reserve
+ * @in.commit_pages: The number of physical pages to allocate
+ * @in.extension: The number of extra pages to allocate on each GPU fault which
+ * grows the region
+ * @in.flags: Flags
+ * @out: Output parameters
+ * @out.flags: Flags
+ * @out.gpu_va: The GPU virtual address which is allocated
+ */
+union kbase_ioctl_mem_alloc {
+   struct {
+      __u64 va_pages;
+      __u64 commit_pages;
+      __u64 extension;
+      __u64 flags;
+   } in;
+   struct {
+      __u64 flags;
+      __u64 gpu_va;
+   } out;
+};
+
+#define KBASE_IOCTL_MEM_ALLOC                                                  \
+   _IOWR(KBASE_IOCTL_TYPE, 5, union kbase_ioctl_mem_alloc)
+
+/**
+ * struct kbase_ioctl_mem_query - Query properties of a GPU memory region
+ * @in: Input parameters
+ * @in.gpu_addr: A GPU address contained within the region
+ * @in.query: The type of query
+ * @out: Output parameters
+ * @out.value: The result of the query
+ *
+ * Use a %KBASE_MEM_QUERY_xxx flag as input for @query.
+ */
+union kbase_ioctl_mem_query {
+   struct {
+      __u64 gpu_addr;
+      __u64 query;
+   } in;
+   struct {
+      __u64 value;
+   } out;
+};
+
+#define KBASE_IOCTL_MEM_QUERY                                                  \
+   _IOWR(KBASE_IOCTL_TYPE, 6, union kbase_ioctl_mem_query)
+
+#define KBASE_MEM_QUERY_COMMIT_SIZE ((__u64)1)
+#define KBASE_MEM_QUERY_VA_SIZE     ((__u64)2)
+#define KBASE_MEM_QUERY_FLAGS       ((__u64)3)
+
+/**
+ * struct kbase_ioctl_mem_free - Free a memory region
+ * @gpu_addr: Handle to the region to free
+ */
+struct kbase_ioctl_mem_free {
+   __u64 gpu_addr;
+};
+
+#define KBASE_IOCTL_MEM_FREE                                                   \
+   _IOW(KBASE_IOCTL_TYPE, 7, struct kbase_ioctl_mem_free)
+
+/**
+ * struct kbase_ioctl_hwcnt_reader_setup - Setup HWC dumper/reader
+ * @buffer_count: requested number of dumping buffers
+ * @fe_bm:        counters selection bitmask (Front end)
+ * @shader_bm:    counters selection bitmask (Shader)
+ * @tiler_bm:     counters selection bitmask (Tiler)
+ * @mmu_l2_bm:    counters selection bitmask (MMU_L2)
+ *
+ * A fd is returned from the ioctl if successful, or a negative value on error
+ */
+struct kbase_ioctl_hwcnt_reader_setup {
+   __u32 buffer_count;
+   __u32 fe_bm;
+   __u32 shader_bm;
+   __u32 tiler_bm;
+   __u32 mmu_l2_bm;
+};
+
+#define KBASE_IOCTL_HWCNT_READER_SETUP                                         \
+   _IOW(KBASE_IOCTL_TYPE, 8, struct kbase_ioctl_hwcnt_reader_setup)
+
+/**
+ * struct kbase_ioctl_hwcnt_values - Values to set dummy the dummy counters to.
+ * @data:    Counter samples for the dummy model.
+ * @size:    Size of the counter sample data.
+ * @padding: Padding.
+ */
+struct kbase_ioctl_hwcnt_values {
+   __u64 data;
+   __u32 size;
+   __u32 padding;
+};
+
+#define KBASE_IOCTL_HWCNT_SET                                                  \
+   _IOW(KBASE_IOCTL_TYPE, 32, struct kbase_ioctl_hwcnt_values)
+
+/**
+ * struct kbase_ioctl_disjoint_query - Query the disjoint counter
+ * @counter:   A counter of disjoint events in the kernel
+ */
+struct kbase_ioctl_disjoint_query {
+   __u32 counter;
+};
+
+#define KBASE_IOCTL_DISJOINT_QUERY                                             \
+   _IOR(KBASE_IOCTL_TYPE, 12, struct kbase_ioctl_disjoint_query)
+
+/**
+ * struct kbase_ioctl_get_ddk_version - Query the kernel version
+ * @version_buffer: Buffer to receive the kernel version string
+ * @size: Size of the buffer
+ * @padding: Padding
+ *
+ * The ioctl will return the number of bytes written into version_buffer
+ * (which includes a NULL byte) or a negative error code
+ *
+ * The ioctl request code has to be _IOW because the data in ioctl struct is
+ * being copied to the kernel, even though the kernel then writes out the
+ * version info to the buffer specified in the ioctl.
+ */
+struct kbase_ioctl_get_ddk_version {
+   __u64 version_buffer;
+   __u32 size;
+   __u32 padding;
+};
+
+#define KBASE_IOCTL_GET_DDK_VERSION                                            \
+   _IOW(KBASE_IOCTL_TYPE, 13, struct kbase_ioctl_get_ddk_version)
+
+/**
+ * struct kbase_ioctl_mem_jit_init_10_2 - Initialize the just-in-time memory
+ *                                        allocator (between kernel driver
+ *                                        version 10.2--11.4)
+ * @va_pages: Number of VA pages to reserve for JIT
+ *
+ * Note that depending on the VA size of the application and GPU, the value
+ * specified in @va_pages may be ignored.
+ *
+ * New code should use KBASE_IOCTL_MEM_JIT_INIT instead, this is kept for
+ * backwards compatibility.
+ */
+struct kbase_ioctl_mem_jit_init_10_2 {
+   __u64 va_pages;
+};
+
+#define KBASE_IOCTL_MEM_JIT_INIT_10_2                                          \
+   _IOW(KBASE_IOCTL_TYPE, 14, struct kbase_ioctl_mem_jit_init_10_2)
+
+/**
+ * struct kbase_ioctl_mem_jit_init_11_5 - Initialize the just-in-time memory
+ *                                        allocator (between kernel driver
+ *                                        version 11.5--11.19)
+ * @va_pages: Number of VA pages to reserve for JIT
+ * @max_allocations: Maximum number of concurrent allocations
+ * @trim_level: Level of JIT allocation trimming to perform on free (0 - 100%)
+ * @group_id: Group ID to be used for physical allocations
+ * @padding: Currently unused, must be zero
+ *
+ * Note that depending on the VA size of the application and GPU, the value
+ * specified in @va_pages may be ignored.
+ *
+ * New code should use KBASE_IOCTL_MEM_JIT_INIT instead, this is kept for
+ * backwards compatibility.
+ */
+struct kbase_ioctl_mem_jit_init_11_5 {
+   __u64 va_pages;
+   __u8 max_allocations;
+   __u8 trim_level;
+   __u8 group_id;
+   __u8 padding[5];
+};
+
+#define KBASE_IOCTL_MEM_JIT_INIT_11_5                                          \
+   _IOW(KBASE_IOCTL_TYPE, 14, struct kbase_ioctl_mem_jit_init_11_5)
+
+/**
+ * struct kbase_ioctl_mem_jit_init - Initialize the just-in-time memory
+ *                                   allocator
+ * @va_pages: Number of GPU virtual address pages to reserve for just-in-time
+ *            memory allocations
+ * @max_allocations: Maximum number of concurrent allocations
+ * @trim_level: Level of JIT allocation trimming to perform on free (0 - 100%)
+ * @group_id: Group ID to be used for physical allocations
+ * @padding: Currently unused, must be zero
+ * @phys_pages: Maximum number of physical pages to allocate just-in-time
+ *
+ * Note that depending on the VA size of the application and GPU, the value
+ * specified in @va_pages may be ignored.
+ */
+struct kbase_ioctl_mem_jit_init {
+   __u64 va_pages;
+   __u8 max_allocations;
+   __u8 trim_level;
+   __u8 group_id;
+   __u8 padding[5];
+   __u64 phys_pages;
+};
+
+#define KBASE_IOCTL_MEM_JIT_INIT                                               \
+   _IOW(KBASE_IOCTL_TYPE, 14, struct kbase_ioctl_mem_jit_init)
+
+/**
+ * struct kbase_ioctl_mem_sync - Perform cache maintenance on memory
+ *
+ * @handle: GPU memory handle (GPU VA)
+ * @user_addr: The address where it is mapped in user space
+ * @size: The number of bytes to synchronise
+ * @type: The direction to synchronise: 0 is sync to memory (clean),
+ * 1 is sync from memory (invalidate). Use the BASE_SYNCSET_OP_xxx constants.
+ * @padding: Padding to round up to a multiple of 8 bytes, must be zero
+ */
+struct kbase_ioctl_mem_sync {
+   __u64 handle;
+   __u64 user_addr;
+   __u64 size;
+   __u8 type;
+   __u8 padding[7];
+};
+
+#define KBASE_IOCTL_MEM_SYNC                                                   \
+   _IOW(KBASE_IOCTL_TYPE, 15, struct kbase_ioctl_mem_sync)
+
+/**
+ * union kbase_ioctl_mem_find_cpu_offset - Find the offset of a CPU pointer
+ *
+ * @in: Input parameters
+ * @in.gpu_addr: The GPU address of the memory region
+ * @in.cpu_addr: The CPU address to locate
+ * @in.size: A size in bytes to validate is contained within the region
+ * @out: Output parameters
+ * @out.offset: The offset from the start of the memory region to @cpu_addr
+ */
+union kbase_ioctl_mem_find_cpu_offset {
+   struct {
+      __u64 gpu_addr;
+      __u64 cpu_addr;
+      __u64 size;
+   } in;
+   struct {
+      __u64 offset;
+   } out;
+};
+
+#define KBASE_IOCTL_MEM_FIND_CPU_OFFSET                                        \
+   _IOWR(KBASE_IOCTL_TYPE, 16, union kbase_ioctl_mem_find_cpu_offset)
+
+/**
+ * struct kbase_ioctl_get_context_id - Get the kernel context ID
+ *
+ * @id: The kernel context ID
+ */
+struct kbase_ioctl_get_context_id {
+   __u32 id;
+};
+
+#define KBASE_IOCTL_GET_CONTEXT_ID                                             \
+   _IOR(KBASE_IOCTL_TYPE, 17, struct kbase_ioctl_get_context_id)
+
+/**
+ * struct kbase_ioctl_tlstream_acquire - Acquire a tlstream fd
+ *
+ * @flags: Flags
+ *
+ * The ioctl returns a file descriptor when successful
+ */
+struct kbase_ioctl_tlstream_acquire {
+   __u32 flags;
+};
+
+#define KBASE_IOCTL_TLSTREAM_ACQUIRE                                           \
+   _IOW(KBASE_IOCTL_TYPE, 18, struct kbase_ioctl_tlstream_acquire)
+
+#define KBASE_IOCTL_TLSTREAM_FLUSH _IO(KBASE_IOCTL_TYPE, 19)
+
+/**
+ * struct kbase_ioctl_mem_commit - Change the amount of memory backing a region
+ *
+ * @gpu_addr: The memory region to modify
+ * @pages:    The number of physical pages that should be present
+ *
+ * The ioctl may return on the following error codes or 0 for success:
+ *   -ENOMEM: Out of memory
+ *   -EINVAL: Invalid arguments
+ */
+struct kbase_ioctl_mem_commit {
+   __u64 gpu_addr;
+   __u64 pages;
+};
+
+#define KBASE_IOCTL_MEM_COMMIT                                                 \
+   _IOW(KBASE_IOCTL_TYPE, 20, struct kbase_ioctl_mem_commit)
+
+/**
+ * union kbase_ioctl_mem_alias - Create an alias of memory regions
+ * @in: Input parameters
+ * @in.flags: Flags, see BASE_MEM_xxx
+ * @in.stride: Bytes between start of each memory region
+ * @in.nents: The number of regions to pack together into the alias
+ * @in.aliasing_info: Pointer to an array of struct base_mem_aliasing_info
+ * @out: Output parameters
+ * @out.flags: Flags, see BASE_MEM_xxx
+ * @out.gpu_va: Address of the new alias
+ * @out.va_pages: Size of the new alias
+ */
+union kbase_ioctl_mem_alias {
+   struct {
+      __u64 flags;
+      __u64 stride;
+      __u64 nents;
+      __u64 aliasing_info;
+   } in;
+   struct {
+      __u64 flags;
+      __u64 gpu_va;
+      __u64 va_pages;
+   } out;
+};
+
+#define KBASE_IOCTL_MEM_ALIAS                                                  \
+   _IOWR(KBASE_IOCTL_TYPE, 21, union kbase_ioctl_mem_alias)
+
+/**
+ * union kbase_ioctl_mem_import - Import memory for use by the GPU
+ * @in: Input parameters
+ * @in.flags: Flags, see BASE_MEM_xxx
+ * @in.phandle: Handle to the external memory
+ * @in.type: Type of external memory, see base_mem_import_type
+ * @in.padding: Amount of extra VA pages to append to the imported buffer
+ * @out: Output parameters
+ * @out.flags: Flags, see BASE_MEM_xxx
+ * @out.gpu_va: Address of the new alias
+ * @out.va_pages: Size of the new alias
+ */
+union kbase_ioctl_mem_import {
+   struct {
+      __u64 flags;
+      __u64 phandle;
+      __u32 type;
+      __u32 padding;
+   } in;
+   struct {
+      __u64 flags;
+      __u64 gpu_va;
+      __u64 va_pages;
+   } out;
+};
+
+#define KBASE_IOCTL_MEM_IMPORT                                                 \
+   _IOWR(KBASE_IOCTL_TYPE, 22, union kbase_ioctl_mem_import)
+
+/**
+ * struct kbase_ioctl_mem_flags_change - Change the flags for a memory region
+ * @gpu_va: The GPU region to modify
+ * @flags: The new flags to set
+ * @mask: Mask of the flags to modify
+ */
+struct kbase_ioctl_mem_flags_change {
+   __u64 gpu_va;
+   __u64 flags;
+   __u64 mask;
+};
+
+#define KBASE_IOCTL_MEM_FLAGS_CHANGE                                           \
+   _IOW(KBASE_IOCTL_TYPE, 23, struct kbase_ioctl_mem_flags_change)
+
+/**
+ * struct kbase_ioctl_stream_create - Create a synchronisation stream
+ * @name: A name to identify this stream. Must be NULL-terminated.
+ *
+ * Note that this is also called a "timeline", but is named stream to avoid
+ * confusion with other uses of the word.
+ *
+ * Unused bytes in @name (after the first NULL byte) must be also be NULL bytes.
+ *
+ * The ioctl returns a file descriptor.
+ */
+struct kbase_ioctl_stream_create {
+   char name[32];
+};
+
+#define KBASE_IOCTL_STREAM_CREATE                                              \
+   _IOW(KBASE_IOCTL_TYPE, 24, struct kbase_ioctl_stream_create)
+
+/**
+ * struct kbase_ioctl_fence_validate - Validate a fd refers to a fence
+ * @fd: The file descriptor to validate
+ */
+struct kbase_ioctl_fence_validate {
+   int fd;
+};
+
+#define KBASE_IOCTL_FENCE_VALIDATE                                             \
+   _IOW(KBASE_IOCTL_TYPE, 25, struct kbase_ioctl_fence_validate)
+
+/**
+ * struct kbase_ioctl_mem_profile_add - Provide profiling information to kernel
+ * @buffer: Pointer to the information
+ * @len: Length
+ * @padding: Padding
+ *
+ * The data provided is accessible through a debugfs file
+ */
+struct kbase_ioctl_mem_profile_add {
+   __u64 buffer;
+   __u32 len;
+   __u32 padding;
+};
+
+#define KBASE_IOCTL_MEM_PROFILE_ADD                                            \
+   _IOW(KBASE_IOCTL_TYPE, 27, struct kbase_ioctl_mem_profile_add)
+
+/**
+ * struct kbase_ioctl_sticky_resource_map - Permanently map an external resource
+ * @count: Number of resources
+ * @address: Array of __u64 GPU addresses of the external resources to map
+ */
+struct kbase_ioctl_sticky_resource_map {
+   __u64 count;
+   __u64 address;
+};
+
+#define KBASE_IOCTL_STICKY_RESOURCE_MAP                                        \
+   _IOW(KBASE_IOCTL_TYPE, 29, struct kbase_ioctl_sticky_resource_map)
+
+/**
+ * struct kbase_ioctl_sticky_resource_map - Unmap a resource mapped which was
+ *                                          previously permanently mapped
+ * @count: Number of resources
+ * @address: Array of __u64 GPU addresses of the external resources to unmap
+ */
+struct kbase_ioctl_sticky_resource_unmap {
+   __u64 count;
+   __u64 address;
+};
+
+#define KBASE_IOCTL_STICKY_RESOURCE_UNMAP                                      \
+   _IOW(KBASE_IOCTL_TYPE, 30, struct kbase_ioctl_sticky_resource_unmap)
+
+/**
+ * union kbase_ioctl_mem_find_gpu_start_and_offset - Find the start address of
+ *                                                   the GPU memory region for
+ *                                                   the given gpu address and
+ *                                                   the offset of that address
+ *                                                   into the region
+ * @in: Input parameters
+ * @in.gpu_addr: GPU virtual address
+ * @in.size: Size in bytes within the region
+ * @out: Output parameters
+ * @out.start: Address of the beginning of the memory region enclosing @gpu_addr
+ *             for the length of @offset bytes
+ * @out.offset: The offset from the start of the memory region to @gpu_addr
+ */
+union kbase_ioctl_mem_find_gpu_start_and_offset {
+   struct {
+      __u64 gpu_addr;
+      __u64 size;
+   } in;
+   struct {
+      __u64 start;
+      __u64 offset;
+   } out;
+};
+
+#define KBASE_IOCTL_MEM_FIND_GPU_START_AND_OFFSET                              \
+   _IOWR(KBASE_IOCTL_TYPE, 31, union kbase_ioctl_mem_find_gpu_start_and_offset)
+
+#define KBASE_IOCTL_CINSTR_GWT_START _IO(KBASE_IOCTL_TYPE, 33)
+
+#define KBASE_IOCTL_CINSTR_GWT_STOP _IO(KBASE_IOCTL_TYPE, 34)
+
+/**
+ * union kbase_ioctl_gwt_dump - Used to collect all GPU write fault addresses.
+ * @in: Input parameters
+ * @in.addr_buffer: Address of buffer to hold addresses of gpu modified areas.
+ * @in.size_buffer: Address of buffer to hold size of modified areas (in pages)
+ * @in.len: Number of addresses the buffers can hold.
+ * @in.padding: padding
+ * @out: Output parameters
+ * @out.no_of_addr_collected: Number of addresses collected into addr_buffer.
+ * @out.more_data_available: Status indicating if more addresses are available.
+ * @out.padding: padding
+ *
+ * This structure is used when performing a call to dump GPU write fault
+ * addresses.
+ */
+union kbase_ioctl_cinstr_gwt_dump {
+   struct {
+      __u64 addr_buffer;
+      __u64 size_buffer;
+      __u32 len;
+      __u32 padding;
+
+   } in;
+   struct {
+      __u32 no_of_addr_collected;
+      __u8 more_data_available;
+      __u8 padding[27];
+   } out;
+};
+
+#define KBASE_IOCTL_CINSTR_GWT_DUMP                                            \
+   _IOWR(KBASE_IOCTL_TYPE, 35, union kbase_ioctl_cinstr_gwt_dump)
+
+/**
+ * struct kbase_ioctl_mem_exec_init - Initialise the EXEC_VA memory zone
+ *
+ * @va_pages: Number of VA pages to reserve for EXEC_VA
+ */
+struct kbase_ioctl_mem_exec_init {
+   __u64 va_pages;
+};
+
+#define KBASE_IOCTL_MEM_EXEC_INIT                                              \
+   _IOW(KBASE_IOCTL_TYPE, 38, struct kbase_ioctl_mem_exec_init)
+
+/**
+ * union kbase_ioctl_get_cpu_gpu_timeinfo - Request zero or more types of
+ *                                          cpu/gpu time (counter values)
+ * @in: Input parameters
+ * @in.request_flags: Bit-flags indicating the requested types.
+ * @in.paddings:      Unused, size alignment matching the out.
+ * @out: Output parameters
+ * @out.sec:           Integer field of the monotonic time, unit in seconds.
+ * @out.nsec:          Fractional sec of the monotonic time, in nano-seconds.
+ * @out.padding:       Unused, for __u64 alignment
+ * @out.timestamp:     System wide timestamp (counter) value.
+ * @out.cycle_counter: GPU cycle counter value.
+ */
+union kbase_ioctl_get_cpu_gpu_timeinfo {
+   struct {
+      __u32 request_flags;
+      __u32 paddings[7];
+   } in;
+   struct {
+      __u64 sec;
+      __u32 nsec;
+      __u32 padding;
+      __u64 timestamp;
+      __u64 cycle_counter;
+   } out;
+};
+
+#define KBASE_IOCTL_GET_CPU_GPU_TIMEINFO                                       \
+   _IOWR(KBASE_IOCTL_TYPE, 50, union kbase_ioctl_get_cpu_gpu_timeinfo)
+
+/**
+ * struct kbase_ioctl_context_priority_check - Check the max possible priority
+ * @priority: Input priority & output priority
+ */
+
+struct kbase_ioctl_context_priority_check {
+   __u8 priority;
+};
+
+#define KBASE_IOCTL_CONTEXT_PRIORITY_CHECK                                     \
+   _IOWR(KBASE_IOCTL_TYPE, 54, struct kbase_ioctl_context_priority_check)
+
+/**
+ * struct kbase_ioctl_set_limited_core_count - Set the limited core count.
+ *
+ * @max_core_count: Maximum core count
+ */
+struct kbase_ioctl_set_limited_core_count {
+   __u8 max_core_count;
+};
+
+#define KBASE_IOCTL_SET_LIMITED_CORE_COUNT                                     \
+   _IOW(KBASE_IOCTL_TYPE, 55, struct kbase_ioctl_set_limited_core_count)
+
+/**
+ * struct kbase_ioctl_kinstr_prfcnt_enum_info - Enum Performance counter
+ *                                              information
+ * @info_item_size:  Performance counter item size in bytes.
+ * @info_item_count: Performance counter item count in the info_list_ptr.
+ * @info_list_ptr:   Performance counter item list pointer which points to a
+ *                   list with info_item_count of items.
+ *
+ * On success: returns info_item_size and info_item_count if info_list_ptr is
+ * NULL, returns performance counter information if info_list_ptr is not NULL.
+ * On error: returns a negative error code.
+ */
+struct kbase_ioctl_kinstr_prfcnt_enum_info {
+   __u32 info_item_size;
+   __u32 info_item_count;
+   __u64 info_list_ptr;
+};
+
+#define KBASE_IOCTL_KINSTR_PRFCNT_ENUM_INFO                                    \
+   _IOWR(KBASE_IOCTL_TYPE, 56, struct kbase_ioctl_kinstr_prfcnt_enum_info)
+
+/**
+ * struct kbase_ioctl_hwcnt_reader_setup - Setup HWC dumper/reader
+ * @in: input parameters.
+ * @in.request_item_count: Number of requests in the requests array.
+ * @in.request_item_size:  Size in bytes of each request in the requests array.
+ * @in.requests_ptr:       Pointer to the requests array.
+ * @out: output parameters.
+ * @out.prfcnt_metadata_item_size: Size of each item in the metadata array for
+ *                                 each sample.
+ * @out.prfcnt_mmap_size_bytes:    Size in bytes that user-space should mmap
+ *                                 for reading performance counter samples.
+ *
+ * A fd is returned from the ioctl if successful, or a negative value on error.
+ */
+union kbase_ioctl_kinstr_prfcnt_setup {
+   struct {
+      __u32 request_item_count;
+      __u32 request_item_size;
+      __u64 requests_ptr;
+   } in;
+   struct {
+      __u32 prfcnt_metadata_item_size;
+      __u32 prfcnt_mmap_size_bytes;
+   } out;
+};
+
+#define KBASE_IOCTL_KINSTR_PRFCNT_SETUP                                        \
+   _IOWR(KBASE_IOCTL_TYPE, 57, union kbase_ioctl_kinstr_prfcnt_setup)
+
+/***************
+ * test ioctls *
+ ***************/
+#if MALI_UNIT_TEST
+/* These ioctls are purely for test purposes and are not used in the production
+ * driver, they therefore may change without notice
+ */
+
+#define KBASE_IOCTL_TEST_TYPE (KBASE_IOCTL_TYPE + 1)
+
+/**
+ * struct kbase_ioctl_tlstream_stats - Read tlstream stats for test purposes
+ * @bytes_collected: number of bytes read by user
+ * @bytes_generated: number of bytes generated by tracepoints
+ */
+struct kbase_ioctl_tlstream_stats {
+   __u32 bytes_collected;
+   __u32 bytes_generated;
+};
+
+#define KBASE_IOCTL_TLSTREAM_STATS                                             \
+   _IOR(KBASE_IOCTL_TEST_TYPE, 2, struct kbase_ioctl_tlstream_stats)
+
+#endif /* MALI_UNIT_TEST */
+
+/* Customer extension range */
+#define KBASE_IOCTL_EXTRA_TYPE (KBASE_IOCTL_TYPE + 2)
+
+/* If the integration needs extra ioctl add them there
+ * like this:
+ *
+ * struct my_ioctl_args {
+ *  ....
+ * }
+ *
+ * #define KBASE_IOCTL_MY_IOCTL \
+ *         _IOWR(KBASE_IOCTL_EXTRA_TYPE, 0, struct my_ioctl_args)
+ */
+
+/**********************************
+ * Definitions for GPU properties *
+ **********************************/
+#define KBASE_GPUPROP_VALUE_SIZE_U8  (0x0)
+#define KBASE_GPUPROP_VALUE_SIZE_U16 (0x1)
+#define KBASE_GPUPROP_VALUE_SIZE_U32 (0x2)
+#define KBASE_GPUPROP_VALUE_SIZE_U64 (0x3)
+
+#define KBASE_GPUPROP_PRODUCT_ID     1
+#define KBASE_GPUPROP_VERSION_STATUS 2
+#define KBASE_GPUPROP_MINOR_REVISION 3
+#define KBASE_GPUPROP_MAJOR_REVISION 4
+/* 5 previously used for GPU speed */
+#define KBASE_GPUPROP_GPU_FREQ_KHZ_MAX 6
+/* 7 previously used for minimum GPU speed */
+#define KBASE_GPUPROP_LOG2_PROGRAM_COUNTER_SIZE 8
+#define KBASE_GPUPROP_TEXTURE_FEATURES_0        9
+#define KBASE_GPUPROP_TEXTURE_FEATURES_1        10
+#define KBASE_GPUPROP_TEXTURE_FEATURES_2        11
+#define KBASE_GPUPROP_GPU_AVAILABLE_MEMORY_SIZE 12
+
+#define KBASE_GPUPROP_L2_LOG2_LINE_SIZE  13
+#define KBASE_GPUPROP_L2_LOG2_CACHE_SIZE 14
+#define KBASE_GPUPROP_L2_NUM_L2_SLICES   15
+
+#define KBASE_GPUPROP_TILER_BIN_SIZE_BYTES    16
+#define KBASE_GPUPROP_TILER_MAX_ACTIVE_LEVELS 17
+
+#define KBASE_GPUPROP_MAX_THREADS            18
+#define KBASE_GPUPROP_MAX_WORKGROUP_SIZE     19
+#define KBASE_GPUPROP_MAX_BARRIER_SIZE       20
+#define KBASE_GPUPROP_MAX_REGISTERS          21
+#define KBASE_GPUPROP_MAX_TASK_QUEUE         22
+#define KBASE_GPUPROP_MAX_THREAD_GROUP_SPLIT 23
+#define KBASE_GPUPROP_IMPL_TECH              24
+
+#define KBASE_GPUPROP_RAW_SHADER_PRESENT            25
+#define KBASE_GPUPROP_RAW_TILER_PRESENT             26
+#define KBASE_GPUPROP_RAW_L2_PRESENT                27
+#define KBASE_GPUPROP_RAW_STACK_PRESENT             28
+#define KBASE_GPUPROP_RAW_L2_FEATURES               29
+#define KBASE_GPUPROP_RAW_CORE_FEATURES             30
+#define KBASE_GPUPROP_RAW_MEM_FEATURES              31
+#define KBASE_GPUPROP_RAW_MMU_FEATURES              32
+#define KBASE_GPUPROP_RAW_AS_PRESENT                33
+#define KBASE_GPUPROP_RAW_JS_PRESENT                34
+#define KBASE_GPUPROP_RAW_JS_FEATURES_0             35
+#define KBASE_GPUPROP_RAW_JS_FEATURES_1             36
+#define KBASE_GPUPROP_RAW_JS_FEATURES_2             37
+#define KBASE_GPUPROP_RAW_JS_FEATURES_3             38
+#define KBASE_GPUPROP_RAW_JS_FEATURES_4             39
+#define KBASE_GPUPROP_RAW_JS_FEATURES_5             40
+#define KBASE_GPUPROP_RAW_JS_FEATURES_6             41
+#define KBASE_GPUPROP_RAW_JS_FEATURES_7             42
+#define KBASE_GPUPROP_RAW_JS_FEATURES_8             43
+#define KBASE_GPUPROP_RAW_JS_FEATURES_9             44
+#define KBASE_GPUPROP_RAW_JS_FEATURES_10            45
+#define KBASE_GPUPROP_RAW_JS_FEATURES_11            46
+#define KBASE_GPUPROP_RAW_JS_FEATURES_12            47
+#define KBASE_GPUPROP_RAW_JS_FEATURES_13            48
+#define KBASE_GPUPROP_RAW_JS_FEATURES_14            49
+#define KBASE_GPUPROP_RAW_JS_FEATURES_15            50
+#define KBASE_GPUPROP_RAW_TILER_FEATURES            51
+#define KBASE_GPUPROP_RAW_TEXTURE_FEATURES_0        52
+#define KBASE_GPUPROP_RAW_TEXTURE_FEATURES_1        53
+#define KBASE_GPUPROP_RAW_TEXTURE_FEATURES_2        54
+#define KBASE_GPUPROP_RAW_GPU_ID                    55
+#define KBASE_GPUPROP_RAW_THREAD_MAX_THREADS        56
+#define KBASE_GPUPROP_RAW_THREAD_MAX_WORKGROUP_SIZE 57
+#define KBASE_GPUPROP_RAW_THREAD_MAX_BARRIER_SIZE   58
+#define KBASE_GPUPROP_RAW_THREAD_FEATURES           59
+#define KBASE_GPUPROP_RAW_COHERENCY_MODE            60
+
+#define KBASE_GPUPROP_COHERENCY_NUM_GROUPS      61
+#define KBASE_GPUPROP_COHERENCY_NUM_CORE_GROUPS 62
+#define KBASE_GPUPROP_COHERENCY_COHERENCY       63
+#define KBASE_GPUPROP_COHERENCY_GROUP_0         64
+#define KBASE_GPUPROP_COHERENCY_GROUP_1         65
+#define KBASE_GPUPROP_COHERENCY_GROUP_2         66
+#define KBASE_GPUPROP_COHERENCY_GROUP_3         67
+#define KBASE_GPUPROP_COHERENCY_GROUP_4         68
+#define KBASE_GPUPROP_COHERENCY_GROUP_5         69
+#define KBASE_GPUPROP_COHERENCY_GROUP_6         70
+#define KBASE_GPUPROP_COHERENCY_GROUP_7         71
+#define KBASE_GPUPROP_COHERENCY_GROUP_8         72
+#define KBASE_GPUPROP_COHERENCY_GROUP_9         73
+#define KBASE_GPUPROP_COHERENCY_GROUP_10        74
+#define KBASE_GPUPROP_COHERENCY_GROUP_11        75
+#define KBASE_GPUPROP_COHERENCY_GROUP_12        76
+#define KBASE_GPUPROP_COHERENCY_GROUP_13        77
+#define KBASE_GPUPROP_COHERENCY_GROUP_14        78
+#define KBASE_GPUPROP_COHERENCY_GROUP_15        79
+
+#define KBASE_GPUPROP_TEXTURE_FEATURES_3     80
+#define KBASE_GPUPROP_RAW_TEXTURE_FEATURES_3 81
+
+#define KBASE_GPUPROP_NUM_EXEC_ENGINES 82
+
+#define KBASE_GPUPROP_RAW_THREAD_TLS_ALLOC 83
+#define KBASE_GPUPROP_TLS_ALLOC            84
+#define KBASE_GPUPROP_RAW_GPU_FEATURES     85
+#ifdef __cpluscplus
+}
+#endif
+
+#endif /* _UAPI_KBASE_IOCTL_H_ */
diff -urN mesa-23.0.0/src/panfrost/csf_test/test.c mesa/src/panfrost/csf_test/test.c
--- mesa-23.0.0/src/panfrost/csf_test/test.c	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/csf_test/test.c	2023-03-06 19:19:32.672308600 +0100
@@ -0,0 +1,1918 @@
+/*
+ * Copyright (C) 2022 Icecream95
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <fcntl.h>
+#include <inttypes.h>
+#include <poll.h>
+#include <stdbool.h>
+#include <stddef.h>
+#include <stdint.h>
+#include <stdlib.h>
+#include <string.h>
+#include <unistd.h>
+#include <sys/ioctl.h>
+#include <sys/mman.h>
+
+#include "util/macros.h"
+
+#include "mali_base_csf_kernel.h"
+#include "mali_base_kernel.h"
+#include "mali_gpu_csf_registers.h"
+#include "mali_kbase_csf_ioctl.h"
+#include "mali_kbase_ioctl.h"
+
+#define PAN_ARCH 10
+#include "genxml/gen_macros.h"
+
+#include "decode.h"
+#include "wrap.h"
+
+#include "compiler/valhall/disassemble.h"
+#include "compiler/nir/nir_builder.h"
+#include "pan_shader.h"
+
+#define CS_EVENT_REGISTER 0x5A
+
+static bool pr = true;
+static bool colour_term = true;
+
+static void
+dump_start(FILE *f)
+{
+   if (colour_term)
+      fprintf(f, "\x1b[90m");
+}
+
+static void
+dump_end(FILE *f)
+{
+   if (colour_term)
+      fprintf(f, "\x1b[39m");
+}
+
+/* TODO: Use KBASE_IOCTL_MEM_SYNC for 32-bit systems */
+static void
+cache_clean(volatile void *addr)
+{
+#ifdef __aarch64__
+   __asm__ volatile("dc cvac, %0" ::"r"(addr) : "memory");
+#endif
+}
+
+static void
+cache_invalidate(volatile void *addr)
+{
+#ifdef __aarch64__
+   __asm__ volatile("dc civac, %0" ::"r"(addr) : "memory");
+#endif
+}
+
+static void
+cache_barrier(void)
+{
+#ifdef __ARM_ARCH
+   __asm__ volatile("dsb sy" ::: "memory");
+#endif
+}
+
+static void
+memory_barrier(void)
+{
+#ifdef __ARM_ARCH
+   __asm__ volatile("dmb sy" ::: "memory");
+#endif
+}
+
+typedef void (*cacheline_op)(volatile void *addr);
+
+#define CACHELINE_SIZE 64
+
+static void
+cacheline_op_range(volatile void *start, unsigned length, cacheline_op op)
+{
+   volatile void *ptr =
+      (volatile void *)((uintptr_t)start & ~((uintptr_t)CACHELINE_SIZE - 1));
+   volatile void *end =
+      (volatile void *)ALIGN_POT((uintptr_t)start + length, CACHELINE_SIZE);
+   for (; ptr < end; ptr += CACHELINE_SIZE)
+      op(ptr);
+}
+
+static void
+cache_clean_range(volatile void *start, unsigned length)
+{
+   cacheline_op_range(start, length, cache_clean);
+}
+
+static void
+cache_invalidate_range(volatile void *start, unsigned length)
+{
+   cacheline_op_range(start, length, cache_invalidate);
+}
+
+struct state;
+struct test;
+
+typedef bool (*section)(struct state *s, struct test *t);
+
+#define CS_QUEUE_COUNT 4 /* compute / vertex / fragment / other */
+#define CS_QUEUE_SIZE  65536
+
+struct state {
+   int page_size;
+   int argc;
+   char **argv;
+
+   int mali_fd;
+   int tl_fd;
+   void *tracking_region;
+   void *csf_user_reg;
+
+   uint8_t *gpuprops;
+   unsigned gpuprops_size;
+   uint32_t gpu_id;
+
+   struct {
+      struct panfrost_ptr normal, exec, coherent, cached, event, ev2;
+   } allocations;
+
+   uint64_t tiler_heap_va;
+   uint64_t tiler_heap_header;
+
+   uint8_t csg_handle;
+   uint32_t csg_uid;
+
+   struct panfrost_ptr cs_mem[CS_QUEUE_COUNT];
+   void *cs_user_io[CS_QUEUE_COUNT];
+   unsigned cs_last_submit[CS_QUEUE_COUNT];
+   struct pan_command_stream cs[CS_QUEUE_COUNT];
+
+   unsigned shader_alloc_offset;
+   mali_ptr compute_shader;
+};
+
+struct test {
+   section part;
+   section cleanup;
+   const char *label;
+
+   struct test *subtests;
+   unsigned sub_length;
+
+   /* for allocation tests */
+   unsigned offset;
+   unsigned flags;
+
+   bool add;
+   bool invalid;
+   bool blit;
+   bool vertex;
+};
+
+/* See STATE and ALLOC macros below */
+#define DEREF_STATE(s, offset) ((void *)s + offset)
+
+static uint64_t
+pan_get_gpuprop(struct state *s, int name)
+{
+   int i = 0;
+   uint64_t x = 0;
+   while (i < s->gpuprops_size) {
+      x = 0;
+      memcpy(&x, s->gpuprops + i, 4);
+      i += 4;
+
+      int size = 1 << (x & 3);
+      int this_name = x >> 2;
+
+      x = 0;
+      memcpy(&x, s->gpuprops + i, size);
+      i += size;
+
+      if (this_name == name)
+         return x;
+   }
+
+   fprintf(stderr, "Unknown prop %i\n", name);
+   return 0;
+}
+
+static bool
+open_kbase(struct state *s, struct test *t)
+{
+   s->mali_fd = open("/dev/mali0", O_RDWR);
+   if (s->mali_fd != -1)
+      return true;
+
+   perror("open(\"/dev/mali0\")");
+   return false;
+}
+
+static bool
+close_kbase(struct state *s, struct test *t)
+{
+   if (getenv("TEST_CHECK_LEAKS")) {
+      int pid = getpid();
+      char cmd_buffer[64] = {0};
+      sprintf(cmd_buffer, "grep /dev/mali /proc/%i/maps", pid);
+      system(cmd_buffer);
+      sprintf(cmd_buffer, "ls -l /proc/%i/fd", pid);
+      system(cmd_buffer);
+   }
+
+   if (s->mali_fd > 0)
+      return close(s->mali_fd) == 0;
+   return true;
+}
+
+static bool
+get_version(struct state *s, struct test *t)
+{
+   struct kbase_ioctl_version_check ver = {0};
+
+   int ret = ioctl(s->mali_fd, KBASE_IOCTL_VERSION_CHECK, &ver);
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_VERSION_CHECK)");
+      return false;
+   }
+
+   if (pr)
+      printf("Major %i Minor %i: ", ver.major, ver.minor);
+   return true;
+}
+
+static bool
+set_flags(struct state *s, struct test *t)
+{
+   struct kbase_ioctl_set_flags flags = {.create_flags = 0};
+
+   int ret = ioctl(s->mali_fd, KBASE_IOCTL_SET_FLAGS, &flags);
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_SET_FLAGS)");
+      return false;
+   }
+   return true;
+}
+
+static bool
+mmap_tracking(struct state *s, struct test *t)
+{
+   s->tracking_region = mmap(NULL, s->page_size, PROT_NONE, MAP_SHARED,
+                             s->mali_fd, BASE_MEM_MAP_TRACKING_HANDLE);
+
+   if (s->tracking_region == MAP_FAILED) {
+      perror("mmap(BASE_MEM_MAP_TRACKING_HANDLE)");
+      s->tracking_region = NULL;
+      return false;
+   }
+   return true;
+}
+
+static bool
+munmap_tracking(struct state *s, struct test *t)
+{
+   if (s->tracking_region)
+      return munmap(s->tracking_region, s->page_size) == 0;
+   return true;
+}
+
+static bool
+get_gpuprops(struct state *s, struct test *t)
+{
+   struct kbase_ioctl_get_gpuprops props = {0};
+
+   int ret = ioctl(s->mali_fd, KBASE_IOCTL_GET_GPUPROPS, &props);
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_GET_GPUPROPS(0))");
+      return false;
+   } else if (!ret) {
+      fprintf(stderr, "GET_GPUPROPS returned zero size\n");
+      return false;
+   }
+
+   s->gpuprops_size = ret;
+   s->gpuprops = calloc(s->gpuprops_size, 1);
+
+   props.size = s->gpuprops_size;
+   props.buffer = (uint64_t)(uintptr_t)s->gpuprops;
+
+   ret = ioctl(s->mali_fd, KBASE_IOCTL_GET_GPUPROPS, &props);
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_GET_GPUPROPS(size))");
+      return false;
+   }
+
+   return true;
+}
+
+static bool
+free_gpuprops(struct state *s, struct test *t)
+{
+   free(s->gpuprops);
+   return true;
+}
+
+static bool
+get_gpu_id(struct state *s, struct test *t)
+{
+   uint64_t gpu_id = pan_get_gpuprop(s, KBASE_GPUPROP_PRODUCT_ID);
+   if (!gpu_id)
+      return false;
+   s->gpu_id = gpu_id;
+
+   uint16_t maj = gpu_id >> 12;
+   uint16_t min = (gpu_id >> 8) & 0xf;
+   uint16_t rev = (gpu_id >> 4) & 0xf;
+
+   uint16_t product = gpu_id & 0xf;
+   uint16_t prod = product | ((maj & 1) << 4);
+
+   const char *names[] = {
+      [1] = "TDUX",      [2] = "G710",      [3] = "G510", [4] = "G310",
+      [7] = "G610",      [16 + 2] = "G715", /* TODO: Immortalis instead of Mali?
+                                             */
+      [16 + 3] = "G615",
+   };
+   const char *name = (prod < ARRAY_SIZE(names)) ? names[prod] : NULL;
+   if (!name)
+      name = "unknown";
+
+   if (pr)
+      printf("v%i.%i.%i Mali-%s (%i): ", maj, min, rev, name, product);
+
+   if (maj < 10) {
+      printf("not v10 or later: ");
+      return false;
+   }
+
+   return true;
+}
+
+static bool
+get_coherency_mode(struct state *s, struct test *t)
+{
+   uint64_t mode = pan_get_gpuprop(s, KBASE_GPUPROP_RAW_COHERENCY_MODE);
+
+   const char *modes[] = {
+      [0] = "ACE-Lite",
+      [1] = "ACE",
+      [31] = "None",
+   };
+   const char *name = (mode < ARRAY_SIZE(modes)) ? modes[mode] : NULL;
+   if (!name)
+      name = "Unknown";
+
+   if (pr)
+      printf("0x%" PRIx64 " (%s): ", mode, name);
+   return true;
+}
+
+static bool
+get_csf_caps(struct state *s, struct test *t)
+{
+   union kbase_ioctl_cs_get_glb_iface iface = {0};
+
+   int ret = ioctl(s->mali_fd, KBASE_IOCTL_CS_GET_GLB_IFACE, &iface);
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_CS_GET_GLB_IFACE(0))");
+      return false;
+   }
+
+   int ver_maj = iface.out.glb_version >> 24;
+   int ver_min = (iface.out.glb_version >> 16) & 0xff;
+   int ver_rev = iface.out.glb_version & 0xffff;
+
+   if (pr)
+      printf("v%i.%i.%i: feature mask 0x%x, %i groups, %i total: ", ver_maj,
+             ver_min, ver_rev, iface.out.features, iface.out.group_num,
+             iface.out.total_stream_num);
+
+   unsigned group_num = iface.out.group_num;
+   unsigned stream_num = iface.out.total_stream_num;
+
+   struct basep_cs_group_control *group_data =
+      calloc(group_num, sizeof(*group_data));
+
+   struct basep_cs_stream_control *stream_data =
+      calloc(stream_num, sizeof(*stream_data));
+
+   iface = (union kbase_ioctl_cs_get_glb_iface){
+      .in = {
+         .max_group_num = group_num,
+         .max_total_stream_num = stream_num,
+         .groups_ptr = (uintptr_t)group_data,
+         .streams_ptr = (uintptr_t)stream_data,
+      }};
+
+   ret = ioctl(s->mali_fd, KBASE_IOCTL_CS_GET_GLB_IFACE, &iface);
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_CS_GET_GLB_IFACE(size))");
+
+      free(group_data);
+      free(stream_data);
+
+      return false;
+   }
+
+   unsigned print_groups = pr ? group_num : 0;
+   unsigned print_streams = pr ? stream_num : 0;
+
+   for (unsigned i = 0; i < print_groups; ++i) {
+      if (i && !memcmp(group_data + i, group_data + i - 1, sizeof(*group_data)))
+         continue;
+
+      fprintf(stderr, "Group %i-: feature mask 0x%x, %i streams\n", i,
+              group_data[i].features, group_data[i].stream_num);
+   }
+
+   for (unsigned i = 0; i < print_streams; ++i) {
+      if (i &&
+          !memcmp(stream_data + i, stream_data + i - 1, sizeof(*stream_data)))
+         continue;
+
+      unsigned reg = stream_data[i].features & 0xff;
+      unsigned score = (stream_data[i].features >> 8) & 0xff;
+      unsigned feat = stream_data[i].features >> 16;
+
+      fprintf(
+         stderr,
+         "Stream %i-: 0x%x work registers, %i scoreboards, iterator mask: 0x%x\n",
+         i, reg, score, feat);
+   }
+
+   free(group_data);
+   free(stream_data);
+
+   return true;
+}
+
+static bool
+mmap_user_reg(struct state *s, struct test *t)
+{
+   s->csf_user_reg = mmap(NULL, s->page_size, PROT_READ, MAP_SHARED, s->mali_fd,
+                          BASEP_MEM_CSF_USER_REG_PAGE_HANDLE);
+
+   if (s->csf_user_reg == MAP_FAILED) {
+      perror("mmap(BASEP_MEM_CSF_USER_REG_PAGE_HANDLE)");
+      s->csf_user_reg = NULL;
+      return false;
+   }
+   return true;
+}
+
+static bool
+munmap_user_reg(struct state *s, struct test *t)
+{
+   if (s->csf_user_reg)
+      return munmap(s->csf_user_reg, s->page_size) == 0;
+   return true;
+}
+
+static bool
+init_mem_exec(struct state *s, struct test *t)
+{
+   struct kbase_ioctl_mem_exec_init init = {
+      .va_pages = 0x100000,
+   };
+
+   int ret = ioctl(s->mali_fd, KBASE_IOCTL_MEM_EXEC_INIT, &init);
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_MEM_EXEC_INIT)");
+      return false;
+   }
+   return true;
+}
+
+static bool
+init_mem_jit(struct state *s, struct test *t)
+{
+   struct kbase_ioctl_mem_jit_init init = {
+      .va_pages = 1 << 25,
+      .max_allocations = 255,
+      .phys_pages = 1 << 25,
+   };
+
+   int ret = ioctl(s->mali_fd, KBASE_IOCTL_MEM_JIT_INIT, &init);
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_MEM_JIT_INIT)");
+      return false;
+   }
+   return true;
+}
+
+static bool
+stream_create(struct state *s, struct test *t)
+{
+   struct kbase_ioctl_stream_create stream = {.name = "stream"};
+
+   s->tl_fd = ioctl(s->mali_fd, KBASE_IOCTL_STREAM_CREATE, &stream);
+
+   if (s->tl_fd == -1) {
+      perror("ioctl(KBASE_IOCTL_STREAM_CREATE)");
+      return false;
+   }
+   return true;
+}
+
+static bool
+stream_destroy(struct state *s, struct test *t)
+{
+   if (s->tl_fd > 0)
+      return close(s->tl_fd) == 0;
+   return true;
+}
+
+static bool
+tiler_heap_create(struct state *s, struct test *t)
+{
+   union kbase_ioctl_cs_tiler_heap_init init = {.in = {
+                                                   .chunk_size = 1 << 21,
+                                                   .initial_chunks = 5,
+                                                   .max_chunks = 200,
+                                                   .target_in_flight = 65535,
+                                                }};
+
+   int ret = ioctl(s->mali_fd, KBASE_IOCTL_CS_TILER_HEAP_INIT, &init);
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_CS_TILER_HEAP_INIT)");
+      return false;
+   }
+
+   s->tiler_heap_va = init.out.gpu_heap_va;
+   s->tiler_heap_header = init.out.first_chunk_va;
+   printf("heap va: %" PRIx64 ", heap header: %" PRIx64 "\n", s->tiler_heap_va,
+          s->tiler_heap_header);
+
+   return true;
+}
+
+static bool
+tiler_heap_term(struct state *s, struct test *t)
+{
+   if (!s->tiler_heap_va)
+      return true;
+
+   struct kbase_ioctl_cs_tiler_heap_term term = {.gpu_heap_va =
+                                                    s->tiler_heap_va};
+
+   int ret = ioctl(s->mali_fd, KBASE_IOCTL_CS_TILER_HEAP_TERM, &term);
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_CS_TILER_HEAP_TERM)");
+      return false;
+   }
+   return true;
+}
+
+static bool
+cs_group_create(struct state *s, struct test *t)
+{
+   union kbase_ioctl_cs_queue_group_create_1_6 create = {
+      .in = {
+         /* Mali *still* only supports a single tiler unit */
+         .tiler_mask = 1,
+         .fragment_mask = ~0ULL,
+         .compute_mask = ~0ULL,
+
+         .cs_min = CS_QUEUE_COUNT,
+
+         .priority = 1,
+         .tiler_max = 1,
+         .fragment_max = 64,
+         .compute_max = 64,
+      }};
+
+   int ret = ioctl(s->mali_fd, KBASE_IOCTL_CS_QUEUE_GROUP_CREATE_1_6, &create);
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_CS_QUEUE_GROUP_CREATE_1_6)");
+      return false;
+   }
+
+   s->csg_handle = create.out.group_handle;
+   s->csg_uid = create.out.group_uid;
+
+   if (pr)
+      printf("CSG handle: %i UID: %i: ", s->csg_handle, s->csg_uid);
+
+   /* Should be at least 1 */
+   if (!s->csg_uid)
+      abort();
+
+   return true;
+}
+
+static bool
+cs_group_term(struct state *s, struct test *t)
+{
+   if (!s->csg_uid)
+      return true;
+
+   struct kbase_ioctl_cs_queue_group_term term = {.group_handle =
+                                                     s->csg_handle};
+
+   int ret = ioctl(s->mali_fd, KBASE_IOCTL_CS_QUEUE_GROUP_TERMINATE, &term);
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_CS_QUEUE_GROUP_TERMINATE)");
+      return false;
+   }
+   return true;
+}
+
+static bool
+decode_init(struct state *s, struct test *t)
+{
+   pandecode_initialize(true);
+   return true;
+}
+
+static bool
+decode_close(struct state *s, struct test *t)
+{
+   pandecode_close();
+   return true;
+}
+
+static struct panfrost_ptr
+alloc_ioctl(struct state *s, union kbase_ioctl_mem_alloc *a)
+{
+   struct panfrost_ptr p = {0};
+
+   uint64_t va_pages = a->in.va_pages;
+   uint64_t flags = a->in.flags;
+
+   int ret = ioctl(s->mali_fd, KBASE_IOCTL_MEM_ALLOC, a);
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_MEM_ALLOC)");
+      return p;
+   }
+
+   if ((flags & BASE_MEM_SAME_VA) &&
+       (!(a->out.flags & BASE_MEM_SAME_VA) || a->out.gpu_va != 0x41000)) {
+
+      fprintf(stderr, "Flags: 0x%" PRIx64 ", VA: 0x%" PRIx64 "\n",
+              (uint64_t)a->out.flags, (uint64_t)a->out.gpu_va);
+      return p;
+   }
+
+   void *ptr = mmap(NULL, s->page_size * va_pages, PROT_READ | PROT_WRITE,
+                    MAP_SHARED, s->mali_fd, a->out.gpu_va);
+
+   if (ptr == MAP_FAILED) {
+      perror("mmap(GPU BO)");
+      return p;
+   }
+
+   uint64_t gpu_va =
+      (a->out.flags & BASE_MEM_SAME_VA) ? (uintptr_t)ptr : a->out.gpu_va;
+
+   pandecode_inject_mmap(gpu_va, ptr, s->page_size * va_pages, NULL);
+
+   p.cpu = ptr;
+   p.gpu = gpu_va;
+
+   memset(p.cpu, 0, s->page_size * va_pages);
+
+   return p;
+}
+
+static struct panfrost_ptr
+alloc_mem(struct state *s, uint64_t size, uint64_t flags)
+{
+   unsigned pages = size / s->page_size;
+
+   union kbase_ioctl_mem_alloc a = {.in = {
+                                       .va_pages = pages,
+                                       .commit_pages = pages,
+                                       .extension = 0,
+                                       .flags = flags,
+                                    }};
+
+   return alloc_ioctl(s, &a);
+}
+
+static void
+alloc_redzone(struct state *s, struct panfrost_ptr p, uint64_t alloc_size)
+{
+   mmap(p.cpu - s->page_size, 1, PROT_NONE,
+        MAP_PRIVATE | MAP_ANONYMOUS | MAP_FIXED_NOREPLACE, -1, 0);
+
+   mmap(p.cpu + alloc_size, 1, PROT_NONE,
+        MAP_PRIVATE | MAP_ANONYMOUS | MAP_FIXED_NOREPLACE, -1, 0);
+}
+
+static bool
+alloc(struct state *s, struct test *t)
+{
+   struct panfrost_ptr *ptr = DEREF_STATE(s, t->offset);
+
+   *ptr = alloc_mem(s, s->page_size, t->flags);
+
+   volatile int *p = (volatile int *)ptr->cpu;
+   *p = 0x12345;
+   if (*p != 0x12345) {
+      printf("Error reading from allocated memory at %p\n", p);
+      return false;
+   }
+   *p = 0;
+   cache_clean(p);
+
+   return true;
+}
+
+static bool
+dealloc(struct state *s, struct test *t)
+{
+   struct panfrost_ptr *ptr = DEREF_STATE(s, t->offset);
+
+   if (ptr->cpu)
+      return munmap(ptr->cpu, s->page_size) == 0;
+   return true;
+}
+
+static bool
+cs_queue_create(struct state *s, struct test *t)
+{
+   for (unsigned i = 0; i < CS_QUEUE_COUNT; ++i) {
+
+      /* Read/write from CPU/GPU, nothing special
+       * like coherency */
+      s->cs_mem[i] = alloc_mem(s, CS_QUEUE_SIZE, 0x200f);
+      s->cs[i].ptr = s->cs_mem[i].cpu;
+
+      if (!s->cs_mem[i].cpu)
+         return false;
+   }
+
+   return true;
+}
+
+static bool
+cs_queue_free(struct state *s, struct test *t)
+{
+   bool pass = true;
+   for (unsigned i = 0; i < CS_QUEUE_COUNT; ++i) {
+      if (s->cs_mem[i].cpu && munmap(s->cs_mem[i].cpu, CS_QUEUE_SIZE))
+         pass = false;
+   }
+   return pass;
+}
+
+static bool
+cs_queue_register(struct state *s, struct test *t)
+{
+   for (unsigned i = 0; i < CS_QUEUE_COUNT; ++i) {
+      struct kbase_ioctl_cs_queue_register reg = {
+         .buffer_gpu_addr = s->cs_mem[i].gpu,
+         .buffer_size = CS_QUEUE_SIZE,
+         .priority = 1,
+      };
+
+      int ret = ioctl(s->mali_fd, KBASE_IOCTL_CS_QUEUE_REGISTER, &reg);
+
+      if (ret == -1) {
+         perror("ioctl(KBASE_IOCTL_CS_QUEUE_REGISTER)");
+         return false;
+      }
+
+      union kbase_ioctl_cs_queue_bind bind = {
+         .in = {
+            .buffer_gpu_addr = s->cs_mem[i].gpu,
+            .group_handle = s->csg_handle,
+            .csi_index = i,
+         }};
+
+      ret = ioctl(s->mali_fd, KBASE_IOCTL_CS_QUEUE_BIND, &bind);
+
+      if (ret == -1) {
+         perror("ioctl(KBASE_IOCTL_CS_QUEUE_BIND)");
+      }
+
+      s->cs_user_io[i] = mmap(
+         NULL, s->page_size * BASEP_QUEUE_NR_MMAP_USER_PAGES,
+         PROT_READ | PROT_WRITE, MAP_SHARED, s->mali_fd, bind.out.mmap_handle);
+
+      if (s->cs_user_io[i] == MAP_FAILED) {
+         perror("mmap(CS USER IO)");
+         s->cs_user_io[i] = NULL;
+         return false;
+      }
+   }
+   return true;
+}
+
+static bool
+cs_queue_term(struct state *s, struct test *t)
+{
+   bool pass = true;
+
+   for (unsigned i = 0; i < CS_QUEUE_COUNT; ++i) {
+      if (s->cs_user_io[i] &&
+          munmap(s->cs_user_io[i],
+                 s->page_size * BASEP_QUEUE_NR_MMAP_USER_PAGES))
+         pass = false;
+
+      struct kbase_ioctl_cs_queue_terminate term = {
+         .buffer_gpu_addr = s->cs_mem[i].gpu,
+      };
+
+      int ret = ioctl(s->mali_fd, KBASE_IOCTL_CS_QUEUE_TERMINATE, &term);
+
+      if (ret == -1)
+         pass = false;
+   }
+   return pass;
+}
+
+#define CS_RING_DOORBELL(s, i) *((uint32_t *)(s->cs_user_io[i])) = 1
+
+#define CS_READ_REGISTER(s, i, r)                                              \
+   *((uint64_t *)(s->cs_user_io[i] + s->page_size * 2 + r))
+
+#define CS_WRITE_REGISTER(s, i, r, v)                                          \
+   *((uint64_t *)(s->cs_user_io[i] + s->page_size + r)) = v
+
+static void
+submit_cs(struct state *s, unsigned i)
+{
+   uintptr_t p = (uintptr_t)s->cs[i].ptr;
+   unsigned pad = (-p) & 63;
+   memset(s->cs[i].ptr, 0, pad);
+
+   unsigned last_offset = s->cs_last_submit[i];
+
+   unsigned insert_offset = p + pad - (uintptr_t)s->cs_mem[i].cpu;
+   insert_offset %= CS_QUEUE_SIZE;
+
+   for (unsigned o = last_offset; o != insert_offset;
+        o = (o + 64) % CS_QUEUE_SIZE)
+      cache_clean(s->cs_mem[i].cpu + o);
+
+   // TODO: Handle wraparound
+   // TODO: Provide a persistent buffer for pandecode to use?
+   if (pr) {
+      dump_start(stderr);
+      pandecode_cs(s->cs_mem[i].gpu + last_offset, insert_offset - last_offset,
+                   s->gpu_id);
+      dump_end(stderr);
+   }
+
+   cache_barrier();
+
+   CS_WRITE_REGISTER(s, i, CS_INSERT, insert_offset);
+   s->cs[i].ptr = s->cs_mem[i].cpu + insert_offset;
+
+   memory_barrier();
+   CS_RING_DOORBELL(s, i);
+   memory_barrier();
+
+   s->cs_last_submit[i] = insert_offset;
+}
+
+/* Returns true if there was a timeout */
+static bool
+wait_event(struct state *s, unsigned timeout_ms)
+{
+   struct pollfd fd = {
+      .fd = s->mali_fd,
+      .events = POLLIN,
+   };
+
+   int ret = poll(&fd, 1, timeout_ms);
+
+   if (ret == -1) {
+      perror("poll(mali_fd)");
+      return true;
+   }
+
+   /* Timeout */
+   if (ret == 0)
+      return true;
+
+   struct base_csf_notification event;
+   ret = read(s->mali_fd, &event, sizeof(event));
+
+   if (ret == -1) {
+      perror("read(mali_fd)");
+      return true;
+   }
+
+   if (ret != sizeof(event)) {
+      fprintf(stderr, "read(mali_fd) returned %i, expected %i!\n", ret,
+              (int)sizeof(event));
+      return false;
+   }
+
+   switch (event.type) {
+   case BASE_CSF_NOTIFICATION_EVENT:
+      fprintf(stderr, "Notification event!\n");
+      return false;
+
+   case BASE_CSF_NOTIFICATION_GPU_QUEUE_GROUP_ERROR:
+      break;
+
+   case BASE_CSF_NOTIFICATION_CPU_QUEUE_DUMP:
+      fprintf(stderr, "No event from mali_fd!\n");
+      return false;
+
+   default:
+      fprintf(stderr, "Unknown event type!\n");
+      return false;
+   }
+
+   struct base_gpu_queue_group_error e = event.payload.csg_error.error;
+
+   switch (e.error_type) {
+   case BASE_GPU_QUEUE_GROUP_ERROR_FATAL: {
+      // See CS_FATAL_EXCEPTION_* in mali_gpu_csf_registers.h
+      fprintf(stderr,
+              "Queue group error: status 0x%x "
+              "sideband 0x%" PRIx64 "\n",
+              e.payload.fatal_group.status,
+              (uint64_t)e.payload.fatal_group.sideband);
+      break;
+   }
+   case BASE_GPU_QUEUE_GROUP_QUEUE_ERROR_FATAL: {
+      unsigned queue = e.payload.fatal_queue.csi_index;
+
+      // See CS_FATAL_EXCEPTION_* in mali_gpu_csf_registers.h
+      fprintf(stderr,
+              "Queue %i error: status 0x%x "
+              "sideband 0x%" PRIx64 ":",
+              queue, e.payload.fatal_queue.status,
+              (uint64_t)e.payload.fatal_queue.sideband);
+
+      unsigned e = CS_READ_REGISTER(s, queue, CS_EXTRACT);
+      pandecode_cs(s->cs_mem[queue].gpu + e, 8, s->gpu_id);
+
+      break;
+   }
+
+   case BASE_GPU_QUEUE_GROUP_ERROR_TIMEOUT:
+      fprintf(stderr, "Command stream timeout!\n");
+      break;
+   case BASE_GPU_QUEUE_GROUP_ERROR_TILER_HEAP_OOM:
+      fprintf(stderr, "Command stream OOM!\n");
+      break;
+   default:
+      fprintf(stderr, "Unknown error type!\n");
+   }
+
+   return false;
+}
+
+static bool
+kick_queue(struct state *s, unsigned i)
+{
+   struct kbase_ioctl_cs_queue_kick kick = {.buffer_gpu_addr =
+                                               s->cs_mem[i].gpu};
+
+   int ret = ioctl(s->mali_fd, KBASE_IOCTL_CS_QUEUE_KICK, &kick);
+
+   if (ret == -1) {
+      perror("ioctl(KBASE_IOCTL_CS_QUEUE_KICK)");
+      return false;
+   }
+
+   return true;
+}
+
+static bool
+wait_cs(struct state *s, unsigned i)
+{
+   unsigned extract_offset = (void *)s->cs[i].ptr - s->cs_mem[i].cpu;
+
+   unsigned timeout_ms = 500;
+
+   bool done_kick = false;
+
+   while (CS_READ_REGISTER(s, i, CS_EXTRACT) != extract_offset) {
+      if (wait_event(s, timeout_ms)) {
+         if (pr)
+            fprintf(stderr, "Event wait timeout!\n");
+
+         unsigned e = CS_READ_REGISTER(s, i, CS_EXTRACT);
+         unsigned a = CS_READ_REGISTER(s, i, CS_ACTIVE);
+
+         if (e != extract_offset) {
+            fprintf(stderr,
+                    "CS_EXTRACT (%i) != %i, "
+                    "CS_ACTIVE (%i) on queue %i:",
+                    e, extract_offset, a, i);
+            /* Decode two instructions instead? */
+            pandecode_cs(s->cs_mem[i].gpu + e, 8, 1);
+
+            if (done_kick) {
+               cache_barrier();
+               return false;
+            } else {
+               fprintf(stderr, "Kicking queue\n");
+               kick_queue(s, i);
+               done_kick = true;
+            }
+         }
+      }
+   }
+
+   cache_barrier();
+
+   return true;
+}
+
+static bool
+cs_init(struct state *s, struct test *t)
+{
+   uint64_t event_init[] = {1, 1, 1};
+   memcpy(s->allocations.event.cpu, event_init, sizeof(event_init));
+
+   for (unsigned i = 0; i < CS_QUEUE_COUNT; ++i) {
+      CS_WRITE_REGISTER(s, i, CS_INSERT, 0);
+      pan_pack_ins(s->cs + i, CS_RESOURCES, cfg)
+      {
+         switch (i) {
+         case 0:
+            cfg.compute = true;
+            break;
+         case 1:
+            cfg.compute = true;
+            cfg.fragment = true;
+            break;
+         case 2:
+            cfg.compute = true;
+            cfg.tiler = true;
+            cfg.idvs = true;
+            break;
+         case 3:
+            cfg.fragment = true;
+            break;
+         }
+      }
+      pan_pack_ins(s->cs + i, CS_SLOT, cfg)
+      {
+         cfg.index = 2;
+      }
+      pan_emit_cs_48(s->cs + i, CS_EVENT_REGISTER, s->allocations.event.gpu);
+      submit_cs(s, i);
+
+      if (!kick_queue(s, i))
+         return false;
+   }
+
+   return true;
+}
+
+static struct panfrost_ptr *
+buffers_elem(struct util_dynarray *buffers, unsigned index)
+{
+   unsigned size = util_dynarray_num_elements(buffers, struct panfrost_ptr);
+
+   if (index >= size) {
+      unsigned grow = index + 1 - size;
+
+      memset(util_dynarray_grow(buffers, struct panfrost_ptr, grow), 0,
+             grow * sizeof(struct panfrost_ptr));
+   }
+
+   return util_dynarray_element(buffers, struct panfrost_ptr, index);
+}
+
+static void
+dump_hex64(FILE *fp, uint64_t *values, unsigned size)
+{
+   bool zero = false;
+   for (unsigned i = 0; i < size / 8; i += 2) {
+      uint64_t a = values[i];
+      uint64_t b = values[i + 1];
+
+      if (!a && !b) {
+         if (!zero)
+            fprintf(fp, "%06X  *\n", i * 8);
+         zero = true;
+         continue;
+      }
+
+      zero = false;
+
+      fprintf(fp, "%06X  %16" PRIx64 " %16" PRIx64 "\n", i * 8, a, b);
+   }
+
+   fprintf(fp, "\n");
+}
+
+static void
+dump_delta(FILE *fp, uint64_t *values, unsigned size)
+{
+   uint64_t old = 0;
+   bool zero = false;
+   bool el = false;
+   for (unsigned i = 0; i < size / 8; ++i) {
+      uint64_t val = values[i];
+      int64_t delta = val - old;
+
+      if (!zero || delta) {
+         fprintf(fp, "%" PRIi64 "\n", delta);
+         el = false;
+      } else if (!el) {
+         fprintf(fp, "...\n");
+         el = true;
+      }
+
+      old = val;
+      zero = (delta == 0);
+   }
+}
+
+static void
+dump_tiler(FILE *fp, uint8_t *values, unsigned size)
+{
+   fflush(stdout);
+   FILE *stream = popen("tiler-hex-read", "w");
+   // TODO!
+   fprintf(stream, "width %i\nheight %i\nmask %i\nvaheap %p\nsize %i\n", 256,
+           256, 6, values, size);
+   pan_hexdump(stream, values, size, false);
+   pclose(stream);
+}
+
+/* TODO: Pass in a filename? */
+static void
+dump_filehex(uint8_t *values, unsigned size)
+{
+   char buf[1024] = {0};
+
+   for (unsigned i = 0; i < 10000; ++i) {
+      snprintf(buf, 1024, "/tmp/fdump.%05i", i);
+
+      int fd = open(buf, O_WRONLY | O_CREAT | O_EXCL, 0666);
+      if (fd == -1)
+         continue;
+
+      FILE *fp = fdopen(fd, "w");
+
+      fprintf(fp, "%p, %u:\n", values, size);
+      pan_hexdump(fp, values, size, false);
+
+      fclose(fp); /* will close fd */
+      break;
+   }
+}
+
+static void
+dump_heatmap(FILE *fp, uint8_t *values, unsigned size, unsigned gran,
+             unsigned length, unsigned stride)
+{
+   unsigned sum = 0;
+   unsigned gr = 0;
+   unsigned st = 0;
+   unsigned ll = 0;
+
+   while (size && !values[size - 1])
+      --size;
+
+   for (unsigned i = 0; i < size; ++i) {
+      sum += values[i];
+
+      if (++gr == gran) {
+         fprintf(fp, " %02x", sum & 0xff);
+         gr = 0;
+         sum = 0;
+      }
+
+      if (++ll == length) {
+         i += stride - length;
+         fprintf(fp, "\n");
+         st = 0;
+         ll = 0;
+      } else if (++st == stride) {
+         fprintf(fp, "\n");
+         st = 0;
+      }
+   }
+   fprintf(fp, " %02x\n", sum & 0xff);
+}
+
+static bool
+cs_test(struct state *s, struct test *t)
+{
+   if (s->argc < 2)
+      return true;
+
+   FILE *f = fopen(s->argv[1], "r");
+
+   struct util_dynarray buffers;
+   util_dynarray_init(&buffers, NULL);
+
+   for (;;) {
+      char *line = NULL;
+      size_t sz = 0;
+      if (getline(&line, &sz, f) == -1)
+         break;
+
+      unsigned long src, dst, offset, src_offset, size, iter, flags;
+      unsigned long gran, stride, length;
+      int read;
+      char *mode;
+
+      if (sscanf(line, "rel%ms %lu+%lu %lu+%lu", &mode, &dst, &offset, &src,
+                 &src_offset) == 5) {
+
+         if (strcmp(mode, "oc") && strcmp(mode, "split")) {
+            fprintf(stderr, "Unknown relocation mode 'rel%s'\n", mode);
+         }
+         bool split = (mode[0] == 's');
+         free(mode);
+
+         struct panfrost_ptr *s = buffers_elem(&buffers, src);
+         struct panfrost_ptr *d = buffers_elem(&buffers, dst);
+
+         if (!s->gpu || !d->gpu) {
+            fprintf(stderr, "relocating to buffer that doesn't exist!\n");
+         }
+
+         uint64_t *dest = d->cpu + offset;
+         uint64_t value = s->gpu + src_offset;
+         if (split) {
+            dest[0] |= (uint32_t)value;
+            dest[1] |= (uint32_t)(value >> 32);
+         } else {
+            *dest |= value;
+         }
+
+      } else if (sscanf(line, "buffer %lu %lu %lx %n", &dst, &size, &flags,
+                        &read) == 3) {
+         line += read;
+
+         struct panfrost_ptr buffer =
+            alloc_mem(s, ALIGN_POT(size, s->page_size), flags);
+
+         alloc_redzone(s, buffer, ALIGN_POT(size, s->page_size));
+
+         *buffers_elem(&buffers, dst) = buffer;
+
+         // printf("buffer %lu == 0x%lx\n", dst, buffer.gpu);
+
+         uint64_t *fill = buffer.cpu;
+
+         for (unsigned i = 0; i < size / 8; ++i) {
+            read = 0;
+            unsigned long long val = 0;
+            if (sscanf(line, "%Lx %n", &val, &read) != 1)
+               break;
+            line += read;
+            fill[i] = val;
+         }
+
+         cache_clean_range(buffer.cpu, size);
+
+      } else if (sscanf(line, "exe %n %lu %lu %lu", &read, &iter, &dst,
+                        &size) == 3) {
+         line += read;
+
+         unsigned iter_mask = 0;
+
+         for (;;) {
+            read = 0;
+            if (sscanf(line, "%lu %lu %lu %n", &iter, &dst, &size, &read) != 3)
+               break;
+            line += read;
+
+            struct panfrost_ptr *d = buffers_elem(&buffers, dst);
+
+            /* TODO: Check 'size' against buffer size */
+
+            pandecode_cs(d->gpu, size, s->gpu_id);
+
+            if (iter > 3) {
+               fprintf(stderr, "execute on out-of-bounds "
+                               "iterator\n");
+               continue;
+            }
+
+            memcpy(s->cs[iter].ptr, d->cpu, size);
+            s->cs[iter].ptr += size / 8;
+
+            iter_mask |= (1 << iter);
+         }
+
+         u_foreach_bit(i, iter_mask)
+            submit_cs(s, i);
+
+         u_foreach_bit(i, iter_mask)
+            kick_queue(s, i);
+
+         u_foreach_bit(i, iter_mask)
+            wait_cs(s, i);
+
+      } else if (sscanf(line, "dump %lu %lu %lu %ms", &src, &offset, &size,
+                        &mode) == 4) {
+
+         struct panfrost_ptr *s = buffers_elem(&buffers, src);
+
+         if (!s->gpu)
+            fprintf(stderr, "dumping buffer that doesn't exist!\n");
+
+         cache_invalidate_range(s->cpu + offset, size);
+
+         if (!strcmp(mode, "hex"))
+            pan_hexdump(stdout, s->cpu + offset, size, true);
+         else if (!strcmp(mode, "hex64"))
+            dump_hex64(stdout, s->cpu + offset, size);
+         else if (!strcmp(mode, "delta"))
+            dump_delta(stdout, s->cpu + offset, size);
+         else if (!strcmp(mode, "tiler"))
+            dump_tiler(stdout, s->cpu + offset, size);
+         else if (!strcmp(mode, "filehex"))
+            dump_filehex(s->cpu + offset, size);
+
+         free(mode);
+
+      } else if (sscanf(line, "heatmap %lu %lu %lu %lu %lu %lu", &src, &offset,
+                        &size, &gran, &length, &stride) == 6) {
+
+         struct panfrost_ptr *s = buffers_elem(&buffers, src);
+
+         if (!s->gpu)
+            fprintf(stderr, "dumping buffer that doesn't exist!\n");
+
+         cache_invalidate_range(s->cpu + offset, size);
+
+         dump_heatmap(stdout, s->cpu + offset, size, gran, length, stride);
+
+      } else if (sscanf(line, "memset %lu %lu %lu %lu", &src, &offset, &gran,
+                        &size) == 4) {
+
+         struct panfrost_ptr *s = buffers_elem(&buffers, src);
+
+         if (!s->gpu)
+            fprintf(stderr, "memset on buffer that doesn't exist!\n");
+
+         memset(s->cpu + offset, gran, size);
+         cache_clean_range(s->cpu + offset, size);
+
+      } else if (sscanf(line, "sleep %lu", &size) == 1) {
+
+         usleep(size * 1000);
+
+      } else if (strcmp(line, "td\n") == 0 || strcmp(line, "td") == 0) {
+
+         void *ptr;
+
+         ptr = mmap(NULL, 1 << 21, PROT_READ | PROT_WRITE, MAP_SHARED,
+                    s->mali_fd, s->tiler_heap_header);
+         pan_hexdump(stdout, ptr, 4096, false);
+         pan_hexdump(stdout, ptr + (1 << 21) - 4096, 4096, false);
+         munmap(ptr, 1 << 21);
+
+         ptr = mmap(NULL, 1 << 21, PROT_READ | PROT_WRITE, MAP_SHARED,
+                    s->mali_fd, s->tiler_heap_header + (1 << 21));
+         pan_hexdump(stdout, ptr, 4096, false);
+         pan_hexdump(stdout, ptr + (1 << 21) - 4096, 4096, false);
+         munmap(ptr, 1 << 21);
+
+      } else {
+         fprintf(stderr, "unknown command '%s'\n", line);
+      }
+   }
+
+   /* Skip following tests */
+   return false;
+}
+
+static void
+pan_cs_evadd(pan_command_stream *c, unsigned offset, unsigned value)
+{
+   pan_emit_cs_32(c, 0x5e, value);
+   pan_pack_ins(c, CS_ADD_IMM, cfg)
+   {
+      cfg.value = offset;
+      cfg.src = 0x5a;
+      cfg.dest = 0x5c;
+   }
+   pan_pack_ins(c, CS_EVADD, cfg)
+   {
+      cfg.value = 0x5e;
+      cfg.addr = 0x5c;
+   }
+}
+
+static bool
+cs_simple(struct state *s, struct test *t)
+{
+   unsigned queue = t->vertex ? 2 : 0;
+
+   pan_command_stream *c = s->cs + queue;
+
+   unsigned dest = t->invalid ? 0x65 : 0x48;
+
+   pan_emit_cs_32(c, dest, 0x1234);
+   pan_cs_evadd(c, 0, 1);
+
+   submit_cs(s, queue);
+   return wait_cs(s, queue);
+}
+
+static bool
+cs_store(struct state *s, struct test *t)
+{
+   pan_command_stream *c = s->cs;
+
+   uint32_t *dest = s->allocations.ev2.cpu + 240;
+   mali_ptr dest_va = s->allocations.ev2.gpu + 240;
+   uint32_t value = 1234;
+   uint32_t add = 4320000;
+
+   *dest = 0;
+   cache_clean(dest);
+
+   unsigned addr_reg = 0x48;
+   unsigned value_reg = 0x4a;
+
+   if (t->invalid)
+      dest_va = 0xfdcba9876543;
+
+   pan_pack_ins(c, CS_WAIT, cfg)
+   {
+      cfg.slots = (1 << 1);
+   }
+   pan_emit_cs_48(c, addr_reg, dest_va);
+   pan_emit_cs_32(c, value_reg, value);
+
+   if (t->add) {
+      pan_pack_ins(c, CS_ADD_IMM, cfg)
+      {
+         cfg.value = add;
+         cfg.src = value_reg;
+         cfg.dest = value_reg;
+      }
+      value += add;
+   }
+
+   pan_pack_ins(c, CS_STR, cfg)
+   {
+      cfg.addr = addr_reg;
+      cfg.register_base = value_reg;
+      cfg.register_mask = 1;
+   }
+   pan_cs_evadd(c, 0, 1);
+
+   submit_cs(s, 0);
+   wait_cs(s, 0);
+
+   cache_invalidate(dest);
+   cache_barrier(); /* Just in case it's needed */
+   uint32_t result = *dest;
+
+   if (t->invalid && result == value) {
+      printf("Got %i, did not expect %i: ", result, value);
+      return false;
+   } else if (result != value) {
+      printf("Got %i, expected %i: ", result, value);
+      return false;
+   }
+
+   return true;
+}
+
+static void
+emit_cs_call(pan_command_stream *c, mali_ptr va, void *start, void *end)
+{
+   cache_clean_range(start, end - start);
+
+   pan_emit_cs_48(c, 0x48, va);
+   pan_emit_cs_32(c, 0x4a, end - start);
+   pan_pack_ins(c, CS_CALL, cfg)
+   {
+      cfg.address = 0x48;
+      cfg.length = 0x4a;
+   }
+}
+
+static bool
+cs_sub(struct state *s, struct test *t)
+{
+   pan_command_stream *c = s->cs;
+   pan_command_stream _i = {.ptr = s->allocations.cached.cpu}, *i = &_i;
+   mali_ptr cs_va = s->allocations.cached.gpu;
+
+   uint32_t *dest = s->allocations.normal.cpu;
+   mali_ptr dest_va = s->allocations.normal.gpu;
+   uint32_t value = 4321;
+
+   *dest = 0;
+   cache_clean(dest);
+
+   unsigned addr_reg = 0x48;
+   unsigned value_reg = 0x4a;
+
+   void *start = i->ptr;
+
+   pan_emit_cs_ins(c, 0x30, 0x5a0000000000);
+
+   pan_pack_ins(i, CS_SLOT, cfg)
+   {
+      cfg.index = 3;
+   }
+   pan_pack_ins(i, CS_WAIT, cfg)
+   {
+      cfg.slots = (1 << 3);
+   }
+   // pan_emit_cs_ins(i, 0x31, 0);
+
+   pan_emit_cs_48(i, addr_reg, dest_va);
+   pan_emit_cs_32(i, value_reg, value);
+   // pan_emit_cs_ins(i, 0x25, 0x01484a00000005ULL);
+   pan_pack_ins(i, CS_STR, cfg)
+   {
+      cfg.addr = addr_reg;
+      cfg.register_base = value_reg;
+      cfg.register_mask = 1;
+   }
+   // pan_emit_cs_ins(i, 0x09, 0);
+   // pan_emit_cs_ins(i, 0x31, 0x100000000);
+
+   // pan_emit_cs_ins(i, 0x24, 0x024a0000f80211ULL);
+
+   /*
+   pan_pack_ins(i, CS_STR_32, cfg) {
+           cfg.unk_1 = 1;
+           cfg.unk_2 = 4;
+           cfg.unk_3 = 1;
+           cfg.addr = addr_reg;
+           cfg.value = value_reg;
+           }*/
+
+   emit_cs_call(c, cs_va, start, i->ptr);
+   pan_cs_evadd(c, 0, 1);
+
+   submit_cs(s, 0);
+   wait_cs(s, 0);
+
+   cache_invalidate(dest);
+   cache_barrier(); /* Just in case it's needed */
+   uint32_t result = *dest;
+
+   if (result != value) {
+      printf("Got %i, expected %i: ", result, value);
+      return false;
+   }
+
+   return true;
+}
+
+static mali_ptr
+upload_shader(struct state *s, struct util_dynarray binary)
+{
+   assert(s->shader_alloc_offset + binary.size < s->page_size);
+
+   mali_ptr va = s->allocations.exec.gpu + s->shader_alloc_offset;
+
+   memcpy(s->allocations.exec.cpu, binary.data, binary.size);
+
+   /* Shouldn't be needed, but just in case... */
+   cache_clean_range(s->allocations.exec.cpu, binary.size);
+
+   s->shader_alloc_offset += binary.size;
+
+   return va;
+}
+
+static bool
+compute_compile(struct state *s, struct test *t)
+{
+   nir_builder _b = nir_builder_init_simple_shader(
+                  MESA_SHADER_COMPUTE, GENX(pan_shader_get_compiler_options)(),
+                  "mem_store"),
+               *b = &_b;
+
+   nir_ssa_def *ptr = nir_load_push_constant(b, 1, 64, nir_imm_int(b, 0));
+
+   nir_ssa_def *value = nir_imm_int(b, 123);
+
+   nir_store_global(b, ptr, 8, value, 1);
+
+   struct panfrost_compile_inputs inputs = {
+      .gpu_id = s->gpu_id,
+      .no_ubo_to_push = true,
+   };
+
+   struct util_dynarray binary = {0};
+   struct pan_shader_info shader_info = {0};
+
+   GENX(pan_shader_compile)(b->shader, &inputs, &binary, &shader_info);
+
+   dump_start(stderr);
+   disassemble_valhall(stderr, binary.data, binary.size, true);
+   dump_end(stderr);
+
+   s->compute_shader = upload_shader(s, binary);
+
+   util_dynarray_fini(&binary);
+   ralloc_free(b->shader);
+
+   return true;
+}
+
+static struct panfrost_ptr
+mem_offset(struct panfrost_ptr ptr, unsigned offset)
+{
+   ptr.cpu += offset;
+   ptr.gpu += offset;
+   return ptr;
+}
+
+static bool
+compute_execute(struct state *s, struct test *t)
+{
+   unsigned queue = t->blit ? 1 : 0;
+
+   pan_command_stream *c = s->cs + queue;
+   pan_command_stream _i = {.ptr = s->allocations.cached.cpu}, *i = &_i;
+   mali_ptr cs_va = s->allocations.cached.gpu;
+
+   struct panfrost_ptr dest = s->allocations.normal;
+   uint32_t value = 123;
+
+   *(uint32_t *)dest.cpu = 0;
+   cache_clean(dest.cpu);
+
+   struct panfrost_ptr fau = mem_offset(dest, 128);
+   *(uint64_t *)fau.cpu = dest.gpu;
+   cache_clean(fau.cpu);
+
+   struct panfrost_ptr local_storage = mem_offset(dest, 192);
+   pan_pack(local_storage.cpu, LOCAL_STORAGE, _)
+      ;
+   cache_clean(local_storage.cpu);
+
+   struct panfrost_ptr shader_program = mem_offset(dest, 256);
+   pan_pack(shader_program.cpu, SHADER_PROGRAM, cfg) {
+      cfg.stage = MALI_SHADER_STAGE_COMPUTE;
+      cfg.primary_shader = true;
+      cfg.register_allocation = MALI_SHADER_REGISTER_ALLOCATION_32_PER_THREAD;
+      cfg.binary = s->compute_shader;
+   }
+   cache_clean(shader_program.cpu);
+
+   void *start = i->ptr;
+
+   pan_pack_ins(i, CS_SLOT, cfg)
+   {
+      cfg.index = 3;
+   }
+   // pan_pack_ins(i, CS_WAIT, cfg) { cfg.slots = 1 << 3; }
+
+   pan_pack_cs(i, COMPUTE_PAYLOAD, cfg)
+   {
+      cfg.workgroup_size_x = 1;
+      cfg.workgroup_size_y = 1;
+      cfg.workgroup_size_z = 1;
+
+      cfg.workgroup_count_x = 1;
+      cfg.workgroup_count_y = 1;
+      cfg.workgroup_count_z = 1;
+
+      cfg.compute.shader = shader_program.gpu;
+      cfg.compute.thread_storage = local_storage.gpu;
+
+      cfg.compute.fau = fau.gpu;
+      cfg.compute.fau_count = 1;
+   }
+
+   pan_pack_ins(i, COMPUTE_LAUNCH, _);
+
+   // pan_emit_cs_32(c, 0x54, 1);
+   // pan_emit_cs_ins(c, 0x24, 0x540000000233);
+   emit_cs_call(c, cs_va, start, i->ptr);
+
+   pan_emit_cs_32(c, 0x4a, 0);
+   pan_emit_cs_ins(c, 0x24, 0x024a0000000211ULL);
+
+   pan_emit_cs_48(c, 0x48, dest.gpu);
+   pan_pack_ins(c, CS_LDR, cfg)
+   {
+      cfg.offset = 0;
+      cfg.register_mask = 1;
+      cfg.addr = 0x48;
+      cfg.register_base = 0x20;
+   }
+   pan_pack_ins(c, CS_WAIT, cfg)
+   {
+      cfg.slots = 1;
+   }
+   pan_pack_ins(c, CS_ADD_IMM, cfg)
+   {
+      cfg.value = 1;
+      cfg.src = 0x20;
+      cfg.dest = 0x20;
+   }
+   pan_pack_ins(c, CS_STR, cfg)
+   {
+      cfg.offset = 64;
+      cfg.register_mask = 1;
+      cfg.addr = 0x48;
+      cfg.register_base = 0x20;
+   }
+
+   pan_cs_evadd(c, 0, 1);
+
+   submit_cs(s, queue);
+   wait_cs(s, queue);
+
+   cache_invalidate(dest.cpu);
+   cache_barrier(); /* Just in case it's needed */
+   uint32_t result = ((uint32_t *)dest.cpu)[0];
+   uint32_t result2 = ((uint32_t *)dest.cpu)[16];
+
+   if (result != value) {
+      printf("Got %i, %i, expected %i: ", result, result2, value);
+      return false;
+   }
+
+   return true;
+}
+
+static bool
+mmu_dump(struct state *s, struct test *t)
+{
+   unsigned size = 1024 * 1024;
+
+   void *mem = mmap(NULL, size, PROT_READ, MAP_SHARED, s->mali_fd,
+                    BASE_MEM_MMU_DUMP_HANDLE);
+   if (mem == MAP_FAILED) {
+      perror("mmap(BASE_MEM_MMU_DUMP_HANDLE)");
+      return false;
+   }
+
+   pan_hexdump(stdout, mem, size, true);
+
+   return true;
+}
+
+#define SUBTEST(s)                                                             \
+   {                                                                           \
+      .label = #s, .subtests = s, .sub_length = ARRAY_SIZE(s)                  \
+   }
+
+#define STATE(item) .offset = offsetof(struct state, item)
+
+#define ALLOC(item) .offset = offsetof(struct state, allocations.item)
+#define ALLOC_TEST(label, item, f)                                             \
+   {                                                                           \
+      alloc, dealloc, label, ALLOC(item), .flags = f                           \
+   }
+
+struct test kbase_main[] = {
+   {open_kbase, close_kbase, "Open kbase device"},
+   {get_version, NULL, "Check version"},
+   {set_flags, NULL, "Set flags"},
+   {mmap_tracking, munmap_tracking, "Map tracking handle"},
+   {get_gpuprops, free_gpuprops, "Get GPU properties"},
+   {get_gpu_id, NULL, "GPU ID"},
+   {get_coherency_mode, NULL, "Coherency mode"},
+   {get_csf_caps, NULL, "CSF caps"},
+   {mmap_user_reg, munmap_user_reg, "Map user register page"},
+   {init_mem_exec, NULL, "Initialise EXEC_VA zone"},
+   {init_mem_jit, NULL, "Initialise JIT allocator"},
+   {stream_create, stream_destroy, "Create synchronisation stream"},
+   {tiler_heap_create, tiler_heap_term, "Create chunked tiler heap"},
+   {cs_group_create, cs_group_term, "Create command stream group"},
+   {decode_init, decode_close, "Initialize pandecode"},
+
+   /* Flags are named in mali_base_csf_kernel.h, omitted for brevity */
+   ALLOC_TEST("Allocate normal memory", normal, 0x200f),
+   ALLOC_TEST("Allocate exectuable memory", exec, 0x2017),
+   ALLOC_TEST("Allocate coherent memory", coherent, 0x280f),
+   ALLOC_TEST("Allocate cached memory", cached, 0x380f),
+   ALLOC_TEST("Allocate CSF event memory", event, 0x8200f),
+   ALLOC_TEST("Allocate CSF event memory 2", ev2, 0x8200f),
+
+   /* These three tests are run for every queue, but later ones are not */
+   {cs_queue_create, cs_queue_free, "Create command stream queues"},
+   {cs_queue_register, cs_queue_term, "Register command stream queues"},
+
+   {cs_test, NULL, "Test command stream"},
+
+   {cs_init, NULL, "Initialise and start command stream queues"},
+   {cs_simple, NULL, "Execute MOV command"},
+   {cs_simple, NULL, "Execute MOV command (again)"},
+   {cs_simple, NULL, "Execute MOV command (vertex)", .vertex = true},
+   //{ cs_simple, NULL, "Execute MOV command (vertex, invalid)", .invalid =
+   //true, .vertex = true },
+   {cs_simple, NULL, "Execute MOV command (vertex, again)", .vertex = true},
+   {cs_store, NULL, "Execute STR command"},
+   //{ cs_store, NULL, "Execute STR command to invalid address", .invalid = true
+   //},
+   {cs_store, NULL, "Execute ADD command", .add = true},
+   {cs_sub, NULL, "Execute STR on iterator"},
+
+   {compute_compile, NULL, "Compile a compute shader"},
+   {compute_execute, NULL, "Execute a compute shader"},
+   {compute_execute, NULL, "Execute compute on blit queue", .blit = true},
+
+   //{ mmu_dump, NULL, "Dump MMU pagetables" },
+};
+
+static void do_test_list(struct state *s, struct test *tests, unsigned length);
+
+static void
+cleanup_test_list(struct state *s, struct test *tests, unsigned length)
+{
+   for (unsigned i = length; i > 0; --i) {
+      unsigned n = i - 1;
+
+      struct test *t = &tests[n];
+      if (!t->cleanup)
+         continue;
+
+      if (pr)
+         printf("[CLEANUP %i] %s: ", n, t->label);
+      if (t->cleanup(s, t)) {
+         if (pr)
+            printf("PASS\n");
+      } else {
+         if (pr)
+            printf("FAIL\n");
+      }
+   }
+}
+
+static unsigned
+interpret_test_list(struct state *s, struct test *tests, unsigned length)
+{
+   for (unsigned i = 0; i < length; ++i) {
+      struct test *t = &tests[i];
+
+      if (pr)
+         printf("[TEST %i] %s: ", i, t->label);
+      if (t->part) {
+         if (t->part(s, t)) {
+            if (pr)
+               printf("PASS\n");
+         } else {
+            if (pr)
+               printf("FAIL\n");
+            if (!getenv("TEST_KEEP_GOING"))
+               return i + 1;
+         }
+      }
+      if (t->subtests)
+         do_test_list(s, t->subtests, t->sub_length);
+   }
+
+   return length;
+}
+
+static void
+do_test_list(struct state *s, struct test *tests, unsigned length)
+{
+   unsigned ran = interpret_test_list(s, tests, length);
+   cleanup_test_list(s, tests, ran);
+}
+
+int
+main(int argc, char *argv[])
+{
+   struct state s = {
+      .page_size = sysconf(_SC_PAGE_SIZE),
+      .argc = argc,
+      .argv = argv,
+   };
+
+   if (getenv("CSF_QUIET"))
+      pr = false;
+
+   if (!strcmp(getenv("TERM"), "dumb"))
+      colour_term = false;
+
+   if (pr)
+      printf("Running Valhall CSF tests\n");
+
+   do_test_list(&s, kbase_main, ARRAY_SIZE(kbase_main));
+}
diff -urN mesa-23.0.0/src/panfrost/ds/.clang-format mesa/src/panfrost/ds/.clang-format
--- mesa-23.0.0/src/panfrost/ds/.clang-format	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/ds/.clang-format	2023-03-06 17:54:35.854495073 +0100
@@ -0,0 +1,21 @@
+BasedOnStyle: WebKit
+AlignTrailingComments: 'true'
+AllowAllParametersOfDeclarationOnNextLine: 'false'
+AllowShortFunctionsOnASingleLine: None
+AlwaysBreakBeforeMultilineStrings: 'true'
+BinPackArguments: 'false'
+BinPackParameters: 'false'
+BreakBeforeBinaryOperators: None
+BreakBeforeBraces: Linux
+ColumnLimit: '100'
+Cpp11BracedListStyle: 'true'
+KeepEmptyLinesAtTheStartOfBlocks: 'false'
+NamespaceIndentation: None
+PointerAlignment: Right
+SortIncludes: 'true'
+SpaceAfterTemplateKeyword: 'false'
+Standard: Cpp11
+TabWidth: '3'
+IndentWidth: '3'
+ConstructorInitializerIndentWidth: '3'
+ContinuationIndentWidth: '3'
diff -urN mesa-23.0.0/src/panfrost/lib/genxml/common.xml mesa/src/panfrost/lib/genxml/common.xml
--- mesa-23.0.0/src/panfrost/lib/genxml/common.xml	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/genxml/common.xml	2023-03-06 17:53:54.638222075 +0100
@@ -46,7 +46,7 @@
     <value name="Constant" value="7"/>
   </enum>
 
-  <struct name="Blend Function" no-direct-packing="true">
+  <struct name="Blend Function" layout="none">
     <!-- Blend equation: A + (B * C) -->
     <field name="A" size="2" start="0" type="Blend Operand A"/>
     <field name="Negate A" size="1" start="3" type="bool"/>
diff -urN mesa-23.0.0/src/panfrost/lib/genxml/decode.c mesa/src/panfrost/lib/genxml/decode.c
--- mesa-23.0.0/src/panfrost/lib/genxml/decode.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/genxml/decode.c	2023-03-06 19:19:31.726302354 +0100
@@ -58,6 +58,14 @@
                         (pandecode_indent + 1) * 2);                           \
    }
 
+#define DUMP_SECTION_CS_V10(A, S, cl, buf, buf_unk, ...)                       \
+   {                                                                           \
+      pan_section_unpack_cs_v10(cl, buf, buf_unk, A, S, temp);                 \
+      pandecode_log(__VA_ARGS__);                                              \
+      pan_section_print(pandecode_dump_stream, A, S, temp,                     \
+                        (pandecode_indent + 1) * 2);                           \
+   }
+
 #define MAP_ADDR(T, addr, cl)                                                  \
    const uint8_t *cl = pandecode_fetch_gpu_mem(addr, pan_size(T));
 
@@ -161,7 +169,7 @@
    if (nonzero_weights)
       DUMP_UNPACKED(TILER_WEIGHTS, w, "Tiler Weights:\n");
 }
-#endif
+#endif /* PAN_ARCH <= 5 */
 
 #if PAN_ARCH >= 5
 static void
@@ -189,7 +197,7 @@
    pandecode_indent--;
    pandecode_log("\n");
 }
-#endif
+#endif /* PAN_ARCH >= 5 */
 
 #if PAN_ARCH >= 6
 static void
@@ -205,7 +213,7 @@
                     samples[2 * i + 1] - 128);
    }
 }
-#endif
+#endif /* PAN_ARCH >= 6 */
 
 static void pandecode_dcd(const struct MALI_DRAW *p,
                           enum mali_job_type job_type, unsigned gpu_id);
@@ -231,12 +239,12 @@
 #if PAN_ARCH >= 6
    pandecode_sample_locations(fb);
 
-   unsigned dcd_size = pan_size(DRAW);
+   unsigned dcd_size = pan_size(DRAW_NO_CS);
 
    if (params.pre_frame_0 != MALI_PRE_POST_FRAME_SHADER_MODE_NEVER) {
       const void *PANDECODE_PTR_VAR(dcd,
                                     params.frame_shader_dcds + (0 * dcd_size));
-      pan_unpack(dcd, DRAW, draw);
+      pan_unpack(dcd, DRAW_NO_CS, draw);
       pandecode_log("Pre frame 0:\n");
       pandecode_dcd(&draw, MALI_JOB_TYPE_FRAGMENT, gpu_id);
    }
@@ -244,7 +252,7 @@
    if (params.pre_frame_1 != MALI_PRE_POST_FRAME_SHADER_MODE_NEVER) {
       const void *PANDECODE_PTR_VAR(dcd,
                                     params.frame_shader_dcds + (1 * dcd_size));
-      pan_unpack(dcd, DRAW, draw);
+      pan_unpack(dcd, DRAW_NO_CS, draw);
       pandecode_log("Pre frame 1:\n");
       pandecode_dcd(&draw, MALI_JOB_TYPE_FRAGMENT, gpu_id);
    }
@@ -252,11 +260,11 @@
    if (params.post_frame != MALI_PRE_POST_FRAME_SHADER_MODE_NEVER) {
       const void *PANDECODE_PTR_VAR(dcd,
                                     params.frame_shader_dcds + (2 * dcd_size));
-      pan_unpack(dcd, DRAW, draw);
+      pan_unpack(dcd, DRAW_NO_CS, draw);
       pandecode_log("Post frame:\n");
       pandecode_dcd(&draw, MALI_JOB_TYPE_FRAGMENT, gpu_id);
    }
-#else
+#else /* PAN_ARCH < 6 */
    DUMP_SECTION(FRAMEBUFFER, LOCAL_STORAGE, fb, "Local Storage:\n");
 
    const void *t = pan_section_ptr(fb, FRAMEBUFFER, TILER);
@@ -287,20 +295,16 @@
    if (is_fragment)
       pandecode_render_target(gpu_va, gpu_id, &params);
 
-   return (struct pandecode_fbd){
-      .rt_count = params.render_target_count,
-      .has_extra = params.has_zs_crc_extension,
-   };
-#else
+   return (struct pandecode_fbd){.rt_count = params.render_target_count,
+                                 .has_extra = params.has_zs_crc_extension};
+#else /* PAN_ARCH < 5 */
    /* Dummy unpack of the padding section to make sure all words are 0.
     * No need to call print here since the section is supposed to be empty.
     */
    pan_section_unpack(fb, FRAMEBUFFER, PADDING_1, padding1);
    pan_section_unpack(fb, FRAMEBUFFER, PADDING_2, padding2);
 
-   return (struct pandecode_fbd){
-      .rt_count = 1,
-   };
+   return (struct pandecode_fbd){.rt_count = 1};
 #endif
 }
 
@@ -348,7 +352,7 @@
    }
    pandecode_log("\n");
 }
-#endif
+#endif /* PAN_ARCH <= 7 */
 
 #if PAN_ARCH >= 5
 static mali_ptr
@@ -365,7 +369,7 @@
    return b.blend_shader ? (b.shader_pc & ~0xf) : 0;
 #endif
 }
-#endif
+#endif /* PAN_ARCH >= 6 || PAN_ARCH == 5 */
 
 #if PAN_ARCH <= 7
 static unsigned
@@ -430,8 +434,9 @@
 
    DUMP_UNPACKED(INVOCATION, invocation, "Invocation:\n")
 }
-#endif
+#endif /* PAN_ARCH <= 7 */
 
+#if PAN_ARCH < 10
 static void
 pandecode_primitive(const void *p)
 {
@@ -459,7 +464,7 @@
                                    primitive.index_count * size);
    } else if (primitive.index_type)
       pandecode_log("// XXX: unexpected index size\n");
-#endif
+#endif /* PAN_ARCH <= 7 */
 }
 
 static void
@@ -471,6 +476,7 @@
 
    DUMP_UNPACKED(PRIMITIVE_SIZE, ps, "Primitive Size:\n")
 }
+#endif /* PAN_ARCH < 10 */
 
 #if PAN_ARCH <= 7
 static void
@@ -502,7 +508,7 @@
    free(ptr);
    pandecode_log("\n");
 }
-#endif
+#endif /* PAN_ARCH <= 7 */
 
 static void
 pandecode_shader_disassemble(mali_ptr shader_ptr, int type, unsigned gpu_id)
@@ -585,7 +591,7 @@
    pandecode_indent--;
    pandecode_log("},\n");
 }
-#endif
+#endif /* PAN_ARCH <= 7 */
 
 #if PAN_ARCH <= 5
 static void
@@ -604,7 +610,7 @@
                              temp.levels, nr_samples, temp.array_size);
    pandecode_indent--;
 }
-#else
+#else /* PAN_ARCH > 5 */
 static void
 pandecode_texture(const void *cl, unsigned tex)
 {
@@ -622,7 +628,7 @@
 
    for (unsigned i = 0; i < plane_count; ++i)
       DUMP_ADDR(PLANE, temp.surfaces + i * pan_size(PLANE), "Plane %u:\n", i);
-#else
+#else /* PAN_ARCH < 9 */
    unsigned nr_samples =
       temp.dimension == MALI_TEXTURE_DIMENSION_3D ? 1 : temp.sample_count;
 
@@ -649,7 +655,7 @@
 
    for (unsigned tex = 0; tex < texture_count; ++tex)
       pandecode_texture(cl + pan_size(TEXTURE) * tex, tex);
-#else
+#else /* PAN_ARCH < 6 */
    mali_ptr *PANDECODE_PTR_VAR(u, textures);
 
    for (int tex = 0; tex < texture_count; ++tex) {
@@ -761,7 +767,7 @@
                pandecode_shader_disassemble(shader, job_type, gpu_id);
          }
       }
-#endif
+#endif /* PAN_ARCH >= 5 */
    } else
       pandecode_log("// XXX: missing shader descriptor\n");
 
@@ -830,7 +836,7 @@
    pandecode_indent--;
    pandecode_log("\n");
 }
-#endif
+#endif /* PAN_ARCH <= 7 */
 
 #if PAN_ARCH >= 6
 static void
@@ -846,6 +852,10 @@
    DUMP_UNPACKED(TILER_CONTEXT, t, "Tiler:\n");
 }
 
+#endif /* PAN_ARCH >= 6 */
+
+#if PAN_ARCH < 10
+#if PAN_ARCH >= 6
 #if PAN_ARCH <= 7
 static void
 pandecode_indexed_vertex_job(const struct MALI_JOB_HEADER *h, mali_ptr job,
@@ -878,8 +888,8 @@
 
    pan_section_unpack(p, INDEXED_VERTEX_JOB, PADDING, padding);
 }
-#endif
-#endif
+#endif /* PAN_ARCH <= 7 */
+#endif /* PAN_ARCH >= 6 */
 
 static void
 pandecode_tiler_job(const struct MALI_JOB_HEADER *h, mali_ptr job,
@@ -915,7 +925,7 @@
    pan_section_unpack(p, TILER_JOB, PADDING, padding);
 #endif
 
-#else
+#else /* PAN_ARCH < 6 */
    pan_section_unpack(p, TILER_JOB, PRIMITIVE, primitive);
    pandecode_primitive_size(
       pan_section_ptr(p, TILER_JOB, PRIMITIVE_SIZE),
@@ -924,12 +934,17 @@
    pandecode_indent--;
    pandecode_log("\n");
 }
+#endif /* PAN_ARCH < 10 */
 
 static void
-pandecode_fragment_job(mali_ptr job, unsigned gpu_id)
+pandecode_fragment_job(mali_ptr job, uint32_t *cs_buf, uint32_t *cs_buf_unk,
+                       unsigned gpu_id)
 {
+#if PAN_ARCH < 10
    struct mali_fragment_job_packed *PANDECODE_PTR_VAR(p, job);
-   pan_section_unpack(p, FRAGMENT_JOB, PAYLOAD, s);
+#endif
+
+   pan_section_unpack_cs_v10(p, cs_buf, cs_buf_unk, FRAGMENT_JOB, PAYLOAD, s);
 
    UNUSED struct pandecode_fbd info =
       pandecode_fbd(s.framebuffer, true, gpu_id);
@@ -947,7 +962,7 @@
       expected_tag |= MALI_FBD_TAG_HAS_ZS_RT;
 
    expected_tag |= MALI_FBD_TAG_IS_MFBD | (MALI_POSITIVE(info.rt_count) << 2);
-#endif
+#endif /* PAN_ARCH >= 5 */
 
    DUMP_UNPACKED(FRAGMENT_JOB_PAYLOAD, s, "Fragment Job Payload:\n");
 
@@ -964,6 +979,8 @@
    pandecode_log("\n");
 }
 
+#if PAN_ARCH < 10
+// TODO: Use the same model as for malloc_vertex jobs?
 static void
 pandecode_write_value_job(mali_ptr job)
 {
@@ -981,6 +998,7 @@
    DUMP_SECTION(CACHE_FLUSH_JOB, PAYLOAD, p, "Cache Flush Payload:\n");
    pandecode_log("\n");
 }
+#endif /* PAN_ARCH < 10 */
 
 #if PAN_ARCH >= 9
 static void
@@ -1063,6 +1081,9 @@
 static void
 pandecode_depth_stencil(mali_ptr addr)
 {
+   if (!addr)
+      return;
+
    MAP_ADDR(DEPTH_STENCIL, addr, cl);
    pan_unpack(cl, DEPTH_STENCIL, desc);
    DUMP_UNPACKED(DEPTH_STENCIL, desc, "Depth/stencil");
@@ -1089,14 +1110,15 @@
 pandecode_dcd(const struct MALI_DRAW *p, enum mali_job_type job_type,
               unsigned gpu_id)
 {
-   mali_ptr frag_shader = 0;
-
    pandecode_depth_stencil(p->depth_stencil);
 
    for (unsigned i = 0; i < p->blend_count; ++i) {
+      MAP_ADDR(SHADER_PROGRAM, p->shader.shader, cl);
+      pan_unpack(cl, SHADER_PROGRAM, desc);
+
       struct mali_blend_packed *PANDECODE_PTR_VAR(blend_descs, p->blend);
 
-      mali_ptr blend_shader = pandecode_blend(blend_descs, i, frag_shader);
+      mali_ptr blend_shader = pandecode_blend(blend_descs, i, desc.binary);
       if (blend_shader) {
          fprintf(pandecode_dump_stream, "Blend shader %u", i);
          pandecode_shader_disassemble(blend_shader, 0, gpu_id);
@@ -1108,21 +1130,34 @@
 }
 
 static void
-pandecode_malloc_vertex_job(mali_ptr job, unsigned gpu_id)
+pandecode_malloc_vertex_job(mali_ptr job, uint32_t *cs_buf,
+                            uint32_t *cs_buf_unk, unsigned gpu_id)
 {
+#if PAN_ARCH < 10
    struct mali_malloc_vertex_job_packed *PANDECODE_PTR_VAR(p, job);
+#endif
 
-   DUMP_SECTION(MALLOC_VERTEX_JOB, PRIMITIVE, p, "Primitive:\n");
-   DUMP_SECTION(MALLOC_VERTEX_JOB, INSTANCE_COUNT, p, "Instance count:\n");
+   DUMP_SECTION_CS_V10(MALLOC_VERTEX_JOB, PRIMITIVE, p, cs_buf, cs_buf_unk,
+                       "Primitive:\n");
+   DUMP_SECTION_CS_V10(MALLOC_VERTEX_JOB, INSTANCE_COUNT, p, cs_buf, cs_buf_unk,
+                       "Instance count:\n");
+#if PAN_ARCH < 10
    DUMP_SECTION(MALLOC_VERTEX_JOB, ALLOCATION, p, "Allocation:\n");
-   DUMP_SECTION(MALLOC_VERTEX_JOB, TILER, p, "Tiler:\n");
-   DUMP_SECTION(MALLOC_VERTEX_JOB, SCISSOR, p, "Scissor:\n");
-   DUMP_SECTION(MALLOC_VERTEX_JOB, PRIMITIVE_SIZE, p, "Primitive Size:\n");
-   DUMP_SECTION(MALLOC_VERTEX_JOB, INDICES, p, "Indices:\n");
+#endif
+   DUMP_SECTION_CS_V10(MALLOC_VERTEX_JOB, TILER, p, cs_buf, cs_buf_unk,
+                       "Tiler:\n");
+   DUMP_SECTION_CS_V10(MALLOC_VERTEX_JOB, SCISSOR, p, cs_buf, cs_buf_unk,
+                       "Scissor:\n");
+   DUMP_SECTION_CS_V10(MALLOC_VERTEX_JOB, PRIMITIVE_SIZE, p, cs_buf, cs_buf_unk,
+                       "Primitive Size:\n");
+   DUMP_SECTION_CS_V10(MALLOC_VERTEX_JOB, INDICES, p, cs_buf, cs_buf_unk,
+                       "Indices:\n");
 
-   pan_section_unpack(p, MALLOC_VERTEX_JOB, DRAW, dcd);
+   pan_section_unpack_cs_v10(p, cs_buf, cs_buf_unk, MALLOC_VERTEX_JOB, DRAW,
+                             dcd);
 
-   pan_section_unpack(p, MALLOC_VERTEX_JOB, TILER, tiler_ptr);
+   pan_section_unpack_cs_v10(p, cs_buf, cs_buf_unk, MALLOC_VERTEX_JOB, TILER,
+                             tiler_ptr);
    pandecode_log("Tiler Job Payload:\n");
    pandecode_indent++;
    if (tiler_ptr.address)
@@ -1133,17 +1168,23 @@
 
    pandecode_dcd(&dcd, 0, gpu_id);
 
-   pan_section_unpack(p, MALLOC_VERTEX_JOB, POSITION, position);
-   pan_section_unpack(p, MALLOC_VERTEX_JOB, VARYING, varying);
+   pan_section_unpack_cs_v10(p, cs_buf, cs_buf_unk, MALLOC_VERTEX_JOB, POSITION,
+                             position);
+   pan_section_unpack_cs_v10(p, cs_buf, cs_buf_unk, MALLOC_VERTEX_JOB, VARYING,
+                             varying);
    pandecode_shader_environment(&position, gpu_id);
    pandecode_shader_environment(&varying, gpu_id);
 }
 
 static void
-pandecode_compute_job(mali_ptr job, unsigned gpu_id)
+pandecode_compute_job(mali_ptr job, uint32_t *cs_buf, uint32_t *cs_buf_unk,
+                      unsigned gpu_id)
 {
+#if PAN_ARCH < 10
    struct mali_compute_job_packed *PANDECODE_PTR_VAR(p, job);
-   pan_section_unpack(p, COMPUTE_JOB, PAYLOAD, payload);
+#endif
+   pan_section_unpack_cs_v10(p, cs_buf, cs_buf_unk, COMPUTE_JOB, PAYLOAD,
+                             payload);
 
    pandecode_shader(payload.compute.shader, "Shader", gpu_id);
    if (payload.compute.thread_storage)
@@ -1155,8 +1196,9 @@
 
    DUMP_UNPACKED(COMPUTE_PAYLOAD, payload, "Compute:\n");
 }
-#endif
+#endif /* PAN_ARCH >= 9 */
 
+#if PAN_ARCH < 10
 /* Entrypoint to start tracing. jc_gpu_va is the GPU address for the first job
  * in the chain; later jobs are found by walking the chain. GPU ID is the
  * more finegrained ID because some details are model-specific even within a
@@ -1212,18 +1254,18 @@
          pandecode_indexed_vertex_job(&h, jc_gpu_va, gpu_id);
          break;
 #endif
-#else
+#else /* PAN_ARCH > 7 */
       case MALI_JOB_TYPE_COMPUTE:
-         pandecode_compute_job(jc_gpu_va, gpu_id);
+         pandecode_compute_job(jc_gpu_va, NULL, NULL, gpu_id);
          break;
 
       case MALI_JOB_TYPE_MALLOC_VERTEX:
-         pandecode_malloc_vertex_job(jc_gpu_va, gpu_id);
+         pandecode_malloc_vertex_job(jc_gpu_va, NULL, NULL, gpu_id);
          break;
 #endif
 
       case MALI_JOB_TYPE_FRAGMENT:
-         pandecode_fragment_job(jc_gpu_va, gpu_id);
+         pandecode_fragment_job(jc_gpu_va, NULL, NULL, gpu_id);
          break;
 
       default:
@@ -1261,3 +1303,526 @@
 
    pandecode_map_read_write();
 }
+#endif
+
+#if PAN_ARCH >= 10
+static void
+pandecode_cs_dump_state(uint32_t *state)
+{
+   uint64_t *st_64 = (uint64_t *)state;
+   /* Only registers below 0x40 seem to be actually be used by jobs */
+   for (unsigned i = 0; i < 0x40 / 4; ++i) {
+      uint64_t v1 = st_64[i * 2];
+      uint64_t v2 = st_64[i * 2 + 1];
+
+      if (!v1 && !v2)
+         continue;
+
+      pandecode_log("0x%2x: 0x%16" PRIx64 " 0x%16" PRIx64 "\n", i * 4, v1, v2);
+   }
+}
+
+/* Assumes eight scoreboards */
+static void
+pandecode_scoreboard_mask(unsigned mask)
+{
+   if (mask == 0xff) {
+      pandecode_log_cont("all");
+      return;
+   } else if (!mask) {
+      pandecode_log_cont("none");
+      return;
+   }
+
+   const char *comma = "";
+   for (unsigned i = 0; i < 8; ++i) {
+      if (mask & (1 << i)) {
+         pandecode_log_cont("%s%i", comma, i);
+         comma = ",";
+      }
+   }
+}
+
+static void
+pandecode_regmask(unsigned base, unsigned mask)
+{
+   switch (mask) {
+   case 0:
+      pandecode_log_cont("(invalid: %02x mask 0)", base);
+      return;
+   case 1:
+      pandecode_log_cont("w%02x", base);
+      return;
+   case 3:
+      pandecode_log_cont("x%02x", base);
+      return;
+   default:
+      break;
+   }
+
+   unsigned first = ffs(mask) - 1;
+   if (first)
+      pandecode_log_cont("{(+%i) ", first);
+   else
+      pandecode_log_cont("{");
+
+   unsigned edges = mask ^ (mask << 1);
+
+   const char *comma = "";
+
+   bool outside = true;
+   unsigned start;
+   u_foreach_bit(i, edges) {
+      if (outside)
+         start = i;
+      else if (i == start + 1)
+         pandecode_log_cont("%sw%02x", comma, base + start);
+      else if (i == start + 2)
+         pandecode_log_cont("%sx%02x", comma, base + start);
+      else
+         pandecode_log_cont("%sw%02x-w%02x", comma, base + start, base + i - 1);
+      outside = !outside;
+
+      if (outside)
+         comma = ", ";
+   }
+
+   pandecode_log_cont("}");
+}
+
+static void pandecode_cs_buffer(uint64_t *commands, unsigned size,
+                                uint32_t *buffer, uint32_t *buffer_unk,
+                                unsigned gpu_id, mali_ptr va);
+
+// Hack hack hackity hack: gpu_id == 1 means "don't decode" (only disassemble)
+static void
+pandecode_cs_command(uint64_t command, mali_ptr va, uint32_t *buffer,
+                     uint32_t *buffer_unk, unsigned gpu_id)
+{
+   uint8_t op = command >> 56;
+   uint8_t addr = (command >> 48) & 0xff;
+   uint64_t value = command & 0xffffffffffffULL;
+
+   uint32_t h = value >> 32;
+   uint32_t l = value;
+
+   uint8_t arg1 = h & 0xff;
+   uint8_t arg2 = h >> 8;
+
+   if (command)
+      pandecode_log("%" PRIx64 " %016" PRIx64 " ", va, command);
+
+   switch (op) {
+   case 0:
+      if (addr || value)
+         pandecode_log("nop %02x, #0x%" PRIx64 "\n", addr, value);
+      break;
+   case 1:
+      buffer_unk[addr] = buffer[addr] = l;
+      buffer_unk[addr + 1] = buffer[addr + 1] = h;
+      pandecode_log("mov x%02x, #0x%" PRIx64 "\n", addr, value);
+      break;
+   case 2:
+      buffer_unk[addr] = buffer[addr] = l;
+      pandecode_log("mov w%02x, #0x%" PRIx64 "\n", addr, value);
+      break;
+   case 3:
+      if (l & 0xff00ffff || h || addr) {
+         pandecode_log("wait (unk %02x), (unk %04x), "
+                       "%i, (unk %04x)\n",
+                       addr, h, l >> 16, l);
+      } else {
+         pandecode_log("wait ");
+         pandecode_scoreboard_mask(l >> 16);
+         pandecode_log_cont("\n");
+      }
+      break;
+   case 4: {
+      uint32_t masked = l & 0xffff0000;
+      unsigned task_increment = l & 0x3fff;
+      unsigned task_axis = (l >> 14) & 3;
+      if (h != 0xff00 || addr || masked)
+         pandecode_log("compute (unk %02x), (unk %04x), "
+                       "(unk %x), inc %i, axis %i\n\n",
+                       addr, h, masked, task_increment, task_axis);
+      else
+         pandecode_log("compute inc %i, axis %i\n\n", task_increment,
+                       task_axis);
+
+      if (gpu_id != 1) {
+         pandecode_indent++;
+
+         pandecode_compute_job(0, buffer, buffer_unk, gpu_id);
+
+         /* The gallium driver emits this even for compute jobs, clear
+          * it from unknown state */
+         pan_unpack_cs(buffer, buffer_unk, SCISSOR, unused_scissor);
+         pandecode_cs_dump_state(buffer_unk);
+
+         pandecode_log("\n");
+         pandecode_indent--;
+      }
+
+      break;
+   }
+   case 6: {
+      /* The meaning of the first argument (in h) is unknown, but it
+       * appears that the second bit must be set. */
+      uint32_t masked = l & 0xfffff8f0;
+      uint8_t mode = l & 0xf;
+      uint8_t index = (l >> 8) & 7;
+      if (addr || masked)
+         pandecode_log("idvs (unk %02x), 0x%04x, (unk %x), "
+                       "mode %i index %i\n\n",
+                       addr, h, masked, mode, index);
+      else
+         pandecode_log("idvs 0x%04x, mode %i index %i\n\n", h, mode, index);
+
+      if (gpu_id != 1) {
+         pandecode_indent++;
+
+         pandecode_malloc_vertex_job(0, buffer, buffer_unk, gpu_id);
+         pandecode_cs_dump_state(buffer_unk);
+
+         pandecode_log("\n");
+         pandecode_indent--;
+      }
+
+      break;
+   }
+   case 7: {
+      uint64_t masked = value & ~0x000100000071;
+      bool tem = value & 1;
+      bool unk = (value >> 32) & 1;
+
+      const char *order = (const char *[]){
+         "z_order",   "horizontal",         "vertical",         "invalid_3",
+         "invalid_4", "reverse_horizontal", "reverse_vertical", "invalid_7",
+      }[(value >> 4) & 7];
+
+      if (addr || masked) {
+         pandecode_log("fragment (unk %02x), (unk %" PRIx64 ")\n\n", addr,
+                       value);
+      } else if (value) {
+         pandecode_log("fragment tem %i, render %s, unk %i\n\n", tem, order,
+                       unk);
+      } else {
+         pandecode_log("fragment\n\n");
+      }
+
+      if (gpu_id != 1) {
+         pandecode_indent++;
+
+         pandecode_fragment_job(0, buffer, buffer_unk, gpu_id);
+         pandecode_cs_dump_state(buffer_unk);
+
+         pandecode_log("\n");
+         pandecode_indent--;
+      }
+
+      break;
+   }
+
+   case 9: {
+      if (addr || l || h > 1)
+         pandecode_log("flush_tiler (unk %02x), (unk %" PRIx64 ")\n", addr,
+                       value);
+      else if (h)
+         pandecode_log("flush_tiler unk\n");
+      else
+         pandecode_log("flush_tiler\n");
+      break;
+   }
+
+   case 16:
+   case 17: {
+      char wid = (op == 16) ? 'w' : 'x';
+
+      if (op == 16) {
+         buffer_unk[addr] = buffer[addr] = buffer[arg2] + l;
+      } else {
+         uint64_t r = buffer[arg2] + ((uint64_t)buffer[arg2 + 1] << 32) + l;
+         buffer_unk[addr] = buffer[addr] = r;
+         buffer_unk[addr + 1] = buffer[addr + 1] = r >> 32;
+      }
+
+      if (arg1)
+         pandecode_log("add %c%02x, (unk %x), %c%02x, #0x%x\n", wid, addr, arg1,
+                       wid, arg2, l);
+      else if ((int32_t)l < 0)
+         pandecode_log("add %c%02x, %c%02x, %i\n", wid, addr, wid, arg2,
+                       (int32_t)l);
+      else if (l)
+         pandecode_log("add %c%02x, %c%02x, #0x%x\n", wid, addr, wid, arg2, l);
+      else
+         pandecode_log("mov %c%02x, %c%02x\n", wid, addr, wid, arg2);
+
+      break;
+   }
+
+   case 20:
+   case 21: {
+      const char *name = (op == 20) ? "ldr" : "str";
+
+      /* The immediate offset must be 4-aligned (though if the
+       * address itself is unaligned, the bits will silently be
+       * masked off).
+       *
+       * Up to 16 32-bit registers can be read or written in a
+       * single instruction, behaviour is similar to LDM or STM
+       * except that a base register is specified.
+       *
+       * These instructions are high latency. Use WAIT 0 to wait for
+       * the result of an LDR, or for a STR to finish.
+       *
+       * For LDR, it is an error for the address register to be
+       * included in the destination register set.
+       */
+
+      if (arg1) {
+         pandecode_log("%s (unk %02x), x%02x, (mask %x), [x%02x, %i]\n", name,
+                       arg1, addr, l >> 16, arg2, (int16_t)l);
+      } else {
+         pandecode_log("%s ", name);
+         pandecode_regmask(addr, l >> 16);
+         pandecode_log_cont(", [x%02x, %i]\n", arg2, (int16_t)l);
+      }
+      break;
+   }
+
+   case 22: {
+      /* The signed 32-bit source register is compared against zero
+       * for these comparisons. For example, .GT means that the
+       * branch is taken if the signed register value is greater
+       * than zero. */
+      const char *comparisons[] = {
+         ".le", ".gt", ".eq",           ".ne",
+         ".lt", ".ge", "" /* always */, ".(invalid: never)",
+      };
+
+      const char *m = comparisons[(l >> 28) & 7];
+
+      int16_t offset = l;
+
+      bool forward = (offset >= 0);
+      if (!forward)
+         offset = -1 - offset;
+
+      if (addr || arg1 || l & 0x8fff0000) {
+         pandecode_log("b%s (unk %02x), w%02x, (unk %02x), "
+                       "(unk 0x%x), %s %i\n",
+                       m, addr, arg2, arg1, l & 0x8fff0000,
+                       forward ? "skip" : "back", offset);
+      } else {
+         pandecode_log("b%s w%02x, %s %i\n", m, arg2, forward ? "skip" : "back",
+                       offset);
+      }
+
+      break;
+   }
+
+   case 23: {
+      if (value >> 3 || addr)
+         pandecode_log("slot (unk %02x), (unk %" PRIx64 "), "
+                       "%i\n",
+                       addr, value >> 3, l & 7);
+      else
+         pandecode_log("slot %i\n", l);
+      break;
+   }
+
+   case 32:
+   case 33: {
+      /* A tail call is similar to a normal call, but reuses the
+       * current stack entry so that execution returns directly to
+       * the parent, rather than pushing a new entry and returning
+       * to the instruction after the call. Using tail calls avoids
+       * the possibility of stack overflow.
+       */
+      const char *name = (op == 32) ? "call" : "tailcall";
+
+      unsigned length = buffer[arg1];
+      uint64_t target = (((uint64_t)buffer[arg2 + 1]) << 32) | buffer[arg2];
+
+      assert(!(length & 7));
+      unsigned instrs = length / 8;
+
+      if (addr || l)
+         pandecode_log(
+            "%s (unk %02x), w%02x (%i instructions), x%02x (0x%" PRIx64
+            "), (unk %x)\n",
+            name, addr, arg1, instrs, arg2, target, l);
+      else
+         pandecode_log("%s w%02x (%i instructions), x%02x (0x%" PRIx64 ")\n",
+                       name, arg1, instrs, arg2, target);
+
+      if (!target || !length)
+         break;
+
+      uint64_t *t = pandecode_fetch_gpu_mem(target, length);
+      pandecode_indent++;
+      pandecode_cs_buffer(t, length, buffer, buffer_unk, gpu_id, target);
+      pandecode_indent--;
+      break;
+   }
+
+   case 34: {
+      /* idvs implies tiler */
+      if (l & ~0xf)
+         pandecode_log("resources 0x%x\n", l);
+      else
+         pandecode_log("resources%s%s%s%s\n", (l & 1) ? " compute" : "",
+                       (l & 2) ? " fragment" : "", (l & 4) ? " tiler" : "",
+                       (l & 8) ? " idvs" : "");
+      break;
+   }
+
+   case 37:
+   case 38:
+   case 51:
+   case 52: {
+      /*
+       * 0b 00100101 / 00100110 -- opcode
+       *    ????0??? -- unk. usually 1, faults if "0" bit set
+       *    aaaaaaaa -- address register
+       *    vvvvvvvv -- 32-bit value register
+       *    00000000 -- seems to act as NOP if nonzero
+       *    mmmmmmmm -- some sort of mask, unknown purpose
+       *    ???????? -- seems to have no effect
+       *    ?????s0u -- 's' disables signal to CPU,
+       *                'u' has unknown purpose (disable GPU signal?)
+       *
+       * The difference between the two opcodes is unknown.
+       *
+       * That the 'mmmmmmmm' byte is somehow a scoreboard mask is
+       * a possibility.
+       */
+
+      const char *name = (op & 1) ? "evadd" : "evstr";
+      const char *type = (op > 50) ? "x" : "w";
+
+      if (addr != 1 || l & 0xff00fffa) {
+         pandecode_log("%s (unk %02x), %s%02x, [x%02x], "
+                       "unk 0x%x, flags 0x%x\n",
+                       name, addr, type, arg1, arg2, l >> 16, (uint16_t)l);
+      } else {
+         pandecode_log("%s %s%02x, [x%02x], unk 0x%x%s%s\n", name, type, arg1,
+                       arg2, l >> 16, l & 0x4 ? "" : ", irq",
+                       l & 0x1 ? ", unk0" : "");
+      }
+
+      break;
+   }
+
+   case 39:
+   case 53: {
+      const char *m = (const char *[]){
+         ".ls",
+         ".hi",
+      }[(l >> 28) & 1];
+      const char *e = (const char *[]){
+         ".inherit",
+         ".no_error",
+      }[l & 1];
+      const char *type = (op > 50) ? "x" : "w";
+
+      /* Wait until the value in the destination register is changed
+       * to pass the comparison. For example, with .LS the value
+       * in memory must be less than or same as the reference to
+       * continue execution. */
+      if (addr || l & ~((1 << 28) | (1 << 0)))
+         pandecode_log("evwait%s%s (unk %02x), %s%02x, "
+                       "[x%02x, unk %x]\n",
+                       m, e, addr, type, arg1, arg2, l);
+      else
+         pandecode_log("evwait%s%s %s%02x, [x%02x]\n", m, e, type, arg1, arg2);
+      break;
+   }
+
+   case 40: {
+      if (addr || l >> 16 || arg1 > 1) {
+         pandecode_log("str type %02x, (unk %02x), "
+                       "(unk %x), [x%02x, %i]\n",
+                       addr, arg1, l >> 16, arg2, (int16_t)l);
+      } else {
+         const char *type = (const char *[]){
+            "timestamp",
+            "cycles",
+         }[arg1];
+
+         pandecode_log("str %s, [x%02x, %i]\n", type, arg2, (int16_t)l);
+      }
+      break;
+   }
+
+   case 48: {
+      if (addr || arg1 || l)
+         pandecode_log("heapctx (unk %02x), "
+                       "x%02x, (unk %02x), (unk %x)\n",
+                       addr, arg2, arg1, l);
+      else
+         pandecode_log("heapctx x%02x\n", arg2);
+      break;
+   }
+
+   case 49: {
+      const char *m = (const char *[]){
+         "vt_start",
+         "vt_end",
+         "unk",
+         "frag_end",
+      }[arg1 & 3];
+
+      if (addr || arg2 || arg1 > 3 || l)
+         pandecode_log("heapinc (unk %02x), "
+                       "(unk %02x), %02x, (unk %x)\n",
+                       addr, arg2, arg1, l);
+      else
+         pandecode_log("heapinc %s\n", m);
+      break;
+   }
+
+   default:
+      /*
+       * UNK 00 30, #0x480000000000 -- takes an eight-byte aligned
+       * memory address.
+       */
+
+      pandecode_log("UNK %02x %02x, #0x%" PRIx64 "\n", addr, op, value);
+      break;
+   }
+}
+
+// TODO: reorder args
+static void
+pandecode_cs_buffer(uint64_t *commands, unsigned size, uint32_t *buffer,
+                    uint32_t *buffer_unk, unsigned gpu_id, mali_ptr va)
+{
+   uint64_t *end = (uint64_t *)((uint8_t *)commands + size);
+
+   for (uint64_t c = *commands; commands < end; c = *(++commands)) {
+      pandecode_cs_command(c, va, buffer, buffer_unk, gpu_id);
+      va += 8;
+   }
+}
+
+// TODO: Does it make sense to pass in the length?
+void
+GENX(pandecode_cs)(mali_ptr cs_gpu_va, unsigned size, unsigned gpu_id)
+{
+   pandecode_dump_file_open();
+
+   // TODO: Pass down the buffer during recursion
+   uint32_t buffer[256] = {0};
+   uint32_t buffer_unk[256] = {0};
+
+   uint64_t *commands = pandecode_fetch_gpu_mem(cs_gpu_va, 1);
+
+   pandecode_log("\n");
+
+   pandecode_cs_buffer(commands, size, buffer, buffer_unk, gpu_id, cs_gpu_va);
+
+   fflush(pandecode_dump_stream);
+   pandecode_map_read_write();
+}
+#endif
diff -urN mesa-23.0.0/src/panfrost/lib/genxml/decode_common.c mesa/src/panfrost/lib/genxml/decode_common.c
--- mesa-23.0.0/src/panfrost/lib/genxml/decode_common.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/genxml/decode_common.c	2023-03-06 19:19:31.731302387 +0100
@@ -202,7 +202,7 @@
 
 static int pandecode_dump_frame_count = 0;
 
-static bool force_stderr = false;
+bool force_stderr = false;
 
 void
 pandecode_dump_file_open(void)
@@ -232,7 +232,7 @@
    }
 }
 
-static void
+void
 pandecode_dump_file_close(void)
 {
    simple_mtx_assert_locked(&pandecode_lock);
@@ -291,8 +291,9 @@
       if (!it->addr || !it->length)
          continue;
 
-      fprintf(pandecode_dump_stream, "Buffer: %s gpu %" PRIx64 "\n\n", it->name,
-              it->gpu_va);
+      fprintf(pandecode_dump_stream,
+              "Buffer: %s gpu %" PRIx64 " length %zu\n\n", it->name, it->gpu_va,
+              it->length);
 
       pan_hexdump(pandecode_dump_stream, it->addr, it->length, false);
       fprintf(pandecode_dump_stream, "\n");
@@ -353,6 +354,27 @@
       break;
    default:
       unreachable("Unsupported architecture");
+   }
+
+   simple_mtx_unlock(&pandecode_lock);
+}
+
+void pandecode_cs_v10(mali_ptr cs_gpu_va, unsigned cs_size, unsigned gpu_id);
+
+void
+pandecode_cs(mali_ptr cs_gpu_va, unsigned cs_size, unsigned gpu_id)
+{
+   simple_mtx_lock(&pandecode_lock);
+
+   switch (pan_arch(gpu_id)) {
+   // Hack hack hackity hack: gpu_id == 1 means "don't decode" (only
+   // disassemble)
+   case 0:
+   case 10:
+      pandecode_cs_v10(cs_gpu_va, cs_size, gpu_id);
+      break;
+   default:
+      unreachable("Unsupported architecture");
    }
 
    simple_mtx_unlock(&pandecode_lock);
diff -urN mesa-23.0.0/src/panfrost/lib/genxml/decode.h mesa/src/panfrost/lib/genxml/decode.h
--- mesa-23.0.0/src/panfrost/lib/genxml/decode.h	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/genxml/decode.h	2023-03-06 19:19:31.728302367 +0100
@@ -51,8 +51,6 @@
 
 void pandecode_map_read_write(void);
 
-void pandecode_dump_mappings(void);
-
 static inline void *
 __pandecode_fetch_gpu_mem(uint64_t gpu_va, size_t size, int line,
                           const char *filename)
@@ -98,6 +96,8 @@
 void pandecode_abort_on_fault_v7(mali_ptr jc_gpu_va);
 void pandecode_abort_on_fault_v9(mali_ptr jc_gpu_va);
 
+void pandecode_cs_v10(mali_ptr cs_gpu_va, unsigned cs_size, unsigned gpu_id);
+
 static inline void
 pan_hexdump(FILE *fp, const uint8_t *hex, size_t cnt, bool with_strings)
 {
@@ -130,7 +130,7 @@
          fprintf(fp, " | ");
          for (unsigned j = i & ~0xF; j <= i; ++j) {
             uint8_t c = hex[j];
-            fputc((c < 32 || c > 128) ? '.' : c, fp);
+            fputc((c < 32 || c > 126) ? '.' : c, fp);
          }
       }
 
diff -urN mesa-23.0.0/src/panfrost/lib/genxml/gen_macros.h mesa/src/panfrost/lib/genxml/gen_macros.h
--- mesa-23.0.0/src/panfrost/lib/genxml/gen_macros.h	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/genxml/gen_macros.h	2023-03-06 19:19:31.679302043 +0100
@@ -93,6 +93,9 @@
 #elif (PAN_ARCH == 9)
 #define GENX(X) X##_v9
 #include "genxml/v9_pack.h"
+#elif (PAN_ARCH == 10)
+#define GENX(X) X##_v10
+#include "genxml/v10_pack.h"
 #else
 #error "Need to add suffixing macro for this architecture"
 #endif
diff -urN mesa-23.0.0/src/panfrost/lib/genxml/gen_pack.py mesa/src/panfrost/lib/genxml/gen_pack.py
--- mesa-23.0.0/src/panfrost/lib/genxml/gen_pack.py	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/genxml/gen_pack.py	2023-03-06 17:54:35.854495073 +0100
@@ -46,6 +46,18 @@
 
 #include "util/bitpack_helpers.h"
 
+/* Most functions assume the caller has done bounds checking */
+typedef struct pan_command_stream {
+   uint64_t *ptr;
+   uint64_t *begin;
+   uint64_t *end;
+   uint64_t gpu;
+} pan_command_stream;
+
+struct pan_command_stream_decoded {
+  uint32_t values[256];
+};
+
 #define __gen_unpack_float(x, y, z) uif(__gen_unpack_uint(x, y, z))
 
 static inline uint32_t
@@ -98,6 +110,20 @@
    return (2*odd + 1) << shift;
 }
 
+static inline void
+__gen_clear_value(uint8_t *restrict cl, uint32_t start, uint32_t end)
+{
+   for (uint32_t byte = start / 8; byte <= end / 8; byte++) {
+      uint8_t m = 0;
+      if (byte == start / 8)
+         m |= 0xff >> (8 - start % 8);
+      if (byte == end / 8)
+         m |= 0xff << (1 + end % 8);
+
+      cl[byte] &= m;
+   }
+}
+
 #define PREFIX1(A) MALI_ ## A
 #define PREFIX2(A, B) MALI_ ## A ## _ ## B
 #define PREFIX4(A, B, C, D) MALI_ ## A ## _ ## B ## _ ## C ## _ ## D
@@ -183,6 +209,96 @@
 
 """
 
+no_cs = "".join([f"""
+#define MALI_{y} MALI_{x}
+#define MALI_{y}_header MALI_{x}_header
+#define MALI_{y}_pack MALI_{x}_pack
+#define MALI_{y}_LENGTH MALI_{x}_LENGTH
+#define MALI_{y}_ALIGN MALI_{x}_ALIGN
+#define mali_{y.lower()}_packed mali_{x.lower()}_packed
+#define MALI_{y}_unpack MALI_{x}_unpack
+#define MALI_{y}_print MALI_{x}_print
+""" for x, y in (("DRAW", "DRAW_NO_CS"), )]) + """
+
+#define pan_pack_cs_v10(dst, _, T, name) pan_pack(dst, T, name)
+
+#define pan_section_pack_cs_v10(dst, _, A, S, name) pan_section_pack(dst, A, S, name)
+
+#define pan_unpack_cs_v10(dst, _, __, T, name) pan_unpack(dst, T, name)
+
+#define pan_section_unpack_cs_v10(src, _, __, A, S, name) pan_section_unpack(src, A, S, name)
+"""
+
+with_cs = """
+#define pan_pack_cs(dst, T, name)                       \\
+   for (struct PREFIX1(T) name = { PREFIX2(T, header) }, \\
+        *_loop_terminate = (void *) (dst);                  \\
+        __builtin_expect(_loop_terminate != NULL, 1);       \\
+        ({ PREFIX2(T, pack_cs)(dst, &name);  \\
+           _loop_terminate = NULL; }))
+
+#define pan_section_pack_cs(dst, A, S, name)                                                         \\
+   for (PREFIX4(A, SECTION, S, TYPE) name = { PREFIX4(A, SECTION, S, header) }, \\
+        *_loop_terminate = (void *) (dst);                                                        \\
+        __builtin_expect(_loop_terminate != NULL, 1);                                             \\
+        ({ PREFIX4(A, SECTION, S, pack_cs) (dst, &name);              \\
+           _loop_terminate = NULL; }))
+
+#define pan_section_pack_cs_v10(_, dst, A, S, name) pan_section_pack_cs(dst, A, S, name)
+
+// TODO: assert that the first argument is NULL
+#define pan_pack_cs_v10(_, dst, T, name) pan_pack_cs(dst, T, name)
+
+#define pan_pack_ins(dst, T, name)                       \\
+   for (struct PREFIX1(T) name = { PREFIX2(T, header) }, \\
+        *_loop_terminate = (void *) (dst);                  \\
+        __builtin_expect(_loop_terminate != NULL, 1);       \\
+        ({ PREFIX2(T, pack_ins)(dst, &name);  \\
+           _loop_terminate = NULL; }))
+
+#define pan_unpack_cs(buf, buf_unk, T, name) \\
+        struct PREFIX1(T) name; \\
+        PREFIX2(T, unpack)(buf, buf_unk, &name)
+
+#define pan_unpack_cs_v10(_, buf, buf_unk, T, name) pan_unpack_cs(buf, buf_unk, T, name)
+
+#define pan_section_unpack_cs_v10(_, buf, buf_unk, A, S, name) \\
+        PREFIX4(A, SECTION, S, TYPE) name;                             \\
+        PREFIX4(A, SECTION, S, unpack)(buf, buf_unk, &name)
+
+static inline void
+pan_emit_cs_ins(pan_command_stream *s, uint8_t op, uint64_t instr)
+{
+   assert(instr < (1ULL << 56));
+   instr |= ((uint64_t)op << 56);
+   *((s->ptr)++) = instr;
+}
+
+static inline void
+pan_emit_cs_32(pan_command_stream *s, uint8_t reg, uint32_t value)
+{
+   pan_emit_cs_ins(s, 2, ((uint64_t) reg << 48) | value);
+}
+
+static inline void
+pan_emit_cs_48(pan_command_stream *s, uint8_t reg, uint64_t value)
+{
+   assert(value < (1ULL << 48));
+   pan_emit_cs_ins(s, 1, ((uint64_t) reg << 48) | value);
+}
+
+static inline void
+pan_emit_cs_64(pan_command_stream *s, uint8_t reg, uint64_t value)
+{
+   if (value < (1ULL << 48)) {
+      pan_emit_cs_48(s, reg, value);
+   } else {
+      pan_emit_cs_32(s, reg, value);
+      pan_emit_cs_32(s, reg + 1, value >> 32);
+   }
+}
+"""
+
 def to_alphanum(name):
     substitutions = {
         ' ': '_',
@@ -297,7 +413,7 @@
 
         if ":" in str(attrs["start"]):
             (word, bit) = attrs["start"].split(":")
-            self.start = (int(word) * 32) + int(bit)
+            self.start = (int(word, 0) * 32) + int(bit)
         else:
             self.start = int(attrs["start"])
 
@@ -331,7 +447,8 @@
             type = 'uint64_t'
         elif self.type == 'int':
             type = 'int32_t'
-        elif self.type in ['uint', 'hex', 'uint/float', 'padded', 'Pixel Format']:
+            # TODO: Convert to tuple
+        elif self.type in ['uint', 'hex', 'register', 'uint/float', 'padded', 'Pixel Format']:
             type = 'uint32_t'
         elif self.type in self.parser.structs:
             type = 'struct ' + self.parser.gen_prefix(safe_name(self.type.upper()))
@@ -385,8 +502,8 @@
                 field.emit_template_struct(dim)
 
     class Word:
-        def __init__(self):
-            self.size = 32
+        def __init__(self, size=32):
+            self.size = size
             self.contributors = []
 
     class FieldRef:
@@ -410,7 +527,7 @@
             end = offset + field.end
             all_fields.append(self.FieldRef(field, field_path, start, end))
 
-    def collect_words(self, fields, offset, path, words):
+    def collect_words(self, fields, offset, path, words, ins=False):
         for field in fields:
             field_path = '{}{}'.format(path, field.name)
             start = offset + field.start
@@ -424,16 +541,27 @@
             contributor = self.FieldRef(field, field_path, start, end)
             first_word = contributor.start // 32
             last_word = contributor.end // 32
+            if ins:
+                assert(last_word < 2)
+                first_word = last_word = 0
+
             for b in range(first_word, last_word + 1):
                 if not b in words:
-                    words[b] = self.Word()
+                    words[b] = self.Word(size=64 if ins else 32)
+
                 words[b].contributors.append(contributor)
 
-    def emit_pack_function(self):
-        self.get_length()
+        return
+
+    def emit_pack_function(self, csf=False, ins=False):
+        if csf:
+            self.length = 256 * 4
+        else:
+            self.get_length()
+            assert(not ins)
 
         words = {}
-        self.collect_words(self.fields, 0, '', words)
+        self.collect_words(self.fields, 0, '', words, ins=ins)
 
         # Validate the modifier is lossless
         for field in self.fields:
@@ -449,25 +577,52 @@
             elif field.modifier[0] == "log2":
                 print("   assert(util_is_power_of_two_nonzero(values->{}));".format(field.name))
 
-        for index in range(self.length // 4):
+        if ins:
+            index_list = (0, )
+        elif csf:
+            index_list = sorted(words)
+        else:
+            index_list = range(self.length // 4)
+
+        for index in index_list:
             # Handle MBZ words
             if not index in words:
-                print("   cl[%2d] = 0;" % index)
+                if ins:
+                    print("   pan_emit_cs_ins(s, 0x%02x, 0);" % self.op)
+                elif not csf:
+                    print("   cl[%2d] = 0;" % index)
                 continue
 
             word = words[index]
 
             word_start = index * 32
 
+            size = 32
+            # Can we move all fields from the next index here?
+            if csf and index % 2 == 0 and index + 1 in words:
+                word_next = words[index + 1]
+                end = max(c.end for c in word_next.contributors)
+                if end - word_start < 48:
+                    size = 48
+                    word.contributors += [x for x in word_next.contributors if not x in word.contributors]
+                    del words[index + 1]
+
             v = None
-            prefix = "   cl[%2d] =" % index
+            if ins:
+                prefix = "   pan_emit_cs_ins(s, 0x%02x," % self.op
+            elif size == 48:
+                prefix = "   pan_emit_cs_48(s, 0x%02x," % index
+            elif csf:
+                prefix = "   pan_emit_cs_32(s, 0x%02x," % index
+            else:
+                prefix = "   cl[%2d] = (" % index
 
             for contributor in word.contributors:
                 field = contributor.field
                 name = field.name
                 start = contributor.start
                 end = contributor.end
-                contrib_word_start = (start // 32) * 32
+                contrib_word_start = (start // word.size) * word.size
                 start -= contrib_word_start
                 end -= contrib_word_start
 
@@ -482,7 +637,7 @@
                     elif field.modifier[0] == "log2":
                         value = "util_logbase2({})".format(value)
 
-                if field.type in ["uint", "hex", "uint/float", "address", "Pixel Format"]:
+                if field.type in ["uint", "hex", "uint/float", "address", "register", "Pixel Format"]:
                     s = "util_bitpack_uint(%s, %d, %d)" % \
                         (value, start, end)
                 elif field.type == "padded":
@@ -505,11 +660,13 @@
 
                 if not s == None:
                     shift = word_start - contrib_word_start
-                    if shift:
+                    if shift > 0:
                         s = "%s >> %d" % (s, shift)
+                    elif shift < 0:
+                        s = "%s << %d" % (s, -shift)
 
                     if contributor == word.contributors[-1]:
-                        print("%s %s;" % (prefix, s))
+                        print("%s %s);" % (prefix, s))
                     else:
                         print("%s %s |" % (prefix, s))
                     prefix = "           "
@@ -528,22 +685,23 @@
         count = (end - start + 1)
         return (((1 << count) - 1) << start)
 
-    def emit_unpack_function(self):
+    def emit_unpack_function(self, csf=False):
         # First, verify there is no garbage in unused bits
         words = {}
         self.collect_words(self.fields, 0, '', words)
 
-        for index in range(self.length // 4):
-            base = index * 32
-            word = words.get(index, self.Word())
-            masks = [self.mask_for_word(index, c.start, c.end) for c in word.contributors]
-            mask = reduce(lambda x,y: x | y, masks, 0)
-
-            ALL_ONES = 0xffffffff
-
-            if mask != ALL_ONES:
-                TMPL = '   if (((const uint32_t *) cl)[{}] & {}) fprintf(stderr, "XXX: Invalid field of {} unpacked at word {}\\n");'
-                print(TMPL.format(index, hex(mask ^ ALL_ONES), self.label, index))
+        if not csf:
+            for index in range(self.length // 4):
+                base = index * 32
+                word = words.get(index, self.Word())
+                masks = [self.mask_for_word(index, c.start, c.end) for c in word.contributors]
+                mask = reduce(lambda x,y: x | y, masks, 0)
+
+                ALL_ONES = 0xffffffff
+
+                if mask != ALL_ONES:
+                    TMPL = '   if (((const uint32_t *) cl)[{}] & {}) fprintf(stderr, "XXX: Invalid field of {} unpacked at word {}\\n");'
+                    print(TMPL.format(index, hex(mask ^ ALL_ONES), self.label, index))
 
         fieldrefs = []
         self.collect_fields(self.fields, 0, '', fieldrefs)
@@ -556,7 +714,7 @@
             args.append(str(fieldref.start))
             args.append(str(fieldref.end))
 
-            if field.type in set(["uint", "hex", "uint/float", "address", "Pixel Format"]):
+            if field.type in set(["uint", "hex", "uint/float", "address", "register", "Pixel Format"]):
                 convert = "__gen_unpack_uint"
             elif field.type in self.parser.enums:
                 convert = "(enum %s)__gen_unpack_uint" % enum_name(field.type)
@@ -588,6 +746,9 @@
                 mask = hex(field.modifier[1] - 1)
                 print('   assert(!(values->{} & {}));'.format(fieldref.path, mask))
 
+            if csf:
+                print('   __gen_clear_value({});'.format(', '.join(['cl_unk'] + args[1:])))
+
     def emit_print_function(self):
         for field in self.fields:
             convert = None
@@ -610,7 +771,7 @@
                 print('   fprintf(fp, "%*s{}: %f\\n", indent, "", {});'.format(name, val))
             elif field.type in ["uint", "hex"] and (field.end - field.start) >= 32:
                 print('   fprintf(fp, "%*s{}: 0x%" PRIx64 "\\n", indent, "", {});'.format(name, val))
-            elif field.type == "hex":
+            elif field.type in ("hex", "register"):
                 print('   fprintf(fp, "%*s{}: 0x%x\\n", indent, "", {});'.format(name, val))
             elif field.type == "uint/float":
                 print('   fprintf(fp, "%*s{}: 0x%X (%f)\\n", indent, "", {}, uif({}));'.format(name, val, val))
@@ -649,9 +810,13 @@
                     print(v6_format_printer)
                 else:
                     print(v7_format_printer)
+                if arch < 10:
+                    print(no_cs)
+                else:
+                    print(with_cs)
         elif name == "struct":
             name = attrs["name"]
-            self.no_direct_packing = attrs.get("no-direct-packing", False)
+            self.layout = attrs.get("layout", "struct")
             object_name = self.gen_prefix(safe_name(name.upper()))
             self.struct = object_name
 
@@ -659,10 +824,16 @@
             if "size" in attrs:
                 self.group.length = int(attrs["size"]) * 4
             self.group.align = int(attrs["align"]) if "align" in attrs else None
+            self.group.op = int(attrs["op"]) if "op" in attrs else None
             self.structs[attrs["name"]] = self.group
+            self.unpacked_alias = self.gen_prefix(safe_name(attrs["unpacked"].upper())) if "unpacked" in attrs else None
         elif name == "field":
-            self.group.fields.append(Field(self, attrs))
             self.values = []
+            self.skip_field = self.layout == "cs" and not attrs["start"].startswith("0x")
+            if self.skip_field:
+                #print(f"#warning Skipping non-CS field {attrs['name']}")
+                return
+            self.group.fields.append(Field(self, attrs))
         elif name == "enum":
             self.values = []
             self.enum = safe_name(attrs["name"])
@@ -675,6 +846,8 @@
             self.values.append(Value(attrs))
         elif name == "aggregate":
             aggregate_name = self.gen_prefix(safe_name(attrs["name"].upper()))
+            # TODO: Make .layout less "global"?
+            self.layout = attrs.get("layout", "struct")
             self.aggregate = Aggregate(self, aggregate_name, attrs)
             self.aggregates[attrs['name']] = self.aggregate
         elif name == "section":
@@ -687,7 +860,8 @@
             self.struct = None
             self.group = None
         elif name  == "field":
-            self.group.fields[-1].values = self.values
+            if not self.skip_field:
+                self.group.fields[-1].values = self.values
         elif name  == "enum":
             self.emit_enum()
             self.enum = None
@@ -717,22 +891,33 @@
         print('')
 
     def emit_template_struct(self, name, group):
-        print("struct %s {" % name)
-        group.emit_template_struct("")
-        print("};\n")
+        if self.unpacked_alias:
+            # TODO: Check the fields match
+            print("#define %s %s" % (name, self.unpacked_alias))
+        else:
+            print("struct %s {" % name)
+            group.emit_template_struct("")
+            print("};\n")
 
     def emit_aggregate(self):
         aggregate = self.aggregate
-        print("struct %s_packed {" % aggregate.name.lower())
-        print("   uint32_t opaque[{}];".format(aggregate.get_size() // 4))
-        print("};\n")
-        print('#define {}_LENGTH {}'.format(aggregate.name.upper(), aggregate.size))
+
+        if self.layout == "struct":
+            print("struct %s_packed {" % aggregate.name.lower())
+            print("   uint32_t opaque[{}];".format(aggregate.get_size() // 4))
+            print("};\n")
+            print('#define {}_LENGTH {}'.format(aggregate.name.upper(), aggregate.size))
+        else:
+            assert(self.layout == "cs")
+
         if aggregate.align != None:
             print('#define {}_ALIGN {}'.format(aggregate.name.upper(), aggregate.align))
         for section in aggregate.sections:
             print('#define {}_SECTION_{}_TYPE struct {}'.format(aggregate.name.upper(), section.name.upper(), section.type_name))
             print('#define {}_SECTION_{}_header {}_header'.format(aggregate.name.upper(), section.name.upper(), section.type_name))
             print('#define {}_SECTION_{}_pack {}_pack'.format(aggregate.name.upper(), section.name.upper(), section.type_name))
+            # TODO: Only when req'd
+            print('#define {}_SECTION_{}_pack_cs {}_pack_cs'.format(aggregate.name.upper(), section.name.upper(), section.type_name))
             print('#define {}_SECTION_{}_unpack {}_unpack'.format(aggregate.name.upper(), section.name.upper(), section.type_name))
             print('#define {}_SECTION_{}_print {}_print'.format(aggregate.name.upper(), section.name.upper(), section.type_name))
             print('#define {}_SECTION_{}_OFFSET {}'.format(aggregate.name.upper(), section.name.upper(), section.offset))
@@ -747,12 +932,32 @@
         print("}\n\n")
 
         # Should be a whole number of words
-        assert((self.group.length % 4) == 0)
+        assert((group.length % 4) == 0)
+
+        print('#define {} {}'.format (name + "_LENGTH", group.length))
+        if group.align != None:
+            print('#define {} {}'.format (name + "_ALIGN", group.align))
+        print('struct {}_packed {{ uint32_t opaque[{}]; }};'.format(name.lower(), group.length // 4))
+
+    def emit_cs_pack_function(self, name, group):
+        print("static inline void\n%s_pack_cs(pan_command_stream * restrict s,\n%sconst struct %s * restrict values)\n{\n" %
+              (name, ' ' * (len(name) + 6), name))
+
+        group.emit_pack_function(csf=True)
 
-        print('#define {} {}'.format (name + "_LENGTH", self.group.length))
-        if self.group.align != None:
-            print('#define {} {}'.format (name + "_ALIGN", self.group.align))
-        print('struct {}_packed {{ uint32_t opaque[{}]; }};'.format(name.lower(), self.group.length // 4))
+        print("}\n\n")
+
+        assert(group.length == 256 * 4)
+
+    def emit_ins_pack_function(self, name, group):
+        print("static inline void\n%s_pack_ins(pan_command_stream * restrict s,\n%sconst struct %s * restrict values)\n{" %
+              (name, ' ' * (len(name) + 6), name))
+
+        group.emit_pack_function(csf=True, ins=True)
+
+        print("}\n\n")
+
+        assert(group.length == 256 * 4)
 
     def emit_unpack_function(self, name, group):
         print("static inline void")
@@ -763,6 +968,18 @@
 
         print("}\n")
 
+    def emit_cs_unpack_function(self, name, group):
+        print("static inline void")
+        print("%s_unpack(const uint32_t * restrict buffer, uint32_t * restrict buffer_unk,\n"
+              "%sstruct %s * restrict values)\n{"
+              "   const uint8_t *cl = (uint8_t *)buffer;\n"
+              "   uint8_t *cl_unk = (uint8_t *)buffer_unk;\n" %
+              (name.upper(), ' ' * (len(name) + 8), name))
+
+        group.emit_unpack_function(csf=True)
+
+        print("}\n")
+
     def emit_print_function(self, name, group):
         print("static inline void")
         print("{}_print(FILE *fp, const struct {} * values, unsigned indent)\n{{".format(name.upper(), name))
@@ -776,14 +993,20 @@
 
         self.emit_template_struct(self.struct, self.group)
         self.emit_header(name)
-        if self.no_direct_packing == False:
+        if self.layout == "struct":
             self.emit_pack_function(self.struct, self.group)
             self.emit_unpack_function(self.struct, self.group)
+        elif self.layout == "cs":
+            self.emit_cs_pack_function(self.struct, self.group)
+            self.emit_cs_unpack_function(self.struct, self.group)
+        elif self.layout == "ins":
+            # TODO: I don't think that the current unpack emit functions would
+            # work
+            self.emit_ins_pack_function(self.struct, self.group)
+        else:
+            assert(self.layout == "none")
         self.emit_print_function(self.struct, self.group)
 
-    def enum_prefix(self, name):
-        return 
-
     def emit_enum(self):
         e_name = enum_name(self.enum)
         prefix = e_name if self.enum != 'Format' else global_prefix
diff -urN mesa-23.0.0/src/panfrost/lib/genxml/meson.build mesa/src/panfrost/lib/genxml/meson.build
--- mesa-23.0.0/src/panfrost/lib/genxml/meson.build	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/genxml/meson.build	2023-03-06 17:54:35.854495073 +0100
@@ -20,7 +20,7 @@
 # SOFTWARE.
 
 pan_packers = []
-foreach packer : ['common', 'v4', 'v5', 'v6', 'v7', 'v9']
+foreach packer : ['common', 'v4', 'v5', 'v6', 'v7', 'v9', 'v10']
   pan_packers += custom_target(
     packer + '_pack.h',
     input : ['gen_pack.py', packer + '.xml'],
@@ -37,7 +37,7 @@
 
 libpanfrost_decode_per_arch = []
 
-foreach ver : ['4', '5', '6', '7', '9']
+foreach ver : ['4', '5', '6', '7', '9', '10']
   libpanfrost_decode_per_arch += static_library(
     'pandecode-arch-v' + ver,
     ['decode.c', pan_packers],
diff -urN mesa-23.0.0/src/panfrost/lib/genxml/v10.xml mesa/src/panfrost/lib/genxml/v10.xml
--- mesa-23.0.0/src/panfrost/lib/genxml/v10.xml	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/genxml/v10.xml	2023-03-06 17:54:35.855495079 +0100
@@ -28,6 +28,13 @@
     <value name="Bounds" value="2"/>
   </enum>
 
+  <!-- Only used for pandecode shader type -->
+  <enum name="Job Type">
+    <value name="Compute" value="4"/>
+    <value name="Tiler" value="7"/>
+    <value name="Fragment" value="9"/>
+  </enum>
+
   <enum name="Shader stage">
     <value name="Compute" value="1"/>
     <value name="Fragment" value="2"/>
@@ -64,6 +71,13 @@
     <value name="Quads" value="14"/>
   </enum>
 
+  <enum name="Exception Access">
+    <value name="None" value="0"/>
+    <value name="Execute" value="2"/>
+    <value name="Read" value="1"/>
+    <value name="Write" value="3"/>
+  </enum>
+
   <enum name="Func">
     <value name="Never" value="0"/>
     <value name="Less" value="1"/>
@@ -323,6 +337,7 @@
     <value name="No Conversion" value="0"/>
     <value name="BT 601" value="3"/>
     <value name="BT 709" value="4"/>
+    <value name="Unk" value="5"/>
     <value name="BT 2020" value="6"/>
   </enum>
 
@@ -360,11 +375,13 @@
   </enum>
 
   <enum name="Block Format">
-    <value name="No Write" value="0"/>
-    <!--- 16x16 block u-interleaved -->
+    <value name="Give me a GPU hang" value="0"/>
+    <!-- 16x16 block u-interleaved -->
     <value name="Tiled U-Interleaved" value="1"/>
     <value name="Linear" value="2"/>
+    <value name="Also Tiled" value="8"/>
     <value name="AFBC" value="12"/>
+    <!-- This seems to actually use a newer AFBC version -->
     <value name="AFBC Tiled" value="13"/>
   </enum>
 
@@ -449,341 +466,6 @@
     <value name="Instance" value="1"/>
   </enum>
 
-  <enum name="CEU Condition">
-    <value name="Lequal" value="0"/>
-    <value name="Greater" value="1"/>
-    <value name="Equal" value="2"/>
-    <value name="Nequal" value="3"/>
-    <value name="Less" value="4"/>
-    <value name="Gequal" value="5"/>
-    <value name="Always" value="6"/>
-  </enum>
-
-  <enum name="CEU State">
-    <value name="Timestamp" value="0"/>
-    <value name="Cycle count" value="1"/>
-    <value name="Disjoint count" value="2"/>
-    <value name="Error status" value="3"/>
-  </enum>
-
-  <enum name="CEU Heap Operation">
-    <value name="Vertex/Tiler Started" value="0"/>
-    <value name="Vertex/Tiler Completed" value="1"/>
-    <value name="Fragment Completed" value="3"/>
-  </enum>
-
-  <enum name="CEU Flush Mode">
-    <value name="None" value="0"/>
-    <value name="Clean" value="1"/>
-    <value name="Clean and invalidate" value="3"/>
-  </enum>
-
-  <enum name="CEU Opcode">
-    <value name="NOP" value="0"/>
-    <value name="MOVE" value="1"/>
-    <value name="MOVE32" value="2"/>
-    <value name="WAIT" value="3"/>
-    <value name="RUN_COMPUTE" value="4"/>
-    <value name="RUN_TILING" value="5"/>
-    <value name="RUN_IDVS" value="6"/>
-    <value name="RUN_FRAGMENT" value="7"/>
-    <value name="RUN_FULLSCREEN" value="8"/>
-    <value name="FINISH_TILING" value="9"/>
-    <value name="FINISH_FRAGMENT" value="11"/>
-    <value name="ADD_IMMEDIATE32" value="16"/>
-    <value name="ADD_IMMEDIATE64" value="17"/>
-    <value name="UMIN32" value="18"/>
-    <value name="LOAD_MULTIPLE" value="20"/>
-    <value name="STORE_MULTIPLE" value="21"/>
-    <value name="BRANCH" value="22"/>
-    <value name="SET_SB_ENTRY" value="23"/>
-    <value name="PROGRESS_WAIT" value="24"/>
-    <!-- SET_EXCEPTION_HANDLER -->
-    <value name="CALL" value="32"/>
-    <value name="JUMP" value="33"/>
-    <value name="REQ_RESOURCE" value="34"/>
-    <value name="FLUSH_CACHE2" value="36"/>
-    <value name="SYNC_ADD32" value="37"/>
-    <value name="SYNC_SET32" value="38"/>
-    <value name="SYNC_WAIT32" value="39"/>
-    <value name="STORE_STATE" value="40"/>
-    <value name="PROT_REGION" value="41"/>
-    <value name="PROGRESS_STORE" value="42"/>
-    <value name="PROGEESS_LOAD" value="43"/>
-    <value name="RUN_COMPUTE_INDIRECT" value="44"/>
-    <value name="ERROR_BARRIER" value="47"/>
-    <value name="HEAP_SET" value="48"/>
-    <value name="HEAP_OPERATION" value="49"/>
-    <value name="TRACE_POINT" value="50"/>
-    <value name="SYNC_ADD64" value="51"/>
-    <value name="SYNC_SET64" value="52"/>
-    <value name="SYNC_WAIT64" value="53"/>
-  </enum>
-
-  <struct name="CEU Base" size="2">
-    <field name="Data" size="56" start="0" type="hex"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode"/>
-  </struct>
-
-  <struct name="CEU NOP" size="2">
-    <field name="Ignored" size="56" start="0" type="hex"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="NOP"/>
-  </struct>
-
-  <struct name="CEU ERROR_BARRIER" size="2">
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="ERROR_BARRIER"/>
-  </struct>
-
-  <struct name="CEU PROGRESS_WAIT" size="2">
-    <field name="Trace Buffer ID" size="5" start="0" type="uint"/>
-    <field name="Source" size="8" start="40" type="uint"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="PROGRESS_WAIT"/>
-  </struct>
-
-  <struct name="CEU PROGRESS_STORE" size="2">
-    <field name="Source" size="8" start="40" type="uint"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="PROGRESS_STORE"/>
-  </struct>
-
-  <struct name="CEU PROGRESS_LOAD" size="2">
-    <field name="Destination" size="8" start="40" type="uint"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="PROGRESS_LOAD"/>
-  </struct>
-
-  <struct name="CEU MOVE" size="2">
-    <field name="Immediate" size="48" start="0" type="hex"/>
-    <field name="Destination" size="8" start="48" type="uint"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="MOVE"/>
-  </struct>
-
-  <struct name="CEU MOVE32" size="2">
-    <field name="Immediate" size="32" start="0" type="hex"/>
-    <field name="Destination" size="8" start="48" type="uint"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="MOVE32"/>
-  </struct>
-
-  <struct name="CEU WAIT" size="2">
-    <field name="Slots" size="8" start="16" type="hex"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="WAIT"/>
-  </struct>
-
-  <struct name="CEU RUN_COMPUTE" size="2">
-    <field name="Task increment" size="14" start="0" type="uint"/>
-    <field name="Task axis" size="2" start="14" type="Task Axis"/>
-    <field name="SRT select" size="2" start="40" type="uint"/>
-    <field name="SPD select" size="2" start="42" type="uint"/>
-    <field name="TSD select" size="2" start="44" type="uint"/>
-    <field name="FAU select" size="2" start="46" type="uint"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="RUN_COMPUTE"/>
-  </struct>
-
-  <struct name="CEU RUN_IDVS" size="2">
-    <field name="Flags override" size="32" start="0" type="hex"/>
-    <field name="Malloc enable" size="1" start="33" type="bool"/>
-    <field name="Draw ID register enable" size="1" start="34" type="bool"/>
-    <field name="Varying SRT select" size="1" start="35" type="bool"/>
-    <field name="Varying FAU select" size="1" start="36" type="bool"/>
-    <field name="Varying TSD select" size="1" start="37" type="bool"/>
-    <field name="Fragment SRT select" size="1" start="38" type="bool"/>
-    <field name="Fragment TSD select" size="1" start="39" type="bool"/>
-    <field name="Draw ID" size="8" start="40" type="uint"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="RUN_IDVS"/>
-  </struct>
-
-  <struct name="CEU RUN_FRAGMENT" size="2">
-    <field name="Enable TEM" size="1" start="0" type="bool"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="RUN_FRAGMENT"/>
-  </struct>
-
-  <struct name="CEU FINISH_TILING" size="2">
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="FINISH_TILING"/>
-  </struct>
-
-  <struct name="CEU FINISH_FRAGMENT" size="2">
-    <field name="Increment Fragment Completed" size="1" start="0" type="bool"/>
-    <field name="Wait mask" size="16" start="16" type="hex"/>
-    <field name="First Heap Chunk" size="8" start="32" type="hex"/>
-    <field name="Last Heap Chunk" size="8" start="40" type="hex"/>
-    <field name="Scoreboard entry" size="4" start="48" type="hex"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="FINISH_FRAGMENT"/>
-  </struct>
-
-  <struct name="CEU HEAP_OPERATION" size="2">
-    <field name="Wait mask" size="16" start="16" type="hex"/>
-    <field name="Operation" size="2" start="32" type="CEU Heap Operation"/>
-    <field name="Scoreboard entry" size="4" start="48" type="uint"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="HEAP_OPERATION"/>
-  </struct>
-
-  <struct name="CEU TRACE_POINT" size="2">
-    <field name="Wait mask" size="16" start="16" type="hex"/>
-    <field name="Base Register" size="8" start="32" type="uint"/>
-    <field name="Register Count" size="8" start="40" type="uint"/>
-    <field name="Scoreboard slot" size="4" start="48" type="hex"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="TRACE_POINT"/>
-  </struct>
-
-  <struct name="CEU CALL" size="2">
-    <field name="Length" size="8" start="32" type="uint"/>
-    <field name="Address" size="8" start="40" type="uint"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="CALL"/>
-  </struct>
-
-  <struct name="CEU JUMP" size="2">
-    <field name="Length" size="8" start="32" type="uint"/>
-    <field name="Address" size="8" start="40" type="uint"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="JUMP"/>
-  </struct>
-
-  <struct name="CEU ADD_IMMEDIATE64" size="2">
-    <field name="Immediate" size="32" start="0" type="int"/>
-    <field name="Source" size="8" start="40" type="uint"/>
-    <field name="Destination" size="8" start="48" type="uint"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="ADD_IMMEDIATE64"/>
-  </struct>
-
-  <struct name="CEU ADD_IMMEDIATE32" size="2">
-    <field name="Immediate" size="32" start="0" type="int"/>
-    <field name="Source" size="8" start="40" type="uint"/>
-    <field name="Destination" size="8" start="48" type="uint"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="ADD_IMMEDIATE32"/>
-  </struct>
-
-  <struct name="CEU UMIN32" size="2">
-    <field name="Source 1" size="8" start="32" type="uint"/>
-    <field name="Source 2" size="8" start="40" type="uint"/>
-    <field name="Destination" size="8" start="48" type="uint"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="UMIN32"/>
-  </struct>
-
-  <struct name="CEU LOAD_MULTIPLE" size="2">
-    <field name="Offset" size="16" start="0" type="int"/>
-    <field name="Mask" size="16" start="16" type="hex"/>
-    <field name="Address" size="8" start="40" type="uint"/>
-    <field name="Base" size="8" start="48" type="uint"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="LOAD_MULTIPLE"/>
-  </struct>
-
-  <struct name="CEU STORE_MULTIPLE" size="2">
-    <field name="Offset" size="16" start="0" type="int"/>
-    <field name="Mask" size="16" start="16" type="hex"/>
-    <field name="Address" size="8" start="40" type="uint"/>
-    <field name="Base" size="8" start="48" type="uint"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="STORE_MULTIPLE"/>
-  </struct>
-
-  <struct name="CEU BRANCH" size="2">
-    <field name="Offset" size="16" start="0" type="int"/>
-    <field name="Condition" size="3" start="28" type="CEU Condition"/>
-    <field name="Value" size="8" start="40" type="uint"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="BRANCH"/>
-  </struct>
-
-  <struct name="CEU PROT_REGION" size="2">
-    <field name="Size (instructions)" size="16" start="0" type="uint"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="PROT_REGION"/>
-  </struct>
-
-  <struct name="CEU SET_SB_ENTRY" size="2">
-    <field name="Endpoint entry" size="4" start="0" type="uint"/>
-    <field name="Other entry" size="4" start="4" type="uint"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="SET_SB_ENTRY"/>
-  </struct>
-
-  <struct name="CEU SYNC_ADD32" size="2">
-    <!-- Iter status is written at [Address + 4], 0 == NO_FAULT, 1 == FAULT -->
-    <field name="Check iter faults" size="1" start="0" type="bool"/>
-    <field name="Sync Update" size="1" start="2" type="bool"/>
-    <field name="Wait mask" size="16" start="16" type="hex"/>
-    <field name="Data" size="8" start="32" type="uint"/>
-    <field name="Address" size="8" start="40" type="uint"/>
-    <field name="Scoreboard slot" size="4" start="48" type="hex"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="SYNC_ADD32"/>
-  </struct>
-
-  <struct name="CEU SYNC_SET32" size="2">
-    <!-- Iter status is written at [Address + 4], 0 == NO_FAULT, 1 == FAULT -->
-    <field name="Check iter faults" size="1" start="0" type="bool"/>
-    <field name="Sync Update" size="1" start="2" type="bool"/>
-    <field name="Wait mask" size="16" start="16" type="hex"/>
-    <field name="Data" size="8" start="32" type="uint"/>
-    <field name="Address" size="8" start="40" type="uint"/>
-    <field name="Scoreboard slot" size="4" start="48" type="hex"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="SYNC_SET32"/>
-  </struct>
-
-  <struct name="CEU STORE_STATE" size="2">
-    <!-- Iter status is written at [Address + 4], 0 == NO_FAULT, 1 == FAULT -->
-    <field name="Offset" size="16" start="0" type="int"/>
-    <field name="Wait mask" size="16" start="16" type="hex"/>
-    <field name="State" size="2" start="32" type="CEU State"/>
-    <field name="Address" size="8" start="40" type="uint"/>
-    <field name="Scoreboard slot" size="4" start="48" type="hex"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="STORE_STATE"/>
-  </struct>
-
-  <struct name="CEU SYNC_ADD64" size="2">
-    <!-- Iter status is written at [Address + 8], 0 == NO_FAULT, 1 == FAULT -->
-    <field name="Check iter faults" size="1" start="0" type="bool"/>
-    <!-- Used for inter-queue synchronization (unblocking a queue waiting on a mem-based fence) -->
-    <field name="Sync Update" size="1" start="2" type="bool"/>
-    <field name="Wait mask" size="16" start="16" type="hex"/>
-    <field name="Data" size="8" start="32" type="uint"/>
-    <field name="Address" size="8" start="40" type="uint"/>
-    <field name="Scoreboard slot" size="4" start="48" type="hex"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="SYNC_ADD64"/>
-  </struct>
-
-  <struct name="CEU SYNC_SET64" size="2">
-    <!-- Iter status is written at [Address + 8], 0 == NO_FAULT, 1 == FAULT -->
-    <field name="Check iter faults" size="1" start="0" type="bool"/>
-    <field name="Sync Update" size="1" start="2" type="bool"/>
-    <field name="Wait mask" size="16" start="16" type="hex"/>
-    <field name="Data" size="8" start="32" type="uint"/>
-    <field name="Address" size="8" start="40" type="uint"/>
-    <field name="Scoreboard slot" size="4" start="48" type="hex"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="SYNC_SET64"/>
-  </struct>
-
-  <struct name="CEU FLUSH_CACHE2" size="2">
-    <field name="L2 flush mode" size="4" start="0" type="CEU Flush Mode"/>
-    <field name="LSC flush mode" size="4" start="4" type="CEU Flush Mode"/>
-    <field name="Other invalidate" size="1" start="9" type="bool"/>
-    <field name="Scoreboard mask" size="16" start="16" type="hex"/>
-    <field name="Latest Flush ID" size="8" start="40" type="uint"/>
-    <field name="Scoreboard entry" size="4" start="48" type="uint"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="FLUSH_CACHE2"/>
-  </struct>
-
-  <struct name="CEU REQ_RESOURCE" size="2">
-    <field name="Compute" size="1" start="0" type="bool"/>
-    <field name="Fragment" size="1" start="1" type="bool"/>
-    <field name="Tiler" size="1" start="2" type="bool"/>
-    <field name="IDVS" size="1" start="3" type="bool"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="REQ_RESOURCE"/>
-  </struct>
-
-  <struct name="CEU SYNC_WAIT32" size="2">
-    <field name="Error reject" size="1" start="0" type="bool"/>
-    <field name="Invert" size="1" start="28" type="bool"/>
-    <field name="Data" size="8" start="32" type="uint"/>
-    <field name="Address" size="8" start="40" type="uint"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="SYNC_WAIT32"/>
-  </struct>
-
-  <struct name="CEU SYNC_WAIT64" size="2">
-    <field name="Error reject" size="1" start="0" type="bool"/>
-    <field name="Invert" size="1" start="28" type="bool"/>
-    <field name="Data" size="8" start="32" type="uint"/>
-    <field name="Address" size="8" start="40" type="uint"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="SYNC_WAIT64"/>
-  </struct>
-
-  <struct name="CEU HEAP_SET" size="2">
-    <field name="Address" size="8" start="40" type="uint"/>
-    <field name="Opcode" size="8" start="56" type="CEU Opcode" default="HEAP_SET"/>
-  </struct>
-
   <struct name="Attribute" size="8" align="32">
     <field name="Type" size="4" start="0:0" type="Descriptor Type" default="Attribute"/>
     <field name="Attribute type" size="4" start="0:4" type="Attribute Type"/>
@@ -842,7 +524,7 @@
     <value name="Constant" value="7"/>
   </enum>
 
-  <struct name="Blend Function" no-direct-packing="true">
+  <struct name="Blend Function" layout="none">
     <!-- Blend equation: A + (B * C) -->
     <field name="A" size="2" start="0" type="Blend Operand A"/>
     <field name="Negate A" size="1" start="3" type="bool"/>
@@ -915,63 +597,44 @@
     <value name="FP32" value="3"/>
   </enum>
 
-  <enum name="Primitive Restart">
-    <value name="None" value="0"/>
-    <value name="Implicit" value="2"/>
-    <value name="Explicit" value="3"/>
-  </enum>
-
-  <enum name="FIFO format">
-    <value name="Auto" value="0"/>
-    <value name="Basic" value="1"/>
-    <value name="Extended" value="2"/>
-  </enum>
-
-  <struct name="Primitive Flags" size="1">
-    <field name="Draw mode" size="8" start="0:0" type="Draw Mode" default="None"/>
-    <field name="Index type" size="3" start="0:8" type="Index Type" default="None"/>
-    <field name="Point size array format" size="2" start="0:11" type="Point Size Array Format"/>
-    <field name="Primitive Index Enable" size="1" start="0:13" type="bool"/>
-    <field name="Primitive Index Writeback" size="1" start="0:14" type="bool"/>
-    <field name="Allow rotating primitives" size="1" start="0:15" type="bool"/>
-    <field name="Low Depth Cull" size="1" start="0:16" type="bool" default="true"/>
-    <field name="High Depth Cull" size="1" start="0:17" type="bool" default="true"/>
-    <field name="Secondary Shader" size="1" start="0:18" type="bool"/>
-    <field name="Primitive restart" size="2" start="0:19" type="Primitive Restart"/>
-    <field name="Layer index enable" size="1" start="0:20" type="bool"/>
-    <field name="Scissor array enable" size="1" start="0:21" type="bool"/>
-    <field name="Position FIFO format" size="2" start="0:22" type="FIFO format"/>
-    <field name="View mask" size="8" start="0:24" type="hex"/>
+  <struct name="Tiler launch" layout="ins" op="5">
   </struct>
 
-  <struct name="Primitive">
-    <field name="Draw mode" size="8" start="0:0" type="Draw Mode" default="None"/>
-    <field name="Index type" size="3" start="0:8" type="Index Type" default="None"/>
-    <field name="Point size array format" size="2" start="0:11" type="Point Size Array Format"/>
-    <field name="Primitive Index Enable" size="1" start="0:13" type="bool"/>
-    <field name="Primitive Index Writeback" size="1" start="0:14" type="bool"/>
-    <field name="Allow rotating primitives" size="1" start="0:15" type="bool"/>
-    <field name="Low Depth Cull" size="1" start="0:16" type="bool" default="true"/>
-    <field name="High Depth Cull" size="1" start="0:17" type="bool" default="true"/>
-    <field name="Secondary Shader" size="1" start="0:18" type="bool"/>
-    <field name="Primitive restart" size="2" start="0:19" type="Primitive Restart"/>
+  <struct name="IDVS launch" layout="ins" op="6">
+    <!-- The low 32 bits are all ORd with register w38 -->
+    <field name="Draw mode" size="8" start="0" type="Draw Mode" default="None"/>
+    <field name="Index type" size="3" start="8" type="Index Type" default="None"/>
+    <field name="Secondary Shader" size="1" start="18" type="bool"/>
+    <field name="Unk" size="16" start="32" type="register" default="0x4a42"/>
+  </struct>
+
+  <struct name="Primitive" layout="cs">
+    <field name="Index count" size="32" start="0x21:0" type="uint"/>
+    <field name="Base vertex offset" size="32" start="0x24:0" type="uint"/>
+    <field name="Instance offset" size="32" start="0x25:0" type="uint"/>
+
+    <!-- These bits can also be set from the IDVS launch instruction...
+         they are combined with a bitwise OR. -->
+    <field name="Draw mode" size="8" start="0x38:0" type="Draw Mode" default="None"/>
+    <field name="Index type" size="3" start="0x38:8" type="Index Type" default="None"/>
+    <field name="Point size array format" size="2" start="0x38:11" type="Point Size Array Format"/>
+    <field name="Primitive Index Enable" size="1" start="0x38:13" type="bool"/>
+    <field name="Primitive Index Writeback" size="1" start="0x38:14" type="bool"/>
+    <field name="Allow rotating primitives" size="1" start="0x38:15" type="bool" default="true"/>
+    <field name="Low Depth Cull" size="1" start="0x38:16" type="bool"/>
+    <field name="High Depth Cull" size="1" start="0x38:17" type="bool"/>
+    <field name="Secondary Shader" size="1" start="0x38:18" type="bool"/>
+    <field name="Primitive restart" size="1" start="0x38:19" type="bool"/>
+    <!-- Set to 2 when writing point size, 1 otherwise. 0 appears to be fine in any case. -->
+    <field name="Unk" size="2" start="0x38:22" type="uint"/>
+
+    <!-- These are likely in 0x38, but bit positions have not been verified -->
     <field name="Layer index enable" size="1" start="0:20" type="bool"/>
     <field name="Scissor array enable" size="1" start="0:21" type="bool"/>
-
-    <field name="Base vertex offset" size="32" start="1:0" type="uint"/>
-    <field name="Instance offset" size="32" start="2:0" type="uint"/>
-    <field name="Index count" size="32" start="3:0" type="uint"/>
   </struct>
 
-  <enum name="Reduction Mode">
-    <value name="Average" value="0"/>
-    <value name="Minimum" value="2"/>
-    <value name="Maximum" value="3"/>
-  </enum>
-
   <struct name="Sampler" size="8" align="32">
     <field name="Type" size="4" start="0:0" type="Descriptor Type" default="Sampler"/>
-    <field name="Reduction mode" size="2" start="0:4" type="Reduction Mode" default="Average"/>
     <field name="Wrap Mode R" size="4" start="0:8" type="Wrap Mode" default="Clamp to Edge"/>
     <field name="Wrap Mode T" size="4" start="0:12" type="Wrap Mode" default="Clamp to Edge"/>
     <field name="Wrap Mode S" size="4" start="0:16" type="Wrap Mode" default="Clamp to Edge"/>
@@ -1079,10 +742,12 @@
 
   <enum name="AFBC Compression Mode">
     <value name="R8" value="0"/>
+    <value name="R8G8 alt" value="1"/>
     <value name="R8G8" value="3"/>
     <value name="R5G6B5" value="4"/>
     <value name="R4G4B4A4" value="5"/>
     <value name="R5G5B5A1" value="6"/>
+    <value name="R8G8B8A8 alt" value="8"/>
     <value name="R8G8B8" value="9"/>
     <value name="R8G8B8A8" value="10"/>
     <value name="R10G10B10A2" value="11"/>
@@ -1090,6 +755,10 @@
     <value name="S8" value="14"/>
     <value name="X24S8" value="15"/>
 
+    <!-- Subsampled maybe? -->
+    <value name="Unk 1" value="16"/>
+    <value name="Unk 2" value="17"/>
+
     <value name="YUV420 6c8" value="32"/>
     <value name="YUV420 2c8" value="34"/>
     <value name="YUV420 1c8" value="35"/>
@@ -1217,11 +886,11 @@
     <field name="Binary" size="64" start="2:0" type="address"/>
   </struct>
 
-  <struct name="Scissor">
-    <field name="Scissor Minimum X" size="16" start="0:0" type="uint"/>
-    <field name="Scissor Minimum Y" size="16" start="0:16" type="uint"/>
-    <field name="Scissor Maximum X" size="16" start="1:0" type="uint"/>
-    <field name="Scissor Maximum Y" size="16" start="1:16" type="uint"/>
+  <struct name="Scissor" layout="cs">
+    <field name="Scissor Minimum X" size="16" start="0x2a:0" type="uint"/>
+    <field name="Scissor Minimum Y" size="16" start="0x2a:16" type="uint"/>
+    <field name="Scissor Maximum X" size="16" start="0x2b:0" type="uint"/>
+    <field name="Scissor Maximum Y" size="16" start="0x2b:16" type="uint"/>
   </struct>
 
   <struct name="Local Storage" size="8" align="64">
@@ -1418,7 +1087,8 @@
     <field name="ZS Clean Pixel Write Enable" size="1" start="0:31" type="bool"/>
 
     <field name="CRC Row Stride" size="32" start="1:0" type="uint"/>
-    <field name="CRC Clear Color" size="64" start="2:0" type="hex"/>
+    <field name="CRC Unk" size="16" start="2:0" type="hex"/>
+    <field name="CRC Clear Color" size="48" start="2:16" type="hex"/>
     <field name="CRC Base" size="64" start="4:0" type="address"/>
 
     <field name="ZS Writeback Base" size="64" start="8:0" type="address"/>
@@ -1456,6 +1126,8 @@
     <field name="Conversion Mode" size="4" start="2:21" type="YUV Conversion Mode"/>
     <field name="Cr Siting" size="3" start="2:25" type="YUV Cr Siting"/>
     <field name="Unsigned Cr Range" size="1" start="2:28" type="bool"/>
+    <!-- TODO: Is planar YUV even supported? Only "Plane 0" appears to
+         be used, and the stride gets ignored. -->
     <field name="Plane 0 Base" size="64" start="4:0" type="address"/>
     <field name="Plane 1 Base" size="64" start="6:0" type="address"/>
     <field name="Plane 2 Base" size="64" start="8:0" type="address"/>
@@ -1465,6 +1137,7 @@
 
   <struct name="Render Target AFBC Overlay" size="16">
     <field name="YUV Transform" size="1" start="2:0" type="bool"/>
+    <!-- Split each block into two half-height blocks -->
     <field name="Split block" size="1" start="2:1" type="bool"/>
     <field name="Wide block" size="1" start="2:2" type="bool"/>
     <field name="Reverse issue order" size="1" start="2:3" type="bool"/>
@@ -1510,6 +1183,9 @@
     <value name="2 MiB" value="3"/>
   </enum>
 
+  <!-- Is this actually correct? Changing it doesn't seem to affect
+       heap memory usage at all. Then again, neither does the chunk
+       size... perhaps those are both wrong, at least for v10.  -->
   <enum name="Chunk Partition">
     <value name="Dynamic" value="0"/>
     <value name="Static 25%" value="1"/>
@@ -1549,12 +1225,21 @@
     <field name="Word5" size="32" start="5:0" type="uint"/>
     <field name="Word6" size="32" start="6:0" type="uint"/>
     <field name="Word7" size="32" start="7:0" type="uint"/>
+    <field name="Word8" size="32" start="8:0" type="uint"/>
+    <field name="Word9" size="32" start="9:0" type="uint"/>
+    <field name="Word10" size="32" start="10:0" type="uint"/>
+    <field name="Word11" size="32" start="11:0" type="uint"/>
+    <field name="Word12" size="32" start="12:0" type="uint"/>
+    <field name="Word13" size="32" start="13:0" type="uint"/>
+    <field name="Word14" size="32" start="14:0" type="uint"/>
+    <field name="Word15" size="32" start="15:0" type="uint"/>
   </struct>
 
-  <struct name="Tiler Context" size="32" align="64">
+  <struct name="Tiler Context" size="48" align="64">
     <field name="Polygon List" size="64" start="0:0" type="address"/>
     <field name="Hierarchy Mask" size="13" start="2:0" type="uint"/>
     <field name="Sample Pattern" size="3" start="2:13" type="Sample Pattern"/>
+    <field name="Update Cost Table" size="1" start="2:16" type="bool"/>
     <field name="Sample test disable" size="1" start="2:17" type="bool"/>
     <field name="First provoking vertex" size="1" start="2:18" type="bool"/>
     <field name="FB Width" size="16" start="3:0" type="uint" modifier="minus(1)"/>
@@ -1562,11 +1247,13 @@
     <field name="Layer count" size="8" start="4:0" type="uint" default="1" modifier="minus(1)"/>
     <field name="Layer offset" size="8" start="4:8" type="uint"/>
     <field name="Heap" size="64" start="6:0" type="address"/>
-    <field name="Geometry buffer size" size="12" start="8:0" type="uint" default="65536" modifier="shr(12)"/>
-    <field name="Geometry buffer" size="52" start="8:12" type="address" modifier="shr(12)"/>
-    <field name="Completed top" size="64" start="10:0" type="address"/>
-    <field name="Completed bottom" size="64" start="12:0" type="address"/>
-    <field name="Private state" size="256" start="24:0" type="Tiler State"/>
+    <!-- The blob places this directly before the heap pointer -->
+    <field name="Scratch" size="64" start="8:0" type="address"/>
+    <!-- Filled in by hardare, passed to HEAPCLEAR -->
+    <field name="Heap Start" size="64" start="10:0" type="address"/>
+    <field name="Heap End" size="64" start="12:0" type="address"/>
+    <field name="Weights" size="256" start="8:0" type="Tiler Weights"/>
+    <field name="State" size="512" start="32:0" type="Tiler State"/>
   </struct>
 
   <struct name="Framebuffer Padding" size="16">
@@ -1577,38 +1264,104 @@
     <section name="Padding" offset="64" type="Framebuffer Padding"/>
   </aggregate>
 
-  <struct name="Fragment Job Payload" size="8">
-    <field name="Bound Min X" size="12" start="0:0" type="uint"/>
-    <field name="Bound Min Y" size="12" start="0:16" type="uint"/>
-    <field name="Bound Max X" size="12" start="1:0" type="uint"/>
-    <field name="Bound Max Y" size="12" start="1:16" type="uint"/>
-    <field name="Tile render order" size="3" start="1:28" type="Tile Render Order" default="Z Order"/>
-    <field name="Has Tile Enable Map" size="1" start="1:31" type="bool"/>
-    <field name="Framebuffer" size="64" start="2:0" type="address"/>
-    <field name="Tile Enable Map" size="64" start="4:0" type="address"/>
-    <field name="Tile Enable Map Row Stride" size="8" start="6:0" type="uint"/>
+  <struct name="Fragment Job Payload" layout="cs">
+    <field name="Bound Min X" size="16" start="0x2a:0" type="uint"/>
+    <field name="Bound Min Y" size="16" start="0x2a:16" type="uint"/>
+    <field name="Bound Max X" size="16" start="0x2b:0" type="uint"/>
+    <field name="Bound Max Y" size="16" start="0x2b:16" type="uint"/>
+    <field name="Framebuffer" size="48" start="0x28:0" type="address"/>
+    <field name="Tile Enable Map" size="48" start="0x2c:0" type="address"/>
+    <!-- Must have 64-byte alignment -->
+    <field name="Tile Enable Map Row Stride" size="9" start="0x2e:0" type="uint"/>
   </struct>
 
-  <struct name="Shader Environment" size="16" align="64">
-    <field name="Attribute offset" start="0:0" size="32" type="uint"/>
-    <field name="FAU count" start="1:0" size="8" type="uint"/>
-    <field name="Resources" start="8:0" size="64" type="address"/>
-    <field name="Shader" start="10:0" size="64" type="address"/>
-    <field name="Thread storage" start="12:0" size="64" type="address"/>
-    <field name="FAU" start="14:0" size="64" type="address"/>
-  </struct>
+  <aggregate name="Fragment Job" align="128">
+    <section name="Payload" offset="32" type="Fragment Job Payload"/>
+  </aggregate>
 
-  <struct name="Compute size workgroup" size="4">
-    <field name="Workgroup size X" start="0" size="10" type="uint" modifier="minus(1)"/>
-    <field name="Workgroup size Y" start="10" size="10" type="uint" modifier="minus(1)"/>
-    <field name="Workgroup size Z" start="20" size="10" type="uint" modifier="minus(1)"/>
-    <field name="Allow merging workgroups" start="31" size="1" type="bool"/>
+  <struct name="Fragment launch" layout="ins" op="7">
+    <field name="Has Tile Enable Map" size="1" start="0" type="bool"/>
+    <field name="Tile render order" size="3" start="4" type="Tile Render Order" default="Z Order"/>
+    <field name="Unk" size="1" start="32" type="bool"/>
   </struct>
 
+  <struct name="Shader Environment" size="16" align="64">
+    <field name="Attribute offset" size="32" start="0:0" type="uint"/>
+    <field name="FAU count" size="8" start="1:0" type="uint"/>
+    <field name="Resources" size="48" start="8:0" type="address"/>
+    <field name="Shader" size="48" start="10:0" type="address"/>
+    <field name="Thread storage" size="48" start="12:0" type="address"/>
+    <field name="FAU" size="64" start="14:0" type="address"/>
+  </struct>
+
+  <struct name="Position Shader Environment" layout="cs" unpacked="Shader Environment">
+    <field name="Attribute offset" size="32" start="0:0" type="uint"/>
+    <field name="Resources" size="48" start="0x00:0" type="address"/>
+    <field name="Shader" size="48" start="0x10:0" type="address"/>
+    <!-- Note that filling the thread storage pointer at 0x1e might
+         also be requried for prefix shaders. -->
+    <field name="Thread storage" size="48" start="0x18:0" type="address"/>
+    <field name="FAU" size="48" start="0x08:0" type="address"/>
+    <field name="FAU count" size="8" start="0x08:56" type="uint"/>
+  </struct>
+
+  <struct name="Varying Shader Environment" layout="cs" unpacked="Shader Environment">
+    <field name="Attribute offset" size="32" start="0:0" type="uint"/>
+    <field name="Resources" size="48" start="0x00:0" type="address"/>
+    <field name="Shader" size="48" start="0x12:0" type="address"/>
+    <!-- These field alias with position shaders -->
+    <field name="Thread storage" size="48" start="0x18:0" type="address"/>
+    <field name="FAU" size="48" start="0x08:0" type="address"/>
+    <field name="FAU count" size="8" start="0x08:56" type="uint"/>
+  </struct>
+
+  <struct name="Fragment Shader Environment" layout="cs" unpacked="Shader Environment">
+    <field name="Attribute offset" size="32" start="0:0" type="uint"/>
+    <field name="Resources" size="48" start="0x04:0" type="address"/>
+    <field name="Shader" size="48" start="0x14:0" type="address"/>
+    <field name="Thread storage" size="48" start="0x18:0" type="address"/>
+    <field name="FAU" size="48" start="0x0c:0" type="address"/>
+    <field name="FAU count" size="8" start="0x0c:56" type="uint"/>
+  </struct>
+
+  <struct name="Compute Shader Environment" layout="cs" unpacked="Shader Environment">
+    <field name="Attribute offset" size="32" start="0:0" type="uint"/>
+    <field name="Resources" size="48" start="0x06:0" type="address"/>
+    <field name="Shader" size="48" start="0x16:0" type="address"/>
+    <field name="Thread storage" size="48" start="0x1e:0" type="address"/>
+    <field name="FAU" size="48" start="0x0e:0" type="address"/>
+    <field name="FAU count" size="8" start="0x0e:56" type="uint"/>
+  </struct>
+
+  <struct name="Compute Payload" layout="cs">
+    <field name="Workgroup size X" size="10" start="0x21:0" type="uint" modifier="minus(1)"/>
+    <field name="Workgroup size Y" size="10" start="0x21:10" type="uint" modifier="minus(1)"/>
+    <field name="Workgroup size Z" size="10" start="0x21:20" type="uint" modifier="minus(1)"/>
+    <field name="Allow merging workgroups" size="1" start="0x21:31" type="bool"/>
+    <field name="Offset X" size="32" start="0x22:0" type="uint"/>
+    <field name="Offset Y" size="32" start="0x23:0" type="uint"/>
+    <field name="Offset Z" size="32" start="0x24:0" type="uint"/>
+    <field name="Workgroup count X" size="32" start="0x25:0" type="uint"/>
+    <field name="Workgroup count Y" size="32" start="0x26:0" type="uint"/>
+    <field name="Workgroup count Z" size="32" start="0x27:0" type="uint"/>
+    <field name="Compute" size="512" start="0x00:0" type="Compute Shader Environment"/>
+  </struct>
+
+  <struct name="Compute Launch" layout="ins" op="4">
+    <field name="Unk 1" size="14" start="0" type="uint" default="1"/>
+    <field name="Unk 2" size="2" start="14" type="uint" default="2"/>
+    <field name="Unk 3" size="8" start="40" type="hex" default="255"/>
+  </struct>
+
+  <!-- Compute job also covers vertex and geometry operations -->
+  <aggregate name="Compute Job" align="128">
+    <section name="Payload" offset="32" type="Compute Payload"/>
+  </aggregate>
+
   <struct name="Resource" size="4" align="16">
-    <field name="Address" start="0:0" size="56" type="address"/>
-    <field name="Contains descriptors" start="1:24" size="1" type="bool" default="true"/>
-    <field name="Size" start="2:0" size="64" type="uint"/> <!-- bytes -->
+    <field name="Address" size="56" start="0:0" type="address"/>
+    <field name="Contains descriptors" size="1" start="1:24" type="bool" default="true"/>
+    <field name="Size" size="64" start="2:0" type="uint"/> <!-- bytes -->
   </struct>
 
   <struct name="Depth/stencil" size="8" align="32">
@@ -1629,9 +1382,8 @@
     <field name="Back value mask" size="8" start="1:24" type="hex"/>
     <field name="Front reference value" size="8" start="2:0" type="hex"/>
     <field name="Back reference value" size="8" start="2:8" type="hex"/>
-    <field name="Shader read only Z/S" size="1" start="4:21" type="bool"/>
-    <field name="Depth cull enable" size="1" start="4:22" type="bool" default="true"/>
-    <field name="Depth clamp mode" size="2" start="4:23" type="Depth Clamp Mode" default="[0, 1]"/>
+    <field name="Depth cull enable" size="1" start="4:22" type="bool"/>
+    <field name="Depth clamp mode" size="2" start="4:23" type="Depth Clamp Mode"/>
     <field name="Depth source" size="2" start="4:25" type="Depth Source" default="Fixed function"/>
     <field name="Depth write enable" size="1" start="4:27" type="bool"/>
     <field name="Depth bias enable" size="1" start="4:28" type="bool"/>
@@ -1641,7 +1393,7 @@
     <field name="Depth bias clamp" size="32" start="7:0" type="float"/>
   </struct>
 
-  <struct name="Vertex Array" size="3">
+  <struct name="Vertex Array" size="4">
     <field name="Packet" size="1" start="0:0" type="bool"/>
 
     <!-- Written by hardware in packet mode -->
@@ -1650,49 +1402,55 @@
     <!-- Written by hardware, leave zero -->
     <field name="Vertex packet stride" size="16" start="2:0" type="uint"/>
     <field name="Vertex attribute stride" size="16" start="2:16" type="uint"/>
+    <!-- Comes from the w26 register -->
+    <field name="Unk" size="24" start="3:0" type="hex"/>
   </struct>
 
-  <struct name="DCD Flags 0" size="1">
-    <field name="Allow forward pixel to kill" size="1" start="0" type="bool"/>
-    <field name="Allow forward pixel to be killed" size="1" start="1" type="bool"/>
-    <field name="Pixel kill operation" size="2" start="2" type="Pixel Kill"/>
-    <field name="ZS update operation" size="2" start="4" type="Pixel Kill"/>
-    <field name="Allow primitive reorder" size="1" start="6" type="bool"/>
-    <field name="Overdraw alpha0" start="7" size="1" type="bool"/>
-    <field name="Overdraw alpha1" size="1" start="8" type="bool"/>
-    <field name="Clean Fragment Write" size="1" start="9" type="bool"/>
-    <field name="Primitive Barrier" size="1" start="10" type="bool"/>
-    <field name="Evaluate per-sample" size="1" start="11" type="bool"/>
-    <field name="Single-sampled lines" size="1" start="13" type="bool"/>
-    <field name="Occlusion query" size="2" start="14" type="Occlusion Mode" default="Disabled"/>
-    <field name="Front face CCW" size="1" start="16" type="bool"/>
-    <field name="Cull front face" size="1" start="17" type="bool"/>
-    <field name="Cull back face" size="1" start="18" type="bool"/>
-    <field name="Multisample enable" size="1" start="19" type="bool"/>
-    <field name="Shader modifies coverage" size="1" start="20" type="bool"/>
-    <field name="Alpha-to-coverage Invert" size="1" start="21" type="bool"/>
-    <field name="Alpha-to-coverage" size="1" start="22" type="bool"/>
-    <field name="Scissor to bounding box" size="1" start="23" type="bool"/>
-  </struct>
-
-  <struct name="DCD Flags 1" size="1">
-    <field name="Sample mask" size="16" start="0" type="uint"/>
-    <field name="Render target mask" start="16" size="8" type="hex"/>
-  </struct>
-
-  <struct name="DCD Flags 2" size="1">
-    <field name="Read mask" size="12" start="0" type="hex"/>
-    <field name="Write mask" size="12" start="12" type="hex"/>
+  <struct name="Draw" layout="cs">
+    <field name="Allow forward pixel to kill" size="1" start="0x39:0" type="bool"/>
+    <field name="Allow forward pixel to be killed" size="1" start="0x39:1" type="bool"/>
+    <field name="Pixel kill operation" size="2" start="0x39:2" type="Pixel Kill"/>
+    <field name="ZS update operation" size="2" start="0x39:4" type="Pixel Kill"/>
+    <field name="Allow primitive reorder" size="1" start="0x39:6" type="bool"/>
+    <field name="Overdraw alpha0" size="1" start="0x39:7" type="bool"/>
+    <field name="Overdraw alpha1" size="1" start="0x39:8" type="bool"/>
+    <field name="Clean Fragment Write" size="1" start="0x39:9" type="bool"/>
+    <field name="Primitive Barrier" size="1" start="0x39:10" type="bool"/>
+    <field name="Evaluate per-sample" size="1" start="0x39:11" type="bool"/>
+    <field name="Single-sampled lines" size="1" start="0x39:13" type="bool"/>
+    <field name="Occlusion query" size="2" start="0x39:14" type="Occlusion Mode" default="Disabled"/>
+    <field name="Front face CCW" size="1" start="0x39:16" type="bool"/>
+    <field name="Cull front face" size="1" start="0x39:17" type="bool"/>
+    <field name="Cull back face" size="1" start="0x39:18" type="bool"/>
+    <field name="Multisample enable" size="1" start="0x39:19" type="bool"/>
+    <field name="Shader modifies coverage" size="1" start="0x39:20" type="bool"/>
+    <field name="Alpha-to-coverage Invert" size="1" start="0x39:21" type="bool"/>
+    <field name="Alpha-to-coverage" size="1" start="0x39:22" type="bool"/>
+    <field name="Scissor to bounding box" size="1" start="0x39:23" type="bool"/>
+    <field name="Sample mask" size="16" start="0x3a:0" type="uint"/>
+    <field name="Render target mask" size="16" start="0x3a:16" type="uint"/>
+
+    <field name="Unk" size="24" start="0x26:0" type="hex" default="0x1000"/>
+
+    <!-- TODO v10, Doesn't exist in register space but is written to the heap -->
+    <field name="Vertex array" size="128" start="2:0" type="Vertex Array"/>
+
+    <field name="Minimum Z" size="32" start="0x2c:0" type="float"/>
+    <field name="Maximum Z" size="32" start="0x2d:0" type="float"/>
+    <field name="Depth/stencil" size="48" start="0x34:0" type="address"/>
+    <field name="Blend count" size="4" start="0x32:0" type="uint"/>
+    <field name="Blend" size="44" start="0x32:4" type="address" modifier="shr(4)"/>
+    <field name="Occlusion" size="48" start="0x2e:0" type="address"/>
+    <field name="Shader" size="512" start="0x0:0" type="Fragment Shader Environment"/>
   </struct>
 
-  <struct name="Draw" align="64">
-    <!-- DCD flags 0 -->
+  <struct name="Draw No CS" align="64" unpacked="Draw">
     <field name="Allow forward pixel to kill" size="1" start="0:0" type="bool"/>
     <field name="Allow forward pixel to be killed" size="1" start="0:1" type="bool"/>
     <field name="Pixel kill operation" size="2" start="0:2" type="Pixel Kill"/>
     <field name="ZS update operation" size="2" start="0:4" type="Pixel Kill"/>
     <field name="Allow primitive reorder" size="1" start="0:6" type="bool"/>
-    <field name="Overdraw alpha0" start="0:7" size="1" type="bool"/>
+    <field name="Overdraw alpha0" size="1" start="0:7" type="bool"/>
     <field name="Overdraw alpha1" size="1" start="0:8" type="bool"/>
     <field name="Clean Fragment Write" size="1" start="0:9" type="bool"/>
     <field name="Primitive Barrier" size="1" start="0:10" type="bool"/>
@@ -1707,23 +1465,204 @@
     <field name="Alpha-to-coverage Invert" size="1" start="0:21" type="bool"/>
     <field name="Alpha-to-coverage" size="1" start="0:22" type="bool"/>
     <field name="Scissor to bounding box" size="1" start="0:23" type="bool"/>
-    <!-- DCD flags 1 -->
     <field name="Sample mask" size="16" start="1:0" type="uint"/>
-    <field name="Render target mask" start="1:16" size="8" type="hex"/>
-    <field name="Vertex array" start="2:0" size="96" type="Vertex Array"/>
-    <field name="Flags 2" start="5:0" size="32" type="DCD Flags 2"/>
-    <field name="Minimum Z" start="6:0" size="32" type="float"/>
-    <field name="Maximum Z" start="7:0" size="32" type="float"/>
-    <field name="Depth/stencil" start="10:0" size="64" type="address"/>
-    <field name="Blend count" start="12:0" size="4" type="uint"/>
-    <field name="Blend" start="12:4" size="60" type="address" modifier="shr(4)"/>
+    <field name="Render target mask" size="8" start="1:16" type="hex"/>
+    <!-- This is correct, but uncommenting would break things! -->
+    <!--<field name="Vertex array" size="128" start="2:0" type="Vertex Array"/>-->
+    <field name="Minimum Z" size="32" start="6:0" type="float"/>
+    <field name="Maximum Z" size="32" start="7:0" type="float"/>
+    <field name="Depth/stencil" size="64" start="10:0" type="address"/>
+    <field name="Blend count" size="4" start="12:0" type="uint"/>
+    <field name="Blend" size="60" start="12:4" type="address" modifier="shr(4)"/>
     <field name="Occlusion" size="64" start="14:0" type="address"/>
-    <field name="Shader" start="16:0" size="512" type="Shader Environment"/>
+    <field name="Shader" size="512" start="16:0" type="Shader Environment"/>
+  </struct>
+
+  <struct name="Count" layout="cs">
+    <!-- I think this is right? -->
+    <field name="Count" size="32" start="0x22:0" type="uint"/>
+  </struct>
+
+  <struct name="Allocation" layout="cs">
+    <!-- "Vertex packet stride" is only written by hardware? -->
+    <field name="Vertex attribute stride" size="16" start="0x30:0" type="uint"/>
+  </struct>
+
+  <struct name="Tiler Pointer" layout="cs">
+    <field name="Address" size="48" start="0x28:0" type="address"/>
+  </struct>
+
+  <struct name="Primitive Size" layout="cs">
+    <field name="Constant" size="32" start="0x3c:0" type="float"/>
+    <field name="Size Array" size="48" start="0x3c:0" type="uint"/>
+  </struct>
+
+  <struct name="Indices" layout="cs">
+    <field name="Address" size="64" start="0x36:0" type="address"/>
+    <!-- This seems different from v9, should it live someplace else? -->
+    <field name="Size" size="32" start="0x27:0" type="uint"/>
+  </struct>
+
+  <!-- TODO: What to do about these? -->
+  <aggregate name="Malloc Vertex Job" layout="cs">
+    <section name="Primitive" offset="32" type="Primitive"/>
+    <section name="Instance Count" offset="48" type="Count"/>
+    <section name="Allocation" offset="52" type="Allocation"/>
+    <section name="Tiler" offset="56" type="Tiler Pointer"/>
+    <section name="Scissor" offset="104" type="Scissor"/>
+    <section name="Primitive Size" offset="112" type="Primitive Size"/>
+    <section name="Indices" offset="120" type="Indices"/>
+    <section name="Draw" offset="128" type="Draw"/>
+    <section name="Position" offset="256" type="Position Shader Environment"/>
+    <section name="Varying" offset="320" type="Varying Shader Environment"/>
+  </aggregate>
+
+  <aggregate name="Tiler Job" layout="cs">
+    <section name="Primitive" offset="32" type="Primitive"/>
+    <section name="Instance Count" offset="48" type="Count"/>
+    <section name="Vertex Count" offset="52" type="Count"/>
+    <section name="Tiler" offset="56" type="Tiler Pointer"/>
+    <section name="Scissor" offset="104" type="Scissor"/>
+    <section name="Primitive Size" offset="112" type="Primitive Size"/>
+    <section name="Indices" offset="120" type="Indices"/>
+    <section name="Draw" offset="128" type="Draw"/>
+  </aggregate>
+
+  <struct name="CS NOP" layout="ins" op="0"/>
+
+  <struct name="CS Add Imm" layout="ins" op="17">
+    <field name="Value" size="32" start="0" type="int"/>
+    <field name="Src" size="8" start="40" type="register"/>
+    <field name="Dest" size="8" start="48" type="register"/>
+  </struct>
+
+  <struct name="CS LDR" layout="ins" op="20">
+      <field name="Offset" size="16" start="0" type="int"/>
+      <field name="Register Mask" size="16" start="16" type="hex"/>
+      <field name="Addr" size="8" start="40" type="register"/>
+      <field name="Register Base" size="8" start="48" type="register"/>
+  </struct>
+
+  <struct name="CS STR" layout="ins" op="21">
+      <field name="Offset" size="16" start="0" type="int"/>
+      <field name="Register Mask" size="16" start="16" type="hex"/>
+      <field name="Addr" size="8" start="40" type="register"/>
+      <field name="Register Base" size="8" start="48" type="register"/>
+  </struct>
+
+  <!-- TODO: The next four are all just about equivalent... they
+       should have a shared definition. -->
+  <struct name="CS EVADD" layout="ins" op="37">
+    <field name="Unk Flag" size="1" start="0" type="bool" default="true"/>
+    <field name="No IRQ" size="1" start="2" type="bool" default="false"/>
+    <field name="Unk 2" size="16" start="16" type="hex" default="253"/>
+    <!-- TODO: Separate types for 32-bit and 64-bit registers -->
+    <field name="Value" size="8" start="32" type="register"/>
+    <field name="Addr" size="8" start="40" type="register"/>
+    <field name="Unk 3" size="8" start="48" type="hex" default="1"/>
+  </struct>
+
+  <struct name="CS EVADD 64" layout="ins" op="51">
+    <field name="Unk Flag" size="1" start="0" type="bool" default="true"/>
+    <field name="No IRQ" size="1" start="2" type="bool" default="false"/>
+    <field name="Unk 2" size="16" start="16" type="hex" default="253"/>
+    <!-- TODO: Separate types for 32-bit and 64-bit registers -->
+    <field name="Value" size="8" start="32" type="register"/>
+    <field name="Addr" size="8" start="40" type="register"/>
+    <field name="Unk 3" size="8" start="48" type="hex" default="1"/>
+  </struct>
+
+  <struct name="CS EVSTR" layout="ins" op="38">
+    <field name="Unk Flag" size="1" start="0" type="bool" default="true"/>
+    <field name="No IRQ" size="1" start="2" type="bool" default="false"/>
+    <field name="Unk 2" size="16" start="16" type="hex" default="253"/>
+    <!-- TODO: Separate types for 32-bit and 64-bit registers -->
+    <field name="Value" size="8" start="32" type="register"/>
+    <field name="Addr" size="8" start="40" type="register"/>
+    <field name="Unk 3" size="8" start="48" type="hex" default="1"/>
+  </struct>
+
+  <struct name="CS EVSTR 64" layout="ins" op="52">
+    <field name="Unk Flag" size="1" start="0" type="bool" default="true"/>
+    <field name="No IRQ" size="1" start="2" type="bool" default="false"/>
+    <field name="Unk 2" size="16" start="16" type="hex" default="253"/>
+    <!-- TODO: Separate types for 32-bit and 64-bit registers -->
+    <field name="Value" size="8" start="32" type="register"/>
+    <field name="Addr" size="8" start="40" type="register"/>
+    <field name="Unk 3" size="8" start="48" type="hex" default="1"/>
+  </struct>
+
+  <!-- Arm uses "greater than" and "less or equal", but I chose these
+       names to match up with AArch64 comparison names - these are
+       unsigned, while branch instructions are signed. -->
+  <enum name="Wait Condition">
+    <value name="Lower or same" value="0"/>
+    <value name="Higher" value="1"/>
+  </enum>
+
+  <!-- TODO: Use a single instruction with two opcodes for bitness -->
+  <struct name="CS EVWAIT" layout="ins" op="39">
+    <field name="No error" size="1" start="0" type="bool"/>
+    <field name="Condition" size="1" start="28" type="Wait Condition"/>
+    <field name="Value" size="8" start="32" type="register"/>
+    <field name="Addr" size="8" start="40" type="register"/>
+  </struct>
+
+  <struct name="CS EVWAIT 64" layout="ins" op="53">
+    <field name="No error" size="1" start="0" type="bool"/>
+    <field name="Condition" size="1" start="28" type="Wait Condition"/>
+    <field name="Value" size="8" start="32" type="register"/>
+    <field name="Addr" size="8" start="40" type="register"/>
   </struct>
 
-  <struct name="Primitive Size" size="2">
-    <field name="Constant" size="32" start="0:0" type="float"/>
-    <field name="Size Array" size="64" start="0:0" type="uint"/>
+  <struct name="CS Slot" layout="ins" op="23">
+    <field name="Index" size="3" start="0" type="uint"/>
   </struct>
 
+  <struct name="CS Wait" layout="ins" op="3">
+    <field name="Slots" size="8" start="16" type="hex"/>
+  </struct>
+
+  <struct name="CS Resources" layout="ins" op="34">
+    <field name="Mask" size="56" start="0" type="hex"/>
+    <field name="Compute" size="1" start="0" type="bool"/>
+    <field name="Fragment" size="1" start="1" type="bool"/>
+    <field name="Tiler" size="1" start="2" type="bool"/>
+    <field name="IDVS" size="1" start="3" type="bool"/>
+  </struct>
+
+  <struct name="CS Call" layout="ins" op="32">
+    <field name="Length" size="8" start="32" type="register"/>
+    <field name="Address" size="8" start="40" type="register"/>
+  </struct>
+
+  <struct name="CS Tailcall" layout="ins" op="33">
+    <field name="Length" size="8" start="32" type="register"/>
+    <field name="Address" size="8" start="40" type="register"/>
+  </struct>
+
+  <struct name="CS Flush Tiler" layout="ins" op="9"/>
+
+  <!-- TODO: What else can the instruction do? -->
+  <struct name="CS HEAPCLEAR" layout="ins" op="11">
+    <field name="Unk 1" size="16" start="0" type="hex" default="1"/>
+    <field name="Slots" size="8" start="16" type="hex"/>
+    <field name="End" size="8" start="32" type="register"/>
+    <field name="Start" size="8" start="40" type="register"/>
+    <field name="Unk 2" size="8" start="48" type="hex" default="2"/>
+  </struct>
+
+  <struct name="CS HEAPCTX" layout="ins" op="48">
+    <field name="Address" size="8" start="40" type="register"/>
+  </struct>
+
+  <enum name="Heap Statistic">
+    <value name="V/T Start" value="0"/>
+    <value name="V/T End" value="1"/>
+    <value name="Fragment End" value="3"/>
+  </enum>
+
+  <struct name="CS HEAPINC" layout="ins" op="49">
+    <field name="Type" size="8" start="32" type="Heap Statistic"/>
+  </struct>
 </panxml>
diff -urN mesa-23.0.0/src/panfrost/lib/genxml/v4.xml mesa/src/panfrost/lib/genxml/v4.xml
--- mesa-23.0.0/src/panfrost/lib/genxml/v4.xml	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/genxml/v4.xml	2023-03-06 17:53:54.639222082 +0100
@@ -446,7 +446,7 @@
     <value name="Constant" value="7"/>
   </enum>
 
-  <struct name="Blend Function" no-direct-packing="true">
+  <struct name="Blend Function" layout="none">
     <!-- Blend equation: A + (B * C) -->
     <field name="A" size="2" start="0" type="Blend Operand A"/>
     <field name="Negate A" size="1" start="3" type="bool"/>
diff -urN mesa-23.0.0/src/panfrost/lib/genxml/v5.xml mesa/src/panfrost/lib/genxml/v5.xml
--- mesa-23.0.0/src/panfrost/lib/genxml/v5.xml	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/genxml/v5.xml	2023-03-06 17:53:54.639222082 +0100
@@ -467,7 +467,7 @@
     <value name="Constant" value="7"/>
   </enum>
 
-  <struct name="Blend Function" no-direct-packing="true">
+  <struct name="Blend Function" layout="none">
     <!-- Blend equation: A + (B * C) -->
     <field name="A" size="2" start="0" type="Blend Operand A"/>
     <field name="Negate A" size="1" start="3" type="bool"/>
diff -urN mesa-23.0.0/src/panfrost/lib/genxml/v6.xml mesa/src/panfrost/lib/genxml/v6.xml
--- mesa-23.0.0/src/panfrost/lib/genxml/v6.xml	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/genxml/v6.xml	2023-03-06 17:53:54.639222082 +0100
@@ -467,7 +467,7 @@
     <value name="Constant" value="7"/>
   </enum>
 
-  <struct name="Blend Function" no-direct-packing="true">
+  <struct name="Blend Function" layout="none">
     <!-- Blend equation: A + (B * C) -->
     <field name="A" size="2" start="0" type="Blend Operand A"/>
     <field name="Negate A" size="1" start="3" type="bool"/>
@@ -689,7 +689,7 @@
     <field name="Stencil from shader" size="1" start="28" type="bool"/>
   </struct>
 
-  <struct name="Compute Preload" size="1" no-direct-packing="true">
+  <struct name="Compute Preload" size="1" layout="none">
     <field name="PC" size="1" start="6" type="bool"/>
     <field name="Local Invocation XY" size="1" start="7" type="bool"/>
     <field name="Local Invocation Z" size="1" start="8" type="bool"/>
@@ -708,7 +708,7 @@
     <value name="8" value="3"/>
   </enum>
 
-  <struct name="Vertex Preload" size="1" no-direct-packing="true">
+  <struct name="Vertex Preload" size="1" layout="none">
     <field name="Warp limit" size="2" start="0" type="Warp Limit"/>
     <field name="PC" size="1" start="6" type="bool"/>
     <field name="Position result address lo" size="1" start="10" type="bool"/>
@@ -717,7 +717,7 @@
     <field name="Instance ID" size="1" start="14" type="bool"/>
   </struct>
 
-  <struct name="Fragment Preload" size="1" no-direct-packing="true">
+  <struct name="Fragment Preload" size="1" layout="none">
     <field name="PC" size="1" start="6" type="bool"/>
     <field name="Coverage" size="1" start="7" type="bool"/>
     <field name="Primitive ID" size="1" start="9" type="bool"/>
diff -urN mesa-23.0.0/src/panfrost/lib/genxml/v7.xml mesa/src/panfrost/lib/genxml/v7.xml
--- mesa-23.0.0/src/panfrost/lib/genxml/v7.xml	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/genxml/v7.xml	2023-03-06 17:53:54.639222082 +0100
@@ -512,7 +512,7 @@
     <value name="Constant" value="7"/>
   </enum>
 
-  <struct name="Blend Function" no-direct-packing="true">
+  <struct name="Blend Function" layout="none">
     <!-- Blend equation: A + (B * C) -->
     <field name="A" size="2" start="0" type="Blend Operand A"/>
     <field name="Negate A" size="1" start="3" type="bool"/>
@@ -754,7 +754,7 @@
     <field name="Shader wait dependency 7" size="1" start="31" type="bool"/>
   </struct>
 
-  <struct name="Compute Preload" size="1" no-direct-packing="true">
+  <struct name="Compute Preload" size="1" layout="none">
     <field name="PC" size="1" start="6" type="bool"/>
     <field name="Local Invocation XY" size="1" start="7" type="bool"/>
     <field name="Local Invocation Z" size="1" start="8" type="bool"/>
@@ -773,7 +773,7 @@
     <value name="8" value="3"/>
   </enum>
 
-  <struct name="Vertex Preload" size="1" no-direct-packing="true">
+  <struct name="Vertex Preload" size="1" layout="none">
     <field name="Warp limit" size="2" start="0" type="Warp Limit"/>
     <field name="PC" size="1" start="6" type="bool"/>
     <field name="Position result address lo" size="1" start="10" type="bool"/>
@@ -782,7 +782,7 @@
     <field name="Instance ID" size="1" start="14" type="bool"/>
   </struct>
 
-  <struct name="Fragment Preload" size="1" no-direct-packing="true">
+  <struct name="Fragment Preload" size="1" layout="none">
     <field name="PC" size="1" start="6" type="bool"/>
     <field name="Coverage" size="1" start="7" type="bool"/>
     <field name="Primitive ID" size="1" start="9" type="bool"/>
@@ -846,13 +846,13 @@
     <field name="Depth Pass" size="3" start="25" type="Stencil Op"/>
   </struct>
 
-  <struct name="LD_VAR Preload" size="1" no-direct-packing="true">
+  <struct name="LD_VAR Preload" size="1" layout="none">
     <field name="Varying Index" size="5" start="4" type="uint"/>
     <field name="Register Format" size="2" start="9" type="Message Preload Register Format"/>
     <field name="Num Components" size="2" start="11" type="uint" modifier="minus(1)" default="1"/>
   </struct>
 
-  <struct name="VAR_TEX Preload" size="1" no-direct-packing="true">
+  <struct name="VAR_TEX Preload" size="1" layout="none">
     <field name="Varying Index" size="3" start="4" type="uint"/>
     <field name="Texture Index" size="2" start="7" type="uint"/>
     <field name="Register Format" size="2" start="9" type="Message Preload Register Format"/>
diff -urN mesa-23.0.0/src/panfrost/lib/genxml/v9.xml mesa/src/panfrost/lib/genxml/v9.xml
--- mesa-23.0.0/src/panfrost/lib/genxml/v9.xml	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/genxml/v9.xml	2023-03-06 17:54:35.855495079 +0100
@@ -526,7 +526,7 @@
     <value name="Constant" value="7"/>
   </enum>
 
-  <struct name="Blend Function" no-direct-packing="true">
+  <struct name="Blend Function" layout="none">
     <!-- Blend equation: A + (B * C) -->
     <field name="A" size="2" start="0" type="Blend Operand A"/>
     <field name="Negate A" size="1" start="3" type="bool"/>
@@ -599,12 +599,6 @@
     <value name="FP32" value="3"/>
   </enum>
 
-  <enum name="Primitive Restart">
-    <value name="None" value="0"/>
-    <value name="Implicit" value="2"/>
-    <value name="Explicit" value="3"/>
-  </enum>
-
   <struct name="Primitive">
     <field name="Draw mode" size="8" start="0:0" type="Draw Mode" default="None"/>
     <field name="Index type" size="3" start="0:8" type="Index Type" default="None"/>
@@ -612,10 +606,10 @@
     <field name="Primitive Index Enable" size="1" start="0:13" type="bool"/>
     <field name="Primitive Index Writeback" size="1" start="0:14" type="bool"/>
     <field name="Allow rotating primitives" size="1" start="0:15" type="bool" default="true"/>
-    <field name="Low Depth Cull" size="1" start="0:16" type="bool" default="true"/>
-    <field name="High Depth Cull" size="1" start="0:17" type="bool" default="true"/>
+    <field name="Low Depth Cull" size="1" start="0:16" type="bool"/>
+    <field name="High Depth Cull" size="1" start="0:17" type="bool"/>
     <field name="Secondary Shader" size="1" start="0:18" type="bool"/>
-    <field name="Primitive restart" size="2" start="0:19" type="Primitive Restart"/>
+    <field name="Primitive restart" size="1" start="0:19" type="bool"/>
     <field name="Layer index enable" size="1" start="0:20" type="bool"/>
     <field name="Scissor array enable" size="1" start="0:21" type="bool"/>
 
@@ -1309,28 +1303,28 @@
   </aggregate>
 
   <struct name="Shader Environment" size="16" align="64">
-    <field name="Attribute offset" start="0:0" size="32" type="uint"/>
-    <field name="FAU count" start="1:0" size="8" type="uint"/>
-    <field name="Resources" start="8:0" size="64" type="address"/>
-    <field name="Shader" start="10:0" size="64" type="address"/>
-    <field name="Thread storage" start="12:0" size="64" type="address"/>
-    <field name="FAU" start="14:0" size="64" type="address"/>
+    <field name="Attribute offset" size="32" start="0:0" type="uint"/>
+    <field name="FAU count" size="8" start="1:0" type="uint"/>
+    <field name="Resources" size="64" start="8:0" type="address"/>
+    <field name="Shader" size="64" start="10:0" type="address"/>
+    <field name="Thread storage" size="64" start="12:0" type="address"/>
+    <field name="FAU" size="64" start="14:0" type="address"/>
   </struct>
 
   <struct name="Compute Payload" size="24">
-    <field name="Workgroup size X" start="0:0" size="10" type="uint" modifier="minus(1)"/>
-    <field name="Workgroup size Y" start="0:10" size="10" type="uint" modifier="minus(1)"/>
-    <field name="Workgroup size Z" start="0:20" size="10" type="uint" modifier="minus(1)"/>
-    <field name="Allow merging workgroups" start="0:31" size="1" type="bool"/>
-    <field name="Task increment" start="1:0" size="14" type="uint" default="1"/>
-    <field name="Task axis" start="1:14" size="2" type="Task Axis"/>
-    <field name="Workgroup count X" start="2:0" size="32" type="uint"/>
-    <field name="Workgroup count Y" start="3:0" size="32" type="uint"/>
-    <field name="Workgroup count Z" start="4:0" size="32" type="uint"/>
-    <field name="Offset X" start="5:0" size="32" type="uint"/>
-    <field name="Offset Y" start="6:0" size="32" type="uint"/>
-    <field name="Offset Z" start="7:0" size="32" type="uint"/>
-    <field name="Compute" start="8:0" size="512" type="Shader Environment"/>
+    <field name="Workgroup size X" size="10" start="0:0" type="uint" modifier="minus(1)"/>
+    <field name="Workgroup size Y" size="10" start="0:10" type="uint" modifier="minus(1)"/>
+    <field name="Workgroup size Z" size="10" start="0:20" type="uint" modifier="minus(1)"/>
+    <field name="Allow merging workgroups" size="1" start="0:31" type="bool"/>
+    <field name="Task increment" size="14" start="1:0" type="uint" default="1"/>
+    <field name="Task axis" size="2" start="1:14" type="Task Axis"/>
+    <field name="Workgroup count X" size="32" start="2:0" type="uint"/>
+    <field name="Workgroup count Y" size="32" start="3:0" type="uint"/>
+    <field name="Workgroup count Z" size="32" start="4:0" type="uint"/>
+    <field name="Offset X" size="32" start="5:0" type="uint"/>
+    <field name="Offset Y" size="32" start="6:0" type="uint"/>
+    <field name="Offset Z" size="32" start="7:0" type="uint"/>
+    <field name="Compute" size="512" start="8:0" type="Shader Environment"/>
   </struct>
 
   <!-- Compute job also covers vertex and geometry operations -->
@@ -1340,9 +1334,9 @@
   </aggregate>
 
   <struct name="Resource" size="4" align="16">
-    <field name="Address" start="0:0" size="56" type="address"/>
-    <field name="Contains descriptors" start="1:24" size="1" type="bool" default="true"/>
-    <field name="Size" start="2:0" size="64" type="uint"/> <!-- bytes -->
+    <field name="Address" size="56" start="0:0" type="address"/>
+    <field name="Contains descriptors" size="1" start="1:24" type="bool" default="true"/>
+    <field name="Size" size="64" start="2:0" type="uint"/> <!-- bytes -->
   </struct>
 
   <struct name="Depth/stencil" size="8" align="32">
@@ -1363,8 +1357,8 @@
     <field name="Back value mask" size="8" start="1:24" type="hex"/>
     <field name="Front reference value" size="8" start="2:0" type="hex"/>
     <field name="Back reference value" size="8" start="2:8" type="hex"/>
-    <field name="Depth cull enable" size="1" start="4:22" type="bool" default="true"/>
-    <field name="Depth clamp mode" size="2" start="4:23" type="Depth Clamp Mode" default="[0, 1]"/>
+    <field name="Depth cull enable" size="1" start="4:22" type="bool"/>
+    <field name="Depth clamp mode" size="2" start="4:23" type="Depth Clamp Mode"/>
     <field name="Depth source" size="2" start="4:25" type="Depth Source" default="Fixed function"/>
     <field name="Depth write enable" size="1" start="4:27" type="bool"/>
     <field name="Depth bias enable" size="1" start="4:28" type="bool"/>
@@ -1374,6 +1368,7 @@
     <field name="Depth bias clamp" size="32" start="7:0" type="float"/>
   </struct>
 
+  <!-- TODO: Is this actually four words? -->
   <struct name="Vertex Array" size="3">
     <field name="Packet" size="1" start="0:0" type="bool"/>
 
@@ -1391,7 +1386,7 @@
     <field name="Pixel kill operation" size="2" start="0:2" type="Pixel Kill"/>
     <field name="ZS update operation" size="2" start="0:4" type="Pixel Kill"/>
     <field name="Allow primitive reorder" size="1" start="0:6" type="bool"/>
-    <field name="Overdraw alpha0" start="0:7" size="1" type="bool"/>
+    <field name="Overdraw alpha0" size="1" start="0:7" type="bool"/>
     <field name="Overdraw alpha1" size="1" start="0:8" type="bool"/>
     <field name="Clean Fragment Write" size="1" start="0:9" type="bool"/>
     <field name="Primitive Barrier" size="1" start="0:10" type="bool"/>
@@ -1407,24 +1402,24 @@
     <field name="Alpha-to-coverage" size="1" start="0:22" type="bool"/>
     <field name="Scissor to bounding box" size="1" start="0:23" type="bool"/>
     <field name="Sample mask" size="16" start="1:0" type="uint"/>
-    <field name="Render target mask" start="1:16" size="8" type="hex"/>
-    <field name="Vertex array" start="2:0" size="96" type="Vertex Array"/>
-    <field name="Minimum Z" start="6:0" size="32" type="float"/>
-    <field name="Maximum Z" start="7:0" size="32" type="float"/>
-    <field name="Depth/stencil" start="10:0" size="64" type="address"/>
-    <field name="Blend count" start="12:0" size="4" type="uint"/>
-    <field name="Blend" start="12:4" size="60" type="address" modifier="shr(4)"/>
+    <field name="Render target mask" size="8" start="1:16" type="hex"/>
+    <field name="Vertex array" size="96" start="2:0" type="Vertex Array"/>
+    <field name="Minimum Z" size="32" start="6:0" type="float"/>
+    <field name="Maximum Z" size="32" start="7:0" type="float"/>
+    <field name="Depth/stencil" size="64" start="10:0" type="address"/>
+    <field name="Blend count" size="4" start="12:0" type="uint"/>
+    <field name="Blend" size="60" start="12:4" type="address" modifier="shr(4)"/>
     <field name="Occlusion" size="64" start="14:0" type="address"/>
-    <field name="Shader" start="16:0" size="512" type="Shader Environment"/>
+    <field name="Shader" size="512" start="16:0" type="Shader Environment"/>
   </struct>
 
   <struct name="Count" size="1">
-    <field name="Count" start="0:0" size="32" type="uint"/>
+    <field name="Count" size="32" start="0:0" type="uint"/>
   </struct>
 
   <struct name="Allocation" size="1">
-    <field name="Vertex packet stride" start="0:0" size="16" type="uint"/>
-    <field name="Vertex attribute stride" start="0:16" size="16" type="uint"/>
+    <field name="Vertex packet stride" size="16" start="0:0" type="uint"/>
+    <field name="Vertex attribute stride" size="16" start="0:16" type="uint"/>
   </struct>
 
   <struct name="Tiler Pointer" size="2">
diff -urN mesa-23.0.0/src/panfrost/lib/meson.build mesa/src/panfrost/lib/meson.build
--- mesa-23.0.0/src/panfrost/lib/meson.build	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/meson.build	2023-03-06 17:54:35.855495079 +0100
@@ -39,7 +39,7 @@
 
 libpanfrost_per_arch = []
 
-foreach ver : ['4', '5', '6', '7', '9']
+foreach ver : ['4', '5', '6', '7', '9', '10']
   libpanfrost_per_arch += static_library(
     'pan-arch-v' + ver,
     [
@@ -59,7 +59,10 @@
 foreach ver : ['7']
   libpanfrost_per_arch += static_library(
     'pan-arch-indirect-v' + ver,
-    'pan_indirect_dispatch.c',
+    [
+      'pan_indirect_dispatch.c',
+      'pan_indirect_draw.c',
+    ],
     include_directories : [inc_include, inc_src, inc_mapi, inc_mesa, inc_gallium, inc_gallium_aux, inc_panfrost_hw],
     c_args : ['-DPAN_ARCH=' + ver],
     gnu_symbol_visibility : 'hidden',
@@ -90,7 +93,7 @@
   include_directories : [inc_include, inc_src, inc_mapi, inc_mesa, inc_gallium, inc_gallium_aux, inc_panfrost_hw],
   c_args : [no_override_init_args],
   gnu_symbol_visibility : 'hidden',
-  dependencies: [dep_libdrm, idep_nir, idep_mesautil],
+  dependencies: [dep_libdrm, idep_nir, libpanfrost_base_dep],
   build_by_default : false,
   link_with: [libpanfrost_pixel_format, libpanfrost_per_arch],
 )
diff -urN mesa-23.0.0/src/panfrost/lib/pan_afbc.c mesa/src/panfrost/lib/pan_afbc.c
--- mesa-23.0.0/src/panfrost/lib/pan_afbc.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/pan_afbc.c	2023-03-06 19:19:31.860303239 +0100
@@ -115,7 +115,7 @@
  * the canonical AFBC internal format if it exists, or NONE if the format
  * cannot be compressed. */
 
-enum pan_afbc_mode
+enum pipe_format
 panfrost_afbc_format(unsigned arch, enum pipe_format format)
 {
    /* sRGB does not change the pixel format itself, only the
@@ -125,42 +125,31 @@
     */
    format = util_format_linear(format);
 
-   /* Luminance-alpha not supported for AFBC on v7+ */
-   switch (format) {
-   case PIPE_FORMAT_A8_UNORM:
-   case PIPE_FORMAT_L8_UNORM:
-   case PIPE_FORMAT_I8_UNORM:
-   case PIPE_FORMAT_L8A8_UNORM:
-      if (arch >= 7)
-         return PAN_AFBC_MODE_INVALID;
-      else
-         break;
-   default:
-      break;
-   }
-
-   /* We handle swizzling orthogonally to AFBC */
+   /* Otherwise swizzling doesn't affect AFBC */
    format = unswizzled_format(format);
 
-   /* clang-format off */
    switch (format) {
-   case PIPE_FORMAT_R8_UNORM:          return PAN_AFBC_MODE_R8;
-   case PIPE_FORMAT_R8G8_UNORM:        return PAN_AFBC_MODE_R8G8;
-   case PIPE_FORMAT_R8G8B8_UNORM:      return PAN_AFBC_MODE_R8G8B8;
-   case PIPE_FORMAT_R8G8B8A8_UNORM:    return PAN_AFBC_MODE_R8G8B8A8;
-   case PIPE_FORMAT_R5G6B5_UNORM:      return PAN_AFBC_MODE_R5G6B5;
-   case PIPE_FORMAT_R5G5B5A1_UNORM:    return PAN_AFBC_MODE_R5G5B5A1;
-   case PIPE_FORMAT_R10G10B10A2_UNORM: return PAN_AFBC_MODE_R10G10B10A2;
-   case PIPE_FORMAT_R4G4B4A4_UNORM:    return PAN_AFBC_MODE_R4G4B4A4;
-   case PIPE_FORMAT_Z16_UNORM:         return PAN_AFBC_MODE_R8G8;
-
-   case PIPE_FORMAT_Z24_UNORM_S8_UINT: return PAN_AFBC_MODE_R8G8B8A8;
-   case PIPE_FORMAT_Z24X8_UNORM:       return PAN_AFBC_MODE_R8G8B8A8;
-   case PIPE_FORMAT_X24S8_UINT:        return PAN_AFBC_MODE_R8G8B8A8;
+   case PIPE_FORMAT_R8_UNORM:
+   case PIPE_FORMAT_R8G8_UNORM:
+   case PIPE_FORMAT_R8G8B8_UNORM:
+   case PIPE_FORMAT_R8G8B8A8_UNORM:
+   case PIPE_FORMAT_R5G6B5_UNORM:
+   case PIPE_FORMAT_R5G5B5A1_UNORM:
+   case PIPE_FORMAT_R10G10B10A2_UNORM:
+   case PIPE_FORMAT_R4G4B4A4_UNORM:
+      return format;
+
+   case PIPE_FORMAT_Z16_UNORM:
+      return PIPE_FORMAT_R8G8_UNORM;
+
+   case PIPE_FORMAT_Z24_UNORM_S8_UINT:
+   case PIPE_FORMAT_Z24X8_UNORM:
+   case PIPE_FORMAT_X24S8_UINT:
+      return PIPE_FORMAT_R8G8B8A8_UNORM;
 
-   default:                            return PAN_AFBC_MODE_INVALID;
+   default:
+      return PIPE_FORMAT_NONE;
    }
-   /* clang-format on */
 }
 
 /* A format may be compressed as AFBC if it has an AFBC internal format */
@@ -169,7 +158,7 @@
 panfrost_format_supports_afbc(const struct panfrost_device *dev,
                               enum pipe_format format)
 {
-   return panfrost_afbc_format(dev->arch, format) != PAN_AFBC_MODE_INVALID;
+   return panfrost_afbc_format(dev->arch, format) != PIPE_FORMAT_NONE;
 }
 
 /* The lossless colour transform (AFBC_FORMAT_MOD_YTR) requires RGB. */
@@ -196,3 +185,12 @@
 {
    return (dev->arch >= 7);
 }
+
+/*
+ * Can this format only be used with AFBC_FORMAT_MOD_NATIVE_SWIZZLE?
+ */
+bool
+panfrost_afbc_only_native(unsigned arch, enum pipe_format format)
+{
+   return (arch >= 7 && format != unswizzled_format(format));
+}
diff -urN mesa-23.0.0/src/panfrost/lib/pan_blend.c mesa/src/panfrost/lib/pan_blend.c
--- mesa-23.0.0/src/panfrost/lib/pan_blend.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/pan_blend.c	2023-03-06 19:19:31.792302789 +0100
@@ -634,8 +634,7 @@
       .logicop_enable = state->logicop_enable,
       .logicop_func = state->logicop_func,
       .rt[0].colormask = rt_state->equation.color_mask,
-      .format[0] = rt_state->format,
-   };
+      .format[0] = rt_state->format};
 
    if (!rt_state->equation.blend_enable) {
       static const nir_lower_blend_channel replace = {
diff -urN mesa-23.0.0/src/panfrost/lib/pan_blitter.c mesa/src/panfrost/lib/pan_blitter.c
--- mesa-23.0.0/src/panfrost/lib/pan_blitter.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/pan_blitter.c	2023-03-06 19:19:31.928303688 +0100
@@ -1156,7 +1156,7 @@
       pan_blitter_emit_blends(pool->dev, blit_shader, &views, NULL, blend.cpu);
    }
 
-   pan_pack(out, DRAW, cfg) {
+   pan_pack(out, DRAW_NO_CS, cfg) {
       if (zs) {
          /* ZS_EMIT requires late update/kill */
          cfg.zs_update_operation = MALI_PIXEL_KILL_FORCE_LATE;
@@ -1230,7 +1230,8 @@
    if (fb->bifrost.pre_post.dcds.gpu)
       return;
 
-   fb->bifrost.pre_post.dcds = pan_pool_alloc_desc_array(desc_pool, 3, DRAW);
+   fb->bifrost.pre_post.dcds =
+      pan_pool_alloc_desc_array(desc_pool, 3, DRAW_NO_CS);
 }
 
 static void
@@ -1241,7 +1242,7 @@
    unsigned dcd_idx = zs ? 1 : 0;
    pan_preload_fb_alloc_pre_post_dcds(desc_pool, fb);
    assert(fb->bifrost.pre_post.dcds.cpu);
-   void *dcd = fb->bifrost.pre_post.dcds.cpu + (dcd_idx * pan_size(DRAW));
+   void *dcd = fb->bifrost.pre_post.dcds.cpu + (dcd_idx * pan_size(DRAW_NO_CS));
 
    /* We only use crc_rt to determine whether to force writes for updating
     * the CRCs, so use a conservative tile size (16x16).
diff -urN mesa-23.0.0/src/panfrost/lib/pan_bo.c mesa/src/panfrost/lib/pan_bo.c
--- mesa-23.0.0/src/panfrost/lib/pan_bo.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/pan_bo.c	2023-03-06 19:19:32.071304632 +0100
@@ -37,6 +37,7 @@
 
 #include "util/os_mman.h"
 
+#include "util/os_file.h"
 #include "util/u_inlines.h"
 #include "util/u_math.h"
 
@@ -71,7 +72,39 @@
          create_bo.flags |= PANFROST_BO_NOEXEC;
    }
 
-   ret = drmIoctl(dev->fd, DRM_IOCTL_PANFROST_CREATE_BO, &create_bo);
+   void *cpu = NULL;
+
+   bool cached = false;
+
+   if (dev->kbase) {
+      if (flags & PAN_BO_CACHEABLE) {
+         if (!(dev->debug & PAN_DBG_UNCACHED_CPU)) {
+            create_bo.flags |= MALI_BO_CACHED_CPU;
+            /* TODO: What if kbase decides not to cache it? */
+            cached = true;
+         }
+         if (dev->debug & PAN_DBG_UNCACHED_GPU)
+            create_bo.flags |= MALI_BO_UNCACHED_GPU;
+      }
+
+      unsigned mali_flags = (flags & PAN_BO_EVENT) ? 0x8200f : 0;
+
+      struct base_ptr p =
+         dev->mali.alloc(&dev->mali, size, create_bo.flags, mali_flags);
+
+      if (p.gpu) {
+         cpu = p.cpu;
+         create_bo.offset = p.gpu;
+         create_bo.handle = kbase_alloc_gem_handle(&dev->mali, p.gpu, -1);
+         if (!cpu)
+            abort();
+         ret = 0;
+      } else {
+         ret = -1;
+      }
+   } else {
+      ret = drmIoctl(dev->fd, DRM_IOCTL_PANFROST_CREATE_BO, &create_bo);
+   }
    if (ret) {
       fprintf(stderr, "DRM_IOCTL_PANFROST_CREATE_BO failed: %m\n");
       return NULL;
@@ -82,29 +115,102 @@
 
    bo->size = create_bo.size;
    bo->ptr.gpu = create_bo.offset;
+   bo->ptr.cpu = cpu;
+   if ((uintptr_t)bo->ptr.cpu != bo->ptr.gpu)
+      bo->free_ioctl = true;
    bo->gem_handle = create_bo.handle;
    bo->flags = flags;
    bo->dev = dev;
    bo->label = label;
+   bo->cached = cached;
+   bo->dmabuf_fd = -1;
    return bo;
 }
 
 static void
 panfrost_bo_free(struct panfrost_bo *bo)
 {
+   struct panfrost_device *dev = bo->dev;
    struct drm_gem_close gem_close = {.handle = bo->gem_handle};
    int ret;
 
-   ret = drmIoctl(bo->dev->fd, DRM_IOCTL_GEM_CLOSE, &gem_close);
+   if (dev->bo_log) {
+      int fd = kbase_gem_handle_get(&dev->mali, bo->gem_handle).fd;
+
+      struct timespec tp;
+      clock_gettime(CLOCK_MONOTONIC_RAW, &tp);
+      fprintf(dev->bo_log,
+              "%" PRIu64 ".%09li memfree %" PRIx64 " to %" PRIx64
+              " size %zu label %s obj (%p,%i,%i)\n",
+              (uint64_t)tp.tv_sec, tp.tv_nsec, bo->ptr.gpu,
+              bo->ptr.gpu + bo->size, bo->size, bo->label, bo, bo->gem_handle,
+              fd);
+      fflush(NULL);
+   }
+
+   if (dev->kbase) {
+      os_munmap(bo->ptr.cpu, bo->size);
+      if (bo->munmap_ptr)
+         os_munmap(bo->munmap_ptr, bo->size);
+      if (bo->free_ioctl)
+         dev->mali.free(&dev->mali, bo->ptr.gpu);
+      kbase_free_gem_handle(&dev->mali, bo->gem_handle);
+      ret = 0;
+   } else {
+      ret = drmIoctl(bo->dev->fd, DRM_IOCTL_GEM_CLOSE, &gem_close);
+   }
    if (ret) {
       fprintf(stderr, "DRM_IOCTL_GEM_CLOSE failed: %m\n");
       assert(0);
    }
 
-   /* BO will be freed with the sparse array, but zero to indicate free */
+   /* BO will be freed with the stable_array, but zero to indicate free */
    memset(bo, 0, sizeof(*bo));
 }
 
+static bool
+panfrost_bo_usage_finished(struct panfrost_bo *bo, bool readers)
+{
+   struct panfrost_device *dev = bo->dev;
+   kbase k = &dev->mali;
+
+   bool ret = true;
+
+   pthread_mutex_lock(&dev->bo_usage_lock);
+   pthread_mutex_lock(&dev->mali.queue_lock);
+
+   util_dynarray_foreach(&bo->usage, struct panfrost_usage, u) {
+      /* Skip if we are only waiting for writers */
+      if (!u->write && !readers)
+         continue;
+
+      /* Usages are ordered, so everything else is also invalid */
+      if (u->queue >= k->event_slot_usage)
+         break;
+
+      struct kbase_event_slot *slot = &k->event_slots[u->queue];
+      uint64_t seqnum = u->seqnum;
+
+      /* There is a race condition, where we can depend on an
+       * unsubmitted batch. In that cade, decrease the seqnum.
+       * Otherwise, skip invalid dependencies. TODO: do GC? */
+      if (slot->last_submit == seqnum)
+         --seqnum;
+      else if (slot->last_submit < seqnum)
+         continue;
+
+      if (slot->last <= seqnum) {
+         ret = false;
+         break;
+      }
+   }
+
+   pthread_mutex_unlock(&dev->mali.queue_lock);
+   pthread_mutex_unlock(&dev->bo_usage_lock);
+
+   return ret;
+}
+
 /* Returns true if the BO is ready, false otherwise.
  * access_type is encoding the type of access one wants to ensure is done.
  * Waiting is always done for writers, but if wait_readers is set then readers
@@ -113,12 +219,15 @@
 bool
 panfrost_bo_wait(struct panfrost_bo *bo, int64_t timeout_ns, bool wait_readers)
 {
+   struct panfrost_device *dev = bo->dev;
    struct drm_panfrost_wait_bo req = {
       .handle = bo->gem_handle,
       .timeout_ns = timeout_ns,
    };
    int ret;
 
+   /* TODO: With driver-handled sync, is gpu_access even worth it? */
+
    /* If the BO has been exported or imported we can't rely on the cached
     * state, we need to call the WAIT_BO ioctl.
     */
@@ -134,10 +243,30 @@
          return true;
    }
 
+   if (dev->kbase && (dev->arch >= 10)) {
+      struct kbase_wait_ctx wait = kbase_wait_init(&dev->mali, timeout_ns);
+      while (kbase_wait_for_event(&wait)) {
+         if (panfrost_bo_usage_finished(bo, wait_readers))
+            break;
+      }
+      kbase_wait_fini(wait);
+
+      bool ret = panfrost_bo_usage_finished(bo, wait_readers);
+      if (bo->flags & PAN_BO_SHARED)
+         ret &= kbase_poll_fd_until(bo->dmabuf_fd, wait_readers, wait.until);
+
+      if (ret)
+         bo->gpu_access &= (wait_readers ? 0 : PAN_BO_ACCESS_READ);
+      return ret;
+   }
+
    /* The ioctl returns >= 0 value when the BO we are waiting for is ready
     * -1 otherwise.
     */
-   ret = drmIoctl(bo->dev->fd, DRM_IOCTL_PANFROST_WAIT_BO, &req);
+   if (dev->kbase)
+      ret = kbase_wait_bo(&dev->mali, bo->gem_handle, timeout_ns, wait_readers);
+   else
+      ret = drmIoctl(dev->fd, DRM_IOCTL_PANFROST_WAIT_BO, &req);
    if (ret != -1) {
       /* Set gpu_access to 0 so that the next call to bo_wait()
        * doesn't have to call the WAIT_BO ioctl.
@@ -153,6 +282,33 @@
    return false;
 }
 
+static void
+panfrost_bo_mem_op(struct panfrost_bo *bo, size_t offset, size_t length,
+                   bool invalidate)
+{
+   struct panfrost_device *dev = bo->dev;
+
+   assert(offset + length <= bo->size);
+
+   if (!bo->cached)
+      return;
+
+   dev->mali.mem_sync(&dev->mali, bo->ptr.gpu, bo->ptr.cpu + offset, length,
+                      invalidate);
+}
+
+void
+panfrost_bo_mem_invalidate(struct panfrost_bo *bo, size_t offset, size_t length)
+{
+   panfrost_bo_mem_op(bo, offset, length, true);
+}
+
+void
+panfrost_bo_mem_clean(struct panfrost_bo *bo, size_t offset, size_t length)
+{
+   panfrost_bo_mem_op(bo, offset, length, false);
+}
+
 /* Helper to calculate the bucket index of a BO */
 
 static unsigned
@@ -197,20 +353,31 @@
 
       /* If the oldest BO in the cache is busy, likely so is
        * everything newer, so bail. */
-      if (!panfrost_bo_wait(entry, dontwait ? 0 : INT64_MAX, PAN_BO_ACCESS_RW))
-         break;
+
+      /* For kbase, BOs are not added to the cache until the GPU is
+       * done with them, so there is no need to wait. */
+      if (!dev->kbase) {
+         if (!panfrost_bo_wait(entry, dontwait ? 0 : INT64_MAX,
+                               PAN_BO_ACCESS_RW))
+            break;
+      }
 
       struct drm_panfrost_madvise madv = {
          .handle = entry->gem_handle,
          .madv = PANFROST_MADV_WILLNEED,
       };
-      int ret;
+      int ret = 0;
 
       /* This one works, splice it out of the cache */
       list_del(&entry->bucket_link);
       list_del(&entry->lru_link);
 
-      ret = drmIoctl(dev->fd, DRM_IOCTL_PANFROST_MADVISE, &madv);
+      if (dev->kbase) {
+         /* With kbase, BOs are never freed from the cache */
+         madv.retained = true;
+      } else {
+         ret = drmIoctl(dev->fd, DRM_IOCTL_PANFROST_MADVISE, &madv);
+      }
       if (!ret && !madv.retained) {
          panfrost_bo_free(entry);
          continue;
@@ -272,7 +439,10 @@
    madv.madv = PANFROST_MADV_DONTNEED;
    madv.retained = 0;
 
-   drmIoctl(dev->fd, DRM_IOCTL_PANFROST_MADVISE, &madv);
+   // TODO: Allow freeing madvise'd BOs with kbase... not that it really
+   // matters for boards with 16 GB RAM
+   if (!dev->kbase)
+      drmIoctl(dev->fd, DRM_IOCTL_PANFROST_MADVISE, &madv);
 
    /* Add us to the bucket */
    list_addtail(&bo->bucket_link, bucket);
@@ -282,6 +452,10 @@
    clock_gettime(CLOCK_MONOTONIC, &time);
    bo->last_used = time.tv_sec;
 
+   /* For kbase, the GPU can't be accessing this BO any more */
+   if (dev->kbase)
+      bo->gpu_access = 0;
+
    /* Let's do some cleanup in the BO cache while we hold the
     * lock.
     */
@@ -346,10 +520,15 @@
 static void
 panfrost_bo_munmap(struct panfrost_bo *bo)
 {
+   /* We can't munmap BOs when using kbase, as that frees the storage and
+    * the GPU might still be using the BO. */
+   if (bo->dev->kbase)
+      return;
+
    if (!bo->ptr.cpu)
       return;
 
-   if (os_munmap((void *)(uintptr_t)bo->ptr.cpu, bo->size)) {
+   if (os_munmap(bo->ptr.cpu, bo->size)) {
       perror("munmap");
       abort();
    }
@@ -384,8 +563,15 @@
    if (!bo)
       bo = panfrost_bo_cache_fetch(dev, size, flags, label, false);
    if (!bo) {
-      panfrost_bo_cache_evict_all(dev);
-      bo = panfrost_bo_alloc(dev, size, flags, label);
+      for (unsigned i = 0; i < 5; ++i) {
+         usleep(20 * 1000 * i * i);
+         if (dev->kbase)
+            kbase_ensure_handle_events(&dev->mali);
+         panfrost_bo_cache_evict_all(dev);
+         bo = panfrost_bo_alloc(dev, size, flags, label);
+         if (bo)
+            break;
+      }
    }
 
    if (!bo) {
@@ -400,8 +586,15 @@
    if (!(flags & (PAN_BO_INVISIBLE | PAN_BO_DELAY_MMAP)))
       panfrost_bo_mmap(bo);
 
+   if ((dev->debug & PAN_DBG_BO_CLEAR) && !(flags & PAN_BO_INVISIBLE)) {
+      memset(bo->ptr.cpu, 0, bo->size);
+      panfrost_bo_mem_clean(bo, 0, bo->size);
+   }
+
    p_atomic_set(&bo->refcnt, 1);
 
+   util_dynarray_init(&bo->usage, NULL);
+
    if (dev->debug & (PAN_DBG_TRACE | PAN_DBG_SYNC)) {
       if (flags & PAN_BO_INVISIBLE)
          pandecode_inject_mmap(bo->ptr.gpu, NULL, bo->size, NULL);
@@ -409,6 +602,17 @@
          pandecode_inject_mmap(bo->ptr.gpu, bo->ptr.cpu, bo->size, NULL);
    }
 
+   if (dev->bo_log) {
+      struct timespec tp;
+      clock_gettime(CLOCK_MONOTONIC_RAW, &tp);
+      fprintf(dev->bo_log,
+              "%" PRIu64 ".%09li alloc %" PRIx64 " to %" PRIx64
+              " size %zu label %s\n",
+              (uint64_t)tp.tv_sec, tp.tv_nsec, bo->ptr.gpu,
+              bo->ptr.gpu + bo->size, bo->size, bo->label);
+      fflush(NULL);
+   }
+
    return bo;
 }
 
@@ -421,6 +625,63 @@
    }
 }
 
+static void
+panfrost_bo_fini(struct panfrost_bo *bo)
+{
+   struct panfrost_device *dev = bo->dev;
+
+   /* When the reference count goes to zero, we need to cleanup */
+   panfrost_bo_munmap(bo);
+
+   if (dev->debug & (PAN_DBG_TRACE | PAN_DBG_SYNC))
+      pandecode_inject_free(bo->ptr.gpu, bo->size);
+
+   /* Rather than freeing the BO now, we'll cache the BO for later
+    * allocations if we're allowed to.
+    */
+   if (!panfrost_bo_cache_put(bo))
+      panfrost_bo_free(bo);
+}
+
+static void
+panfrost_bo_free_gpu(void *data)
+{
+   struct panfrost_bo *bo = data;
+   struct panfrost_device *dev = bo->dev;
+
+   /* Don't free if there are still references */
+   if (p_atomic_dec_return(&bo->gpu_refcnt))
+      return;
+
+   pthread_mutex_lock(&dev->bo_map_lock);
+
+   /* Someone might have imported this BO while we were waiting for the
+    * lock, let's make sure it's still not referenced before freeing it.
+    */
+   if (p_atomic_read(&bo->refcnt) != 0) {
+      pthread_mutex_unlock(&dev->bo_map_lock);
+      return;
+   }
+
+   if (dev->bo_log) {
+      int fd = kbase_gem_handle_get(&dev->mali, bo->gem_handle).fd;
+
+      struct timespec tp;
+      clock_gettime(CLOCK_MONOTONIC_RAW, &tp);
+      fprintf(dev->bo_log,
+              "%" PRIu64 ".%09li gpufree %" PRIx64 " to %" PRIx64
+              " size %zu label %s obj (%p,%i,%i)\n",
+              (uint64_t)tp.tv_sec, tp.tv_nsec, bo->ptr.gpu,
+              bo->ptr.gpu + bo->size, bo->size, bo->label, bo, bo->gem_handle,
+              fd);
+      fflush(NULL);
+   }
+
+   panfrost_bo_fini(bo);
+
+   pthread_mutex_unlock(&dev->bo_map_lock);
+}
+
 void
 panfrost_bo_unreference(struct panfrost_bo *bo)
 {
@@ -433,24 +694,63 @@
 
    struct panfrost_device *dev = bo->dev;
 
+   if (dev->bo_log) {
+      int fd = kbase_gem_handle_get(&dev->mali, bo->gem_handle).fd;
+
+      struct timespec tp;
+      clock_gettime(CLOCK_MONOTONIC_RAW, &tp);
+      fprintf(dev->bo_log,
+              "%" PRIu64 ".%09li free %" PRIx64 " to %" PRIx64
+              " size %zu label %s obj (%p,%i,%i)\n",
+              (uint64_t)tp.tv_sec, tp.tv_nsec, bo->ptr.gpu,
+              bo->ptr.gpu + bo->size, bo->size, bo->label, bo, bo->gem_handle,
+              fd);
+      fflush(NULL);
+   }
+
    pthread_mutex_lock(&dev->bo_map_lock);
 
    /* Someone might have imported this BO while we were waiting for the
     * lock, let's make sure it's still not referenced before freeing it.
     */
-   if (p_atomic_read(&bo->refcnt) == 0) {
-      /* When the reference count goes to zero, we need to cleanup */
-      panfrost_bo_munmap(bo);
+   if (p_atomic_read(&bo->refcnt) != 0) {
+      pthread_mutex_unlock(&dev->bo_map_lock);
+      return;
+   }
 
-      if (dev->debug & (PAN_DBG_TRACE | PAN_DBG_SYNC))
-         pandecode_inject_free(bo->ptr.gpu, bo->size);
+   util_dynarray_fini(&bo->usage);
 
-      /* Rather than freeing the BO now, we'll cache the BO for later
-       * allocations if we're allowed to.
+   if (dev->kbase) {
+      /* Assume that all queues are using this BO, and so free the
+       * BO only after all currently-submitted jobs have finished.
+       * This could eventually be optimised to only wait on a subset
+       * of queues.
        */
-      if (!panfrost_bo_cache_put(bo))
-         panfrost_bo_free(bo);
+      bool added = dev->mali.callback_all_queues(&dev->mali, &bo->gpu_refcnt,
+                                                 panfrost_bo_free_gpu, bo);
+
+      if (added) {
+         pthread_mutex_unlock(&dev->bo_map_lock);
+         return;
+      }
+   }
+
+   if (dev->bo_log) {
+      int fd = kbase_gem_handle_get(&dev->mali, bo->gem_handle).fd;
+
+      struct timespec tp;
+      clock_gettime(CLOCK_MONOTONIC_RAW, &tp);
+      fprintf(dev->bo_log,
+              "%" PRIu64 ".%09li immfree %" PRIx64 " to %" PRIx64
+              " size %zu label %s obj (%p,%i,%i)\n",
+              (uint64_t)tp.tv_sec, tp.tv_nsec, bo->ptr.gpu,
+              bo->ptr.gpu + bo->size, bo->size, bo->label, bo, bo->gem_handle,
+              fd);
+      fflush(NULL);
    }
+
+   panfrost_bo_fini(bo);
+
    pthread_mutex_unlock(&dev->bo_map_lock);
 }
 
@@ -462,23 +762,44 @@
       0,
    };
    ASSERTED int ret;
+   kbase_handle handle = {.fd = -1};
    unsigned gem_handle;
 
-   pthread_mutex_lock(&dev->bo_map_lock);
- 
-   ret = drmPrimeFDToHandle(dev->fd, fd, &gem_handle);
-   assert(!ret);
+   if (dev->kbase) {
+      gem_handle = dev->mali.import_dmabuf(&dev->mali, fd);
+      if (gem_handle == -1)
+         return NULL;
+   } else {
+      ret = drmPrimeFDToHandle(dev->fd, fd, &gem_handle);
+      assert(!ret);
+   }
 
+   pthread_mutex_lock(&dev->bo_map_lock);
    bo = pan_lookup_bo(dev, gem_handle);
 
+   bool found = false;
+
    if (!bo->dev) {
       get_bo_offset.handle = gem_handle;
-      ret = drmIoctl(dev->fd, DRM_IOCTL_PANFROST_GET_BO_OFFSET, &get_bo_offset);
-      assert(!ret);
+      if (dev->kbase) {
+         handle = kbase_gem_handle_get(&dev->mali, gem_handle);
+         get_bo_offset.offset = handle.va;
+      } else {
+         ret =
+            drmIoctl(dev->fd, DRM_IOCTL_PANFROST_GET_BO_OFFSET, &get_bo_offset);
+         assert(!ret);
+      }
 
       bo->dev = dev;
-      bo->ptr.gpu = (mali_ptr)get_bo_offset.offset;
       bo->size = lseek(fd, 0, SEEK_END);
+      bo->ptr.gpu = (mali_ptr)get_bo_offset.offset;
+      if (dev->kbase &&
+          (sizeof(void *) > 4 || get_bo_offset.offset < (1LL << 32))) {
+         bo->ptr.cpu = (void *)(uintptr_t)get_bo_offset.offset;
+      } else if (dev->kbase) {
+         bo->ptr.cpu = dev->mali.mmap_import(&dev->mali, bo->ptr.gpu, bo->size);
+         bo->free_ioctl = true;
+      }
       /* Sometimes this can fail and return -1. size of -1 is not
        * a nice thing for mmap to try mmap. Be more robust also
        * for zero sized maps and fail nicely too
@@ -489,8 +810,21 @@
       }
       bo->flags = PAN_BO_SHARED;
       bo->gem_handle = gem_handle;
+      util_dynarray_init(&bo->usage, NULL);
+      if (dev->kbase) {
+         /* kbase always maps dma-bufs with caching */
+         bo->cached = true;
+
+         /* Importing duplicates the FD, so we cache the FD
+          * from the handle */
+         bo->dmabuf_fd = handle.fd;
+      } else {
+         bo->dmabuf_fd = -1;
+      }
       p_atomic_set(&bo->refcnt, 1);
    } else {
+      found = true;
+
       /* bo->refcnt == 0 can happen if the BO
        * was being released but panfrost_bo_import() acquired the
        * lock before panfrost_bo_unreference(). In that case, refcnt
@@ -508,12 +842,36 @@
    }
    pthread_mutex_unlock(&dev->bo_map_lock);
 
+   if (dev->bo_log) {
+      int new_fd = kbase_gem_handle_get(&dev->mali, bo->gem_handle).fd;
+
+      struct timespec tp;
+      clock_gettime(CLOCK_MONOTONIC_RAW, &tp);
+      fprintf(dev->bo_log,
+              "%" PRIu64 ".%09li import %" PRIx64 " to %" PRIx64
+              " size %zu fd %i new %i handle %i found %i\n",
+              (uint64_t)tp.tv_sec, tp.tv_nsec, bo->ptr.gpu,
+              bo->ptr.gpu + bo->size, bo->size, fd, new_fd, gem_handle, found);
+      fflush(NULL);
+   }
+
    return bo;
 }
 
 int
 panfrost_bo_export(struct panfrost_bo *bo)
 {
+   struct panfrost_device *dev = bo->dev;
+
+   if (bo->dmabuf_fd != -1) {
+      assert(bo->flags & PAN_BO_SHARED);
+
+      return os_dupfd_cloexec(bo->dmabuf_fd);
+   }
+
+   if (dev->kbase)
+      return -1;
+
    struct drm_prime_handle args = {
       .handle = bo->gem_handle,
       .flags = DRM_CLOEXEC,
diff -urN mesa-23.0.0/src/panfrost/lib/pan_bo.h mesa/src/panfrost/lib/pan_bo.h
--- mesa-23.0.0/src/panfrost/lib/pan_bo.h	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/pan_bo.h	2023-03-06 19:19:31.797302823 +0100
@@ -28,6 +28,7 @@
 
 #include <time.h>
 #include "util/list.h"
+#include "util/u_dynarray.h"
 #include "panfrost-job.h"
 
 /* Flags for allocated memory */
@@ -50,6 +51,12 @@
  * cached locally */
 #define PAN_BO_SHARED (1 << 4)
 
+/* Use event memory, required for CSF events to be signaled to the kernel */
+#define PAN_BO_EVENT (1 << 5)
+
+/* Use the caching policy for resource BOs */
+#define PAN_BO_CACHEABLE (1 << 6)
+
 /* GPU access flags */
 
 /* BO is either shared (can be accessed by more than one GPU batch) or private
@@ -80,6 +87,12 @@
    mali_ptr gpu;
 };
 
+struct panfrost_usage {
+   uint32_t queue;
+   bool write;
+   uint64_t seqnum;
+};
+
 struct panfrost_bo {
    /* Must be first for casting */
    struct list_head bucket_link;
@@ -95,11 +108,16 @@
    /* Atomic reference count */
    int32_t refcnt;
 
+   /* Reference count for GPU jobs */
+   int32_t gpu_refcnt;
+
    struct panfrost_device *dev;
 
    /* Mapping for the entire object (all levels) */
    struct panfrost_ptr ptr;
 
+   struct util_dynarray usage;
+
    /* Size of all entire trees */
    size_t size;
 
@@ -115,10 +133,30 @@
 
    /* Human readable description of the BO for debugging. */
    const char *label;
+
+   /* Sometimes we don't access the BO through kbase's mapping of the
+    * memory, in that case we need to save the pointer to pass to
+    * munmap to avoid leaking memory. */
+   void *munmap_ptr;
+
+   /* For 32-bit applications we may not even be able to that, because
+    * the VA may be too high for kbase to map to an equivalent CPU
+    * address, in which case we must use the memory free icotl. */
+   bool free_ioctl;
+
+   /* Is the BO cached CPU-side? */
+   bool cached;
+
+   /* File descriptor for the dma-buf */
+   int dmabuf_fd;
 };
 
 bool panfrost_bo_wait(struct panfrost_bo *bo, int64_t timeout_ns,
                       bool wait_readers);
+void panfrost_bo_mem_invalidate(struct panfrost_bo *bo, size_t offset,
+                                size_t length);
+void panfrost_bo_mem_clean(struct panfrost_bo *bo, size_t offset,
+                           size_t length);
 void panfrost_bo_reference(struct panfrost_bo *bo);
 void panfrost_bo_unreference(struct panfrost_bo *bo);
 struct panfrost_bo *panfrost_bo_create(struct panfrost_device *dev, size_t size,
diff -urN mesa-23.0.0/src/panfrost/lib/pan_clear.c mesa/src/panfrost/lib/pan_clear.c
--- mesa-23.0.0/src/panfrost/lib/pan_clear.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/pan_clear.c	2023-03-06 19:19:31.808302895 +0100
@@ -79,16 +79,15 @@
    unsigned int_a, frac_a;
 };
 
-/* clang-format off */
 static const struct mali_tib_layout tib_layouts[] = {
-   [MALI_COLOR_BUFFER_INTERNAL_FORMAT_R8G8B8A8]    = {  8, 0,  8, 0,  8, 0, 8, 0 },
-   [MALI_COLOR_BUFFER_INTERNAL_FORMAT_R10G10B10A2] = { 10, 0, 10, 0, 10, 0, 2, 0 },
-   [MALI_COLOR_BUFFER_INTERNAL_FORMAT_R8G8B8A2]    = {  8, 2,  8, 2,  8, 2, 2, 0 },
-   [MALI_COLOR_BUFFER_INTERNAL_FORMAT_R4G4B4A4]    = {  4, 4,  4, 4,  4, 4, 4, 4 },
-   [MALI_COLOR_BUFFER_INTERNAL_FORMAT_R5G6B5A0]    = {  5, 5,  6, 4,  5, 5, 0, 2 },
-   [MALI_COLOR_BUFFER_INTERNAL_FORMAT_R5G5B5A1]    = {  5, 5,  5, 5,  5, 5, 1, 1 },
+   [MALI_COLOR_BUFFER_INTERNAL_FORMAT_R8G8B8A8] = {8, 0, 8, 0, 8, 0, 8, 0},
+   [MALI_COLOR_BUFFER_INTERNAL_FORMAT_R10G10B10A2] = {10, 0, 10, 0, 10, 0, 2,
+                                                      0},
+   [MALI_COLOR_BUFFER_INTERNAL_FORMAT_R8G8B8A2] = {8, 2, 8, 2, 8, 2, 2, 0},
+   [MALI_COLOR_BUFFER_INTERNAL_FORMAT_R4G4B4A4] = {4, 4, 4, 4, 4, 4, 4, 4},
+   [MALI_COLOR_BUFFER_INTERNAL_FORMAT_R5G6B5A0] = {5, 5, 6, 4, 5, 5, 0, 2},
+   [MALI_COLOR_BUFFER_INTERNAL_FORMAT_R5G5B5A1] = {5, 5, 5, 5, 5, 5, 1, 1},
 };
-/* clang-format on */
 
 /* Raw values are stored as-is but replicated for multisampling */
 
diff -urN mesa-23.0.0/src/panfrost/lib/pan_cs.c mesa/src/panfrost/lib/pan_cs.c
--- mesa-23.0.0/src/panfrost/lib/pan_cs.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/pan_cs.c	2023-03-06 19:19:32.096304797 +0100
@@ -297,9 +297,15 @@
    ext->crc_render_target = rt_crc;
 
    if (fb->rts[rt_crc].clear) {
+#if PAN_ARCH < 10
+      // todo v10
       uint32_t clear_val = fb->rts[rt_crc].clear_value[0];
       ext->crc_clear_color = clear_val | 0xc000000000000000 |
                              (((uint64_t)clear_val & 0xffff) << 32);
+#else
+      // TODO: Is this correct?
+      ext->crc_unk = 0x1f;
+#endif
    }
 #endif
 }
@@ -369,27 +375,42 @@
 static enum mali_color_format
 pan_mfbd_raw_format(unsigned bits)
 {
-   /* clang-format off */
    switch (bits) {
-   case    8: return MALI_COLOR_FORMAT_RAW8;
-   case   16: return MALI_COLOR_FORMAT_RAW16;
-   case   24: return MALI_COLOR_FORMAT_RAW24;
-   case   32: return MALI_COLOR_FORMAT_RAW32;
-   case   48: return MALI_COLOR_FORMAT_RAW48;
-   case   64: return MALI_COLOR_FORMAT_RAW64;
-   case   96: return MALI_COLOR_FORMAT_RAW96;
-   case  128: return MALI_COLOR_FORMAT_RAW128;
-   case  192: return MALI_COLOR_FORMAT_RAW192;
-   case  256: return MALI_COLOR_FORMAT_RAW256;
-   case  384: return MALI_COLOR_FORMAT_RAW384;
-   case  512: return MALI_COLOR_FORMAT_RAW512;
-   case  768: return MALI_COLOR_FORMAT_RAW768;
-   case 1024: return MALI_COLOR_FORMAT_RAW1024;
-   case 1536: return MALI_COLOR_FORMAT_RAW1536;
-   case 2048: return MALI_COLOR_FORMAT_RAW2048;
-   default: unreachable("invalid raw bpp");
+   case 8:
+      return MALI_COLOR_FORMAT_RAW8;
+   case 16:
+      return MALI_COLOR_FORMAT_RAW16;
+   case 24:
+      return MALI_COLOR_FORMAT_RAW24;
+   case 32:
+      return MALI_COLOR_FORMAT_RAW32;
+   case 48:
+      return MALI_COLOR_FORMAT_RAW48;
+   case 64:
+      return MALI_COLOR_FORMAT_RAW64;
+   case 96:
+      return MALI_COLOR_FORMAT_RAW96;
+   case 128:
+      return MALI_COLOR_FORMAT_RAW128;
+   case 192:
+      return MALI_COLOR_FORMAT_RAW192;
+   case 256:
+      return MALI_COLOR_FORMAT_RAW256;
+   case 384:
+      return MALI_COLOR_FORMAT_RAW384;
+   case 512:
+      return MALI_COLOR_FORMAT_RAW512;
+   case 768:
+      return MALI_COLOR_FORMAT_RAW768;
+   case 1024:
+      return MALI_COLOR_FORMAT_RAW1024;
+   case 1536:
+      return MALI_COLOR_FORMAT_RAW1536;
+   case 2048:
+      return MALI_COLOR_FORMAT_RAW2048;
+   default:
+      unreachable("invalid raw bpp");
    }
-   /* clang-format on */
 }
 
 static void
@@ -438,7 +459,8 @@
    cfg->swizzle = panfrost_translate_swizzle_4(swizzle);
 }
 
-#if PAN_ARCH >= 9
+/* Don't define for later gens as this is not a GENX function */
+#if PAN_ARCH == 9
 enum mali_afbc_compression_mode
 pan_afbc_compression_mode(enum pipe_format format)
 {
@@ -452,23 +474,38 @@
     * needs to handle the subset of formats returned by
     * panfrost_afbc_format.
     */
-   /* clang-format off */
    switch (panfrost_afbc_format(PAN_ARCH, format)) {
-   case PAN_AFBC_MODE_R8:          return MALI_AFBC_COMPRESSION_MODE_R8;
-   case PAN_AFBC_MODE_R8G8:        return MALI_AFBC_COMPRESSION_MODE_R8G8;
-   case PAN_AFBC_MODE_R5G6B5:      return MALI_AFBC_COMPRESSION_MODE_R5G6B5;
-   case PAN_AFBC_MODE_R4G4B4A4:    return MALI_AFBC_COMPRESSION_MODE_R4G4B4A4;
-   case PAN_AFBC_MODE_R5G5B5A1:    return MALI_AFBC_COMPRESSION_MODE_R5G5B5A1;
-   case PAN_AFBC_MODE_R8G8B8:      return MALI_AFBC_COMPRESSION_MODE_R8G8B8;
-   case PAN_AFBC_MODE_R8G8B8A8:    return MALI_AFBC_COMPRESSION_MODE_R8G8B8A8;
-   case PAN_AFBC_MODE_R10G10B10A2: return MALI_AFBC_COMPRESSION_MODE_R10G10B10A2;
-   case PAN_AFBC_MODE_R11G11B10:   return MALI_AFBC_COMPRESSION_MODE_R11G11B10;
-   case PAN_AFBC_MODE_S8:          return MALI_AFBC_COMPRESSION_MODE_S8;
-   case PAN_AFBC_MODE_INVALID:     unreachable("Invalid AFBC format");
+   case PIPE_FORMAT_R8_UNORM:
+      return MALI_AFBC_COMPRESSION_MODE_R8;
+   case PIPE_FORMAT_R8G8_UNORM:
+      return MALI_AFBC_COMPRESSION_MODE_R8G8;
+   case PIPE_FORMAT_R5G6B5_UNORM:
+      return MALI_AFBC_COMPRESSION_MODE_R5G6B5;
+   case PIPE_FORMAT_R5G5B5A1_UNORM:
+      return MALI_AFBC_COMPRESSION_MODE_R5G5B5A1;
+   case PIPE_FORMAT_R4G4B4A4_UNORM:
+      return MALI_AFBC_COMPRESSION_MODE_R4G4B4A4;
+   case PIPE_FORMAT_R8G8B8_UNORM:
+      return MALI_AFBC_COMPRESSION_MODE_R8G8B8;
+   case PIPE_FORMAT_R8G8B8A8_UNORM:
+      return MALI_AFBC_COMPRESSION_MODE_R8G8B8A8;
+   case PIPE_FORMAT_R10G10B10A2_UNORM:
+      return MALI_AFBC_COMPRESSION_MODE_R10G10B10A2;
+   case PIPE_FORMAT_R11G11B10_FLOAT:
+      return MALI_AFBC_COMPRESSION_MODE_R11G11B10;
+   case PIPE_FORMAT_S8_UINT:
+      return MALI_AFBC_COMPRESSION_MODE_S8;
+   case PIPE_FORMAT_NONE:
+      fprintf(stderr, "invalid format for AFBC: %s\n",
+              util_format_name(format));
+      fflush(NULL);
+      abort();
+   default:
+      fprintf(stderr, "unknown canonical AFBC format: %s\n",
+              util_format_name(format));
+      fflush(NULL);
+      abort();
    }
-   /* clang-format on */
-
-   unreachable("all AFBC formats handled");
 }
 #endif
 
@@ -580,6 +617,7 @@
           */
          cfg.tls_address_mode = MALI_ADDRESS_MODE_PACKED;
 
+         /* The shift is only used for packed mode */
          assert((info->tls.ptr & 4095) == 0);
          cfg.tls_base_pointer = info->tls.ptr >> 8;
 #else
@@ -744,6 +782,9 @@
 #if PAN_ARCH >= 6
       bool force_clean_write = pan_force_clean_write(fb, tile_size);
 
+#if PAN_ARCH >= 9
+      cfg.frame_argument = 0x10000;
+#endif
       cfg.sample_locations =
          panfrost_sample_positions(dev, pan_sample_pattern(fb->nr_samples));
       cfg.pre_frame_0 = pan_fix_frame_shader_mode(fb->bifrost.pre_post.modes[0],
@@ -955,7 +996,7 @@
    pan_pack(out, TILER_HEAP, heap) {
       heap.size = dev->tiler_heap->size;
       heap.base = dev->tiler_heap->ptr.gpu;
-      heap.bottom = dev->tiler_heap->ptr.gpu;
+      heap.bottom = dev->tiler_heap->ptr.gpu + 64;
       heap.top = dev->tiler_heap->ptr.gpu + dev->tiler_heap->size;
    }
 }
@@ -963,30 +1004,39 @@
 void
 GENX(pan_emit_tiler_ctx)(const struct panfrost_device *dev, unsigned fb_width,
                          unsigned fb_height, unsigned nr_samples,
-                         bool first_provoking_vertex, mali_ptr heap, void *out)
+                         bool first_provoking_vertex, mali_ptr heap,
+                         mali_ptr scratch, void *out)
 {
    unsigned max_levels = dev->tiler_features.max_levels;
    assert(max_levels >= 2);
 
    pan_pack(out, TILER_CONTEXT, tiler) {
-      /* TODO: Select hierarchy mask more effectively */
-      tiler.hierarchy_mask = (max_levels >= 8) ? 0xFF : 0x28;
+      /* TODO: Select hierarchy mask more effectively. */
 
-      /* For large framebuffers, disable the smallest bin size to
-       * avoid pathological tiler memory usage. Required to avoid OOM
-       * on dEQP-GLES31.functional.fbo.no_attachments.maximums.all on
-       * Mali-G57.
+      /* Disable the smallest hierarchy level. This is required to
+       * use 32x32 tiles on v10, and helps reduce tiler heap memory
+       * usage for other GPUs. The rasteriser can efficiently skip
+       * primitives not entering the current quadrant of a tile, so
+       * this should not hurt performance much.
+       * Even for GPUs earlier than v10, cores get fed tiles in
+       * 32x32 pixel blocks, so making all of the tiles use the same
+       * set of primitive lists could help with performance.
+       * Maybe then v10 should disable two levels?
        */
-      if (MAX2(fb_width, fb_height) >= 4096)
-         tiler.hierarchy_mask &= ~1;
+      tiler.hierarchy_mask = (max_levels >= 8) ? 0xFE : 0x28;
 
       tiler.fb_width = fb_width;
       tiler.fb_height = fb_height;
       tiler.heap = heap;
+#if PAN_ARCH >= 10
+      tiler.scratch = scratch;
+#endif
       tiler.sample_pattern = pan_sample_pattern(nr_samples);
 #if PAN_ARCH >= 9
       tiler.first_provoking_vertex = first_provoking_vertex;
 #endif
+      tiler.state.word1 = 31;
+      tiler.state.word3 = 0x10000000;
    }
 }
 #endif
@@ -995,24 +1045,45 @@
 GENX(pan_emit_fragment_job)(const struct pan_fb_info *fb, mali_ptr fbd,
                             void *out)
 {
+#if PAN_ARCH < 10
    pan_section_pack(out, FRAGMENT_JOB, HEADER, header) {
       header.type = MALI_JOB_TYPE_FRAGMENT;
       header.index = 1;
    }
+#endif
 
-   pan_section_pack(out, FRAGMENT_JOB, PAYLOAD, payload) {
-      payload.bound_min_x = fb->extent.minx >> MALI_TILE_SHIFT;
-      payload.bound_min_y = fb->extent.miny >> MALI_TILE_SHIFT;
-      payload.bound_max_x = fb->extent.maxx >> MALI_TILE_SHIFT;
-      payload.bound_max_y = fb->extent.maxy >> MALI_TILE_SHIFT;
+#if PAN_ARCH < 10
+#define BOUND_SHIFT MALI_TILE_SHIFT
+#else
+#define BOUND_SHIFT 0
+#endif
+
+   pan_section_pack_cs_v10(out, fb->cs_fragment, FRAGMENT_JOB, PAYLOAD, payload)
+   {
+      payload.bound_min_x = fb->extent.minx >> BOUND_SHIFT;
+      payload.bound_min_y = fb->extent.miny >> BOUND_SHIFT;
+      payload.bound_max_x = fb->extent.maxx >> BOUND_SHIFT;
+      payload.bound_max_y = fb->extent.maxy >> BOUND_SHIFT;
       payload.framebuffer = fbd;
 
 #if PAN_ARCH >= 5
       if (fb->tile_map.base) {
+#if PAN_ARCH < 0
          payload.has_tile_enable_map = true;
+#endif
          payload.tile_enable_map = fb->tile_map.base;
          payload.tile_enable_map_row_stride = fb->tile_map.stride;
       }
+#else
+      assert(!fb->tile_map.base);
 #endif
    }
+
+#if PAN_ARCH >= 10
+   /* TODO: Do this here? */
+   pan_pack_ins(fb->cs_fragment, FRAGMENT_LAUNCH, launch)
+   {
+      launch.has_tile_enable_map = !!fb->tile_map.base;
+   }
+#endif
 }
diff -urN mesa-23.0.0/src/panfrost/lib/pan_cs.h mesa/src/panfrost/lib/pan_cs.h
--- mesa-23.0.0/src/panfrost/lib/pan_cs.h	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/pan_cs.h	2023-03-06 19:19:31.810302908 +0100
@@ -121,6 +121,8 @@
    /* Only used on Valhall */
    bool sprite_coord_origin;
    bool first_provoking_vertex;
+
+   pan_command_stream *cs_fragment;
 };
 
 static inline unsigned
@@ -162,7 +164,7 @@
 void GENX(pan_emit_tiler_ctx)(const struct panfrost_device *dev,
                               unsigned fb_width, unsigned fb_height,
                               unsigned nr_samples, bool first_provoking_vertex,
-                              mali_ptr heap, void *out);
+                              mali_ptr heap, mali_ptr scratch, void *out);
 #endif
 
 void GENX(pan_emit_fragment_job)(const struct pan_fb_info *fb, mali_ptr fbd,
diff -urN mesa-23.0.0/src/panfrost/lib/pan_device.h mesa/src/panfrost/lib/pan_device.h
--- mesa-23.0.0/src/panfrost/lib/pan_device.h	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/pan_device.h	2023-03-06 19:19:31.930303701 +0100
@@ -34,10 +34,11 @@
 #include "renderonly/renderonly.h"
 #include "util/bitset.h"
 #include "util/list.h"
-#include "util/sparse_array.h"
+#include "util/stable_array.h"
 #include "util/u_dynarray.h"
 
 #include "panfrost/util/pan_ir.h"
+#include "pan_base.h"
 #include "pan_pool.h"
 #include "pan_util.h"
 
@@ -79,6 +80,59 @@
    pthread_mutex_t lock;
 };
 
+enum pan_indirect_draw_flags {
+   PAN_INDIRECT_DRAW_NO_INDEX = 0 << 0,
+   PAN_INDIRECT_DRAW_1B_INDEX = 1 << 0,
+   PAN_INDIRECT_DRAW_2B_INDEX = 2 << 0,
+   PAN_INDIRECT_DRAW_4B_INDEX = 3 << 0,
+   PAN_INDIRECT_DRAW_INDEX_SIZE_MASK = 3 << 0,
+   PAN_INDIRECT_DRAW_HAS_PSIZ = 1 << 2,
+   PAN_INDIRECT_DRAW_PRIMITIVE_RESTART = 1 << 3,
+   PAN_INDIRECT_DRAW_UPDATE_PRIM_SIZE = 1 << 4,
+   PAN_INDIRECT_DRAW_IDVS = 1 << 5,
+   PAN_INDIRECT_DRAW_LAST_FLAG = PAN_INDIRECT_DRAW_IDVS,
+   PAN_INDIRECT_DRAW_FLAGS_MASK = (PAN_INDIRECT_DRAW_LAST_FLAG << 1) - 1,
+   PAN_INDIRECT_DRAW_MIN_MAX_SEARCH_1B_INDEX = PAN_INDIRECT_DRAW_LAST_FLAG << 1,
+   PAN_INDIRECT_DRAW_MIN_MAX_SEARCH_2B_INDEX,
+   PAN_INDIRECT_DRAW_MIN_MAX_SEARCH_4B_INDEX,
+   PAN_INDIRECT_DRAW_MIN_MAX_SEARCH_1B_INDEX_PRIM_RESTART,
+   PAN_INDIRECT_DRAW_MIN_MAX_SEARCH_2B_INDEX_PRIM_RESTART,
+   PAN_INDIRECT_DRAW_MIN_MAX_SEARCH_3B_INDEX_PRIM_RESTART,
+   PAN_INDIRECT_DRAW_NUM_SHADERS,
+};
+
+struct pan_indirect_draw_shader {
+   struct panfrost_ubo_push push;
+   mali_ptr rsd;
+};
+
+struct pan_indirect_draw_shaders {
+   struct pan_indirect_draw_shader shaders[PAN_INDIRECT_DRAW_NUM_SHADERS];
+
+   /* Take the lock when initializing the draw shaders context or when
+    * allocating from the binary pool.
+    */
+   pthread_mutex_t lock;
+
+   /* A memory pool for shader binaries. We currently don't allocate a
+    * single BO for all shaders up-front because estimating shader size
+    * is not trivial, and changes to the compiler might influence this
+    * estimation.
+    */
+   struct pan_pool *bin_pool;
+
+   /* BO containing all renderer states attached to the compute shaders.
+    * Those are built at shader compilation time and re-used every time
+    * panfrost_emit_indirect_draw() is called.
+    */
+   struct panfrost_bo *states;
+
+   /* Varying memory is allocated dynamically by compute jobs from this
+    * heap.
+    */
+   struct panfrost_bo *varying_heap;
+};
+
 struct pan_indirect_dispatch {
    struct panfrost_ubo_push push;
    struct panfrost_bo *bin;
@@ -129,6 +183,7 @@
    void *memctx;
 
    int fd;
+   bool kbase;
 
    /* Properties of the GPU in use */
    unsigned arch;
@@ -151,6 +206,9 @@
    const struct panfrost_model *model;
    bool has_afbc;
 
+   /* Does the kernel support dma-buf fence import/export? */
+   bool has_dmabuf_fence;
+
    /* Table of formats, indexed by a PIPE format */
    const struct panfrost_format *formats;
 
@@ -164,8 +222,11 @@
 
    struct renderonly *ro;
 
+   /* Hold this while updating usage field of BOs */
+   pthread_mutex_t bo_usage_lock;
+
    pthread_mutex_t bo_map_lock;
-   struct util_sparse_array bo_map;
+   struct stable_array bo_map;
 
    struct {
       pthread_mutex_t lock;
@@ -186,6 +247,7 @@
 
    struct pan_blitter blitter;
    struct pan_blend_shaders blend_shaders;
+   struct pan_indirect_draw_shaders indirect_draw_shaders;
    struct pan_indirect_dispatch indirect_dispatch;
 
    /* Tiler heap shared across all tiler jobs, allocated against the
@@ -209,12 +271,18 @@
     * unconditionally on Bifrost, and useful for sharing with Midgard */
 
    struct panfrost_bo *sample_positions;
+
+   struct kbase_ mali;
+
+   FILE *bo_log;
 };
 
 void panfrost_open_device(void *memctx, int fd, struct panfrost_device *dev);
 
 void panfrost_close_device(struct panfrost_device *dev);
 
+bool panfrost_check_dmabuf_fence(struct panfrost_device *dev);
+
 bool panfrost_supports_compressed_format(struct panfrost_device *dev,
                                          unsigned fmt);
 
@@ -225,12 +293,19 @@
 void panfrost_query_sample_position(enum mali_sample_pattern pattern,
                                     unsigned sample_idx, float *out);
 
-unsigned panfrost_query_l2_slices(const struct panfrost_device *dev);
+unsigned panfrost_query_l2_slices(struct panfrost_device *dev);
 
 static inline struct panfrost_bo *
 pan_lookup_bo(struct panfrost_device *dev, uint32_t gem_handle)
 {
-   return (struct panfrost_bo *)util_sparse_array_get(&dev->bo_map, gem_handle);
+   return stable_array_get(&dev->bo_map, struct panfrost_bo, gem_handle);
+}
+
+static inline struct panfrost_bo *
+pan_lookup_bo_existing(struct panfrost_device *dev, uint32_t gem_handle)
+{
+   return stable_array_get_existing(&dev->bo_map, struct panfrost_bo,
+                                    gem_handle);
 }
 
 static inline bool
diff -urN mesa-23.0.0/src/panfrost/lib/pan_earlyzs.c mesa/src/panfrost/lib/pan_earlyzs.c
--- mesa-23.0.0/src/panfrost/lib/pan_earlyzs.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/pan_earlyzs.c	2023-03-06 19:19:31.931303707 +0100
@@ -92,8 +92,7 @@
 
    return (struct pan_earlyzs_state){
       .update = late_update ? PAN_EARLYZS_FORCE_LATE : early_mode,
-      .kill = late_kill ? PAN_EARLYZS_FORCE_LATE : early_mode,
-   };
+      .kill = late_kill ? PAN_EARLYZS_FORCE_LATE : early_mode};
 }
 
 /*
diff -urN mesa-23.0.0/src/panfrost/lib/pan_format.c mesa/src/panfrost/lib/pan_format.c
--- mesa-23.0.0/src/panfrost/lib/pan_format.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/pan_format.c	2023-03-06 19:19:32.026304335 +0100
@@ -45,26 +45,20 @@
 
 #if PAN_ARCH <= 6
 #define BFMT2(pipe, internal, writeback, srgb)                                 \
-   [PIPE_FORMAT_##pipe] = {                                                    \
-      MALI_COLOR_BUFFER_INTERNAL_FORMAT_##internal,                            \
-      MALI_COLOR_FORMAT_##writeback,                                           \
-      {                                                                        \
-         MALI_BLEND_PU_##internal | (srgb ? (1 << 20) : 0) |                   \
-            PAN_V6_SWIZZLE(R, G, B, A),                                        \
-         MALI_BLEND_AU_##internal | (srgb ? (1 << 20) : 0) |                   \
-            PAN_V6_SWIZZLE(R, G, B, A),                                        \
-      },                                                                       \
-   }
+   [PIPE_FORMAT_##                                                             \
+      pipe] = {MALI_COLOR_BUFFER_INTERNAL_FORMAT_##internal,                   \
+               MALI_COLOR_FORMAT_##writeback,                                  \
+               {MALI_BLEND_PU_##internal | (srgb ? (1 << 20) : 0) |            \
+                   PAN_V6_SWIZZLE(R, G, B, A),                                 \
+                MALI_BLEND_AU_##internal | (srgb ? (1 << 20) : 0) |            \
+                   PAN_V6_SWIZZLE(R, G, B, A)}}
 #else
 #define BFMT2(pipe, internal, writeback, srgb)                                 \
-   [PIPE_FORMAT_##pipe] = {                                                    \
-      MALI_COLOR_BUFFER_INTERNAL_FORMAT_##internal,                            \
-      MALI_COLOR_FORMAT_##writeback,                                           \
-      {                                                                        \
-         MALI_BLEND_PU_##internal | (srgb ? (1 << 20) : 0),                    \
-         MALI_BLEND_AU_##internal | (srgb ? (1 << 20) : 0),                    \
-      },                                                                       \
-   }
+   [PIPE_FORMAT_##                                                             \
+      pipe] = {MALI_COLOR_BUFFER_INTERNAL_FORMAT_##internal,                   \
+               MALI_COLOR_FORMAT_##writeback,                                  \
+               {MALI_BLEND_PU_##internal | (srgb ? (1 << 20) : 0),             \
+                MALI_BLEND_AU_##internal | (srgb ? (1 << 20) : 0)}}
 #endif
 
 #define BFMT(pipe, internal_and_writeback)                                     \
@@ -152,8 +146,7 @@
 #define FMT(pipe, mali, swizzle, srgb, flags)                                  \
    [PIPE_FORMAT_##pipe] = {                                                    \
       .hw = (V6_##swizzle) | ((MALI_##mali) << 12) | (((SRGB_##srgb)) << 20),  \
-      .bind = FLAGS_##flags,                                                   \
-   }
+      .bind = FLAGS_##flags}
 #else
 
 #define MALI_RGB_COMPONENT_ORDER_R001 MALI_RGB_COMPONENT_ORDER_RGB1
@@ -163,409 +156,407 @@
 #define MALI_RGB_COMPONENT_ORDER_ABG1 MALI_RGB_COMPONENT_ORDER_1BGR
 
 #define FMT(pipe, mali, swizzle, srgb, flags)                                  \
-   [PIPE_FORMAT_##pipe] = {                                                    \
-      .hw = (MALI_RGB_COMPONENT_ORDER_##swizzle) | ((MALI_##mali) << 12) |     \
-            (((SRGB_##srgb)) << 20),                                           \
-      .bind = FLAGS_##flags,                                                   \
-   }
+   [PIPE_FORMAT_##pipe] = {.hw = (MALI_RGB_COMPONENT_ORDER_##swizzle) |        \
+                                 ((MALI_##mali) << 12) |                       \
+                                 (((SRGB_##srgb)) << 20),                      \
+                           .bind = FLAGS_##flags}
 #endif
 
-/* clang-format off */
 const struct panfrost_format GENX(panfrost_pipe_format)[PIPE_FORMAT_COUNT] = {
-   FMT(NONE,                    CONSTANT,        0000, L, VTR_),
+   FMT(NONE, CONSTANT, 0000, L, VTR_),
 
 #if PAN_ARCH <= 7
-   FMT(ETC1_RGB8,               ETC2_RGB8,       RGB1, L, _T__),
-   FMT(ETC2_RGB8,               ETC2_RGB8,       RGB1, L, _T__),
-   FMT(ETC2_SRGB8,              ETC2_RGB8,       RGB1, S, _T__),
-   FMT(ETC2_R11_UNORM,          ETC2_R11_UNORM,  R001, L, _T__),
-   FMT(ETC2_RGBA8,              ETC2_RGBA8,      RGBA, L, _T__),
-   FMT(ETC2_SRGBA8,             ETC2_RGBA8,      RGBA, S, _T__),
-   FMT(ETC2_RG11_UNORM,         ETC2_RG11_UNORM, RG01, L, _T__),
-   FMT(ETC2_R11_SNORM,          ETC2_R11_SNORM,  R001, L, _T__),
-   FMT(ETC2_RG11_SNORM,         ETC2_RG11_SNORM, RG01, L, _T__),
-   FMT(ETC2_RGB8A1,             ETC2_RGB8A1,     RGBA, L, _T__),
-   FMT(ETC2_SRGB8A1,            ETC2_RGB8A1,     RGBA, S, _T__),
-   FMT(DXT1_RGB,                BC1_UNORM,       RGB1, L, _T__),
-   FMT(DXT1_RGBA,               BC1_UNORM,       RGBA, L, _T__),
-   FMT(DXT1_SRGB,               BC1_UNORM,       RGB1, S, _T__),
-   FMT(DXT1_SRGBA,              BC1_UNORM,       RGBA, S, _T__),
-   FMT(DXT3_RGBA,               BC2_UNORM,       RGBA, L, _T__),
-   FMT(DXT3_SRGBA,              BC2_UNORM,       RGBA, S, _T__),
-   FMT(DXT5_RGBA,               BC3_UNORM,       RGBA, L, _T__),
-   FMT(DXT5_SRGBA,              BC3_UNORM,       RGBA, S, _T__),
-   FMT(RGTC1_UNORM,             BC4_UNORM,       R001, L, _T__),
-   FMT(RGTC1_SNORM,             BC4_SNORM,       R001, L, _T__),
-   FMT(RGTC2_UNORM,             BC5_UNORM,       RG01, L, _T__),
-   FMT(RGTC2_SNORM,             BC5_SNORM,       RG01, L, _T__),
-   FMT(BPTC_RGB_FLOAT,          BC6H_SF16,       RGB1, L, _T__),
-   FMT(BPTC_RGB_UFLOAT,         BC6H_UF16,       RGB1, L, _T__),
-   FMT(BPTC_RGBA_UNORM,         BC7_UNORM,       RGBA, L, _T__),
-   FMT(BPTC_SRGBA,              BC7_UNORM,       RGBA, S, _T__),
-   FMT(ASTC_4x4,                ASTC_2D_HDR,     RGBA, L, _T__),
-   FMT(ASTC_5x4,                ASTC_2D_HDR,     RGBA, L, _T__),
-   FMT(ASTC_5x5,                ASTC_2D_HDR,     RGBA, L, _T__),
-   FMT(ASTC_6x5,                ASTC_2D_HDR,     RGBA, L, _T__),
-   FMT(ASTC_6x6,                ASTC_2D_HDR,     RGBA, L, _T__),
-   FMT(ASTC_8x5,                ASTC_2D_HDR,     RGBA, L, _T__),
-   FMT(ASTC_8x6,                ASTC_2D_HDR,     RGBA, L, _T__),
-   FMT(ASTC_8x8,                ASTC_2D_HDR,     RGBA, L, _T__),
-   FMT(ASTC_10x5,               ASTC_2D_HDR,     RGBA, L, _T__),
-   FMT(ASTC_10x6,               ASTC_2D_HDR,     RGBA, L, _T__),
-   FMT(ASTC_10x8,               ASTC_2D_HDR,     RGBA, L, _T__),
-   FMT(ASTC_10x10,              ASTC_2D_HDR,     RGBA, L, _T__),
-   FMT(ASTC_12x10,              ASTC_2D_HDR,     RGBA, L, _T__),
-   FMT(ASTC_12x12,              ASTC_2D_HDR,     RGBA, L, _T__),
-   FMT(ASTC_4x4_SRGB,           ASTC_2D_LDR,     RGBA, S, _T__),
-   FMT(ASTC_5x4_SRGB,           ASTC_2D_LDR,     RGBA, S, _T__),
-   FMT(ASTC_5x5_SRGB,           ASTC_2D_LDR,     RGBA, S, _T__),
-   FMT(ASTC_6x5_SRGB,           ASTC_2D_LDR,     RGBA, S, _T__),
-   FMT(ASTC_6x6_SRGB,           ASTC_2D_LDR,     RGBA, S, _T__),
-   FMT(ASTC_8x5_SRGB,           ASTC_2D_LDR,     RGBA, S, _T__),
-   FMT(ASTC_8x6_SRGB,           ASTC_2D_LDR,     RGBA, S, _T__),
-   FMT(ASTC_8x8_SRGB,           ASTC_2D_LDR,     RGBA, S, _T__),
-   FMT(ASTC_10x5_SRGB,          ASTC_2D_LDR,     RGBA, S, _T__),
-   FMT(ASTC_10x6_SRGB,          ASTC_2D_LDR,     RGBA, S, _T__),
-   FMT(ASTC_10x8_SRGB,          ASTC_2D_LDR,     RGBA, S, _T__),
-   FMT(ASTC_10x10_SRGB,         ASTC_2D_LDR,     RGBA, S, _T__),
-   FMT(ASTC_12x10_SRGB,         ASTC_2D_LDR,     RGBA, S, _T__),
-   FMT(ASTC_12x12_SRGB,         ASTC_2D_LDR,     RGBA, S, _T__),
-   FMT(ASTC_3x3x3,              ASTC_3D_HDR,     RGBA, L, _T__),
-   FMT(ASTC_4x3x3,              ASTC_3D_HDR,     RGBA, L, _T__),
-   FMT(ASTC_4x4x3,              ASTC_3D_HDR,     RGBA, L, _T__),
-   FMT(ASTC_4x4x4,              ASTC_3D_HDR,     RGBA, L, _T__),
-   FMT(ASTC_5x4x4,              ASTC_3D_HDR,     RGBA, L, _T__),
-   FMT(ASTC_5x5x4,              ASTC_3D_HDR,     RGBA, L, _T__),
-   FMT(ASTC_5x5x5,              ASTC_3D_HDR,     RGBA, L, _T__),
-   FMT(ASTC_6x5x5,              ASTC_3D_HDR,     RGBA, L, _T__),
-   FMT(ASTC_6x6x5,              ASTC_3D_HDR,     RGBA, L, _T__),
-   FMT(ASTC_6x6x6,              ASTC_3D_HDR,     RGBA, L, _T__),
-   FMT(ASTC_3x3x3_SRGB,         ASTC_3D_LDR,     RGBA, S, _T__),
-   FMT(ASTC_4x3x3_SRGB,         ASTC_3D_LDR,     RGBA, S, _T__),
-   FMT(ASTC_4x4x3_SRGB,         ASTC_3D_LDR,     RGBA, S, _T__),
-   FMT(ASTC_4x4x4_SRGB,         ASTC_3D_LDR,     RGBA, S, _T__),
-   FMT(ASTC_5x4x4_SRGB,         ASTC_3D_LDR,     RGBA, S, _T__),
-   FMT(ASTC_5x5x4_SRGB,         ASTC_3D_LDR,     RGBA, S, _T__),
-   FMT(ASTC_5x5x5_SRGB,         ASTC_3D_LDR,     RGBA, S, _T__),
-   FMT(ASTC_6x5x5_SRGB,         ASTC_3D_LDR,     RGBA, S, _T__),
-   FMT(ASTC_6x6x5_SRGB,         ASTC_3D_LDR,     RGBA, S, _T__),
-   FMT(ASTC_6x6x6_SRGB,         ASTC_3D_LDR,     RGBA, S, _T__),
+   FMT(ETC1_RGB8, ETC2_RGB8, RGB1, L, _T__),
+   FMT(ETC2_RGB8, ETC2_RGB8, RGB1, L, _T__),
+   FMT(ETC2_SRGB8, ETC2_RGB8, RGB1, S, _T__),
+   FMT(ETC2_R11_UNORM, ETC2_R11_UNORM, R001, L, _T__),
+   FMT(ETC2_RGBA8, ETC2_RGBA8, RGBA, L, _T__),
+   FMT(ETC2_SRGBA8, ETC2_RGBA8, RGBA, S, _T__),
+   FMT(ETC2_RG11_UNORM, ETC2_RG11_UNORM, RG01, L, _T__),
+   FMT(ETC2_R11_SNORM, ETC2_R11_SNORM, R001, L, _T__),
+   FMT(ETC2_RG11_SNORM, ETC2_RG11_SNORM, RG01, L, _T__),
+   FMT(ETC2_RGB8A1, ETC2_RGB8A1, RGBA, L, _T__),
+   FMT(ETC2_SRGB8A1, ETC2_RGB8A1, RGBA, S, _T__),
+   FMT(DXT1_RGB, BC1_UNORM, RGB1, L, _T__),
+   FMT(DXT1_RGBA, BC1_UNORM, RGBA, L, _T__),
+   FMT(DXT1_SRGB, BC1_UNORM, RGB1, S, _T__),
+   FMT(DXT1_SRGBA, BC1_UNORM, RGBA, S, _T__),
+   FMT(DXT3_RGBA, BC2_UNORM, RGBA, L, _T__),
+   FMT(DXT3_SRGBA, BC2_UNORM, RGBA, S, _T__),
+   FMT(DXT5_RGBA, BC3_UNORM, RGBA, L, _T__),
+   FMT(DXT5_SRGBA, BC3_UNORM, RGBA, S, _T__),
+   FMT(RGTC1_UNORM, BC4_UNORM, R001, L, _T__),
+   FMT(RGTC1_SNORM, BC4_SNORM, R001, L, _T__),
+   FMT(RGTC2_UNORM, BC5_UNORM, RG01, L, _T__),
+   FMT(RGTC2_SNORM, BC5_SNORM, RG01, L, _T__),
+   FMT(BPTC_RGB_FLOAT, BC6H_SF16, RGB1, L, _T__),
+   FMT(BPTC_RGB_UFLOAT, BC6H_UF16, RGB1, L, _T__),
+   FMT(BPTC_RGBA_UNORM, BC7_UNORM, RGBA, L, _T__),
+   FMT(BPTC_SRGBA, BC7_UNORM, RGBA, S, _T__),
+   FMT(ASTC_4x4, ASTC_2D_HDR, RGBA, L, _T__),
+   FMT(ASTC_5x4, ASTC_2D_HDR, RGBA, L, _T__),
+   FMT(ASTC_5x5, ASTC_2D_HDR, RGBA, L, _T__),
+   FMT(ASTC_6x5, ASTC_2D_HDR, RGBA, L, _T__),
+   FMT(ASTC_6x6, ASTC_2D_HDR, RGBA, L, _T__),
+   FMT(ASTC_8x5, ASTC_2D_HDR, RGBA, L, _T__),
+   FMT(ASTC_8x6, ASTC_2D_HDR, RGBA, L, _T__),
+   FMT(ASTC_8x8, ASTC_2D_HDR, RGBA, L, _T__),
+   FMT(ASTC_10x5, ASTC_2D_HDR, RGBA, L, _T__),
+   FMT(ASTC_10x6, ASTC_2D_HDR, RGBA, L, _T__),
+   FMT(ASTC_10x8, ASTC_2D_HDR, RGBA, L, _T__),
+   FMT(ASTC_10x10, ASTC_2D_HDR, RGBA, L, _T__),
+   FMT(ASTC_12x10, ASTC_2D_HDR, RGBA, L, _T__),
+   FMT(ASTC_12x12, ASTC_2D_HDR, RGBA, L, _T__),
+   FMT(ASTC_4x4_SRGB, ASTC_2D_LDR, RGBA, S, _T__),
+   FMT(ASTC_5x4_SRGB, ASTC_2D_LDR, RGBA, S, _T__),
+   FMT(ASTC_5x5_SRGB, ASTC_2D_LDR, RGBA, S, _T__),
+   FMT(ASTC_6x5_SRGB, ASTC_2D_LDR, RGBA, S, _T__),
+   FMT(ASTC_6x6_SRGB, ASTC_2D_LDR, RGBA, S, _T__),
+   FMT(ASTC_8x5_SRGB, ASTC_2D_LDR, RGBA, S, _T__),
+   FMT(ASTC_8x6_SRGB, ASTC_2D_LDR, RGBA, S, _T__),
+   FMT(ASTC_8x8_SRGB, ASTC_2D_LDR, RGBA, S, _T__),
+   FMT(ASTC_10x5_SRGB, ASTC_2D_LDR, RGBA, S, _T__),
+   FMT(ASTC_10x6_SRGB, ASTC_2D_LDR, RGBA, S, _T__),
+   FMT(ASTC_10x8_SRGB, ASTC_2D_LDR, RGBA, S, _T__),
+   FMT(ASTC_10x10_SRGB, ASTC_2D_LDR, RGBA, S, _T__),
+   FMT(ASTC_12x10_SRGB, ASTC_2D_LDR, RGBA, S, _T__),
+   FMT(ASTC_12x12_SRGB, ASTC_2D_LDR, RGBA, S, _T__),
+   FMT(ASTC_3x3x3, ASTC_3D_HDR, RGBA, L, _T__),
+   FMT(ASTC_4x3x3, ASTC_3D_HDR, RGBA, L, _T__),
+   FMT(ASTC_4x4x3, ASTC_3D_HDR, RGBA, L, _T__),
+   FMT(ASTC_4x4x4, ASTC_3D_HDR, RGBA, L, _T__),
+   FMT(ASTC_5x4x4, ASTC_3D_HDR, RGBA, L, _T__),
+   FMT(ASTC_5x5x4, ASTC_3D_HDR, RGBA, L, _T__),
+   FMT(ASTC_5x5x5, ASTC_3D_HDR, RGBA, L, _T__),
+   FMT(ASTC_6x5x5, ASTC_3D_HDR, RGBA, L, _T__),
+   FMT(ASTC_6x6x5, ASTC_3D_HDR, RGBA, L, _T__),
+   FMT(ASTC_6x6x6, ASTC_3D_HDR, RGBA, L, _T__),
+   FMT(ASTC_3x3x3_SRGB, ASTC_3D_LDR, RGBA, S, _T__),
+   FMT(ASTC_4x3x3_SRGB, ASTC_3D_LDR, RGBA, S, _T__),
+   FMT(ASTC_4x4x3_SRGB, ASTC_3D_LDR, RGBA, S, _T__),
+   FMT(ASTC_4x4x4_SRGB, ASTC_3D_LDR, RGBA, S, _T__),
+   FMT(ASTC_5x4x4_SRGB, ASTC_3D_LDR, RGBA, S, _T__),
+   FMT(ASTC_5x5x4_SRGB, ASTC_3D_LDR, RGBA, S, _T__),
+   FMT(ASTC_5x5x5_SRGB, ASTC_3D_LDR, RGBA, S, _T__),
+   FMT(ASTC_6x5x5_SRGB, ASTC_3D_LDR, RGBA, S, _T__),
+   FMT(ASTC_6x6x5_SRGB, ASTC_3D_LDR, RGBA, S, _T__),
+   FMT(ASTC_6x6x6_SRGB, ASTC_3D_LDR, RGBA, S, _T__),
 #else
    /* Map to interchange format, as compression is specified in the plane
     * descriptor on Valhall.
     */
-   FMT(ETC1_RGB8,               RGBA8_UNORM,     RGB1, L, _T__),
-   FMT(ETC2_RGB8,               RGBA8_UNORM,     RGB1, L, _T__),
-   FMT(ETC2_SRGB8,              RGBA8_UNORM,     RGB1, S, _T__),
-   FMT(ETC2_R11_UNORM,          R16_UNORM,       R001, L, _T__),
-   FMT(ETC2_RGBA8,              RGBA8_UNORM,     RGBA, L, _T__),
-   FMT(ETC2_SRGBA8,             RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(ETC2_RG11_UNORM,         RG16_UNORM,      RG01, L, _T__),
-   FMT(ETC2_R11_SNORM,          R16_SNORM,       R001, L, _T__),
-   FMT(ETC2_RG11_SNORM,         RG16_SNORM,      RG01, L, _T__),
-   FMT(ETC2_RGB8A1,             RGBA8_UNORM,     RGBA, L, _T__),
-   FMT(ETC2_SRGB8A1,            RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(DXT1_RGB,                RGBA8_UNORM,     RGB1, L, _T__),
-   FMT(DXT1_RGBA,               RGBA8_UNORM,     RGBA, L, _T__),
-   FMT(DXT1_SRGB,               RGBA8_UNORM,     RGB1, S, _T__),
-   FMT(DXT1_SRGBA,              RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(DXT3_RGBA,               RGBA8_UNORM,     RGBA, L, _T__),
-   FMT(DXT3_SRGBA,              RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(DXT5_RGBA,               RGBA8_UNORM,     RGBA, L, _T__),
-   FMT(DXT5_SRGBA,              RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(RGTC1_UNORM,             R16_UNORM,       R001, L, _T__),
-   FMT(RGTC1_SNORM,             R16_SNORM,       R001, L, _T__),
-   FMT(RGTC2_UNORM,             RG16_UNORM,      RG01, L, _T__),
-   FMT(RGTC2_SNORM,             RG16_SNORM,      RG01, L, _T__),
-   FMT(BPTC_RGB_FLOAT,          RGBA16F,         RGB1, L, _T__),
-   FMT(BPTC_RGB_UFLOAT,         RGBA16F,         RGB1, L, _T__),
-   FMT(BPTC_RGBA_UNORM,         RGBA8_UNORM,     RGBA, L, _T__),
-   FMT(BPTC_SRGBA,              RGBA8_UNORM,     RGBA, S, _T__),
+   FMT(ETC1_RGB8, RGBA8_UNORM, RGB1, L, _T__),
+   FMT(ETC2_RGB8, RGBA8_UNORM, RGB1, L, _T__),
+   FMT(ETC2_SRGB8, RGBA8_UNORM, RGB1, S, _T__),
+   FMT(ETC2_R11_UNORM, R16_UNORM, R001, L, _T__),
+   FMT(ETC2_RGBA8, RGBA8_UNORM, RGBA, L, _T__),
+   FMT(ETC2_SRGBA8, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(ETC2_RG11_UNORM, RG16_UNORM, RG01, L, _T__),
+   FMT(ETC2_R11_SNORM, R16_SNORM, R001, L, _T__),
+   FMT(ETC2_RG11_SNORM, RG16_SNORM, RG01, L, _T__),
+   FMT(ETC2_RGB8A1, RGBA8_UNORM, RGBA, L, _T__),
+   FMT(ETC2_SRGB8A1, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(DXT1_RGB, RGBA8_UNORM, RGB1, L, _T__),
+   FMT(DXT1_RGBA, RGBA8_UNORM, RGBA, L, _T__),
+   FMT(DXT1_SRGB, RGBA8_UNORM, RGB1, S, _T__),
+   FMT(DXT1_SRGBA, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(DXT3_RGBA, RGBA8_UNORM, RGBA, L, _T__),
+   FMT(DXT3_SRGBA, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(DXT5_RGBA, RGBA8_UNORM, RGBA, L, _T__),
+   FMT(DXT5_SRGBA, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(RGTC1_UNORM, R16_UNORM, R001, L, _T__),
+   FMT(RGTC1_SNORM, R16_SNORM, R001, L, _T__),
+   FMT(RGTC2_UNORM, RG16_UNORM, RG01, L, _T__),
+   FMT(RGTC2_SNORM, RG16_SNORM, RG01, L, _T__),
+   FMT(BPTC_RGB_FLOAT, RGBA16F, RGB1, L, _T__),
+   FMT(BPTC_RGB_UFLOAT, RGBA16F, RGB1, L, _T__),
+   FMT(BPTC_RGBA_UNORM, RGBA8_UNORM, RGBA, L, _T__),
+   FMT(BPTC_SRGBA, RGBA8_UNORM, RGBA, S, _T__),
 
    /* Mesa does not yet support astc_decode_mode extensions, so non-sRGB
     * formats must be assumed to be wide.
     */
-   FMT(ASTC_4x4,                RGBA16F,         RGBA, L, _T__),
-   FMT(ASTC_5x4,                RGBA16F,         RGBA, L, _T__),
-   FMT(ASTC_5x5,                RGBA16F,         RGBA, L, _T__),
-   FMT(ASTC_6x5,                RGBA16F,         RGBA, L, _T__),
-   FMT(ASTC_6x6,                RGBA16F,         RGBA, L, _T__),
-   FMT(ASTC_8x5,                RGBA16F,         RGBA, L, _T__),
-   FMT(ASTC_8x6,                RGBA16F,         RGBA, L, _T__),
-   FMT(ASTC_8x8,                RGBA16F,         RGBA, L, _T__),
-   FMT(ASTC_10x5,               RGBA16F,         RGBA, L, _T__),
-   FMT(ASTC_10x6,               RGBA16F,         RGBA, L, _T__),
-   FMT(ASTC_10x8,               RGBA16F,         RGBA, L, _T__),
-   FMT(ASTC_10x10,              RGBA16F,         RGBA, L, _T__),
-   FMT(ASTC_12x10,              RGBA16F,         RGBA, L, _T__),
-   FMT(ASTC_12x12,              RGBA16F,         RGBA, L, _T__),
-   FMT(ASTC_3x3x3,              RGBA16F,         RGBA, L, _T__),
-   FMT(ASTC_4x3x3,              RGBA16F,         RGBA, L, _T__),
-   FMT(ASTC_4x4x3,              RGBA16F,         RGBA, L, _T__),
-   FMT(ASTC_4x4x4,              RGBA16F,         RGBA, L, _T__),
-   FMT(ASTC_5x4x4,              RGBA16F,         RGBA, L, _T__),
-   FMT(ASTC_5x5x4,              RGBA16F,         RGBA, L, _T__),
-   FMT(ASTC_5x5x5,              RGBA16F,         RGBA, L, _T__),
-   FMT(ASTC_6x5x5,              RGBA16F,         RGBA, L, _T__),
-   FMT(ASTC_6x6x5,              RGBA16F,         RGBA, L, _T__),
-   FMT(ASTC_6x6x6,              RGBA16F,         RGBA, L, _T__),
+   FMT(ASTC_4x4, RGBA16F, RGBA, L, _T__),
+   FMT(ASTC_5x4, RGBA16F, RGBA, L, _T__),
+   FMT(ASTC_5x5, RGBA16F, RGBA, L, _T__),
+   FMT(ASTC_6x5, RGBA16F, RGBA, L, _T__),
+   FMT(ASTC_6x6, RGBA16F, RGBA, L, _T__),
+   FMT(ASTC_8x5, RGBA16F, RGBA, L, _T__),
+   FMT(ASTC_8x6, RGBA16F, RGBA, L, _T__),
+   FMT(ASTC_8x8, RGBA16F, RGBA, L, _T__),
+   FMT(ASTC_10x5, RGBA16F, RGBA, L, _T__),
+   FMT(ASTC_10x6, RGBA16F, RGBA, L, _T__),
+   FMT(ASTC_10x8, RGBA16F, RGBA, L, _T__),
+   FMT(ASTC_10x10, RGBA16F, RGBA, L, _T__),
+   FMT(ASTC_12x10, RGBA16F, RGBA, L, _T__),
+   FMT(ASTC_12x12, RGBA16F, RGBA, L, _T__),
+   FMT(ASTC_3x3x3, RGBA16F, RGBA, L, _T__),
+   FMT(ASTC_4x3x3, RGBA16F, RGBA, L, _T__),
+   FMT(ASTC_4x4x3, RGBA16F, RGBA, L, _T__),
+   FMT(ASTC_4x4x4, RGBA16F, RGBA, L, _T__),
+   FMT(ASTC_5x4x4, RGBA16F, RGBA, L, _T__),
+   FMT(ASTC_5x5x4, RGBA16F, RGBA, L, _T__),
+   FMT(ASTC_5x5x5, RGBA16F, RGBA, L, _T__),
+   FMT(ASTC_6x5x5, RGBA16F, RGBA, L, _T__),
+   FMT(ASTC_6x6x5, RGBA16F, RGBA, L, _T__),
+   FMT(ASTC_6x6x6, RGBA16F, RGBA, L, _T__),
 
    /* By definition, sRGB formats are narrow */
-   FMT(ASTC_4x4_SRGB,           RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(ASTC_5x4_SRGB,           RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(ASTC_5x5_SRGB,           RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(ASTC_6x5_SRGB,           RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(ASTC_6x6_SRGB,           RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(ASTC_8x5_SRGB,           RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(ASTC_8x6_SRGB,           RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(ASTC_8x8_SRGB,           RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(ASTC_10x5_SRGB,          RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(ASTC_10x6_SRGB,          RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(ASTC_10x8_SRGB,          RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(ASTC_10x10_SRGB,         RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(ASTC_12x10_SRGB,         RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(ASTC_12x12_SRGB,         RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(ASTC_3x3x3_SRGB,         RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(ASTC_4x3x3_SRGB,         RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(ASTC_4x4x3_SRGB,         RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(ASTC_4x4x4_SRGB,         RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(ASTC_5x4x4_SRGB,         RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(ASTC_5x5x4_SRGB,         RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(ASTC_5x5x5_SRGB,         RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(ASTC_6x5x5_SRGB,         RGBA8_UNORM,     RGBA, S, _T__),
-   FMT(ASTC_6x6x5_SRGB,         RGBA8_UNORM,     RGBA, S, _T__),
+   FMT(ASTC_4x4_SRGB, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(ASTC_5x4_SRGB, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(ASTC_5x5_SRGB, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(ASTC_6x5_SRGB, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(ASTC_6x6_SRGB, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(ASTC_8x5_SRGB, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(ASTC_8x6_SRGB, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(ASTC_8x8_SRGB, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(ASTC_10x5_SRGB, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(ASTC_10x6_SRGB, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(ASTC_10x8_SRGB, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(ASTC_10x10_SRGB, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(ASTC_12x10_SRGB, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(ASTC_12x12_SRGB, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(ASTC_3x3x3_SRGB, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(ASTC_4x3x3_SRGB, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(ASTC_4x4x3_SRGB, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(ASTC_4x4x4_SRGB, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(ASTC_5x4x4_SRGB, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(ASTC_5x5x4_SRGB, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(ASTC_5x5x5_SRGB, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(ASTC_6x5x5_SRGB, RGBA8_UNORM, RGBA, S, _T__),
+   FMT(ASTC_6x6x5_SRGB, RGBA8_UNORM, RGBA, S, _T__),
 #endif
 
-   FMT(R5G6B5_UNORM,            RGB565,          RGB1, L, VTR_),
-   FMT(B5G6R5_UNORM,            RGB565,          BGR1, L, VTR_),
-   FMT(R5G5B5X1_UNORM,          RGB5_A1_UNORM,   RGB1, L, VT__),
-   FMT(B5G5R5X1_UNORM,          RGB5_A1_UNORM,   BGR1, L, VT__),
-   FMT(R5G5B5A1_UNORM,          RGB5_A1_UNORM,   RGBA, L, VTR_),
-   FMT(B5G5R5A1_UNORM,          RGB5_A1_UNORM,   BGRA, L, VTR_),
-   FMT(R10G10B10X2_UNORM,       RGB10_A2_UNORM,  RGB1, L, VTR_),
-   FMT(B10G10R10X2_UNORM,       RGB10_A2_UNORM,  BGR1, L, VTR_),
-   FMT(R10G10B10A2_UNORM,       RGB10_A2_UNORM,  RGBA, L, VTR_),
-   FMT(B10G10R10A2_UNORM,       RGB10_A2_UNORM,  BGRA, L, VTR_),
+   FMT(R5G6B5_UNORM, RGB565, RGB1, L, VTR_),
+   FMT(B5G6R5_UNORM, RGB565, BGR1, L, VTR_),
+   FMT(R5G5B5X1_UNORM, RGB5_A1_UNORM, RGB1, L, VT__),
+   FMT(B5G5R5X1_UNORM, RGB5_A1_UNORM, BGR1, L, VT__),
+   FMT(R5G5B5A1_UNORM, RGB5_A1_UNORM, RGBA, L, VTR_),
+   FMT(B5G5R5A1_UNORM, RGB5_A1_UNORM, BGRA, L, VTR_),
+   FMT(R10G10B10X2_UNORM, RGB10_A2_UNORM, RGB1, L, VTR_),
+   FMT(B10G10R10X2_UNORM, RGB10_A2_UNORM, BGR1, L, VTR_),
+   FMT(R10G10B10A2_UNORM, RGB10_A2_UNORM, RGBA, L, VTR_),
+   FMT(B10G10R10A2_UNORM, RGB10_A2_UNORM, BGRA, L, VTR_),
 #if PAN_ARCH <= 5
-   FMT(R10G10B10X2_SNORM,       RGB10_A2_SNORM,  RGB1, L, VT__),
-   FMT(R10G10B10A2_SNORM,       RGB10_A2_SNORM,  RGBA, L, VT__),
-   FMT(B10G10R10A2_SNORM,       RGB10_A2_SNORM,  BGRA, L, VT__),
-   FMT(R3G3B2_UNORM,            RGB332_UNORM,    RGB1, L, VT__),
+   FMT(R10G10B10X2_SNORM, RGB10_A2_SNORM, RGB1, L, VT__),
+   FMT(R10G10B10A2_SNORM, RGB10_A2_SNORM, RGBA, L, VT__),
+   FMT(B10G10R10A2_SNORM, RGB10_A2_SNORM, BGRA, L, VT__),
+   FMT(R3G3B2_UNORM, RGB332_UNORM, RGB1, L, VT__),
 #else
-   FMT(R10G10B10X2_SNORM,       RGB10_A2_SNORM,  RGB1, L, V___),
-   FMT(R10G10B10A2_SNORM,       RGB10_A2_SNORM,  RGBA, L, V___),
-   FMT(B10G10R10A2_SNORM,       RGB10_A2_SNORM,  BGRA, L, V___),
+   FMT(R10G10B10X2_SNORM, RGB10_A2_SNORM, RGB1, L, V___),
+   FMT(R10G10B10A2_SNORM, RGB10_A2_SNORM, RGBA, L, V___),
+   FMT(B10G10R10A2_SNORM, RGB10_A2_SNORM, BGRA, L, V___),
 #endif
-   FMT(R10G10B10A2_UINT,        RGB10_A2UI,      RGBA, L, VTR_),
-   FMT(B10G10R10A2_UINT,        RGB10_A2UI,      BGRA, L, VTR_),
-   FMT(R10G10B10A2_USCALED,     RGB10_A2UI,      RGBA, L, V___),
-   FMT(B10G10R10A2_USCALED,     RGB10_A2UI,      BGRA, L, V___),
-   FMT(R10G10B10A2_SINT,        RGB10_A2I,       RGBA, L, VTR_),
-   FMT(B10G10R10A2_SINT,        RGB10_A2I,       BGRA, L, VTR_),
-   FMT(R10G10B10A2_SSCALED,     RGB10_A2I,       RGBA, L, V___),
-   FMT(B10G10R10A2_SSCALED,     RGB10_A2I,       BGRA, L, V___),
-   FMT(R8_SSCALED,              R8I,             R001, L, V___),
-   FMT(R8G8_SSCALED,            RG8I,            RG01, L, V___),
-   FMT(R8G8B8_SSCALED,          RGB8I,           RGB1, L, V___),
-   FMT(B8G8R8_SSCALED,          RGB8I,           BGR1, L, V___),
-   FMT(R8G8B8A8_SSCALED,        RGBA8I,          RGBA, L, V___),
-   FMT(B8G8R8A8_SSCALED,        RGBA8I,          BGRA, L, V___),
-   FMT(A8B8G8R8_SSCALED,        RGBA8I,          ABGR, L, V___),
-   FMT(R8_USCALED,              R8UI,            R001, L, V___),
-   FMT(R8G8_USCALED,            RG8UI,           RG01, L, V___),
-   FMT(R8G8B8_USCALED,          RGB8UI,          RGB1, L, V___),
-   FMT(B8G8R8_USCALED,          RGB8UI,          BGR1, L, V___),
-   FMT(R8G8B8A8_USCALED,        RGBA8UI,         RGBA, L, V___),
-   FMT(B8G8R8A8_USCALED,        RGBA8UI,         BGRA, L, V___),
-   FMT(A8B8G8R8_USCALED,        RGBA8UI,         ABGR, L, V___),
-   FMT(R16_USCALED,             R16UI,           R001, L, V___),
-   FMT(R16G16_USCALED,          RG16UI,          RG01, L, V___),
-   FMT(R16G16B16A16_USCALED,    RGBA16UI,        RGBA, L, V___),
-   FMT(R16_SSCALED,             R16I,            R001, L, V___),
-   FMT(R16G16_SSCALED,          RG16I,           RG01, L, V___),
-   FMT(R16G16B16A16_SSCALED,    RGBA16I,         RGBA, L, V___),
-   FMT(R32_USCALED,             R32UI,           R001, L, V___),
-   FMT(R32G32_USCALED,          RG32UI,          RG01, L, V___),
-   FMT(R32G32B32_USCALED,       RGB32UI,         RGB1, L, V___),
-   FMT(R32G32B32A32_USCALED,    RGBA32UI,        RGBA, L, V___),
-   FMT(R32_SSCALED,             R32I,            R001, L, V___),
-   FMT(R32G32_SSCALED,          RG32I,           RG01, L, V___),
-   FMT(R32G32B32_SSCALED,       RGB32I,          RGB1, L, V___),
-   FMT(R32G32B32A32_SSCALED,    RGBA32I,         RGBA, L, V___),
-   FMT(R32_FIXED,               R32_FIXED,       R001, L, V___),
-   FMT(R32G32_FIXED,            RG32_FIXED,      RG01, L, V___),
-   FMT(R32G32B32_FIXED,         RGB32_FIXED,     RGB1, L, V___),
-   FMT(R32G32B32A32_FIXED,      RGBA32_FIXED,    RGBA, L, V___),
-   FMT(R11G11B10_FLOAT,         R11F_G11F_B10F,  RGB1, L, VTR_),
-   FMT(R9G9B9E5_FLOAT,          R9F_G9F_B9F_E5F, RGB1, L, VT__),
-   FMT(R8_SNORM,                R8_SNORM,        R001, L, VT__),
-   FMT(R16_SNORM,               R16_SNORM,       R001, L, VT__),
-   FMT(R8G8_SNORM,              RG8_SNORM,       RG01, L, VT__),
-   FMT(R16G16_SNORM,            RG16_SNORM,      RG01, L, VT__),
-   FMT(R8G8B8_SNORM,            RGB8_SNORM,      RGB1, L, VT__),
-   FMT(R8G8B8A8_SNORM,          RGBA8_SNORM,     RGBA, L, VT__),
-   FMT(R16G16B16A16_SNORM,      RGBA16_SNORM,    RGBA, L, VT__),
-   FMT(I8_SINT,                 R8I,             RRRR, L, VTR_),
-   FMT(L8_SINT,                 R8I,             RRR1, L, VTR_),
-   FMT(I8_UINT,                 R8UI,            RRRR, L, VTR_),
-   FMT(L8_UINT,                 R8UI,            RRR1, L, VTR_),
-   FMT(I16_SINT,                R16I,            RRRR, L, VTR_),
-   FMT(L16_SINT,                R16I,            RRR1, L, VTR_),
-   FMT(I16_UINT,                R16UI,           RRRR, L, VTR_),
-   FMT(L16_UINT,                R16UI,           RRR1, L, VTR_),
-   FMT(I32_SINT,                R32I,            RRRR, L, VTR_),
-   FMT(L32_SINT,                R32I,            RRR1, L, VTR_),
-   FMT(I32_UINT,                R32UI,           RRRR, L, VTR_),
-   FMT(L32_UINT,                R32UI,           RRR1, L, VTR_),
-   FMT(B8G8R8_UINT,             RGB8UI,          BGR1, L, VTR_),
-   FMT(B8G8R8A8_UINT,           RGBA8UI,         BGRA, L, VTR_),
-   FMT(B8G8R8_SINT,             RGB8I,           BGR1, L, VTR_),
-   FMT(B8G8R8A8_SINT,           RGBA8I,          BGRA, L, VTR_),
-   FMT(A8R8G8B8_UINT,           RGBA8UI,         GBAR, L, VTR_),
-   FMT(A8B8G8R8_UINT,           RGBA8UI,         ABGR, L, VTR_),
-   FMT(R8_UINT,                 R8UI,            R001, L, VTR_),
-   FMT(R16_UINT,                R16UI,           R001, L, VTR_),
-   FMT(R32_UINT,                R32UI,           R001, L, VTR_),
-   FMT(R8G8_UINT,               RG8UI,           RG01, L, VTR_),
-   FMT(R16G16_UINT,             RG16UI,          RG01, L, VTR_),
-   FMT(R32G32_UINT,             RG32UI,          RG01, L, VTR_),
-   FMT(R8G8B8_UINT,             RGB8UI,          RGB1, L, VTR_),
-   FMT(R32G32B32_UINT,          RGB32UI,         RGB1, L, VTR_),
-   FMT(R8G8B8A8_UINT,           RGBA8UI,         RGBA, L, VTR_),
-   FMT(R16G16B16A16_UINT,       RGBA16UI,        RGBA, L, VTR_),
-   FMT(R32G32B32A32_UINT,       RGBA32UI,        RGBA, L, VTR_),
-   FMT(R32_FLOAT,               R32F,            R001, L, VTR_),
-   FMT(R32G32_FLOAT,            RG32F,           RG01, L, VTR_),
-   FMT(R32G32B32_FLOAT,         RGB32F,          RGB1, L, VTR_),
-   FMT(R32G32B32A32_FLOAT,      RGBA32F,         RGBA, L, VTR_),
-   FMT(R8_UNORM,                R8_UNORM,        R001, L, VTR_),
-   FMT(R16_UNORM,               R16_UNORM,       R001, L, VTR_),
-   FMT(R8G8_UNORM,              RG8_UNORM,       RG01, L, VTR_),
-   FMT(R16G16_UNORM,            RG16_UNORM,      RG01, L, VTR_),
-   FMT(R8G8B8_UNORM,            RGB8_UNORM,      RGB1, L, VTR_),
+   FMT(R10G10B10A2_UINT, RGB10_A2UI, RGBA, L, VTR_),
+   FMT(B10G10R10A2_UINT, RGB10_A2UI, BGRA, L, VTR_),
+   FMT(R10G10B10A2_USCALED, RGB10_A2UI, RGBA, L, V___),
+   FMT(B10G10R10A2_USCALED, RGB10_A2UI, BGRA, L, V___),
+   FMT(R10G10B10A2_SINT, RGB10_A2I, RGBA, L, VTR_),
+   FMT(B10G10R10A2_SINT, RGB10_A2I, BGRA, L, VTR_),
+   FMT(R10G10B10A2_SSCALED, RGB10_A2I, RGBA, L, V___),
+   FMT(B10G10R10A2_SSCALED, RGB10_A2I, BGRA, L, V___),
+   FMT(R8_SSCALED, R8I, R001, L, V___),
+   FMT(R8G8_SSCALED, RG8I, RG01, L, V___),
+   FMT(R8G8B8_SSCALED, RGB8I, RGB1, L, V___),
+   FMT(B8G8R8_SSCALED, RGB8I, BGR1, L, V___),
+   FMT(R8G8B8A8_SSCALED, RGBA8I, RGBA, L, V___),
+   FMT(B8G8R8A8_SSCALED, RGBA8I, BGRA, L, V___),
+   FMT(A8B8G8R8_SSCALED, RGBA8I, ABGR, L, V___),
+   FMT(R8_USCALED, R8UI, R001, L, V___),
+   FMT(R8G8_USCALED, RG8UI, RG01, L, V___),
+   FMT(R8G8B8_USCALED, RGB8UI, RGB1, L, V___),
+   FMT(B8G8R8_USCALED, RGB8UI, BGR1, L, V___),
+   FMT(R8G8B8A8_USCALED, RGBA8UI, RGBA, L, V___),
+   FMT(B8G8R8A8_USCALED, RGBA8UI, BGRA, L, V___),
+   FMT(A8B8G8R8_USCALED, RGBA8UI, ABGR, L, V___),
+   FMT(R16_USCALED, R16UI, R001, L, V___),
+   FMT(R16G16_USCALED, RG16UI, RG01, L, V___),
+   FMT(R16G16B16A16_USCALED, RGBA16UI, RGBA, L, V___),
+   FMT(R16_SSCALED, R16I, R001, L, V___),
+   FMT(R16G16_SSCALED, RG16I, RG01, L, V___),
+   FMT(R16G16B16A16_SSCALED, RGBA16I, RGBA, L, V___),
+   FMT(R32_USCALED, R32UI, R001, L, V___),
+   FMT(R32G32_USCALED, RG32UI, RG01, L, V___),
+   FMT(R32G32B32_USCALED, RGB32UI, RGB1, L, V___),
+   FMT(R32G32B32A32_USCALED, RGBA32UI, RGBA, L, V___),
+   FMT(R32_SSCALED, R32I, R001, L, V___),
+   FMT(R32G32_SSCALED, RG32I, RG01, L, V___),
+   FMT(R32G32B32_SSCALED, RGB32I, RGB1, L, V___),
+   FMT(R32G32B32A32_SSCALED, RGBA32I, RGBA, L, V___),
+   FMT(R32_FIXED, R32_FIXED, R001, L, V___),
+   FMT(R32G32_FIXED, RG32_FIXED, RG01, L, V___),
+   FMT(R32G32B32_FIXED, RGB32_FIXED, RGB1, L, V___),
+   FMT(R32G32B32A32_FIXED, RGBA32_FIXED, RGBA, L, V___),
+   FMT(R11G11B10_FLOAT, R11F_G11F_B10F, RGB1, L, VTR_),
+   FMT(R9G9B9E5_FLOAT, R9F_G9F_B9F_E5F, RGB1, L, VT__),
+   FMT(R8_SNORM, R8_SNORM, R001, L, VT__),
+   FMT(R16_SNORM, R16_SNORM, R001, L, VT__),
+   FMT(R8G8_SNORM, RG8_SNORM, RG01, L, VT__),
+   FMT(R16G16_SNORM, RG16_SNORM, RG01, L, VT__),
+   FMT(R8G8B8_SNORM, RGB8_SNORM, RGB1, L, VT__),
+   FMT(R8G8B8A8_SNORM, RGBA8_SNORM, RGBA, L, VT__),
+   FMT(R16G16B16A16_SNORM, RGBA16_SNORM, RGBA, L, VT__),
+   FMT(I8_SINT, R8I, RRRR, L, VTR_),
+   FMT(L8_SINT, R8I, RRR1, L, VTR_),
+   FMT(I8_UINT, R8UI, RRRR, L, VTR_),
+   FMT(L8_UINT, R8UI, RRR1, L, VTR_),
+   FMT(I16_SINT, R16I, RRRR, L, VTR_),
+   FMT(L16_SINT, R16I, RRR1, L, VTR_),
+   FMT(I16_UINT, R16UI, RRRR, L, VTR_),
+   FMT(L16_UINT, R16UI, RRR1, L, VTR_),
+   FMT(I32_SINT, R32I, RRRR, L, VTR_),
+   FMT(L32_SINT, R32I, RRR1, L, VTR_),
+   FMT(I32_UINT, R32UI, RRRR, L, VTR_),
+   FMT(L32_UINT, R32UI, RRR1, L, VTR_),
+   FMT(B8G8R8_UINT, RGB8UI, BGR1, L, VTR_),
+   FMT(B8G8R8A8_UINT, RGBA8UI, BGRA, L, VTR_),
+   FMT(B8G8R8_SINT, RGB8I, BGR1, L, VTR_),
+   FMT(B8G8R8A8_SINT, RGBA8I, BGRA, L, VTR_),
+   FMT(A8R8G8B8_UINT, RGBA8UI, GBAR, L, VTR_),
+   FMT(A8B8G8R8_UINT, RGBA8UI, ABGR, L, VTR_),
+   FMT(R8_UINT, R8UI, R001, L, VTR_),
+   FMT(R16_UINT, R16UI, R001, L, VTR_),
+   FMT(R32_UINT, R32UI, R001, L, VTR_),
+   FMT(R8G8_UINT, RG8UI, RG01, L, VTR_),
+   FMT(R16G16_UINT, RG16UI, RG01, L, VTR_),
+   FMT(R32G32_UINT, RG32UI, RG01, L, VTR_),
+   FMT(R8G8B8_UINT, RGB8UI, RGB1, L, VTR_),
+   FMT(R32G32B32_UINT, RGB32UI, RGB1, L, VTR_),
+   FMT(R8G8B8A8_UINT, RGBA8UI, RGBA, L, VTR_),
+   FMT(R16G16B16A16_UINT, RGBA16UI, RGBA, L, VTR_),
+   FMT(R32G32B32A32_UINT, RGBA32UI, RGBA, L, VTR_),
+   FMT(R32_FLOAT, R32F, R001, L, VTR_),
+   FMT(R32G32_FLOAT, RG32F, RG01, L, VTR_),
+   FMT(R32G32B32_FLOAT, RGB32F, RGB1, L, VTR_),
+   FMT(R32G32B32A32_FLOAT, RGBA32F, RGBA, L, VTR_),
+   FMT(R8_UNORM, R8_UNORM, R001, L, VTR_),
+   FMT(R16_UNORM, R16_UNORM, R001, L, VTR_),
+   FMT(R8G8_UNORM, RG8_UNORM, RG01, L, VTR_),
+   FMT(R16G16_UNORM, RG16_UNORM, RG01, L, VTR_),
+   FMT(R8G8B8_UNORM, RGB8_UNORM, RGB1, L, VTR_),
 
-   /* 32-bit NORM is not texturable in v7 onwards. It's renderable
-    * everywhere, but rendering without texturing is not useful.
-    */
+/* 32-bit NORM is not texturable in v7 onwards. It's renderable
+ * everywhere, but rendering without texturing is not useful.
+ */
 #if PAN_ARCH <= 6
-   FMT(R32_UNORM,               R32_UNORM,       R001, L, VTR_),
-   FMT(R32G32_UNORM,            RG32_UNORM,      RG01, L, VTR_),
-   FMT(R32G32B32_UNORM,         RGB32_UNORM,     RGB1, L, VT__),
-   FMT(R32G32B32A32_UNORM,      RGBA32_UNORM,    RGBA, L, VTR_),
-   FMT(R32_SNORM,               R32_SNORM,       R001, L, VT__),
-   FMT(R32G32_SNORM,            RG32_SNORM,      RG01, L, VT__),
-   FMT(R32G32B32_SNORM,         RGB32_SNORM,     RGB1, L, VT__),
-   FMT(R32G32B32A32_SNORM,      RGBA32_SNORM,    RGBA, L, VT__),
+   FMT(R32_UNORM, R32_UNORM, R001, L, VTR_),
+   FMT(R32G32_UNORM, RG32_UNORM, RG01, L, VTR_),
+   FMT(R32G32B32_UNORM, RGB32_UNORM, RGB1, L, VT__),
+   FMT(R32G32B32A32_UNORM, RGBA32_UNORM, RGBA, L, VTR_),
+   FMT(R32_SNORM, R32_SNORM, R001, L, VT__),
+   FMT(R32G32_SNORM, RG32_SNORM, RG01, L, VT__),
+   FMT(R32G32B32_SNORM, RGB32_SNORM, RGB1, L, VT__),
+   FMT(R32G32B32A32_SNORM, RGBA32_SNORM, RGBA, L, VT__),
 #else
-   FMT(R32_UNORM,               R32_UNORM,       R001, L, V___),
-   FMT(R32G32_UNORM,            RG32_UNORM,      RG01, L, V___),
-   FMT(R32G32B32_UNORM,         RGB32_UNORM,     RGB1, L, V___),
-   FMT(R32G32B32A32_UNORM,      RGBA32_UNORM,    RGBA, L, V___),
-   FMT(R32_SNORM,               R32_SNORM,       R001, L, V___),
-   FMT(R32G32_SNORM,            RG32_SNORM,      RG01, L, V___),
-   FMT(R32G32B32_SNORM,         RGB32_SNORM,     RGB1, L, V___),
-   FMT(R32G32B32A32_SNORM,      RGBA32_SNORM,    RGBA, L, V___),
+   FMT(R32_UNORM, R32_UNORM, R001, L, V___),
+   FMT(R32G32_UNORM, RG32_UNORM, RG01, L, V___),
+   FMT(R32G32B32_UNORM, RGB32_UNORM, RGB1, L, V___),
+   FMT(R32G32B32A32_UNORM, RGBA32_UNORM, RGBA, L, V___),
+   FMT(R32_SNORM, R32_SNORM, R001, L, V___),
+   FMT(R32G32_SNORM, RG32_SNORM, RG01, L, V___),
+   FMT(R32G32B32_SNORM, RGB32_SNORM, RGB1, L, V___),
+   FMT(R32G32B32A32_SNORM, RGBA32_SNORM, RGBA, L, V___),
 #endif
 
    /* Don't allow render/texture for 48-bit  */
-   FMT(R16G16B16_UNORM,         RGB16_UNORM,     RGB1, L, V___),
-   FMT(R16G16B16_SINT,          RGB16I,          RGB1, L, V___),
-   FMT(R16G16B16_FLOAT,         RGB16F,          RGB1, L, V___),
-   FMT(R16G16B16_USCALED,       RGB16UI,         RGB1, L, V___),
-   FMT(R16G16B16_SSCALED,       RGB16I,          RGB1, L, V___),
-   FMT(R16G16B16_SNORM,         RGB16_SNORM,     RGB1, L, V___),
-   FMT(R16G16B16_UINT,          RGB16UI,         RGB1, L, V___),
-   FMT(R4G4B4A4_UNORM,          RGBA4_UNORM,     RGBA, L, VTR_),
-   FMT(B4G4R4A4_UNORM,          RGBA4_UNORM,     BGRA, L, VTR_),
-   FMT(R16G16B16A16_UNORM,      RGBA16_UNORM,    RGBA, L, VTR_),
-   FMT(B8G8R8A8_UNORM,          RGBA8_UNORM,     BGRA, L, VTR_),
-   FMT(B8G8R8X8_UNORM,          RGBA8_UNORM,     BGR1, L, VTR_),
-   FMT(A8R8G8B8_UNORM,          RGBA8_UNORM,     GBAR, L, VTR_),
-   FMT(X8R8G8B8_UNORM,          RGBA8_UNORM,     GBA1, L, VTR_),
-   FMT(A8B8G8R8_UNORM,          RGBA8_UNORM,     ABGR, L, VTR_),
-   FMT(X8B8G8R8_UNORM,          RGBA8_UNORM,     ABG1, L, VTR_),
-   FMT(R8G8B8X8_UNORM,          RGBA8_UNORM,     RGB1, L, VTR_),
-   FMT(R8G8B8A8_UNORM,          RGBA8_UNORM,     RGBA, L, VTR_),
-   FMT(R8G8B8X8_SNORM,          RGBA8_SNORM,     RGB1, L, VT__),
-   FMT(R8G8B8X8_SRGB,           RGBA8_UNORM,     RGB1, S, VTR_),
-   FMT(R8G8B8X8_UINT,           RGBA8UI,         RGB1, L, VTR_),
-   FMT(R8G8B8X8_SINT,           RGBA8I,          RGB1, L, VTR_),
-   FMT(L8_UNORM,                R8_UNORM,        RRR1, L, VTR_),
-   FMT(I8_UNORM,                R8_UNORM,        RRRR, L, VTR_),
-   FMT(L16_UNORM,               R16_UNORM,       RRR1, L, VT__),
-   FMT(I16_UNORM,               R16_UNORM,       RRRR, L, VT__),
-   FMT(L8_SNORM,                R8_SNORM,        RRR1, L, VT__),
-   FMT(I8_SNORM,                R8_SNORM,        RRRR, L, VT__),
-   FMT(L16_SNORM,               R16_SNORM,       RRR1, L, VT__),
-   FMT(I16_SNORM,               R16_SNORM,       RRRR, L, VT__),
-   FMT(L16_FLOAT,               R16F,            RRR1, L, VTR_),
-   FMT(I16_FLOAT,               RG16F,           RRRR, L, VTR_),
-   FMT(L8_SRGB,                 R8_UNORM,        RRR1, S, VTR_),
-   FMT(R8_SRGB,                 R8_UNORM,        R001, S, VTR_),
-   FMT(R8G8_SRGB,               RG8_UNORM,       RG01, S, VTR_),
-   FMT(R8G8B8_SRGB,             RGB8_UNORM,      RGB1, S, VTR_),
-   FMT(B8G8R8_SRGB,             RGB8_UNORM,      BGR1, S, VTR_),
-   FMT(R8G8B8A8_SRGB,           RGBA8_UNORM,     RGBA, S, VTR_),
-   FMT(A8B8G8R8_SRGB,           RGBA8_UNORM,     ABGR, S, VTR_),
-   FMT(X8B8G8R8_SRGB,           RGBA8_UNORM,     ABG1, S, VTR_),
-   FMT(B8G8R8A8_SRGB,           RGBA8_UNORM,     BGRA, S, VTR_),
-   FMT(B8G8R8X8_SRGB,           RGBA8_UNORM,     BGR1, S, VTR_),
-   FMT(A8R8G8B8_SRGB,           RGBA8_UNORM,     GBAR, S, VTR_),
-   FMT(X8R8G8B8_SRGB,           RGBA8_UNORM,     GBA1, S, VTR_),
-   FMT(R8_SINT,                 R8I,             R001, L, VTR_),
-   FMT(R16_SINT,                R16I,            R001, L, VTR_),
-   FMT(R32_SINT,                R32I,            R001, L, VTR_),
-   FMT(R16_FLOAT,               R16F,            R001, L, VTR_),
-   FMT(R8G8_SINT,               RG8I,            RG01, L, VTR_),
-   FMT(R16G16_SINT,             RG16I,           RG01, L, VTR_),
-   FMT(R32G32_SINT,             RG32I,           RG01, L, VTR_),
-   FMT(R16G16_FLOAT,            RG16F,           RG01, L, VTR_),
-   FMT(R8G8B8_SINT,             RGB8I,           RGB1, L, VTR_),
-   FMT(R32G32B32_SINT,          RGB32I,          RGB1, L, VTR_),
-   FMT(R8G8B8A8_SINT,           RGBA8I,          RGBA, L, VTR_),
-   FMT(R16G16B16A16_SINT,       RGBA16I,         RGBA, L, VTR_),
-   FMT(R32G32B32A32_SINT,       RGBA32I,         RGBA, L, VTR_),
-   FMT(R16G16B16A16_FLOAT,      RGBA16F,         RGBA, L, VTR_),
-   FMT(R16G16B16X16_UNORM,      RGBA16_UNORM,    RGB1, L, VTR_),
-   FMT(R16G16B16X16_SNORM,      RGBA16_SNORM,    RGB1, L, VT__),
-   FMT(R16G16B16X16_FLOAT,      RGBA16F,         RGB1, L, VTR_),
-   FMT(R16G16B16X16_UINT,       RGBA16UI,        RGB1, L, VTR_),
-   FMT(R16G16B16X16_SINT,       RGBA16I,         RGB1, L, VTR_),
-   FMT(R32G32B32X32_FLOAT,      RGBA32F,         RGB1, L, VTR_),
-   FMT(R32G32B32X32_UINT,       RGBA32UI,        RGB1, L, VTR_),
-   FMT(R32G32B32X32_SINT,       RGBA32I,         RGB1, L, VTR_),
+   FMT(R16G16B16_UNORM, RGB16_UNORM, RGB1, L, V___),
+   FMT(R16G16B16_SINT, RGB16I, RGB1, L, V___),
+   FMT(R16G16B16_FLOAT, RGB16F, RGB1, L, V___),
+   FMT(R16G16B16_USCALED, RGB16UI, RGB1, L, V___),
+   FMT(R16G16B16_SSCALED, RGB16I, RGB1, L, V___),
+   FMT(R16G16B16_SNORM, RGB16_SNORM, RGB1, L, V___),
+   FMT(R16G16B16_UINT, RGB16UI, RGB1, L, V___),
+   FMT(R4G4B4A4_UNORM, RGBA4_UNORM, RGBA, L, VTR_),
+   FMT(B4G4R4A4_UNORM, RGBA4_UNORM, BGRA, L, VTR_),
+   FMT(R16G16B16A16_UNORM, RGBA16_UNORM, RGBA, L, VTR_),
+   FMT(B8G8R8A8_UNORM, RGBA8_UNORM, BGRA, L, VTR_),
+   FMT(B8G8R8X8_UNORM, RGBA8_UNORM, BGR1, L, VTR_),
+   FMT(A8R8G8B8_UNORM, RGBA8_UNORM, GBAR, L, VTR_),
+   FMT(X8R8G8B8_UNORM, RGBA8_UNORM, GBA1, L, VTR_),
+   FMT(A8B8G8R8_UNORM, RGBA8_UNORM, ABGR, L, VTR_),
+   FMT(X8B8G8R8_UNORM, RGBA8_UNORM, ABG1, L, VTR_),
+   FMT(R8G8B8X8_UNORM, RGBA8_UNORM, RGB1, L, VTR_),
+   FMT(R8G8B8A8_UNORM, RGBA8_UNORM, RGBA, L, VTR_),
+   FMT(R8G8B8X8_SNORM, RGBA8_SNORM, RGB1, L, VT__),
+   FMT(R8G8B8X8_SRGB, RGBA8_UNORM, RGB1, S, VTR_),
+   FMT(R8G8B8X8_UINT, RGBA8UI, RGB1, L, VTR_),
+   FMT(R8G8B8X8_SINT, RGBA8I, RGB1, L, VTR_),
+   FMT(L8_UNORM, R8_UNORM, RRR1, L, VTR_),
+   FMT(I8_UNORM, R8_UNORM, RRRR, L, VTR_),
+   FMT(L16_UNORM, R16_UNORM, RRR1, L, VT__),
+   FMT(I16_UNORM, R16_UNORM, RRRR, L, VT__),
+   FMT(L8_SNORM, R8_SNORM, RRR1, L, VT__),
+   FMT(I8_SNORM, R8_SNORM, RRRR, L, VT__),
+   FMT(L16_SNORM, R16_SNORM, RRR1, L, VT__),
+   FMT(I16_SNORM, R16_SNORM, RRRR, L, VT__),
+   FMT(L16_FLOAT, R16F, RRR1, L, VTR_),
+   FMT(I16_FLOAT, RG16F, RRRR, L, VTR_),
+   FMT(L8_SRGB, R8_UNORM, RRR1, S, VTR_),
+   FMT(R8_SRGB, R8_UNORM, R001, S, VTR_),
+   FMT(R8G8_SRGB, RG8_UNORM, RG01, S, VTR_),
+   FMT(R8G8B8_SRGB, RGB8_UNORM, RGB1, S, VTR_),
+   FMT(B8G8R8_SRGB, RGB8_UNORM, BGR1, S, VTR_),
+   FMT(R8G8B8A8_SRGB, RGBA8_UNORM, RGBA, S, VTR_),
+   FMT(A8B8G8R8_SRGB, RGBA8_UNORM, ABGR, S, VTR_),
+   FMT(X8B8G8R8_SRGB, RGBA8_UNORM, ABG1, S, VTR_),
+   FMT(B8G8R8A8_SRGB, RGBA8_UNORM, BGRA, S, VTR_),
+   FMT(B8G8R8X8_SRGB, RGBA8_UNORM, BGR1, S, VTR_),
+   FMT(A8R8G8B8_SRGB, RGBA8_UNORM, GBAR, S, VTR_),
+   FMT(X8R8G8B8_SRGB, RGBA8_UNORM, GBA1, S, VTR_),
+   FMT(R8_SINT, R8I, R001, L, VTR_),
+   FMT(R16_SINT, R16I, R001, L, VTR_),
+   FMT(R32_SINT, R32I, R001, L, VTR_),
+   FMT(R16_FLOAT, R16F, R001, L, VTR_),
+   FMT(R8G8_SINT, RG8I, RG01, L, VTR_),
+   FMT(R16G16_SINT, RG16I, RG01, L, VTR_),
+   FMT(R32G32_SINT, RG32I, RG01, L, VTR_),
+   FMT(R16G16_FLOAT, RG16F, RG01, L, VTR_),
+   FMT(R8G8B8_SINT, RGB8I, RGB1, L, VTR_),
+   FMT(R32G32B32_SINT, RGB32I, RGB1, L, VTR_),
+   FMT(R8G8B8A8_SINT, RGBA8I, RGBA, L, VTR_),
+   FMT(R16G16B16A16_SINT, RGBA16I, RGBA, L, VTR_),
+   FMT(R32G32B32A32_SINT, RGBA32I, RGBA, L, VTR_),
+   FMT(R16G16B16A16_FLOAT, RGBA16F, RGBA, L, VTR_),
+   FMT(R16G16B16X16_UNORM, RGBA16_UNORM, RGB1, L, VTR_),
+   FMT(R16G16B16X16_SNORM, RGBA16_SNORM, RGB1, L, VT__),
+   FMT(R16G16B16X16_FLOAT, RGBA16F, RGB1, L, VTR_),
+   FMT(R16G16B16X16_UINT, RGBA16UI, RGB1, L, VTR_),
+   FMT(R16G16B16X16_SINT, RGBA16I, RGB1, L, VTR_),
+   FMT(R32G32B32X32_FLOAT, RGBA32F, RGB1, L, VTR_),
+   FMT(R32G32B32X32_UINT, RGBA32UI, RGB1, L, VTR_),
+   FMT(R32G32B32X32_SINT, RGBA32I, RGB1, L, VTR_),
 
 #if PAN_ARCH <= 6
-   FMT(Z16_UNORM,               R16_UNORM,       RRRR, L, _T_Z),
-   FMT(Z24_UNORM_S8_UINT,       Z24X8_UNORM,     RRRR, L, _T_Z),
-   FMT(Z24X8_UNORM,             Z24X8_UNORM,     RRRR, L, _T_Z),
-   FMT(Z32_FLOAT,               R32F,            RRRR, L, _T_Z),
-   FMT(Z32_FLOAT_S8X24_UINT,    RG32F,           RRRR, L, _T_Z),
-   FMT(X32_S8X24_UINT,          X32_S8X24,       GGGG, L, _T_Z),
-   FMT(X24S8_UINT,              RGBA8UI,         AAAA, L, _T_Z),
-   FMT(S8_UINT,                 R8UI,            RRRR, L, _T__),
-
-   FMT(A8_UNORM,                R8_UNORM,        000R, L, VTR_),
-   FMT(L8A8_UNORM,              RG8_UNORM,       RRRG, L, VTR_),
-   FMT(L8A8_SRGB,               RG8_UNORM,       RRRG, S, VTR_),
+   FMT(Z16_UNORM, R16_UNORM, RRRR, L, _T_Z),
+   FMT(Z24_UNORM_S8_UINT, Z24X8_UNORM, RRRR, L, _T_Z),
+   FMT(Z24X8_UNORM, Z24X8_UNORM, RRRR, L, _T_Z),
+   FMT(Z32_FLOAT, R32F, RRRR, L, _T_Z),
+   FMT(Z32_FLOAT_S8X24_UINT, RG32F, RRRR, L, _T_Z),
+   FMT(X32_S8X24_UINT, X32_S8X24, GGGG, L, _T_Z),
+   FMT(X24S8_UINT, RGBA8UI, AAAA, L, _T_Z),
+   FMT(S8_UINT, R8UI, RRRR, L, _T__),
+
+   FMT(A8_UNORM, R8_UNORM, 000R, L, VTR_),
+   FMT(L8A8_UNORM, RG8_UNORM, RRRG, L, VTR_),
+   FMT(L8A8_SRGB, RG8_UNORM, RRRG, S, VTR_),
 
    /* These formats were removed in v7 */
-   FMT(A8_SNORM,                R8_SNORM,        000R, L, VT__),
-   FMT(A8_SINT,                 R8I,             000R, L, VTR_),
-   FMT(A8_UINT,                 R8UI,            000R, L, VTR_),
-   FMT(A16_SINT,                R16I,            000R, L, VTR_),
-   FMT(A16_UINT,                R16UI,           000R, L, VTR_),
-   FMT(A32_SINT,                R32I,            000R, L, VTR_),
-   FMT(A32_UINT,                R32UI,           000R, L, VTR_),
-   FMT(A16_UNORM,               R16_UNORM,       000R, L, VT__),
-   FMT(A16_SNORM,               R16_SNORM,       000R, L, VT__),
-   FMT(A16_FLOAT,               R16F,            000R, L, VTR_),
+   FMT(A8_SNORM, R8_SNORM, 000R, L, VT__),
+   FMT(A8_SINT, R8I, 000R, L, VTR_),
+   FMT(A8_UINT, R8UI, 000R, L, VTR_),
+   FMT(A16_SINT, R16I, 000R, L, VTR_),
+   FMT(A16_UINT, R16UI, 000R, L, VTR_),
+   FMT(A32_SINT, R32I, 000R, L, VTR_),
+   FMT(A32_UINT, R32UI, 000R, L, VTR_),
+   FMT(A16_UNORM, R16_UNORM, 000R, L, VT__),
+   FMT(A16_SNORM, R16_SNORM, 000R, L, VT__),
+   FMT(A16_FLOAT, R16F, 000R, L, VTR_),
 
 #else
-   FMT(Z16_UNORM,               Z16_UNORM,       RGBA, L, _T_Z),
-   FMT(Z24_UNORM_S8_UINT,       Z24X8_UNORM,     RGBA, L, _T_Z),
-   FMT(Z24X8_UNORM,             Z24X8_UNORM,     RGBA, L, _T_Z),
-   FMT(Z32_FLOAT,               R32F,            RGBA, L, _T_Z),
+   FMT(Z16_UNORM, Z16_UNORM, RGBA, L, _T_Z),
+   FMT(Z24_UNORM_S8_UINT, Z24X8_UNORM, RGBA, L, _T_Z),
+   FMT(Z24X8_UNORM, Z24X8_UNORM, RGBA, L, _T_Z),
+   FMT(Z32_FLOAT, R32F, RGBA, L, _T_Z),
 
 #if PAN_ARCH >= 9
    /* Specify interchange formats, the actual format for depth/stencil is
@@ -575,80 +566,23 @@
     * "0s00" and "S8 GRBA" is logically "s000". For Bifrost compatibility
     * we want stencil in the red channel, so we use the GRBA swizzles.
     */
-   FMT(Z32_FLOAT_S8X24_UINT,    R32F,            GRBA, L, _T_Z),
-   FMT(X32_S8X24_UINT,          S8,              GRBA, L, _T__),
-   FMT(X24S8_UINT,              S8,              GRBA, L, _T_Z),
-   FMT(S8_UINT,                 S8,              GRBA, L, _T__),
+   FMT(Z32_FLOAT_S8X24_UINT, R32F, GRBA, L, _T_Z),
+   FMT(X32_S8X24_UINT, S8, GRBA, L, _T__),
+   FMT(X24S8_UINT, S8, GRBA, L, _T_Z),
+   FMT(S8_UINT, S8, GRBA, L, _T__),
 
 #else
    /* Specify real formats on Bifrost */
-   FMT(Z32_FLOAT_S8X24_UINT,    Z32_X32,         RGBA, L, _T_Z),
-   FMT(X32_S8X24_UINT,          X32_S8X24,       GRBA, L, _T__),
-   FMT(X24S8_UINT,              X24S8,           GRBA, L, _T_Z),
-   FMT(S8_UINT,                 S8,              GRBA, L, _T__),
+   FMT(Z32_FLOAT_S8X24_UINT, Z32_X32, RGBA, L, _T_Z),
+   FMT(X32_S8X24_UINT, X32_S8X24, GRBA, L, _T__),
+   FMT(X24S8_UINT, X24S8, GRBA, L, _T_Z),
+   FMT(S8_UINT, S8, GRBA, L, _T__),
 
    /* Obsolete formats removed in Valhall */
-   FMT(A8_UNORM,                A8_UNORM,        000A, L, VTR_),
-   FMT(L8A8_UNORM,              R8A8_UNORM,      RRRA, L, VTR_),
-   FMT(L8A8_SRGB,               R8A8_UNORM,      RRRA, S, VTR_),
+   FMT(A8_UNORM, A8_UNORM, 000A, L, VTR_),
+   FMT(L8A8_UNORM, R8A8_UNORM, RRRA, L, VTR_),
+   FMT(L8A8_SRGB, R8A8_UNORM, RRRA, S, VTR_),
 #endif
 
 #endif
 };
-/* clang-format on */
-
-#if PAN_ARCH == 7
-/*
- * Decompose a component ordering swizzle into a component ordering (applied
- * first) and a swizzle (applied second). The output ordering "pre" is allowed
- * with compression and the swizzle "post" is a bijection.
- *
- * These properties allow any component ordering to be used with compression, by
- * using the output "pre" ordering, composing the API swizzle with the "post"
- * ordering, and applying the inverse of the "post" ordering to the border
- * colour to undo what we compose into the API swizzle.
- *
- * Note that "post" is a swizzle, not a component ordering, which means it is
- * inverted from the ordering. E.g. ARGB ordering uses a GBAR (YZWX) swizzle.
- */
-struct pan_decomposed_swizzle
-GENX(pan_decompose_swizzle)(enum mali_rgb_component_order order)
-{
-#define CASE(case_, pre_, R_, G_, B_, A_)                                      \
-   case MALI_RGB_COMPONENT_ORDER_##case_:                                      \
-      return (struct pan_decomposed_swizzle){                                  \
-         MALI_RGB_COMPONENT_ORDER_##pre_,                                      \
-         {                                                                     \
-            PIPE_SWIZZLE_##R_,                                                 \
-            PIPE_SWIZZLE_##G_,                                                 \
-            PIPE_SWIZZLE_##B_,                                                 \
-            PIPE_SWIZZLE_##A_,                                                 \
-         },                                                                    \
-      };
-
-   switch (order) {
-      CASE(RGBA, RGBA, X, Y, Z, W);
-      CASE(GRBA, RGBA, Y, X, Z, W);
-      CASE(BGRA, RGBA, Z, Y, X, W);
-      CASE(ARGB, RGBA, Y, Z, W, X);
-      CASE(AGRB, RGBA, Z, Y, W, X);
-      CASE(ABGR, RGBA, W, Z, Y, X);
-      CASE(RGB1, RGB1, X, Y, Z, W);
-      CASE(GRB1, RGB1, Y, X, Z, W);
-      CASE(BGR1, RGB1, Z, Y, X, W);
-      CASE(1RGB, RGB1, Y, Z, W, X);
-      CASE(1GRB, RGB1, Z, Y, W, X);
-      CASE(1BGR, RGB1, W, Z, Y, X);
-      CASE(RRRR, RRRR, X, Y, Z, W);
-      CASE(RRR1, RRR1, X, Y, Z, W);
-      CASE(RRRA, RRRA, X, Y, Z, W);
-      CASE(000A, 000A, X, Y, Z, W);
-      CASE(0001, 0001, X, Y, Z, W);
-      CASE(0000, 0000, X, Y, Z, W);
-   default:
-      unreachable("Invalid case for texturing");
-   }
-
-#undef CASE
-}
-#endif
diff -urN mesa-23.0.0/src/panfrost/lib/pan_format.h mesa/src/panfrost/lib/pan_format.h
--- mesa-23.0.0/src/panfrost/lib/pan_format.h	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/pan_format.h	2023-03-06 19:19:32.027304341 +0100
@@ -82,17 +82,4 @@
    }
 }
 
-#if PAN_ARCH == 7
-struct pan_decomposed_swizzle {
-   /* Component ordering to apply first */
-   enum mali_rgb_component_order pre;
-
-   /* Bijective swizzle applied after */
-   unsigned char post[4];
-};
-
-struct pan_decomposed_swizzle
-   GENX(pan_decompose_swizzle)(enum mali_rgb_component_order order);
-#endif
-
 #endif
diff -urN mesa-23.0.0/src/panfrost/lib/pan_indirect_dispatch.h mesa/src/panfrost/lib/pan_indirect_dispatch.h
--- mesa-23.0.0/src/panfrost/lib/pan_indirect_dispatch.h	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/pan_indirect_dispatch.h	2023-03-06 19:19:31.814302935 +0100
@@ -25,7 +25,7 @@
 #define __PAN_INDIRECT_DISPATCH_SHADERS_H__
 
 #include "genxml/gen_macros.h"
-#include "pan_scoreboard.h"
+#include "pan_device.h"
 
 struct pan_device;
 struct pan_scoreboard;
diff -urN mesa-23.0.0/src/panfrost/lib/pan_indirect_draw.c mesa/src/panfrost/lib/pan_indirect_draw.c
--- mesa-23.0.0/src/panfrost/lib/pan_indirect_draw.c	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/lib/pan_indirect_draw.c	2023-03-06 19:19:32.045304460 +0100
@@ -0,0 +1,1335 @@
+/*
+ * Copyright (C) 2021 Collabora, Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ *
+ */
+
+#include "pan_indirect_draw.h"
+#include <stdio.h>
+#include "compiler/nir/nir_builder.h"
+#include "util/macros.h"
+#include "util/u_memory.h"
+#include "pan_bo.h"
+#include "pan_encoder.h"
+#include "pan_pool.h"
+#include "pan_scoreboard.h"
+#include "pan_shader.h"
+#include "pan_util.h"
+
+#define WORD(x) ((x)*4)
+
+#define LOOP                                                                   \
+   for (nir_loop *l = nir_push_loop(b); l != NULL; nir_pop_loop(b, l), l = NULL)
+#define BREAK    nir_jump(b, nir_jump_break)
+#define CONTINUE nir_jump(b, nir_jump_continue)
+
+#define IF(cond) nir_push_if(b, cond);
+#define ELSE     nir_push_else(b, NULL);
+#define ENDIF    nir_pop_if(b, NULL);
+
+#define MIN_MAX_JOBS 128
+
+struct draw_data {
+   nir_ssa_def *draw_buf;
+   nir_ssa_def *draw_buf_stride;
+   nir_ssa_def *index_buf;
+   nir_ssa_def *restart_index;
+   nir_ssa_def *vertex_count;
+   nir_ssa_def *start_instance;
+   nir_ssa_def *instance_count;
+   nir_ssa_def *vertex_start;
+   nir_ssa_def *index_bias;
+   nir_ssa_def *draw_ctx;
+   nir_ssa_def *min_max_ctx;
+};
+
+struct instance_size {
+   nir_ssa_def *raw;
+   nir_ssa_def *padded;
+   nir_ssa_def *packed;
+};
+
+struct jobs_data {
+   nir_ssa_def *vertex_job;
+   nir_ssa_def *tiler_job;
+   nir_ssa_def *base_vertex_offset;
+   nir_ssa_def *first_vertex_sysval;
+   nir_ssa_def *base_vertex_sysval;
+   nir_ssa_def *base_instance_sysval;
+   nir_ssa_def *offset_start;
+   nir_ssa_def *invocation;
+};
+
+struct varyings_data {
+   nir_ssa_def *varying_bufs;
+   nir_ssa_def *pos_ptr;
+   nir_ssa_def *psiz_ptr;
+   nir_variable *mem_ptr;
+};
+
+struct attribs_data {
+   nir_ssa_def *attrib_count;
+   nir_ssa_def *attrib_bufs;
+   nir_ssa_def *attribs;
+};
+
+struct indirect_draw_shader_builder {
+   nir_builder b;
+   const struct panfrost_device *dev;
+   unsigned flags;
+   bool index_min_max_search;
+   unsigned index_size;
+   struct draw_data draw;
+   struct instance_size instance_size;
+   struct jobs_data jobs;
+   struct varyings_data varyings;
+   struct attribs_data attribs;
+};
+
+/* Describes an indirect draw (see glDrawArraysIndirect()) */
+
+struct indirect_draw_info {
+   uint32_t count;
+   uint32_t instance_count;
+   uint32_t start;
+   uint32_t start_instance;
+};
+
+struct indirect_indexed_draw_info {
+   uint32_t count;
+   uint32_t instance_count;
+   uint32_t start;
+   int32_t index_bias;
+   uint32_t start_instance;
+};
+
+/* Store the min/max index in a separate context. This is not supported yet, but
+ * the DDK seems to put all min/max search jobs at the beginning of the job
+ * chain when multiple indirect draws are issued to avoid the serialization
+ * caused by the draw patching jobs which have the suppress_prefetch flag set.
+ * Merging the min/max and draw contexts would prevent such optimizations (draw
+ * contexts are shared by all indirect draw in a batch).
+ */
+
+struct min_max_context {
+   uint32_t min;
+   uint32_t max;
+};
+
+/* Per-batch context shared by all indirect draws queued to a given batch. */
+
+struct indirect_draw_context {
+   /* Pointer to the top of the varying heap. */
+   mali_ptr varying_mem;
+};
+
+/* Indirect draw shader inputs. Those are stored in FAU. */
+
+struct indirect_draw_inputs {
+   /* indirect_draw_context pointer */
+   mali_ptr draw_ctx;
+
+   /* min_max_context pointer */
+   mali_ptr min_max_ctx;
+
+   /* Pointer to an array of indirect_draw_info objects */
+   mali_ptr draw_buf;
+
+   /* Pointer to an uint32_t containing the number of draws to issue */
+   mali_ptr draw_count_ptr;
+
+   /* index buffer */
+   mali_ptr index_buf;
+
+   /* {base,first}_{vertex,instance} sysvals */
+   mali_ptr first_vertex_sysval;
+   mali_ptr base_vertex_sysval;
+   mali_ptr base_instance_sysval;
+
+   /* Pointers to various cmdstream structs that need to be patched */
+   mali_ptr vertex_job;
+   mali_ptr tiler_job;
+   mali_ptr attrib_bufs;
+   mali_ptr attribs;
+   mali_ptr varying_bufs;
+   uint32_t draw_count;
+   uint32_t draw_buf_stride;
+   uint32_t restart_index;
+   uint32_t attrib_count;
+} PACKED;
+
+#define get_input_field(b, name)                                               \
+   nir_load_push_constant(                                                     \
+      b, 1, sizeof(((struct indirect_draw_inputs *)0)->name) * 8,              \
+      nir_imm_int(b, 0), .base = offsetof(struct indirect_draw_inputs, name))
+
+static nir_ssa_def *
+get_address(nir_builder *b, nir_ssa_def *base, nir_ssa_def *offset)
+{
+   return nir_iadd(b, base, nir_u2u64(b, offset));
+}
+
+static nir_ssa_def *
+get_address_imm(nir_builder *b, nir_ssa_def *base, unsigned offset)
+{
+   return get_address(b, base, nir_imm_int(b, offset));
+}
+
+static nir_ssa_def *
+load_global(nir_builder *b, nir_ssa_def *addr, unsigned ncomps,
+            unsigned bit_size)
+{
+   return nir_load_global(b, addr, 4, ncomps, bit_size);
+}
+
+static void
+store_global(nir_builder *b, nir_ssa_def *addr, nir_ssa_def *value,
+             unsigned ncomps)
+{
+   nir_store_global(b, addr, 4, value, (1 << ncomps) - 1);
+}
+
+static nir_ssa_def *
+get_draw_ctx_data(struct indirect_draw_shader_builder *builder, unsigned offset,
+                  unsigned size)
+{
+   nir_builder *b = &builder->b;
+   return load_global(b, get_address_imm(b, builder->draw.draw_ctx, offset), 1,
+                      size);
+}
+
+static void
+set_draw_ctx_data(struct indirect_draw_shader_builder *builder, unsigned offset,
+                  nir_ssa_def *value, unsigned size)
+{
+   nir_builder *b = &builder->b;
+   store_global(b, get_address_imm(b, builder->draw.draw_ctx, offset), value,
+                1);
+}
+
+#define get_draw_ctx_field(builder, name)                                      \
+   get_draw_ctx_data(builder, offsetof(struct indirect_draw_context, name),    \
+                     sizeof(((struct indirect_draw_context *)0)->name) * 8)
+
+#define set_draw_ctx_field(builder, name, val)                                 \
+   set_draw_ctx_data(builder, offsetof(struct indirect_draw_context, name),    \
+                     val,                                                      \
+                     sizeof(((struct indirect_draw_context *)0)->name) * 8)
+
+static nir_ssa_def *
+get_min_max_ctx_data(struct indirect_draw_shader_builder *builder,
+                     unsigned offset, unsigned size)
+{
+   nir_builder *b = &builder->b;
+   return load_global(b, get_address_imm(b, builder->draw.min_max_ctx, offset),
+                      1, size);
+}
+
+#define get_min_max_ctx_field(builder, name)                                   \
+   get_min_max_ctx_data(builder, offsetof(struct min_max_context, name),       \
+                        sizeof(((struct min_max_context *)0)->name) * 8)
+
+static void
+update_min(struct indirect_draw_shader_builder *builder, nir_ssa_def *val)
+{
+   nir_builder *b = &builder->b;
+   nir_ssa_def *addr = get_address_imm(b, builder->draw.min_max_ctx,
+                                       offsetof(struct min_max_context, min));
+   nir_global_atomic_umin(b, 32, addr, val);
+}
+
+static void
+update_max(struct indirect_draw_shader_builder *builder, nir_ssa_def *val)
+{
+   nir_builder *b = &builder->b;
+   nir_ssa_def *addr = get_address_imm(b, builder->draw.min_max_ctx,
+                                       offsetof(struct min_max_context, max));
+   nir_global_atomic_umax(b, 32, addr, val);
+}
+
+#define get_draw_field(b, draw_ptr, field)                                     \
+   load_global(b,                                                              \
+               get_address_imm(b, draw_ptr,                                    \
+                               offsetof(struct indirect_draw_info, field)),    \
+               1, sizeof(((struct indirect_draw_info *)0)->field) * 8)
+
+#define get_indexed_draw_field(b, draw_ptr, field)                             \
+   load_global(                                                                \
+      b,                                                                       \
+      get_address_imm(b, draw_ptr,                                             \
+                      offsetof(struct indirect_indexed_draw_info, field)),     \
+      1, sizeof(((struct indirect_indexed_draw_info *)0)->field) * 8)
+
+static void
+extract_inputs(struct indirect_draw_shader_builder *builder)
+{
+   nir_builder *b = &builder->b;
+
+   builder->draw.draw_ctx = get_input_field(b, draw_ctx);
+   builder->draw.draw_buf = get_input_field(b, draw_buf);
+   builder->draw.draw_buf_stride = get_input_field(b, draw_buf_stride);
+
+   if (builder->index_size) {
+      builder->draw.index_buf = get_input_field(b, index_buf);
+      builder->draw.min_max_ctx = get_input_field(b, min_max_ctx);
+      if (builder->flags & PAN_INDIRECT_DRAW_PRIMITIVE_RESTART) {
+         builder->draw.restart_index = get_input_field(b, restart_index);
+      }
+   }
+
+   if (builder->index_min_max_search)
+      return;
+
+   builder->jobs.first_vertex_sysval = get_input_field(b, first_vertex_sysval);
+   builder->jobs.base_vertex_sysval = get_input_field(b, base_vertex_sysval);
+   builder->jobs.base_instance_sysval =
+      get_input_field(b, base_instance_sysval);
+   builder->jobs.vertex_job = get_input_field(b, vertex_job);
+   builder->jobs.tiler_job = get_input_field(b, tiler_job);
+   builder->attribs.attrib_bufs = get_input_field(b, attrib_bufs);
+   builder->attribs.attribs = get_input_field(b, attribs);
+   builder->attribs.attrib_count = get_input_field(b, attrib_count);
+   builder->varyings.varying_bufs = get_input_field(b, varying_bufs);
+   builder->varyings.mem_ptr =
+      nir_local_variable_create(b->impl, glsl_uint64_t_type(), "var_mem_ptr");
+   nir_store_var(b, builder->varyings.mem_ptr,
+                 get_draw_ctx_field(builder, varying_mem), 3);
+}
+
+static void
+init_shader_builder(struct indirect_draw_shader_builder *builder,
+                    const struct panfrost_device *dev, unsigned flags,
+                    unsigned index_size, bool index_min_max_search)
+{
+   memset(builder, 0, sizeof(*builder));
+   builder->dev = dev;
+   builder->flags = flags;
+   builder->index_size = index_size;
+
+   builder->index_min_max_search = index_min_max_search;
+
+   if (index_min_max_search) {
+      builder->b = nir_builder_init_simple_shader(
+         MESA_SHADER_COMPUTE, GENX(pan_shader_get_compiler_options)(),
+         "indirect_draw_min_max_index(index_size=%d)", builder->index_size);
+   } else {
+      builder->b = nir_builder_init_simple_shader(
+         MESA_SHADER_COMPUTE, GENX(pan_shader_get_compiler_options)(),
+         "indirect_draw(index_size=%d%s%s%s%s)", builder->index_size,
+         flags & PAN_INDIRECT_DRAW_HAS_PSIZ ? ",psiz" : "",
+         flags & PAN_INDIRECT_DRAW_PRIMITIVE_RESTART ? ",primitive_restart"
+                                                     : "",
+         flags & PAN_INDIRECT_DRAW_UPDATE_PRIM_SIZE ? ",update_primitive_size"
+                                                    : "",
+         flags & PAN_INDIRECT_DRAW_IDVS ? ",idvs" : "");
+   }
+
+   extract_inputs(builder);
+}
+
+static void
+update_dcd(struct indirect_draw_shader_builder *builder, nir_ssa_def *job_ptr,
+           unsigned draw_offset)
+{
+   nir_builder *b = &builder->b;
+   nir_ssa_def *draw_w01 =
+      load_global(b, get_address_imm(b, job_ptr, draw_offset + WORD(0)), 2, 32);
+   nir_ssa_def *draw_w0 = nir_channel(b, draw_w01, 0);
+
+   /* Update DRAW.{instance_size,offset_start} */
+   nir_ssa_def *instance_size =
+      nir_bcsel(b, nir_ult(b, builder->draw.instance_count, nir_imm_int(b, 2)),
+                nir_imm_int(b, 0), builder->instance_size.packed);
+   draw_w01 = nir_vec2(b,
+                       nir_ior(b, nir_iand_imm(b, draw_w0, 0xffff),
+                               nir_ishl(b, instance_size, nir_imm_int(b, 16))),
+                       builder->jobs.offset_start);
+   store_global(b, get_address_imm(b, job_ptr, draw_offset + WORD(0)), draw_w01,
+                2);
+}
+
+static void
+update_job(struct indirect_draw_shader_builder *builder,
+           enum mali_job_type type)
+{
+   nir_builder *b = &builder->b;
+   nir_ssa_def *job_ptr = type == MALI_JOB_TYPE_VERTEX
+                             ? builder->jobs.vertex_job
+                             : builder->jobs.tiler_job;
+
+   /* Update the invocation words. */
+   store_global(b, get_address_imm(b, job_ptr, WORD(8)),
+                builder->jobs.invocation, 2);
+
+   unsigned draw_offset = type == MALI_JOB_TYPE_VERTEX
+                             ? pan_section_offset(COMPUTE_JOB, DRAW)
+                             : pan_section_offset(TILER_JOB, DRAW);
+   unsigned prim_offset = pan_section_offset(TILER_JOB, PRIMITIVE);
+   unsigned psiz_offset = pan_section_offset(TILER_JOB, PRIMITIVE_SIZE);
+   unsigned index_size = builder->index_size;
+
+   if (type == MALI_JOB_TYPE_TILER) {
+      /* Update PRIMITIVE.{base_vertex_offset,count} */
+      store_global(b, get_address_imm(b, job_ptr, prim_offset + WORD(1)),
+                   builder->jobs.base_vertex_offset, 1);
+      store_global(b, get_address_imm(b, job_ptr, prim_offset + WORD(3)),
+                   nir_iadd_imm(b, builder->draw.vertex_count, -1), 1);
+
+      if (index_size) {
+         nir_ssa_def *addr = get_address_imm(b, job_ptr, prim_offset + WORD(4));
+         nir_ssa_def *indices = load_global(b, addr, 1, 64);
+         nir_ssa_def *offset =
+            nir_imul_imm(b, builder->draw.vertex_start, index_size);
+
+         indices = get_address(b, indices, offset);
+         store_global(b, addr, indices, 2);
+      }
+
+      /* Update PRIMITIVE_SIZE.size_array */
+      if ((builder->flags & PAN_INDIRECT_DRAW_HAS_PSIZ) &&
+          (builder->flags & PAN_INDIRECT_DRAW_UPDATE_PRIM_SIZE)) {
+         store_global(b, get_address_imm(b, job_ptr, psiz_offset + WORD(0)),
+                      builder->varyings.psiz_ptr, 2);
+      }
+
+      /* Update DRAW.position */
+      store_global(b, get_address_imm(b, job_ptr, draw_offset + WORD(4)),
+                   builder->varyings.pos_ptr, 2);
+   }
+
+   update_dcd(builder, job_ptr, draw_offset);
+
+   if (builder->flags & PAN_INDIRECT_DRAW_IDVS) {
+      assert(type == MALI_JOB_TYPE_TILER);
+
+      update_dcd(builder, job_ptr,
+                 pan_section_offset(INDEXED_VERTEX_JOB, VERTEX_DRAW));
+   }
+}
+
+static void
+split_div(nir_builder *b, nir_ssa_def *div, nir_ssa_def **r_e, nir_ssa_def **d)
+{
+   /* TODO: Lower this 64bit div to something GPU-friendly */
+   nir_ssa_def *r = nir_imax(b, nir_ufind_msb(b, div), nir_imm_int(b, 0));
+   nir_ssa_def *div64 = nir_u2u64(b, div);
+   nir_ssa_def *half_div64 = nir_u2u64(b, nir_ushr_imm(b, div, 1));
+   nir_ssa_def *f0 = nir_iadd(
+      b, nir_ishl(b, nir_imm_int64(b, 1), nir_iadd_imm(b, r, 32)), half_div64);
+   nir_ssa_def *fi = nir_idiv(b, f0, div64);
+   nir_ssa_def *ff = nir_isub(b, f0, nir_imul(b, fi, div64));
+   nir_ssa_def *e = nir_bcsel(b, nir_ult(b, half_div64, ff),
+                              nir_imm_int(b, 1 << 5), nir_imm_int(b, 0));
+   *d = nir_iand_imm(b, nir_u2u32(b, fi), ~(1 << 31));
+   *r_e = nir_ior(b, r, e);
+}
+
+static void
+update_vertex_attrib_buf(struct indirect_draw_shader_builder *builder,
+                         nir_ssa_def *attrib_buf_ptr,
+                         enum mali_attribute_type type, nir_ssa_def *div1,
+                         nir_ssa_def *div2)
+{
+   nir_builder *b = &builder->b;
+   unsigned type_mask = BITFIELD_MASK(6);
+   nir_ssa_def *w01 = load_global(b, attrib_buf_ptr, 2, 32);
+   nir_ssa_def *w0 = nir_channel(b, w01, 0);
+   nir_ssa_def *w1 = nir_channel(b, w01, 1);
+
+   /* Word 0 and 1 of the attribute descriptor contain the type,
+    * pointer and the the divisor exponent.
+    */
+   w0 = nir_iand_imm(b, nir_channel(b, w01, 0), ~type_mask);
+   w0 = nir_ior(b, w0, nir_imm_int(b, type));
+   w1 = nir_ior(b, w1, nir_ishl(b, div1, nir_imm_int(b, 24)));
+
+   store_global(b, attrib_buf_ptr, nir_vec2(b, w0, w1), 2);
+
+   if (type == MALI_ATTRIBUTE_TYPE_1D_NPOT_DIVISOR) {
+      /* If the divisor is not a power of two, the divisor numerator
+       * is passed in word 1 of the continuation attribute (word 5
+       * if we consider the attribute and its continuation as a
+       * single attribute).
+       */
+      assert(div2);
+      store_global(b, get_address_imm(b, attrib_buf_ptr, WORD(5)), div2, 1);
+   }
+}
+
+static void
+zero_attrib_buf_stride(struct indirect_draw_shader_builder *builder,
+                       nir_ssa_def *attrib_buf_ptr)
+{
+   /* Stride is an unadorned 32-bit uint at word 2 */
+   nir_builder *b = &builder->b;
+   store_global(b, get_address_imm(b, attrib_buf_ptr, WORD(2)),
+                nir_imm_int(b, 0), 1);
+}
+
+static void
+adjust_attrib_offset(struct indirect_draw_shader_builder *builder,
+                     nir_ssa_def *attrib_ptr, nir_ssa_def *attrib_buf_ptr,
+                     nir_ssa_def *instance_div)
+{
+   nir_builder *b = &builder->b;
+   nir_ssa_def *zero = nir_imm_int(b, 0);
+   nir_ssa_def *two = nir_imm_int(b, 2);
+   nir_ssa_def *sub_cur_offset =
+      nir_iand(b, nir_ine(b, builder->jobs.offset_start, zero),
+               nir_uge(b, builder->draw.instance_count, two));
+
+   nir_ssa_def *add_base_inst_offset =
+      nir_iand(b, nir_ine(b, builder->draw.start_instance, zero),
+               nir_ine(b, instance_div, zero));
+
+   IF(nir_ior(b, sub_cur_offset, add_base_inst_offset))
+   {
+      nir_ssa_def *offset =
+         load_global(b, get_address_imm(b, attrib_ptr, WORD(1)), 1, 32);
+      nir_ssa_def *stride =
+         load_global(b, get_address_imm(b, attrib_buf_ptr, WORD(2)), 1, 32);
+
+      /* Per-instance data needs to be offset in response to a
+       * delayed start in an indexed draw.
+       */
+
+      IF(add_base_inst_offset)
+      {
+         offset = nir_iadd(
+            b, offset,
+            nir_idiv(b, nir_imul(b, stride, builder->draw.start_instance),
+                     instance_div));
+      }
+      ENDIF
+
+      IF(sub_cur_offset)
+      {
+         offset = nir_isub(b, offset,
+                           nir_imul(b, stride, builder->jobs.offset_start));
+      }
+      ENDIF
+
+      store_global(b, get_address_imm(b, attrib_ptr, WORD(1)), offset, 1);
+   }
+   ENDIF
+}
+
+/* x is power of two or zero <===> x has 0 (zero) or 1 (POT) bits set */
+
+static nir_ssa_def *
+nir_is_power_of_two_or_zero(nir_builder *b, nir_ssa_def *x)
+{
+   return nir_ult(b, nir_bit_count(b, x), nir_imm_int(b, 2));
+}
+
+/* Based on panfrost_emit_vertex_data() */
+
+static void
+update_vertex_attribs(struct indirect_draw_shader_builder *builder)
+{
+   nir_builder *b = &builder->b;
+   nir_variable *attrib_idx_var =
+      nir_local_variable_create(b->impl, glsl_uint_type(), "attrib_idx");
+   nir_store_var(b, attrib_idx_var, nir_imm_int(b, 0), 1);
+
+#if PAN_ARCH <= 5
+   nir_ssa_def *single_instance =
+      nir_ult(b, builder->draw.instance_count, nir_imm_int(b, 2));
+#endif
+
+   LOOP
+   {
+      nir_ssa_def *attrib_idx = nir_load_var(b, attrib_idx_var);
+      IF(nir_uge(b, attrib_idx, builder->attribs.attrib_count))
+      BREAK;
+      ENDIF
+
+      nir_ssa_def *attrib_buf_ptr = get_address(
+         b, builder->attribs.attrib_bufs,
+         nir_imul_imm(b, attrib_idx, 2 * pan_size(ATTRIBUTE_BUFFER)));
+      nir_ssa_def *attrib_ptr =
+         get_address(b, builder->attribs.attribs,
+                     nir_imul_imm(b, attrib_idx, pan_size(ATTRIBUTE)));
+
+      nir_ssa_def *r_e, *d;
+
+#if PAN_ARCH <= 5
+      IF(nir_ieq_imm(b, attrib_idx, PAN_VERTEX_ID))
+      {
+         nir_ssa_def *r_p = nir_bcsel(b, single_instance, nir_imm_int(b, 0x9f),
+                                      builder->instance_size.packed);
+
+         store_global(b, get_address_imm(b, attrib_buf_ptr, WORD(4)),
+                      nir_ishl(b, r_p, nir_imm_int(b, 24)), 1);
+
+         nir_store_var(b, attrib_idx_var, nir_iadd_imm(b, attrib_idx, 1), 1);
+         CONTINUE;
+      }
+      ENDIF
+
+      IF(nir_ieq_imm(b, attrib_idx, PAN_INSTANCE_ID))
+      {
+         split_div(b, builder->instance_size.padded, &r_e, &d);
+         nir_ssa_def *default_div = nir_ior(
+            b, single_instance,
+            nir_ult(b, builder->instance_size.padded, nir_imm_int(b, 2)));
+         r_e = nir_bcsel(b, default_div, nir_imm_int(b, 0x3f), r_e);
+         d = nir_bcsel(b, default_div, nir_imm_int(b, (1u << 31) - 1), d);
+         store_global(b, get_address_imm(b, attrib_buf_ptr, WORD(1)),
+                      nir_vec2(b, nir_ishl(b, r_e, nir_imm_int(b, 24)), d), 2);
+         nir_store_var(b, attrib_idx_var, nir_iadd_imm(b, attrib_idx, 1), 1);
+         CONTINUE;
+      }
+      ENDIF
+#endif
+
+      nir_ssa_def *instance_div =
+         load_global(b, get_address_imm(b, attrib_buf_ptr, WORD(7)), 1, 32);
+
+      nir_ssa_def *div =
+         nir_imul(b, instance_div, builder->instance_size.padded);
+
+      nir_ssa_def *multi_instance =
+         nir_uge(b, builder->draw.instance_count, nir_imm_int(b, 2));
+
+      IF(nir_ine(b, div, nir_imm_int(b, 0)))
+      {
+         IF(multi_instance){IF(nir_is_power_of_two_or_zero(b, div)){
+            nir_ssa_def *exp =
+               nir_imax(b, nir_ufind_msb(b, div), nir_imm_int(b, 0));
+         update_vertex_attrib_buf(builder, attrib_buf_ptr,
+                                  MALI_ATTRIBUTE_TYPE_1D_POT_DIVISOR, exp,
+                                  NULL);
+      }
+      ELSE
+      {
+         split_div(b, div, &r_e, &d);
+         update_vertex_attrib_buf(builder, attrib_buf_ptr,
+                                  MALI_ATTRIBUTE_TYPE_1D_NPOT_DIVISOR, r_e, d);
+      }
+      ENDIF
+   }
+   ELSE
+   {
+      /* Single instance with a non-0 divisor: all
+       * accesses should point to attribute 0 */
+      zero_attrib_buf_stride(builder, attrib_buf_ptr);
+   }
+   ENDIF
+
+   adjust_attrib_offset(builder, attrib_ptr, attrib_buf_ptr, instance_div);
+}
+ELSE
+IF(multi_instance)
+{
+   update_vertex_attrib_buf(builder, attrib_buf_ptr,
+                            MALI_ATTRIBUTE_TYPE_1D_MODULUS,
+                            builder->instance_size.packed, NULL);
+}
+ENDIF ENDIF
+
+   nir_store_var(b, attrib_idx_var, nir_iadd_imm(b, attrib_idx, 1), 1);
+}
+}
+
+static nir_ssa_def *
+update_varying_buf(struct indirect_draw_shader_builder *builder,
+                   nir_ssa_def *varying_buf_ptr, nir_ssa_def *vertex_count)
+{
+   nir_builder *b = &builder->b;
+
+   nir_ssa_def *stride =
+      load_global(b, get_address_imm(b, varying_buf_ptr, WORD(2)), 1, 32);
+   nir_ssa_def *size = nir_imul(b, stride, vertex_count);
+   nir_ssa_def *aligned_size = nir_iand_imm(b, nir_iadd_imm(b, size, 63), ~63);
+   nir_ssa_def *var_mem_ptr = nir_load_var(b, builder->varyings.mem_ptr);
+   nir_ssa_def *w0 = nir_ior(b, nir_unpack_64_2x32_split_x(b, var_mem_ptr),
+                             nir_imm_int(b, MALI_ATTRIBUTE_TYPE_1D));
+   nir_ssa_def *w1 = nir_unpack_64_2x32_split_y(b, var_mem_ptr);
+   store_global(b, get_address_imm(b, varying_buf_ptr, WORD(0)),
+                nir_vec4(b, w0, w1, stride, size), 4);
+
+   nir_store_var(b, builder->varyings.mem_ptr,
+                 get_address(b, var_mem_ptr, aligned_size), 3);
+
+   return var_mem_ptr;
+}
+
+/* Based on panfrost_emit_varying_descriptor() */
+
+static void
+update_varyings(struct indirect_draw_shader_builder *builder)
+{
+   nir_builder *b = &builder->b;
+   nir_ssa_def *vertex_count =
+      nir_imul(b, builder->instance_size.padded, builder->draw.instance_count);
+   nir_ssa_def *buf_ptr =
+      get_address_imm(b, builder->varyings.varying_bufs,
+                      PAN_VARY_GENERAL * pan_size(ATTRIBUTE_BUFFER));
+   update_varying_buf(builder, buf_ptr, vertex_count);
+
+   buf_ptr = get_address_imm(b, builder->varyings.varying_bufs,
+                             PAN_VARY_POSITION * pan_size(ATTRIBUTE_BUFFER));
+   builder->varyings.pos_ptr =
+      update_varying_buf(builder, buf_ptr, vertex_count);
+
+   if (builder->flags & PAN_INDIRECT_DRAW_HAS_PSIZ) {
+      buf_ptr = get_address_imm(b, builder->varyings.varying_bufs,
+                                PAN_VARY_PSIZ * pan_size(ATTRIBUTE_BUFFER));
+      builder->varyings.psiz_ptr =
+         update_varying_buf(builder, buf_ptr, vertex_count);
+   }
+
+   set_draw_ctx_field(builder, varying_mem,
+                      nir_load_var(b, builder->varyings.mem_ptr));
+}
+
+/* Based on panfrost_pack_work_groups_compute() */
+
+static void
+get_invocation(struct indirect_draw_shader_builder *builder)
+{
+   nir_builder *b = &builder->b;
+   nir_ssa_def *one = nir_imm_int(b, 1);
+   nir_ssa_def *max_vertex = nir_usub_sat(b, builder->instance_size.raw, one);
+   nir_ssa_def *max_instance =
+      nir_usub_sat(b, builder->draw.instance_count, one);
+   nir_ssa_def *split =
+      nir_bcsel(b, nir_ieq_imm(b, max_instance, 0), nir_imm_int(b, 32),
+                nir_iadd_imm(b, nir_ufind_msb(b, max_vertex), 1));
+
+   builder->jobs.invocation =
+      nir_vec2(b, nir_ior(b, max_vertex, nir_ishl(b, max_instance, split)),
+               nir_ior(b, nir_ishl(b, split, nir_imm_int(b, 22)),
+                       nir_imm_int(b, 2 << 28)));
+}
+
+static nir_ssa_def *
+nir_align_pot(nir_builder *b, nir_ssa_def *val, unsigned pot)
+{
+   assert(pot != 0 && util_is_power_of_two_or_zero(pot));
+
+   return nir_iand_imm(b, nir_iadd_imm(b, val, pot - 1), ~(pot - 1));
+}
+
+/* Based on panfrost_padded_vertex_count() */
+
+static nir_ssa_def *
+get_padded_count(nir_builder *b, nir_ssa_def *val, nir_ssa_def **packed)
+{
+   nir_ssa_def *one = nir_imm_int(b, 1);
+   nir_ssa_def *zero = nir_imm_int(b, 0);
+   nir_ssa_def *eleven = nir_imm_int(b, 11);
+   nir_ssa_def *four = nir_imm_int(b, 4);
+
+   nir_ssa_def *exp =
+      nir_usub_sat(b, nir_imax(b, nir_ufind_msb(b, val), zero), four);
+   nir_ssa_def *base = nir_ushr(b, val, exp);
+
+   base = nir_iadd(
+      b, base,
+      nir_bcsel(b, nir_ine(b, val, nir_ishl(b, base, exp)), one, zero));
+
+   nir_ssa_def *rshift = nir_imax(b, nir_find_lsb(b, base), zero);
+   exp = nir_iadd(b, exp, rshift);
+   base = nir_ushr(b, base, rshift);
+   base = nir_iadd(b, base, nir_bcsel(b, nir_uge(b, base, eleven), one, zero));
+   rshift = nir_imax(b, nir_find_lsb(b, base), zero);
+   exp = nir_iadd(b, exp, rshift);
+   base = nir_ushr(b, base, rshift);
+
+   *packed =
+      nir_ior(b, exp, nir_ishl(b, nir_ushr_imm(b, base, 1), nir_imm_int(b, 5)));
+   return nir_ishl(b, base, exp);
+}
+
+static void
+update_jobs(struct indirect_draw_shader_builder *builder)
+{
+   get_invocation(builder);
+
+   if (!(builder->flags & PAN_INDIRECT_DRAW_IDVS))
+      update_job(builder, MALI_JOB_TYPE_VERTEX);
+
+   update_job(builder, MALI_JOB_TYPE_TILER);
+}
+
+static void
+set_null_job(struct indirect_draw_shader_builder *builder, nir_ssa_def *job_ptr)
+{
+   nir_builder *b = &builder->b;
+   nir_ssa_def *w4 = get_address_imm(b, job_ptr, WORD(4));
+   nir_ssa_def *val = load_global(b, w4, 1, 32);
+
+   /* Set job type to NULL (AKA NOOP) */
+   val = nir_ior(b, nir_iand_imm(b, val, 0xffffff01),
+                 nir_imm_int(b, MALI_JOB_TYPE_NULL << 1));
+   store_global(b, w4, val, 1);
+}
+
+static void
+get_instance_size(struct indirect_draw_shader_builder *builder)
+{
+   nir_builder *b = &builder->b;
+
+   if (!builder->index_size) {
+      builder->jobs.base_vertex_offset = nir_imm_int(b, 0);
+      builder->jobs.offset_start = builder->draw.vertex_start;
+      builder->instance_size.raw = builder->draw.vertex_count;
+      return;
+   }
+
+   unsigned index_size = builder->index_size;
+   nir_ssa_def *min = get_min_max_ctx_field(builder, min);
+   nir_ssa_def *max = get_min_max_ctx_field(builder, max);
+
+   /* We handle unaligned indices here to avoid the extra complexity in
+    * the min/max search job.
+    */
+   if (builder->index_size < 4) {
+      nir_variable *min_var =
+         nir_local_variable_create(b->impl, glsl_uint_type(), "min");
+      nir_store_var(b, min_var, min, 1);
+      nir_variable *max_var =
+         nir_local_variable_create(b->impl, glsl_uint_type(), "max");
+      nir_store_var(b, max_var, max, 1);
+
+      nir_ssa_def *base =
+         get_address(b, builder->draw.index_buf,
+                     nir_imul_imm(b, builder->draw.vertex_start, index_size));
+      nir_ssa_def *offset =
+         nir_iand_imm(b, nir_unpack_64_2x32_split_x(b, base), 3);
+      nir_ssa_def *end = nir_iadd(
+         b, offset, nir_imul_imm(b, builder->draw.vertex_count, index_size));
+      nir_ssa_def *aligned_end = nir_iand_imm(b, end, ~3);
+      unsigned shift = index_size * 8;
+      unsigned mask = (1 << shift) - 1;
+
+      base = nir_iand(b, base, nir_imm_int64(b, ~3ULL));
+
+      /* Unaligned start offset, we need to ignore any data that's
+       * outside the requested range. We also handle ranges that are
+       * covering less than 2 words here.
+       */
+      IF(nir_ior(b, nir_ine(b, offset, nir_imm_int(b, 0)),
+                 nir_ieq(b, aligned_end, nir_imm_int(b, 0))))
+      {
+         min = nir_load_var(b, min_var);
+         max = nir_load_var(b, max_var);
+
+         nir_ssa_def *val = load_global(b, base, 1, 32);
+         for (unsigned i = 0; i < sizeof(uint32_t); i += index_size) {
+            nir_ssa_def *oob = nir_ior(b, nir_ult(b, nir_imm_int(b, i), offset),
+                                       nir_uge(b, nir_imm_int(b, i), end));
+            nir_ssa_def *data = nir_iand_imm(b, val, mask);
+
+            min = nir_umin(b, min,
+                           nir_bcsel(b, oob, nir_imm_int(b, UINT32_MAX), data));
+            max = nir_umax(b, max, nir_bcsel(b, oob, nir_imm_int(b, 0), data));
+            val = nir_ushr_imm(b, val, shift);
+         }
+
+         nir_store_var(b, min_var, min, 1);
+         nir_store_var(b, max_var, max, 1);
+      }
+      ENDIF
+
+      nir_ssa_def *remaining = nir_isub(b, end, aligned_end);
+
+      /* The last word contains less than 4bytes of data, we need to
+       * discard anything falling outside the requested range.
+       */
+      IF(nir_iand(b, nir_ine(b, end, aligned_end),
+                  nir_ine(b, aligned_end, nir_imm_int(b, 0))))
+      {
+         min = nir_load_var(b, min_var);
+         max = nir_load_var(b, max_var);
+
+         nir_ssa_def *val =
+            load_global(b, get_address(b, base, aligned_end), 1, 32);
+         for (unsigned i = 0; i < sizeof(uint32_t); i += index_size) {
+            nir_ssa_def *oob = nir_uge(b, nir_imm_int(b, i), remaining);
+            nir_ssa_def *data = nir_iand_imm(b, val, mask);
+
+            min = nir_umin(b, min,
+                           nir_bcsel(b, oob, nir_imm_int(b, UINT32_MAX), data));
+            max = nir_umax(b, max, nir_bcsel(b, oob, nir_imm_int(b, 0), data));
+            val = nir_ushr_imm(b, val, shift);
+         }
+
+         nir_store_var(b, min_var, min, 1);
+         nir_store_var(b, max_var, max, 1);
+      }
+      ENDIF
+
+      min = nir_load_var(b, min_var);
+      max = nir_load_var(b, max_var);
+   }
+
+   builder->jobs.base_vertex_offset = nir_ineg(b, min);
+   builder->jobs.offset_start = nir_iadd(b, min, builder->draw.index_bias);
+   builder->instance_size.raw = nir_iadd_imm(b, nir_usub_sat(b, max, min), 1);
+}
+
+/* Patch a draw sequence */
+
+static void
+patch(struct indirect_draw_shader_builder *builder)
+{
+   unsigned index_size = builder->index_size;
+   nir_builder *b = &builder->b;
+
+   nir_ssa_def *draw_ptr = builder->draw.draw_buf;
+
+   if (index_size) {
+      builder->draw.vertex_count = get_indexed_draw_field(b, draw_ptr, count);
+      builder->draw.start_instance =
+         get_indexed_draw_field(b, draw_ptr, start_instance);
+      builder->draw.instance_count =
+         get_indexed_draw_field(b, draw_ptr, instance_count);
+      builder->draw.vertex_start = get_indexed_draw_field(b, draw_ptr, start);
+      builder->draw.index_bias =
+         get_indexed_draw_field(b, draw_ptr, index_bias);
+   } else {
+      builder->draw.vertex_count = get_draw_field(b, draw_ptr, count);
+      builder->draw.start_instance =
+         get_draw_field(b, draw_ptr, start_instance);
+      builder->draw.instance_count =
+         get_draw_field(b, draw_ptr, instance_count);
+      builder->draw.vertex_start = get_draw_field(b, draw_ptr, start);
+   }
+
+   assert(builder->draw.vertex_count->num_components);
+
+   nir_ssa_def *num_vertices =
+      nir_imul(b, builder->draw.vertex_count, builder->draw.instance_count);
+
+   IF(nir_ieq(b, num_vertices, nir_imm_int(b, 0)))
+   {
+      /* If there's nothing to draw, turn the vertex/tiler jobs into
+       * null jobs.
+       */
+      if (!(builder->flags & PAN_INDIRECT_DRAW_IDVS))
+         set_null_job(builder, builder->jobs.vertex_job);
+
+      set_null_job(builder, builder->jobs.tiler_job);
+   }
+   ELSE
+   {
+      get_instance_size(builder);
+
+      nir_ssa_def *count = builder->instance_size.raw;
+
+      /* IDVS requires padding to a multiple of 4 */
+      if (builder->flags & PAN_INDIRECT_DRAW_IDVS)
+         count = nir_align_pot(b, count, 4);
+
+      builder->instance_size.padded =
+         get_padded_count(b, count, &builder->instance_size.packed);
+
+      update_varyings(builder);
+      update_jobs(builder);
+      update_vertex_attribs(builder);
+
+      IF(nir_ine(b, builder->jobs.first_vertex_sysval, nir_imm_int64(b, 0)))
+      {
+         store_global(b, builder->jobs.first_vertex_sysval,
+                      builder->jobs.offset_start, 1);
+      }
+      ENDIF
+
+      IF(nir_ine(b, builder->jobs.base_vertex_sysval, nir_imm_int64(b, 0)))
+      {
+         store_global(b, builder->jobs.base_vertex_sysval,
+                      index_size ? builder->draw.index_bias : nir_imm_int(b, 0),
+                      1);
+      }
+      ENDIF
+
+      IF(nir_ine(b, builder->jobs.base_instance_sysval, nir_imm_int64(b, 0)))
+      {
+         store_global(b, builder->jobs.base_instance_sysval,
+                      builder->draw.start_instance, 1);
+      }
+      ENDIF
+   }
+   ENDIF
+}
+
+/* Search the min/max index in the range covered by the indirect draw call */
+
+static void
+get_index_min_max(struct indirect_draw_shader_builder *builder)
+{
+   nir_ssa_def *restart_index = builder->draw.restart_index;
+   unsigned index_size = builder->index_size;
+   nir_builder *b = &builder->b;
+
+   nir_ssa_def *draw_ptr = builder->draw.draw_buf;
+
+   builder->draw.vertex_count = get_draw_field(b, draw_ptr, count);
+   builder->draw.vertex_start = get_draw_field(b, draw_ptr, start);
+
+   nir_ssa_def *thread_id =
+      nir_channel(b, nir_load_global_invocation_id(b, 32), 0);
+   nir_variable *min_var =
+      nir_local_variable_create(b->impl, glsl_uint_type(), "min");
+   nir_store_var(b, min_var, nir_imm_int(b, UINT32_MAX), 1);
+   nir_variable *max_var =
+      nir_local_variable_create(b->impl, glsl_uint_type(), "max");
+   nir_store_var(b, max_var, nir_imm_int(b, 0), 1);
+
+   nir_ssa_def *base =
+      get_address(b, builder->draw.index_buf,
+                  nir_imul_imm(b, builder->draw.vertex_start, index_size));
+
+   nir_ssa_def *start = nir_iand_imm(b, nir_unpack_64_2x32_split_x(b, base), 3);
+   nir_ssa_def *end = nir_iadd(
+      b, start, nir_imul_imm(b, builder->draw.vertex_count, index_size));
+
+   base = nir_iand(b, base, nir_imm_int64(b, ~3ULL));
+
+   /* Align on 4 bytes, non-aligned indices are handled in the indirect draw
+    * job. */
+   start = nir_iand_imm(b, nir_iadd_imm(b, start, 3), ~3);
+   end = nir_iand_imm(b, end, ~3);
+
+   /* Add the job offset. */
+   start = nir_iadd(b, start, nir_imul_imm(b, thread_id, sizeof(uint32_t)));
+
+   nir_variable *offset_var =
+      nir_local_variable_create(b->impl, glsl_uint_type(), "offset");
+   nir_store_var(b, offset_var, start, 1);
+
+   LOOP
+   {
+      nir_ssa_def *offset = nir_load_var(b, offset_var);
+      IF(nir_uge(b, offset, end))
+      BREAK;
+      ENDIF
+
+      nir_ssa_def *val = load_global(b, get_address(b, base, offset), 1, 32);
+      nir_ssa_def *old_min = nir_load_var(b, min_var);
+      nir_ssa_def *old_max = nir_load_var(b, max_var);
+      nir_ssa_def *new_min;
+      nir_ssa_def *new_max;
+
+      /* TODO: use 8/16 bit arithmetic when index_size < 4. */
+      for (unsigned i = 0; i < 4; i += index_size) {
+         nir_ssa_def *data = nir_ushr_imm(b, val, i * 8);
+         data = nir_iand_imm(b, data, (1ULL << (index_size * 8)) - 1);
+         new_min = nir_umin(b, old_min, data);
+         new_max = nir_umax(b, old_max, data);
+         if (restart_index) {
+            new_min =
+               nir_bcsel(b, nir_ine(b, restart_index, data), new_min, old_min);
+            new_max =
+               nir_bcsel(b, nir_ine(b, restart_index, data), new_max, old_max);
+         }
+         old_min = new_min;
+         old_max = new_max;
+      }
+
+      nir_store_var(b, min_var, new_min, 1);
+      nir_store_var(b, max_var, new_max, 1);
+      nir_store_var(b, offset_var,
+                    nir_iadd_imm(b, offset, MIN_MAX_JOBS * sizeof(uint32_t)),
+                    1);
+   }
+
+   IF(nir_ult(b, start, end))
+   update_min(builder, nir_load_var(b, min_var));
+   update_max(builder, nir_load_var(b, max_var));
+   ENDIF
+}
+
+static unsigned
+get_shader_id(unsigned flags, unsigned index_size, bool index_min_max_search)
+{
+   if (!index_min_max_search) {
+      flags &= PAN_INDIRECT_DRAW_FLAGS_MASK;
+      flags &= ~PAN_INDIRECT_DRAW_INDEX_SIZE_MASK;
+      if (index_size)
+         flags |= (util_logbase2(index_size) + 1);
+      return flags;
+   }
+
+   return ((flags & PAN_INDIRECT_DRAW_PRIMITIVE_RESTART)
+              ? PAN_INDIRECT_DRAW_MIN_MAX_SEARCH_1B_INDEX_PRIM_RESTART
+              : PAN_INDIRECT_DRAW_MIN_MAX_SEARCH_1B_INDEX) +
+          util_logbase2(index_size);
+}
+
+static void
+create_indirect_draw_shader(struct panfrost_device *dev, unsigned flags,
+                            unsigned index_size, bool index_min_max_search)
+{
+   assert(flags < PAN_INDIRECT_DRAW_NUM_SHADERS);
+   struct indirect_draw_shader_builder builder;
+   init_shader_builder(&builder, dev, flags, index_size, index_min_max_search);
+
+   nir_builder *b = &builder.b;
+
+   if (index_min_max_search)
+      get_index_min_max(&builder);
+   else
+      patch(&builder);
+
+   struct panfrost_compile_inputs inputs = {
+      .gpu_id = dev->gpu_id,
+      .fixed_sysval_ubo = -1,
+      .no_ubo_to_push = true,
+   };
+   struct pan_shader_info shader_info;
+   struct util_dynarray binary;
+
+   util_dynarray_init(&binary, NULL);
+   GENX(pan_shader_compile)(b->shader, &inputs, &binary, &shader_info);
+
+   assert(!shader_info.tls_size);
+   assert(!shader_info.wls_size);
+   assert(!shader_info.sysvals.sysval_count);
+
+   shader_info.push.count =
+      DIV_ROUND_UP(sizeof(struct indirect_draw_inputs), 4);
+
+   unsigned shader_id = get_shader_id(flags, index_size, index_min_max_search);
+   struct pan_indirect_draw_shader *draw_shader =
+      &dev->indirect_draw_shaders.shaders[shader_id];
+   void *state = dev->indirect_draw_shaders.states->ptr.cpu +
+                 (shader_id * pan_size(RENDERER_STATE));
+
+   pthread_mutex_lock(&dev->indirect_draw_shaders.lock);
+   if (!draw_shader->rsd) {
+      mali_ptr address = pan_pool_upload_aligned(
+         dev->indirect_draw_shaders.bin_pool, binary.data, binary.size,
+         PAN_ARCH >= 6 ? 128 : 64);
+
+      util_dynarray_fini(&binary);
+
+      pan_pack(state, RENDERER_STATE, cfg) {
+         pan_shader_prepare_rsd(&shader_info, address, &cfg);
+      }
+
+      draw_shader->push = shader_info.push;
+      draw_shader->rsd = dev->indirect_draw_shaders.states->ptr.gpu +
+                         (shader_id * pan_size(RENDERER_STATE));
+   }
+   pthread_mutex_unlock(&dev->indirect_draw_shaders.lock);
+
+   ralloc_free(b->shader);
+}
+
+static mali_ptr
+get_renderer_state(struct panfrost_device *dev, unsigned flags,
+                   unsigned index_size, bool index_min_max_search)
+{
+   unsigned shader_id = get_shader_id(flags, index_size, index_min_max_search);
+   struct pan_indirect_draw_shader *info =
+      &dev->indirect_draw_shaders.shaders[shader_id];
+
+   if (!info->rsd) {
+      create_indirect_draw_shader(dev, flags, index_size, index_min_max_search);
+      assert(info->rsd);
+   }
+
+   return info->rsd;
+}
+
+static mali_ptr
+get_tls(const struct panfrost_device *dev)
+{
+   return dev->indirect_draw_shaders.states->ptr.gpu +
+          (PAN_INDIRECT_DRAW_NUM_SHADERS * pan_size(RENDERER_STATE));
+}
+
+static void
+panfrost_indirect_draw_alloc_deps(struct panfrost_device *dev)
+{
+   pthread_mutex_lock(&dev->indirect_draw_shaders.lock);
+   if (dev->indirect_draw_shaders.states)
+      goto out;
+
+   unsigned state_bo_size =
+      (PAN_INDIRECT_DRAW_NUM_SHADERS * pan_size(RENDERER_STATE)) +
+      pan_size(LOCAL_STORAGE);
+
+   dev->indirect_draw_shaders.states =
+      panfrost_bo_create(dev, state_bo_size, 0, "Indirect draw states");
+
+   /* Prepare the thread storage descriptor now since it's invariant. */
+   void *tsd = dev->indirect_draw_shaders.states->ptr.cpu +
+               (PAN_INDIRECT_DRAW_NUM_SHADERS * pan_size(RENDERER_STATE));
+   pan_pack(tsd, LOCAL_STORAGE, ls) {
+      ls.wls_instances = MALI_LOCAL_STORAGE_NO_WORKGROUP_MEM;
+   };
+
+   /* FIXME: Currently allocating 512M of growable memory, meaning that we
+    * only allocate what we really use, the problem is:
+    * - allocation happens 2M at a time, which might be more than we
+    *   actually need
+    * - the memory is attached to the device to speed up subsequent
+    *   indirect draws, but that also means it's never shrinked
+    */
+   dev->indirect_draw_shaders.varying_heap = panfrost_bo_create(
+      dev, 512 * 1024 * 1024, PAN_BO_INVISIBLE | PAN_BO_GROWABLE,
+      "Indirect draw varying heap");
+
+out:
+   pthread_mutex_unlock(&dev->indirect_draw_shaders.lock);
+}
+
+static unsigned
+panfrost_emit_index_min_max_search(
+   struct pan_pool *pool, struct pan_scoreboard *scoreboard,
+   const struct pan_indirect_draw_info *draw_info,
+   const struct indirect_draw_inputs *inputs,
+   struct indirect_draw_context *draw_ctx)
+{
+   struct panfrost_device *dev = pool->dev;
+   unsigned index_size = draw_info->index_size;
+
+   if (!index_size)
+      return 0;
+
+   mali_ptr rsd =
+      get_renderer_state(dev, draw_info->flags, draw_info->index_size, true);
+   struct panfrost_ptr job = pan_pool_alloc_desc(pool, COMPUTE_JOB);
+   void *invocation = pan_section_ptr(job.cpu, COMPUTE_JOB, INVOCATION);
+   panfrost_pack_work_groups_compute(invocation, 1, 1, 1, MIN_MAX_JOBS, 1, 1,
+                                     false, false);
+
+   pan_section_pack(job.cpu, COMPUTE_JOB, PARAMETERS, cfg) {
+      cfg.job_task_split = 7;
+   }
+
+   pan_section_pack(job.cpu, COMPUTE_JOB, DRAW, cfg) {
+      cfg.state = rsd;
+      cfg.thread_storage = get_tls(pool->dev);
+      cfg.push_uniforms =
+         pan_pool_upload_aligned(pool, inputs, sizeof(*inputs), 16);
+   }
+
+   return panfrost_add_job(pool, scoreboard, MALI_JOB_TYPE_COMPUTE, false,
+                           false, 0, 0, &job, false);
+}
+
+unsigned
+GENX(panfrost_emit_indirect_draw)(
+   struct pan_pool *pool, struct pan_scoreboard *scoreboard,
+   const struct pan_indirect_draw_info *draw_info, struct panfrost_ptr *ctx)
+{
+   struct panfrost_device *dev = pool->dev;
+
+   /* Currently only tested on Bifrost, but the logic should be the same
+    * on Midgard.
+    */
+   assert(pan_is_bifrost(dev));
+
+   panfrost_indirect_draw_alloc_deps(dev);
+
+   struct panfrost_ptr job = pan_pool_alloc_desc(pool, COMPUTE_JOB);
+   mali_ptr rsd =
+      get_renderer_state(dev, draw_info->flags, draw_info->index_size, false);
+
+   struct indirect_draw_context draw_ctx = {
+      .varying_mem = dev->indirect_draw_shaders.varying_heap->ptr.gpu,
+   };
+
+   struct panfrost_ptr draw_ctx_ptr = *ctx;
+   if (!draw_ctx_ptr.cpu) {
+      draw_ctx_ptr =
+         pan_pool_alloc_aligned(pool, sizeof(draw_ctx), sizeof(mali_ptr));
+   }
+
+   struct indirect_draw_inputs inputs = {
+      .draw_ctx = draw_ctx_ptr.gpu,
+      .draw_buf = draw_info->draw_buf,
+      .index_buf = draw_info->index_buf,
+      .first_vertex_sysval = draw_info->first_vertex_sysval,
+      .base_vertex_sysval = draw_info->base_vertex_sysval,
+      .base_instance_sysval = draw_info->base_instance_sysval,
+      .vertex_job = draw_info->vertex_job,
+      .tiler_job = draw_info->tiler_job,
+      .attrib_bufs = draw_info->attrib_bufs,
+      .attribs = draw_info->attribs,
+      .varying_bufs = draw_info->varying_bufs,
+      .attrib_count = draw_info->attrib_count,
+   };
+
+   if (draw_info->index_size) {
+      inputs.restart_index = draw_info->restart_index;
+
+      struct panfrost_ptr min_max_ctx_ptr =
+         pan_pool_alloc_aligned(pool, sizeof(struct min_max_context), 4);
+      struct min_max_context *ctx = min_max_ctx_ptr.cpu;
+
+      ctx->min = UINT32_MAX;
+      ctx->max = 0;
+      inputs.min_max_ctx = min_max_ctx_ptr.gpu;
+   }
+
+   void *invocation = pan_section_ptr(job.cpu, COMPUTE_JOB, INVOCATION);
+   panfrost_pack_work_groups_compute(invocation, 1, 1, 1, 1, 1, 1, false,
+                                     false);
+
+   pan_section_pack(job.cpu, COMPUTE_JOB, PARAMETERS, cfg) {
+      cfg.job_task_split = 2;
+   }
+
+   pan_section_pack(job.cpu, COMPUTE_JOB, DRAW, cfg) {
+      cfg.state = rsd;
+      cfg.thread_storage = get_tls(pool->dev);
+      cfg.push_uniforms =
+         pan_pool_upload_aligned(pool, &inputs, sizeof(inputs), 16);
+   }
+
+   unsigned global_dep = draw_info->last_indirect_draw;
+   unsigned local_dep = panfrost_emit_index_min_max_search(
+      pool, scoreboard, draw_info, &inputs, &draw_ctx);
+
+   if (!ctx->cpu) {
+      *ctx = draw_ctx_ptr;
+      memcpy(ctx->cpu, &draw_ctx, sizeof(draw_ctx));
+   }
+
+   return panfrost_add_job(pool, scoreboard, MALI_JOB_TYPE_COMPUTE, false, true,
+                           local_dep, global_dep, &job, false);
+}
+
+void
+GENX(panfrost_init_indirect_draw_shaders)(struct panfrost_device *dev,
+                                          struct pan_pool *bin_pool)
+{
+   /* We allocate the states and varying_heap BO lazily to avoid
+    * reserving memory when indirect draws are not used.
+    */
+   pthread_mutex_init(&dev->indirect_draw_shaders.lock, NULL);
+   dev->indirect_draw_shaders.bin_pool = bin_pool;
+}
+
+void
+GENX(panfrost_cleanup_indirect_draw_shaders)(struct panfrost_device *dev)
+{
+   panfrost_bo_unreference(dev->indirect_draw_shaders.states);
+   panfrost_bo_unreference(dev->indirect_draw_shaders.varying_heap);
+   pthread_mutex_destroy(&dev->indirect_draw_shaders.lock);
+}
diff -urN mesa-23.0.0/src/panfrost/lib/pan_indirect_draw.h mesa/src/panfrost/lib/pan_indirect_draw.h
--- mesa-23.0.0/src/panfrost/lib/pan_indirect_draw.h	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/lib/pan_indirect_draw.h	2023-03-06 19:19:31.815302941 +0100
@@ -0,0 +1,60 @@
+/*
+ * Copyright (C) 2021 Collabora, Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef __PAN_INDIRECT_DRAW_SHADERS_H__
+#define __PAN_INDIRECT_DRAW_SHADERS_H__
+
+#include "genxml/gen_macros.h"
+
+struct pan_device;
+struct pan_scoreboard;
+struct pan_pool;
+
+struct pan_indirect_draw_info {
+   mali_ptr draw_buf;
+   mali_ptr index_buf;
+   mali_ptr first_vertex_sysval;
+   mali_ptr base_vertex_sysval;
+   mali_ptr base_instance_sysval;
+   mali_ptr vertex_job;
+   mali_ptr tiler_job;
+   mali_ptr attrib_bufs;
+   mali_ptr attribs;
+   mali_ptr varying_bufs;
+   unsigned attrib_count;
+   uint32_t restart_index;
+   unsigned flags;
+   unsigned index_size;
+   unsigned last_indirect_draw;
+};
+
+unsigned GENX(panfrost_emit_indirect_draw)(
+   struct pan_pool *pool, struct pan_scoreboard *scoreboard,
+   const struct pan_indirect_draw_info *draw_info, struct panfrost_ptr *ctx);
+
+void GENX(panfrost_init_indirect_draw_shaders)(struct panfrost_device *dev,
+                                               struct pan_pool *bin_pool);
+
+void GENX(panfrost_cleanup_indirect_draw_shaders)(struct panfrost_device *dev);
+
+#endif
diff -urN mesa-23.0.0/src/panfrost/lib/pan_layout.c mesa/src/panfrost/lib/pan_layout.c
--- mesa-23.0.0/src/panfrost/lib/pan_layout.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/pan_layout.c	2023-03-06 19:19:32.050304493 +0100
@@ -27,14 +27,18 @@
 #include "util/u_math.h"
 #include "pan_texture.h"
 
-/*
- * List of supported modifiers, in descending order of preference. AFBC is
+/* List of supported modifiers, in descending order of preference. AFBC is
  * faster than u-interleaved tiling which is faster than linear. Within AFBC,
- * enabling the YUV-like transform is typically a win where possible.
- */
+ * enabling the YUV-like transform is typically a win where possible. */
+
 uint64_t pan_best_modifiers[PAN_MODIFIER_COUNT] = {
    DRM_FORMAT_MOD_ARM_AFBC(AFBC_FORMAT_MOD_BLOCK_SIZE_16x16 |
                            AFBC_FORMAT_MOD_TILED | AFBC_FORMAT_MOD_SC |
+                           AFBC_FORMAT_MOD_SPARSE | AFBC_FORMAT_MOD_YTR |
+                           AFBC_FORMAT_MOD_NATIVE_SWIZZLE),
+
+   DRM_FORMAT_MOD_ARM_AFBC(AFBC_FORMAT_MOD_BLOCK_SIZE_16x16 |
+                           AFBC_FORMAT_MOD_TILED | AFBC_FORMAT_MOD_SC |
                            AFBC_FORMAT_MOD_SPARSE | AFBC_FORMAT_MOD_YTR),
 
    DRM_FORMAT_MOD_ARM_AFBC(AFBC_FORMAT_MOD_BLOCK_SIZE_16x16 |
@@ -192,16 +196,17 @@
 #define CHECKSUM_TILE_HEIGHT    16
 #define CHECKSUM_BYTES_PER_TILE 8
 
-unsigned
-panfrost_compute_checksum_size(struct pan_image_slice_layout *slice,
-                               unsigned width, unsigned height)
+struct pan_image_slice_crc
+panfrost_compute_checksum_size(unsigned width, unsigned height)
 {
    unsigned tile_count_x = DIV_ROUND_UP(width, CHECKSUM_TILE_WIDTH);
    unsigned tile_count_y = DIV_ROUND_UP(height, CHECKSUM_TILE_HEIGHT);
 
-   slice->crc.stride = tile_count_x * CHECKSUM_BYTES_PER_TILE;
-
-   return slice->crc.stride * tile_count_y;
+   struct pan_image_slice_crc ret = {
+      .stride = tile_count_x * CHECKSUM_BYTES_PER_TILE,
+      .size = ret.stride * tile_count_y,
+   };
+   return ret;
 }
 
 unsigned
@@ -224,11 +229,12 @@
       panfrost_block_size(layout->modifier, layout->format);
 
    if (drm_is_afbc(layout->modifier)) {
-      unsigned width = u_minify(layout->width, level);
-      unsigned alignment =
+      unsigned align_w =
          block_size.width * pan_afbc_tile_size(layout->modifier);
 
-      width = ALIGN_POT(width, alignment);
+      unsigned width = u_minify(layout->width, level);
+      width = ALIGN_POT(width, align_w);
+
       return width * util_format_get_blocksize(layout->format);
    } else {
       return row_stride / block_size.height;
@@ -379,8 +385,7 @@
 
       /* Add a checksum region if necessary */
       if (layout->crc) {
-         slice->crc.size = panfrost_compute_checksum_size(slice, width, height);
-
+         slice->crc = panfrost_compute_checksum_size(width, height);
          slice->crc.offset = offset;
          offset += slice->crc.size;
          slice->size += slice->crc.size;
diff -urN mesa-23.0.0/src/panfrost/lib/pan_pool.h mesa/src/panfrost/lib/pan_pool.h
--- mesa-23.0.0/src/panfrost/lib/pan_pool.h	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/pan_pool.h	2023-03-06 19:19:32.102304836 +0100
@@ -129,4 +129,16 @@
 #define pan_pool_alloc_desc_aggregate(pool, ...)                               \
    pan_pool_alloc_descs(pool, PAN_DESC_AGGREGATE(__VA_ARGS__))
 
+#ifdef PAN_ARCH
+#if PAN_ARCH < 10
+
+#define pan_pool_alloc_desc_cs_v10(pool, name) pan_pool_alloc_desc(pool, name)
+
+#else /* PAN_ARCH >= 10 */
+
+#define pan_pool_alloc_desc_cs_v10(pool, name) ((struct panfrost_ptr){0})
+
+#endif
+#endif /* PAN_ARCH */
+
 #endif
diff -urN mesa-23.0.0/src/panfrost/lib/pan_props.c mesa/src/panfrost/lib/pan_props.c
--- mesa-23.0.0/src/panfrost/lib/pan_props.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/pan_props.c	2023-03-06 19:19:32.060304559 +0100
@@ -24,13 +24,16 @@
  *   Alyssa Rosenzweig <alyssa.rosenzweig@collabora.com>
  */
 
+#include <fcntl.h>
 #include <xf86drm.h>
 
+#include "dma-uapi/dma-buf.h"
 #include "drm-uapi/panfrost_drm.h"
 #include "util/hash_table.h"
 #include "util/macros.h"
 #include "util/u_math.h"
 #include "util/u_thread.h"
+#include "pan_base.h"
 #include "pan_bo.h"
 #include "pan_device.h"
 #include "pan_encoder.h"
@@ -52,26 +55,30 @@
    }
 
 /* Table of supported Mali GPUs */
-/* clang-format off */
 const struct panfrost_model panfrost_model_list[] = {
-        MODEL(0x620, "T620",    "T62x", NO_ANISO,          8192, {}),
-        MODEL(0x720, "T720",    "T72x", NO_ANISO,          8192, { .no_hierarchical_tiling = true }),
-        MODEL(0x750, "T760",    "T76x", NO_ANISO,          8192, {}),
-        MODEL(0x820, "T820",    "T82x", NO_ANISO,          8192, { .no_hierarchical_tiling = true }),
-        MODEL(0x830, "T830",    "T83x", NO_ANISO,          8192, { .no_hierarchical_tiling = true }),
-        MODEL(0x860, "T860",    "T86x", NO_ANISO,          8192, {}),
-        MODEL(0x880, "T880",    "T88x", NO_ANISO,          8192, {}),
-
-        MODEL(0x6000, "G71",    "TMIx", NO_ANISO,          8192, {}),
-        MODEL(0x6221, "G72",    "THEx", 0x0030 /* r0p3 */, 16384, {}),
-        MODEL(0x7090, "G51",    "TSIx", 0x1010 /* r1p1 */, 16384, {}),
-        MODEL(0x7093, "G31",    "TDVx", HAS_ANISO,         16384, {}),
-        MODEL(0x7211, "G76",    "TNOx", HAS_ANISO,         16384, {}),
-        MODEL(0x7212, "G52",    "TGOx", HAS_ANISO,         16384, {}),
-        MODEL(0x7402, "G52 r1", "TGOx", HAS_ANISO,         16384, {}),
-        MODEL(0x9093, "G57",    "TNAx", HAS_ANISO,         16384, {}),
+   MODEL(0x620, "T620", "T62x", NO_ANISO, 8192, {}),
+   MODEL(0x720, "T720", "T72x", NO_ANISO, 8192,
+         {.no_hierarchical_tiling = true}),
+   MODEL(0x750, "T760", "T76x", NO_ANISO, 8192, {}),
+   MODEL(0x820, "T820", "T82x", NO_ANISO, 8192,
+         {.no_hierarchical_tiling = true}),
+   MODEL(0x830, "T830", "T83x", NO_ANISO, 8192,
+         {.no_hierarchical_tiling = true}),
+   MODEL(0x860, "T860", "T86x", NO_ANISO, 8192, {}),
+   MODEL(0x880, "T880", "T88x", NO_ANISO, 8192, {}),
+
+   MODEL(0x6000, "G71", "TMIx", NO_ANISO, 8192, {}),
+   MODEL(0x6221, "G72", "THEx", 0x0030 /* r0p3 */, 16384, {}),
+   MODEL(0x7090, "G51", "TSIx", 0x1010 /* r1p1 */, 16384, {}),
+   MODEL(0x7093, "G31", "TDVx", HAS_ANISO, 16384, {}),
+   MODEL(0x7211, "G76", "TNOx", HAS_ANISO, 16384, {}),
+   MODEL(0x7212, "G52", "TGOx", HAS_ANISO, 16384, {}),
+   MODEL(0x7402, "G52 r1", "TGOx", HAS_ANISO, 16384, {}),
+   MODEL(0x9093, "G57", "TNAx", HAS_ANISO, 16384, {}),
+   MODEL(0xa867, "G610", "LODx", HAS_ANISO, 65536, {}),
+   /* Matching the kbase dummy model, probably not real GPUs */
+   MODEL(0xa802, "G710", "TODx", HAS_ANISO, 65536, {}),
 };
-/* clang-format on */
 
 #undef NO_ANISO
 #undef HAS_ANISO
@@ -96,16 +103,27 @@
  * information about devices */
 
 static __u64
-panfrost_query_raw(int fd, enum drm_panfrost_param param, bool required,
-                   unsigned default_value)
+panfrost_query_raw(struct panfrost_device *dev, enum drm_panfrost_param param,
+                   bool required, unsigned default_value)
 {
+   if (dev->kbase) {
+      uint64_t value;
+      bool ret = dev->mali.get_pan_gpuprop(&dev->mali, param, &value);
+      if (ret) {
+         return value;
+      } else {
+         assert(!required);
+         return default_value;
+      }
+   }
+
    struct drm_panfrost_get_param get_param = {
       0,
    };
    ASSERTED int ret;
 
    get_param.param = param;
-   ret = drmIoctl(fd, DRM_IOCTL_PANFROST_GET_PARAM, &get_param);
+   ret = drmIoctl(dev->fd, DRM_IOCTL_PANFROST_GET_PARAM, &get_param);
 
    if (ret) {
       assert(!required);
@@ -116,49 +134,48 @@
 }
 
 static unsigned
-panfrost_query_gpu_version(int fd)
+panfrost_query_gpu_version(struct panfrost_device *dev)
 {
-   return panfrost_query_raw(fd, DRM_PANFROST_PARAM_GPU_PROD_ID, true, 0);
+   return panfrost_query_raw(dev, DRM_PANFROST_PARAM_GPU_PROD_ID, true, 0);
 }
 
 static unsigned
-panfrost_query_gpu_revision(int fd)
+panfrost_query_gpu_revision(struct panfrost_device *dev)
 {
-   return panfrost_query_raw(fd, DRM_PANFROST_PARAM_GPU_REVISION, true, 0);
+   return panfrost_query_raw(dev, DRM_PANFROST_PARAM_GPU_REVISION, true, 0);
 }
 
 unsigned
-panfrost_query_l2_slices(const struct panfrost_device *dev)
+panfrost_query_l2_slices(struct panfrost_device *dev)
 {
    /* Query MEM_FEATURES register */
    uint32_t mem_features =
-      panfrost_query_raw(dev->fd, DRM_PANFROST_PARAM_MEM_FEATURES, true, 0);
+      panfrost_query_raw(dev, DRM_PANFROST_PARAM_MEM_FEATURES, true, 0);
 
    /* L2_SLICES is MEM_FEATURES[11:8] minus(1) */
    return ((mem_features >> 8) & 0xF) + 1;
 }
 
 static struct panfrost_tiler_features
-panfrost_query_tiler_features(int fd)
+panfrost_query_tiler_features(struct panfrost_device *dev)
 {
    /* Default value (2^9 bytes and 8 levels) to match old behaviour */
    uint32_t raw =
-      panfrost_query_raw(fd, DRM_PANFROST_PARAM_TILER_FEATURES, false, 0x809);
+      panfrost_query_raw(dev, DRM_PANFROST_PARAM_TILER_FEATURES, false, 0x809);
 
    /* Bin size is log2 in the first byte, max levels in the second byte */
    return (struct panfrost_tiler_features){
       .bin_size = (1 << (raw & BITFIELD_MASK(5))),
-      .max_levels = (raw >> 8) & BITFIELD_MASK(4),
-   };
+      .max_levels = (raw >> 8) & BITFIELD_MASK(4)};
 }
 
 static unsigned
-panfrost_query_core_count(int fd, unsigned *core_id_range)
+panfrost_query_core_count(struct panfrost_device *dev, unsigned *core_id_range)
 {
    /* On older kernels, worst-case to 16 cores */
 
    unsigned mask =
-      panfrost_query_raw(fd, DRM_PANFROST_PARAM_SHADER_PRESENT, false, 0xffff);
+      panfrost_query_raw(dev, DRM_PANFROST_PARAM_SHADER_PRESENT, false, 0xffff);
 
    /* Some cores might be absent. In some cases, we care
     * about the range of core IDs (that is, the greatest core ID + 1). If
@@ -198,16 +215,16 @@
 }
 
 static unsigned
-panfrost_query_thread_tls_alloc(int fd, unsigned major)
+panfrost_query_thread_tls_alloc(struct panfrost_device *dev, unsigned major)
 {
    unsigned tls =
-      panfrost_query_raw(fd, DRM_PANFROST_PARAM_THREAD_TLS_ALLOC, false, 0);
+      panfrost_query_raw(dev, DRM_PANFROST_PARAM_THREAD_TLS_ALLOC, false, 0);
 
    return (tls > 0) ? tls : panfrost_max_thread_count(major);
 }
 
 static uint32_t
-panfrost_query_compressed_formats(int fd)
+panfrost_query_compressed_formats(struct panfrost_device *dev)
 {
    /* If unspecified, assume ASTC/ETC only. Factory default for Juno, and
     * should exist on any Mali configuration. All hardware should report
@@ -221,7 +238,7 @@
                           (1 << MALI_ASTC_3D_HDR) | (1 << MALI_ASTC_2D_LDR) |
                           (1 << MALI_ASTC_2D_HDR);
 
-   return panfrost_query_raw(fd, DRM_PANFROST_PARAM_TEXTURE_FEATURES0, false,
+   return panfrost_query_raw(dev, DRM_PANFROST_PARAM_TEXTURE_FEATURES0, false,
                              default_set);
 }
 
@@ -244,10 +261,10 @@
  * may omit it, signaled as a nonzero value in the AFBC_FEATURES property. */
 
 static bool
-panfrost_query_afbc(int fd, unsigned arch)
+panfrost_query_afbc(struct panfrost_device *dev, unsigned arch)
 {
    unsigned reg =
-      panfrost_query_raw(fd, DRM_PANFROST_PARAM_AFBC_FEATURES, false, 0);
+      panfrost_query_raw(dev, DRM_PANFROST_PARAM_AFBC_FEATURES, false, 0);
 
    return (arch >= 5) && (reg == 0);
 }
@@ -274,24 +291,40 @@
 void
 panfrost_open_device(void *memctx, int fd, struct panfrost_device *dev)
 {
+   if (kbase_open(&dev->mali, fd, 4, (dev->debug & PAN_DBG_LOG))) {
+      dev->kbase = true;
+      fd = -1;
+   }
+
    dev->fd = fd;
    dev->memctx = memctx;
-   dev->gpu_id = panfrost_query_gpu_version(fd);
+   dev->gpu_id = panfrost_query_gpu_version(dev);
    dev->arch = pan_arch(dev->gpu_id);
-   dev->kernel_version = drmGetVersion(fd);
-   dev->revision = panfrost_query_gpu_revision(fd);
+   if (dev->kbase) {
+      dev->kernel_version = calloc(1, sizeof(drmVersion));
+      *dev->kernel_version = (drmVersion){
+         .version_major = 1,
+         .version_minor = 999,
+      };
+   } else {
+      dev->kernel_version = drmGetVersion(fd);
+   }
+   dev->revision = panfrost_query_gpu_revision(dev);
    dev->model = panfrost_get_model(dev->gpu_id);
 
    /* If we don't recognize the model, bail early */
    if (!dev->model)
       return;
 
-   dev->core_count = panfrost_query_core_count(fd, &dev->core_id_range);
-   dev->thread_tls_alloc = panfrost_query_thread_tls_alloc(fd, dev->arch);
+   if (dev->debug & PAN_DBG_BO_LOG)
+      dev->bo_log = fopen("/tmp/bo_log", "w");
+
+   dev->core_count = panfrost_query_core_count(dev, &dev->core_id_range);
+   dev->thread_tls_alloc = panfrost_query_thread_tls_alloc(dev, dev->arch);
    dev->optimal_tib_size = panfrost_query_optimal_tib_size(dev);
-   dev->compressed_formats = panfrost_query_compressed_formats(fd);
-   dev->tiler_features = panfrost_query_tiler_features(fd);
-   dev->has_afbc = panfrost_query_afbc(fd, dev->arch);
+   dev->compressed_formats = panfrost_query_compressed_formats(dev);
+   dev->tiler_features = panfrost_query_tiler_features(dev);
+   dev->has_afbc = panfrost_query_afbc(dev, dev->arch);
 
    if (dev->arch <= 6)
       dev->formats = panfrost_pipe_format_v6;
@@ -300,8 +333,10 @@
    else
       dev->formats = panfrost_pipe_format_v9;
 
-   util_sparse_array_init(&dev->bo_map, sizeof(struct panfrost_bo), 512);
+   stable_array_init(&dev->bo_map, struct panfrost_bo);
 
+   pthread_mutex_init(&dev->bo_usage_lock, NULL);
+   pthread_mutex_init(&dev->bo_map_lock, NULL);
    pthread_mutex_init(&dev->bo_cache.lock, NULL);
    list_inithead(&dev->bo_cache.lru);
 
@@ -316,8 +351,10 @@
     * active for a single job chain at once, so a single heap can be
     * shared across batches/contextes */
 
-   dev->tiler_heap = panfrost_bo_create(
-      dev, 128 * 1024 * 1024, PAN_BO_INVISIBLE | PAN_BO_GROWABLE, "Tiler heap");
+   if (dev->arch < 10)
+      dev->tiler_heap =
+         panfrost_bo_create(dev, 128 * 1024 * 1024,
+                            PAN_BO_INVISIBLE | PAN_BO_GROWABLE, "Tiler heap");
 
    pthread_mutex_init(&dev->submit_lock, NULL);
 
@@ -337,9 +374,98 @@
       panfrost_bo_unreference(dev->sample_positions);
       panfrost_bo_cache_evict_all(dev);
       pthread_mutex_destroy(&dev->bo_cache.lock);
-      util_sparse_array_finish(&dev->bo_map);
+      pthread_mutex_destroy(&dev->bo_map_lock);
+      pthread_mutex_destroy(&dev->bo_usage_lock);
+      stable_array_fini(&dev->bo_map);
+   }
+
+   if (dev->kbase)
+      free(dev->kernel_version);
+   else
+      drmFreeVersion(dev->kernel_version);
+   if (dev->kbase)
+      dev->mali.close(&dev->mali);
+   else
+      close(dev->fd);
+}
+
+bool
+panfrost_check_dmabuf_fence(struct panfrost_device *dev)
+{
+   bool ret = false;
+   int err;
+
+   /* This function is only useful for kbase, where we can't create
+    * dma-bufs from the kbase FD. */
+   if (!dev->ro)
+      goto out;
+
+   struct drm_mode_create_dumb create_dumb = {
+      .width = 16,
+      .height = 16,
+      .bpp = 32,
+   };
+
+   err = drmIoctl(dev->ro->kms_fd, DRM_IOCTL_MODE_CREATE_DUMB, &create_dumb);
+   if (err < 0) {
+      fprintf(stderr,
+              "DRM_IOCTL_MODE_CREATE_DUMB failed "
+              "for fence check: %s\n",
+              strerror(errno));
+      goto out;
+   }
+
+   int fd;
+   err =
+      drmPrimeHandleToFD(dev->ro->kms_fd, create_dumb.handle, O_CLOEXEC, &fd);
+   if (err < 0) {
+      fprintf(stderr, "failed to export buffer for fence check: %s\n",
+              strerror(errno));
+      goto free_dumb;
    }
 
-   drmFreeVersion(dev->kernel_version);
-   close(dev->fd);
+   struct dma_buf_export_sync_file export = {
+      .flags = DMA_BUF_SYNC_RW,
+   };
+
+   /* ENOTTY is returned if the ioctl is unsupported */
+
+   err = drmIoctl(fd, DMA_BUF_IOCTL_EXPORT_SYNC_FILE, &export);
+   if (err < 0) {
+      if (errno != ENOTTY)
+         fprintf(stderr, "failed to export fence: %s\n", strerror(errno));
+      goto free_fd;
+   }
+
+   struct dma_buf_import_sync_file import = {
+      .flags = DMA_BUF_SYNC_RW,
+      .fd = export.fd,
+   };
+
+   err = drmIoctl(fd, DMA_BUF_IOCTL_IMPORT_SYNC_FILE, &import);
+   if (err < 0) {
+      if (errno != ENOTTY)
+         fprintf(stderr, "failed to import fence: %s\n", strerror(errno));
+      goto free_sync;
+   }
+
+   /* We made it this far, the kernel must support the ioctls */
+   ret = true;
+
+free_sync:
+   close(export.fd);
+
+free_fd:
+   close(fd);
+
+   /* Some compilers don't like goto to a declaration */
+   struct drm_mode_destroy_dumb destroy_dumb;
+free_dumb:
+   destroy_dumb = (struct drm_mode_destroy_dumb){
+      .handle = create_dumb.handle,
+   };
+   drmIoctl(dev->ro->kms_fd, DRM_IOCTL_MODE_DESTROY_DUMB, &destroy_dumb);
+
+out:
+   return ret;
 }
diff -urN mesa-23.0.0/src/panfrost/lib/pan_samples.c mesa/src/panfrost/lib/pan_samples.c
--- mesa-23.0.0/src/panfrost/lib/pan_samples.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/pan_samples.c	2023-03-06 19:19:31.859303232 +0100
@@ -60,72 +60,63 @@
 #define SAMPLE8(x, y) SAMPLE16((x)*2, (y)*2)
 #define SAMPLE4(x, y) SAMPLE16((x)*4, (y)*4)
 
-/* clang-format off */
-const struct mali_sample_positions sample_position_lut[] = {
-   [MALI_SAMPLE_PATTERN_SINGLE_SAMPLED] = {
-      .positions = {
-         SAMPLE4(0, 0)
-      },
-      .origin = SAMPLE4(0, 0)
-   },
-
-   [MALI_SAMPLE_PATTERN_ORDERED_4X_GRID] = {
-      .positions = {
-         SAMPLE4(-1, -1),
-         SAMPLE4( 1, -1),
-         SAMPLE4(-1,  1),
-         SAMPLE4( 1,  1),
-      },
-      .origin = SAMPLE4(0, 0)
-   },
-
-   [MALI_SAMPLE_PATTERN_ROTATED_4X_GRID] = {
-      .positions = {
-         SAMPLE8(-1, -3),
-         SAMPLE8( 3, -1),
-         SAMPLE8(-3,  1),
-         SAMPLE8( 1,  3),
-      },
-      .origin = SAMPLE8(0, 0)
-   },
-
-   [MALI_SAMPLE_PATTERN_D3D_8X_GRID] = {
-      .positions = {
-         SAMPLE16( 1, -3),
-         SAMPLE16(-1,  3),
-         SAMPLE16( 5,  1),
-         SAMPLE16(-3, -5),
-         SAMPLE16(-5,  5),
-         SAMPLE16(-7, -1),
-         SAMPLE16( 3,  7),
-         SAMPLE16( 7,  -7),
-      },
-      .origin = SAMPLE16(0, 0)
-   },
-
-   [MALI_SAMPLE_PATTERN_D3D_16X_GRID] = {
-      .positions = {
-         SAMPLE16( 1,  1),
-         SAMPLE16(-1, -3),
-         SAMPLE16(-3,  2),
-         SAMPLE16( 4, -1),
-         SAMPLE16(-5, -2),
-         SAMPLE16( 2,  5),
-         SAMPLE16( 5,  3),
-         SAMPLE16( 3, -5),
-         SAMPLE16(-2,  6),
-         SAMPLE16( 0,  7),
-         SAMPLE16(-4, -6),
-         SAMPLE16(-6,  4),
-         SAMPLE16(-8,  0),
-         SAMPLE16( 7, -4),
-         SAMPLE16( 6,  7),
-         SAMPLE16(-7, -8),
-      },
-      .origin = SAMPLE16(0, 0)
-   }
-};
-/* clang-format on */
+const struct mali_sample_positions
+   sample_position_lut[] =
+      {[MALI_SAMPLE_PATTERN_SINGLE_SAMPLED] = {.positions = {SAMPLE4(0, 0)},
+                                               .origin = SAMPLE4(0, 0)},
+
+       [MALI_SAMPLE_PATTERN_ORDERED_4X_GRID] = {.positions =
+                                                   {
+                                                      SAMPLE4(-1, -1),
+                                                      SAMPLE4(1, -1),
+                                                      SAMPLE4(-1, 1),
+                                                      SAMPLE4(1, 1),
+                                                   },
+                                                .origin = SAMPLE4(0, 0)},
+
+       [MALI_SAMPLE_PATTERN_ROTATED_4X_GRID] = {.positions =
+                                                   {
+                                                      SAMPLE8(-1, -3),
+                                                      SAMPLE8(3, -1),
+                                                      SAMPLE8(-3, 1),
+                                                      SAMPLE8(1, 3),
+                                                   },
+                                                .origin = SAMPLE8(0, 0)},
+
+       [MALI_SAMPLE_PATTERN_D3D_8X_GRID] = {.positions =
+                                               {
+                                                  SAMPLE16(1, -3),
+                                                  SAMPLE16(-1, 3),
+                                                  SAMPLE16(5, 1),
+                                                  SAMPLE16(-3, -5),
+                                                  SAMPLE16(-5, 5),
+                                                  SAMPLE16(-7, -1),
+                                                  SAMPLE16(3, 7),
+                                                  SAMPLE16(7, -7),
+                                               },
+                                            .origin = SAMPLE16(0, 0)},
+
+       [MALI_SAMPLE_PATTERN_D3D_16X_GRID] = {.positions =
+                                                {
+                                                   SAMPLE16(1, 1),
+                                                   SAMPLE16(-1, -3),
+                                                   SAMPLE16(-3, 2),
+                                                   SAMPLE16(4, -1),
+                                                   SAMPLE16(-5, -2),
+                                                   SAMPLE16(2, 5),
+                                                   SAMPLE16(5, 3),
+                                                   SAMPLE16(3, -5),
+                                                   SAMPLE16(-2, 6),
+                                                   SAMPLE16(0, 7),
+                                                   SAMPLE16(-4, -6),
+                                                   SAMPLE16(-6, 4),
+                                                   SAMPLE16(-8, 0),
+                                                   SAMPLE16(7, -4),
+                                                   SAMPLE16(6, 7),
+                                                   SAMPLE16(-7, -8),
+
+                                                },
+                                             .origin = SAMPLE16(0, 0)}};
 
 mali_ptr
 panfrost_sample_positions(const struct panfrost_device *dev,
diff -urN mesa-23.0.0/src/panfrost/lib/pan_scoreboard.h mesa/src/panfrost/lib/pan_scoreboard.h
--- mesa-23.0.0/src/panfrost/lib/pan_scoreboard.h	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/pan_scoreboard.h	2023-03-06 19:19:32.062304572 +0100
@@ -55,6 +55,7 @@
 };
 
 #ifdef PAN_ARCH
+#if PAN_ARCH < 10
 /*
  * There are various types of Mali jobs:
  *
@@ -263,6 +264,7 @@
    scoreboard->first_job = transfer.gpu;
    return transfer;
 }
+#endif /* PAN_ARCH < 10 */
 #endif /* PAN_ARCH */
 
 #endif
diff -urN mesa-23.0.0/src/panfrost/lib/pan_texture.c mesa/src/panfrost/lib/pan_texture.c
--- mesa-23.0.0/src/panfrost/lib/pan_texture.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/pan_texture.c	2023-03-06 19:19:32.118304942 +0100
@@ -282,50 +282,48 @@
 
 #if PAN_ARCH >= 9
 
-/* clang-format off */
-#define CLUMP_FMT(pipe, mali) [PIPE_FORMAT_ ## pipe] = MALI_CLUMP_FORMAT_ ## mali
+#define CLUMP_FMT(pipe, mali) [PIPE_FORMAT_##pipe] = MALI_CLUMP_FORMAT_##mali
 static enum mali_clump_format special_clump_formats[PIPE_FORMAT_COUNT] = {
-   CLUMP_FMT(X32_S8X24_UINT,  X32S8X24),
-   CLUMP_FMT(X24S8_UINT,      X24S8),
-   CLUMP_FMT(S8X24_UINT,      S8X24),
-   CLUMP_FMT(S8_UINT,         S8),
-   CLUMP_FMT(L4A4_UNORM,      L4A4),
-   CLUMP_FMT(L8A8_UNORM,      L8A8),
-   CLUMP_FMT(L8A8_UINT,       L8A8),
-   CLUMP_FMT(L8A8_SINT,       L8A8),
-   CLUMP_FMT(A8_UNORM,        A8),
-   CLUMP_FMT(A8_UINT,         A8),
-   CLUMP_FMT(A8_SINT,         A8),
-   CLUMP_FMT(ETC1_RGB8,       ETC2_RGB8),
-   CLUMP_FMT(ETC2_RGB8,       ETC2_RGB8),
-   CLUMP_FMT(ETC2_SRGB8,      ETC2_RGB8),
-   CLUMP_FMT(ETC2_RGB8A1,     ETC2_RGB8A1),
-   CLUMP_FMT(ETC2_SRGB8A1,    ETC2_RGB8A1),
-   CLUMP_FMT(ETC2_RGBA8,      ETC2_RGBA8),
-   CLUMP_FMT(ETC2_SRGBA8,     ETC2_RGBA8),
-   CLUMP_FMT(ETC2_R11_UNORM,  ETC2_R11_UNORM),
-   CLUMP_FMT(ETC2_R11_SNORM,  ETC2_R11_SNORM),
+   CLUMP_FMT(X32_S8X24_UINT, X32S8X24),
+   CLUMP_FMT(X24S8_UINT, X24S8),
+   CLUMP_FMT(S8X24_UINT, S8X24),
+   CLUMP_FMT(S8_UINT, S8),
+   CLUMP_FMT(L4A4_UNORM, L4A4),
+   CLUMP_FMT(L8A8_UNORM, L8A8),
+   CLUMP_FMT(L8A8_UINT, L8A8),
+   CLUMP_FMT(L8A8_SINT, L8A8),
+   CLUMP_FMT(A8_UNORM, A8),
+   CLUMP_FMT(A8_UINT, A8),
+   CLUMP_FMT(A8_SINT, A8),
+   CLUMP_FMT(ETC1_RGB8, ETC2_RGB8),
+   CLUMP_FMT(ETC2_RGB8, ETC2_RGB8),
+   CLUMP_FMT(ETC2_SRGB8, ETC2_RGB8),
+   CLUMP_FMT(ETC2_RGB8A1, ETC2_RGB8A1),
+   CLUMP_FMT(ETC2_SRGB8A1, ETC2_RGB8A1),
+   CLUMP_FMT(ETC2_RGBA8, ETC2_RGBA8),
+   CLUMP_FMT(ETC2_SRGBA8, ETC2_RGBA8),
+   CLUMP_FMT(ETC2_R11_UNORM, ETC2_R11_UNORM),
+   CLUMP_FMT(ETC2_R11_SNORM, ETC2_R11_SNORM),
    CLUMP_FMT(ETC2_RG11_UNORM, ETC2_RG11_UNORM),
    CLUMP_FMT(ETC2_RG11_SNORM, ETC2_RG11_SNORM),
-   CLUMP_FMT(DXT1_RGB,        BC1_UNORM),
-   CLUMP_FMT(DXT1_RGBA,       BC1_UNORM),
-   CLUMP_FMT(DXT1_SRGB,       BC1_UNORM),
-   CLUMP_FMT(DXT1_SRGBA,      BC1_UNORM),
-   CLUMP_FMT(DXT3_RGBA,       BC2_UNORM),
-   CLUMP_FMT(DXT3_SRGBA,      BC2_UNORM),
-   CLUMP_FMT(DXT5_RGBA,       BC3_UNORM),
-   CLUMP_FMT(DXT5_SRGBA,      BC3_UNORM),
-   CLUMP_FMT(RGTC1_UNORM,     BC4_UNORM),
-   CLUMP_FMT(RGTC1_SNORM,     BC4_SNORM),
-   CLUMP_FMT(RGTC2_UNORM,     BC5_UNORM),
-   CLUMP_FMT(RGTC2_SNORM,     BC5_SNORM),
-   CLUMP_FMT(BPTC_RGB_FLOAT,  BC6H_SF16),
+   CLUMP_FMT(DXT1_RGB, BC1_UNORM),
+   CLUMP_FMT(DXT1_RGBA, BC1_UNORM),
+   CLUMP_FMT(DXT1_SRGB, BC1_UNORM),
+   CLUMP_FMT(DXT1_SRGBA, BC1_UNORM),
+   CLUMP_FMT(DXT3_RGBA, BC2_UNORM),
+   CLUMP_FMT(DXT3_SRGBA, BC2_UNORM),
+   CLUMP_FMT(DXT5_RGBA, BC3_UNORM),
+   CLUMP_FMT(DXT5_SRGBA, BC3_UNORM),
+   CLUMP_FMT(RGTC1_UNORM, BC4_UNORM),
+   CLUMP_FMT(RGTC1_SNORM, BC4_SNORM),
+   CLUMP_FMT(RGTC2_UNORM, BC5_UNORM),
+   CLUMP_FMT(RGTC2_SNORM, BC5_SNORM),
+   CLUMP_FMT(BPTC_RGB_FLOAT, BC6H_SF16),
    CLUMP_FMT(BPTC_RGB_UFLOAT, BC6H_UF16),
    CLUMP_FMT(BPTC_RGBA_UNORM, BC7_UNORM),
-   CLUMP_FMT(BPTC_SRGBA,      BC7_UNORM),
+   CLUMP_FMT(BPTC_SRGBA, BC7_UNORM),
 };
 #undef CLUMP_FMT
-/* clang-format on */
 
 static enum mali_clump_format
 panfrost_clump_format(enum pipe_format format)
@@ -550,8 +548,7 @@
 {
    const struct pan_image_layout *layout = &iview->image->layout;
    enum pipe_format format = iview->format;
-   uint32_t mali_format = dev->formats[format].hw;
-   unsigned char swizzle[4];
+   unsigned swizzle;
 
    if (PAN_ARCH >= 7 && util_format_is_depth_or_stencil(format)) {
       /* v7+ doesn't have an _RRRR component order, combine the
@@ -563,27 +560,13 @@
          PIPE_SWIZZLE_X,
          PIPE_SWIZZLE_X,
       };
+      unsigned char patched_swizzle[4];
 
-      util_format_compose_swizzles(replicate_x, iview->swizzle, swizzle);
-   } else if (PAN_ARCH == 7) {
-#if PAN_ARCH == 7
-      /* v7 (only) restricts component orders when AFBC is in use.
-       * Rather than restrict AFBC, we use an allowed component order
-       * with an invertible swizzle composed.
-       */
-      enum mali_rgb_component_order orig = mali_format & BITFIELD_MASK(12);
-      struct pan_decomposed_swizzle decomposed =
-         GENX(pan_decompose_swizzle)(orig);
-
-      /* Apply the new component order */
-      mali_format = (mali_format & ~orig) | decomposed.pre;
-
-      /* Compose the new swizzle */
-      util_format_compose_swizzles(decomposed.post, iview->swizzle, swizzle);
-#endif
+      util_format_compose_swizzles(replicate_x, iview->swizzle,
+                                   patched_swizzle);
+      swizzle = panfrost_translate_swizzle_4(patched_swizzle);
    } else {
-      STATIC_ASSERT(sizeof(swizzle) == sizeof(iview->swizzle));
-      memcpy(swizzle, iview->swizzle, sizeof(swizzle));
+      swizzle = panfrost_translate_swizzle_4(iview->swizzle);
    }
 
    panfrost_emit_texture_payload(iview, format, payload->cpu);
@@ -612,14 +595,14 @@
 
    pan_pack(out, TEXTURE, cfg) {
       cfg.dimension = iview->dim;
-      cfg.format = mali_format;
+      cfg.format = dev->formats[format].hw;
       cfg.width = width;
       cfg.height = u_minify(layout->height, iview->first_level);
       if (iview->dim == MALI_TEXTURE_DIMENSION_3D)
          cfg.depth = u_minify(layout->depth, iview->first_level);
       else
          cfg.sample_count = layout->nr_samples;
-      cfg.swizzle = panfrost_translate_swizzle_4(swizzle);
+      cfg.swizzle = swizzle;
 #if PAN_ARCH >= 9
       cfg.texel_interleave = (layout->modifier != DRM_FORMAT_MOD_LINEAR) ||
                              util_format_is_compressed(format);
diff -urN mesa-23.0.0/src/panfrost/lib/pan_texture.h mesa/src/panfrost/lib/pan_texture.h
--- mesa-23.0.0/src/panfrost/lib/pan_texture.h	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/pan_texture.h	2023-03-06 19:19:32.121304962 +0100
@@ -44,9 +44,15 @@
 extern "C" {
 #endif
 
-#define PAN_MODIFIER_COUNT 6
+#define PAN_MODIFIER_COUNT 7
 extern uint64_t pan_best_modifiers[PAN_MODIFIER_COUNT];
 
+struct pan_image_slice_crc {
+   unsigned offset;
+   unsigned stride;
+   unsigned size;
+};
+
 struct pan_image_slice_layout {
    unsigned offset;
 
@@ -80,11 +86,7 @@
 
    /* If checksumming is enabled following the slice, what
     * is its offset/stride? */
-   struct {
-      unsigned offset;
-      unsigned stride;
-      unsigned size;
-   } crc;
+   struct pan_image_slice_crc crc;
 
    unsigned size;
 };
@@ -141,34 +143,15 @@
    } buf;
 };
 
-unsigned panfrost_compute_checksum_size(struct pan_image_slice_layout *slice,
-                                        unsigned width, unsigned height);
+struct pan_image_slice_crc panfrost_compute_checksum_size(unsigned width,
+                                                          unsigned height);
 
-/* AFBC format mode. The ordering is intended to match the Valhall hardware enum
- * ("AFBC Compression Mode"), but this enum is required in software on older
- * hardware for correct handling of texture views. Defining the enum lets us
- * unify these code paths.
- */
-enum pan_afbc_mode {
-   PAN_AFBC_MODE_R8,
-   PAN_AFBC_MODE_R8G8,
-   PAN_AFBC_MODE_R5G6B5,
-   PAN_AFBC_MODE_R4G4B4A4,
-   PAN_AFBC_MODE_R5G5B5A1,
-   PAN_AFBC_MODE_R8G8B8,
-   PAN_AFBC_MODE_R8G8B8A8,
-   PAN_AFBC_MODE_R10G10B10A2,
-   PAN_AFBC_MODE_R11G11B10,
-   PAN_AFBC_MODE_S8,
-
-   /* Sentintel signalling a format that cannot be compressed */
-   PAN_AFBC_MODE_INVALID
-};
+/* AFBC */
 
 bool panfrost_format_supports_afbc(const struct panfrost_device *dev,
                                    enum pipe_format format);
 
-enum pan_afbc_mode panfrost_afbc_format(unsigned arch, enum pipe_format format);
+enum pipe_format panfrost_afbc_format(unsigned arch, enum pipe_format format);
 
 #define AFBC_HEADER_BYTES_PER_TILE 16
 
@@ -176,6 +159,8 @@
 
 bool panfrost_afbc_can_tile(const struct panfrost_device *dev);
 
+bool panfrost_afbc_only_native(unsigned arch, enum pipe_format format);
+
 /*
  * Represents the block size of a single plane. For AFBC, this represents the
  * superblock size. For u-interleaving, this represents the tile size.
diff -urN mesa-23.0.0/src/panfrost/lib/pan_util.h mesa/src/panfrost/lib/pan_util.h
--- mesa-23.0.0/src/panfrost/lib/pan_util.h	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/pan_util.h	2023-03-06 19:19:32.125304989 +0100
@@ -47,10 +47,16 @@
 #define PAN_DBG_LINEAR   0x1000
 #define PAN_DBG_NO_CACHE 0x2000
 #define PAN_DBG_DUMP     0x4000
-
 #ifndef NDEBUG
 #define PAN_DBG_OVERFLOW 0x8000
 #endif
+#define PAN_DBG_TILER        0x010000
+#define PAN_DBG_BO_LOG       0x020000
+#define PAN_DBG_BO_CLEAR     0x040000
+#define PAN_DBG_UNCACHED_GPU 0x100000
+#define PAN_DBG_UNCACHED_CPU 0x200000
+#define PAN_DBG_LOG          0x400000
+#define PAN_DBG_GOFASTER     0x800000
 
 struct panfrost_device;
 
diff -urN mesa-23.0.0/src/panfrost/lib/tests/test-blend.c mesa/src/panfrost/lib/tests/test-blend.c
--- mesa-23.0.0/src/panfrost/lib/tests/test-blend.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/tests/test-blend.c	2023-03-06 19:19:31.754302539 +0100
@@ -36,45 +36,38 @@
    uint32_t hardware;
 };
 
-/* clang-format off */
-#define RGBA(key, value) \
-   .rgb_ ## key = value, \
-   .alpha_ ## key = value
+#define RGBA(key, value) .rgb_##key = value, .alpha_##key = value
 
 static const struct test blend_tests[] = {
-   {
-      "Replace",
-      {
-         .blend_enable = false,
-         .color_mask = 0xF,
-      },
-      .constant_mask = 0x0,
-      .reads_dest = false,
-      .opaque = true,
-      .fixed_function = true,
-      .alpha_zero_nop = false,
-      .alpha_one_store = false,
-      .hardware = 0xF0122122
-   },
-   {
-      "Alpha",
-      {
-         .blend_enable = true,
-         .color_mask = 0xF,
-
-         RGBA(func, BLEND_FUNC_ADD),
-         RGBA(src_factor, BLEND_FACTOR_SRC_ALPHA),
-         RGBA(dst_factor, BLEND_FACTOR_SRC_ALPHA),
-         RGBA(invert_dst_factor, true),
-      },
-      .constant_mask = 0x0,
-      .reads_dest = true,
-      .opaque = false,
-      .fixed_function = true,
-      .alpha_zero_nop = true,
-      .alpha_one_store = true,
-      .hardware = 0xF0503503
-   },
+   {"Replace",
+    {
+       .blend_enable = false,
+       .color_mask = 0xF,
+    },
+    .constant_mask = 0x0,
+    .reads_dest = false,
+    .opaque = true,
+    .fixed_function = true,
+    .alpha_zero_nop = false,
+    .alpha_one_store = false,
+    .hardware = 0xF0122122},
+   {"Alpha",
+    {
+       .blend_enable = true,
+       .color_mask = 0xF,
+
+       RGBA(func, BLEND_FUNC_ADD),
+       RGBA(src_factor, BLEND_FACTOR_SRC_ALPHA),
+       RGBA(dst_factor, BLEND_FACTOR_SRC_ALPHA),
+       RGBA(invert_dst_factor, true),
+    },
+    .constant_mask = 0x0,
+    .reads_dest = true,
+    .opaque = false,
+    .fixed_function = true,
+    .alpha_zero_nop = true,
+    .alpha_one_store = true,
+    .hardware = 0xF0503503},
    {
       "Additive",
       {
@@ -95,25 +88,23 @@
       .alpha_one_store = false,
       .hardware = 0xF0932932 /* equivalently 0xF0923923 */
    },
-   {
-      "Additive-Alpha",
-      {
-         .blend_enable = true,
-         .color_mask = 0xF,
-
-         RGBA(func, BLEND_FUNC_ADD),
-         RGBA(src_factor, BLEND_FACTOR_SRC_ALPHA),
-         RGBA(dst_factor, BLEND_FACTOR_ZERO),
-         RGBA(invert_dst_factor, true),
-      },
-      .constant_mask = 0x0,
-      .reads_dest = true,
-      .opaque = false,
-      .fixed_function = true,
-      .alpha_zero_nop = true,
-      .alpha_one_store = false,
-      .hardware = 0xF0523523
-   },
+   {"Additive-Alpha",
+    {
+       .blend_enable = true,
+       .color_mask = 0xF,
+
+       RGBA(func, BLEND_FUNC_ADD),
+       RGBA(src_factor, BLEND_FACTOR_SRC_ALPHA),
+       RGBA(dst_factor, BLEND_FACTOR_ZERO),
+       RGBA(invert_dst_factor, true),
+    },
+    .constant_mask = 0x0,
+    .reads_dest = true,
+    .opaque = false,
+    .fixed_function = true,
+    .alpha_zero_nop = true,
+    .alpha_one_store = false,
+    .hardware = 0xF0523523},
    {
       "Subtractive",
       {
@@ -171,20 +162,18 @@
       .alpha_one_store = false,
       .hardware = 0xF0231231 /* equivalently 0xF0321321 */
    },
-   {
-      "Replace masked",
-      {
-         .blend_enable = false,
-         .color_mask = 0x3,
-      },
-      .constant_mask = 0x0,
-      .reads_dest = true,
-      .opaque = false,
-      .fixed_function = true,
-      .alpha_zero_nop = false,
-      .alpha_one_store = false,
-      .hardware = 0x30122122
-   },
+   {"Replace masked",
+    {
+       .blend_enable = false,
+       .color_mask = 0x3,
+    },
+    .constant_mask = 0x0,
+    .reads_dest = true,
+    .opaque = false,
+    .fixed_function = true,
+    .alpha_zero_nop = false,
+    .alpha_one_store = false,
+    .hardware = 0x30122122},
    {
       "Modulate masked",
       {
@@ -230,11 +219,11 @@
          .rgb_func = BLEND_FUNC_ADD,
          .rgb_src_factor = BLEND_FACTOR_ZERO,
          .rgb_invert_src_factor = true,
-         .rgb_dst_factor= BLEND_FACTOR_ZERO,
+         .rgb_dst_factor = BLEND_FACTOR_ZERO,
 
          .alpha_func = BLEND_FUNC_ADD,
          .alpha_src_factor = BLEND_FACTOR_DST_COLOR,
-         .alpha_dst_factor= BLEND_FACTOR_SRC_COLOR,
+         .alpha_dst_factor = BLEND_FACTOR_SRC_COLOR,
       },
       .constant_mask = 0x0,
       .reads_dest = true,
@@ -253,11 +242,11 @@
          .rgb_func = BLEND_FUNC_ADD,
          .rgb_src_factor = BLEND_FACTOR_ZERO,
          .rgb_invert_src_factor = true,
-         .rgb_dst_factor= BLEND_FACTOR_ZERO,
+         .rgb_dst_factor = BLEND_FACTOR_ZERO,
 
          .alpha_func = BLEND_FUNC_ADD,
          .alpha_src_factor = BLEND_FACTOR_DST_ALPHA,
-         .alpha_dst_factor= BLEND_FACTOR_SRC_COLOR,
+         .alpha_dst_factor = BLEND_FACTOR_SRC_COLOR,
       },
       .constant_mask = 0x0,
       .reads_dest = true,
@@ -276,11 +265,11 @@
          .rgb_func = BLEND_FUNC_ADD,
          .rgb_src_factor = BLEND_FACTOR_ZERO,
          .rgb_invert_src_factor = true,
-         .rgb_dst_factor= BLEND_FACTOR_ZERO,
+         .rgb_dst_factor = BLEND_FACTOR_ZERO,
 
          .alpha_func = BLEND_FUNC_ADD,
          .alpha_src_factor = BLEND_FACTOR_DST_ALPHA,
-         .alpha_dst_factor= BLEND_FACTOR_SRC_ALPHA,
+         .alpha_dst_factor = BLEND_FACTOR_SRC_ALPHA,
       },
       .constant_mask = 0x0,
       .reads_dest = true,
@@ -289,9 +278,7 @@
       .alpha_zero_nop = false,
       .alpha_one_store = false,
       .hardware = 0xC0431132 /* 0 + dest * (2*src); equivalent 0xC0431122 */
-   }
-};
-/* clang-format on */
+   }};
 
 #define ASSERT_EQ(x, y)                                                        \
    do {                                                                        \
diff -urN mesa-23.0.0/src/panfrost/lib/tests/test-clear.c mesa/src/panfrost/lib/tests/test-clear.c
--- mesa-23.0.0/src/panfrost/lib/tests/test-clear.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/tests/test-clear.c	2023-03-06 19:19:31.781302717 +0100
@@ -52,105 +52,111 @@
 #define D (true)
 #define _ (false)
 
-/* clang-format off */
 static const struct test clear_tests[] = {
    /* Basic tests */
-   { PIPE_FORMAT_R8G8B8A8_UNORM,    D, F(0.0,   0.0, 0.0, 0.0),   RRRR(0x00000000) },
-   { PIPE_FORMAT_R8G8B8A8_UNORM,    _, F(0.0,   0.0, 0.0, 0.0),   RRRR(0x00000000) },
-   { PIPE_FORMAT_R8G8B8A8_UNORM,    D, F(1.0,   0.0, 0.0, 1.0),   RRRR(0xFF0000FF) },
-   { PIPE_FORMAT_R8G8B8A8_UNORM,    _, F(1.0,   0.0, 0.0, 1.0),   RRRR(0xFF0000FF) },
-   { PIPE_FORMAT_B8G8R8A8_UNORM,    D, F(1.0,   0.0, 0.0, 1.0),   RRRR(0xFF0000FF) },
-   { PIPE_FORMAT_B8G8R8A8_UNORM,    _, F(1.0,   0.0, 0.0, 1.0),   RRRR(0xFF0000FF) },
-   { PIPE_FORMAT_R8G8B8A8_UNORM,    D, F(0.664, 0.0, 0.0, 0.0),   RRRR(0x000000A9) },
-   { PIPE_FORMAT_R8G8B8A8_UNORM,    _, F(0.664, 0.0, 0.0, 0.0),   RRRR(0x000000A9) },
-   { PIPE_FORMAT_R4G4B4A4_UNORM,    D, F(0.664, 0.0, 0.0, 0.0),   RRRR(0x0000009F) },
-   { PIPE_FORMAT_R4G4B4A4_UNORM,    _, F(0.664, 0.0, 0.0, 0.0),   RRRR(0x000000A0) },
+   {PIPE_FORMAT_R8G8B8A8_UNORM, D, F(0.0, 0.0, 0.0, 0.0), RRRR(0x00000000)},
+   {PIPE_FORMAT_R8G8B8A8_UNORM, _, F(0.0, 0.0, 0.0, 0.0), RRRR(0x00000000)},
+   {PIPE_FORMAT_R8G8B8A8_UNORM, D, F(1.0, 0.0, 0.0, 1.0), RRRR(0xFF0000FF)},
+   {PIPE_FORMAT_R8G8B8A8_UNORM, _, F(1.0, 0.0, 0.0, 1.0), RRRR(0xFF0000FF)},
+   {PIPE_FORMAT_B8G8R8A8_UNORM, D, F(1.0, 0.0, 0.0, 1.0), RRRR(0xFF0000FF)},
+   {PIPE_FORMAT_B8G8R8A8_UNORM, _, F(1.0, 0.0, 0.0, 1.0), RRRR(0xFF0000FF)},
+   {PIPE_FORMAT_R8G8B8A8_UNORM, D, F(0.664, 0.0, 0.0, 0.0), RRRR(0x000000A9)},
+   {PIPE_FORMAT_R8G8B8A8_UNORM, _, F(0.664, 0.0, 0.0, 0.0), RRRR(0x000000A9)},
+   {PIPE_FORMAT_R4G4B4A4_UNORM, D, F(0.664, 0.0, 0.0, 0.0), RRRR(0x0000009F)},
+   {PIPE_FORMAT_R4G4B4A4_UNORM, _, F(0.664, 0.0, 0.0, 0.0), RRRR(0x000000A0)},
 
    /* Test rounding to nearest even. The values are cherrypicked to multiply
     * out to a fractional part of 0.5. The first test should round down and
     * second test should round up. */
 
-   { PIPE_FORMAT_R4G4B4A4_UNORM,    D, F(0.41875, 0.0, 0.0, 1.0), RRRR(0xF0000064) },
-   { PIPE_FORMAT_R4G4B4A4_UNORM,    D, F(0.40625, 0.0, 0.0, 1.0), RRRR(0xF0000062) },
+   {PIPE_FORMAT_R4G4B4A4_UNORM, D, F(0.41875, 0.0, 0.0, 1.0), RRRR(0xF0000064)},
+   {PIPE_FORMAT_R4G4B4A4_UNORM, D, F(0.40625, 0.0, 0.0, 1.0), RRRR(0xF0000062)},
 
    /* Testing rounding direction when dithering is disabled. The packed value
     * is what gets rounded. This behaves as round-to-even with the cutoff point
     * at 2^-m + 2^-n for an m:n representation. */
 
-   { PIPE_FORMAT_R4G4B4A4_UNORM,    _, F(0.03125, 0.0, 0.0, 1.0),  RRRR(0xF0000000) },
-   { PIPE_FORMAT_R4G4B4A4_UNORM,    _, F(0.03125, 0.0, 0.0, 1.0),  RRRR(0xF0000000) },
-   { PIPE_FORMAT_R4G4B4A4_UNORM,    _, F(0.03333, 0.0, 0.0, 1.0),  RRRR(0xF0000000) },
-   { PIPE_FORMAT_R4G4B4A4_UNORM,    _, F(0.03334, 0.0, 0.0, 1.0),  RRRR(0xF0000010) },
-   { PIPE_FORMAT_R4G4B4A4_UNORM,    _, F(0.09375, 0.0, 0.0, 1.0),  RRRR(0xF0000010) },
+   {PIPE_FORMAT_R4G4B4A4_UNORM, _, F(0.03125, 0.0, 0.0, 1.0), RRRR(0xF0000000)},
+   {PIPE_FORMAT_R4G4B4A4_UNORM, _, F(0.03125, 0.0, 0.0, 1.0), RRRR(0xF0000000)},
+   {PIPE_FORMAT_R4G4B4A4_UNORM, _, F(0.03333, 0.0, 0.0, 1.0), RRRR(0xF0000000)},
+   {PIPE_FORMAT_R4G4B4A4_UNORM, _, F(0.03334, 0.0, 0.0, 1.0), RRRR(0xF0000010)},
+   {PIPE_FORMAT_R4G4B4A4_UNORM, _, F(0.09375, 0.0, 0.0, 1.0), RRRR(0xF0000010)},
 
    /* Check all the special formats with different edge cases */
 
-   { PIPE_FORMAT_R4G4B4A4_UNORM,    D, F(0.127, 2.4, -1.0, 0.5), RRRR(0x7800F01E) },
-   { PIPE_FORMAT_R5G5B5A1_UNORM,    D, F(0.127, 2.4, -1.0, 0.5), RRRR(0x400F807E) },
-   { PIPE_FORMAT_R5G6B5_UNORM,      D, F(0.127, 2.4, -1.0, 0.5), RRRR(0x000FC07E) },
-   { PIPE_FORMAT_R10G10B10A2_UNORM, D, F(0.127, 2.4, -1.0, 0.5), RRRR(0x800FFC82) },
-   { PIPE_FORMAT_R8G8B8A8_SRGB,     D, F(0.127, 2.4, -1.0, 0.5), RRRR(0x8000FF64) },
-
-   { PIPE_FORMAT_R4G4B4A4_UNORM,    D, F(0.718, 0.18, 1.0, 2.0), RRRR(0xF0F02BAC) },
-   { PIPE_FORMAT_R5G6B5_UNORM,      D, F(0.718, 0.18, 1.0, 2.0), RRRR(0x3E02D6C8) },
-   { PIPE_FORMAT_R5G5B5A1_UNORM,    D, F(0.718, 0.18, 1.0, 2.0), RRRR(0xBE02CEC8) },
-   { PIPE_FORMAT_R10G10B10A2_UNORM, D, F(0.718, 0.18, 1.0, 2.0), RRRR(0xFFF2E2DF) },
-   { PIPE_FORMAT_R8G8B8A8_SRGB,     D, F(0.718, 0.18, 1.0, 2.0), RRRR(0xFFFF76DC) },
+   {PIPE_FORMAT_R4G4B4A4_UNORM, D, F(0.127, 2.4, -1.0, 0.5), RRRR(0x7800F01E)},
+   {PIPE_FORMAT_R5G5B5A1_UNORM, D, F(0.127, 2.4, -1.0, 0.5), RRRR(0x400F807E)},
+   {PIPE_FORMAT_R5G6B5_UNORM, D, F(0.127, 2.4, -1.0, 0.5), RRRR(0x000FC07E)},
+   {PIPE_FORMAT_R10G10B10A2_UNORM, D, F(0.127, 2.4, -1.0, 0.5),
+    RRRR(0x800FFC82)},
+   {PIPE_FORMAT_R8G8B8A8_SRGB, D, F(0.127, 2.4, -1.0, 0.5), RRRR(0x8000FF64)},
+
+   {PIPE_FORMAT_R4G4B4A4_UNORM, D, F(0.718, 0.18, 1.0, 2.0), RRRR(0xF0F02BAC)},
+   {PIPE_FORMAT_R5G6B5_UNORM, D, F(0.718, 0.18, 1.0, 2.0), RRRR(0x3E02D6C8)},
+   {PIPE_FORMAT_R5G5B5A1_UNORM, D, F(0.718, 0.18, 1.0, 2.0), RRRR(0xBE02CEC8)},
+   {PIPE_FORMAT_R10G10B10A2_UNORM, D, F(0.718, 0.18, 1.0, 2.0),
+    RRRR(0xFFF2E2DF)},
+   {PIPE_FORMAT_R8G8B8A8_SRGB, D, F(0.718, 0.18, 1.0, 2.0), RRRR(0xFFFF76DC)},
 
    /* Check that values are padded when dithering is disabled */
 
-   { PIPE_FORMAT_R4G4B4A4_UNORM,    _, F(0.718, 0.18, 1.0, 2.0), RRRR(0xF0F030B0) },
-   { PIPE_FORMAT_R5G6B5_UNORM,      _, F(0.718, 0.18, 1.0, 2.0), RRRR(0x3E02C2C0) },
-   { PIPE_FORMAT_R5G5B5A1_UNORM,    _, F(0.718, 0.18, 1.0, 2.0), RRRR(0xBE0302C0) },
-   { PIPE_FORMAT_R10G10B10A2_UNORM, _, F(0.718, 0.18, 1.0, 2.0), RRRR(0xFFF2E2DF) },
-   { PIPE_FORMAT_R8G8B8A8_SRGB,     _, F(0.718, 0.18, 1.0, 2.0), RRRR(0xFFFF76DC) },
+   {PIPE_FORMAT_R4G4B4A4_UNORM, _, F(0.718, 0.18, 1.0, 2.0), RRRR(0xF0F030B0)},
+   {PIPE_FORMAT_R5G6B5_UNORM, _, F(0.718, 0.18, 1.0, 2.0), RRRR(0x3E02C2C0)},
+   {PIPE_FORMAT_R5G5B5A1_UNORM, _, F(0.718, 0.18, 1.0, 2.0), RRRR(0xBE0302C0)},
+   {PIPE_FORMAT_R10G10B10A2_UNORM, _, F(0.718, 0.18, 1.0, 2.0),
+    RRRR(0xFFF2E2DF)},
+   {PIPE_FORMAT_R8G8B8A8_SRGB, _, F(0.718, 0.18, 1.0, 2.0), RRRR(0xFFFF76DC)},
 
    /* Check that blendable tilebuffer values are invariant under swizzling */
 
-   { PIPE_FORMAT_B4G4R4A4_UNORM,    D, F(0.127, 2.4, -1.0, 0.5), RRRR(0x7800F01E) },
-   { PIPE_FORMAT_B5G5R5A1_UNORM,    D, F(0.127, 2.4, -1.0, 0.5), RRRR(0x400F807E) },
-   { PIPE_FORMAT_B5G6R5_UNORM,      D, F(0.127, 2.4, -1.0, 0.5), RRRR(0x000FC07E) },
-   { PIPE_FORMAT_B10G10R10A2_UNORM, D, F(0.127, 2.4, -1.0, 0.5), RRRR(0x800FFC82) },
-   { PIPE_FORMAT_B8G8R8A8_SRGB,     D, F(0.127, 2.4, -1.0, 0.5), RRRR(0x8000FF64) },
-
-   { PIPE_FORMAT_B4G4R4A4_UNORM,    D, F(0.718, 0.18, 1.0, 2.0), RRRR(0xF0F02BAC) },
-   { PIPE_FORMAT_B5G6R5_UNORM,      D, F(0.718, 0.18, 1.0, 2.0), RRRR(0x3E02D6C8) },
-   { PIPE_FORMAT_B5G5R5A1_UNORM,    D, F(0.718, 0.18, 1.0, 2.0), RRRR(0xBE02CEC8) },
-   { PIPE_FORMAT_B10G10R10A2_UNORM, D, F(0.718, 0.18, 1.0, 2.0), RRRR(0xFFF2E2DF) },
-   { PIPE_FORMAT_B8G8R8A8_SRGB,     D, F(0.718, 0.18, 1.0, 2.0), RRRR(0xFFFF76DC) },
-
-   { PIPE_FORMAT_B4G4R4A4_UNORM,    _, F(0.718, 0.18, 1.0, 2.0), RRRR(0xF0F030B0) },
-   { PIPE_FORMAT_B5G6R5_UNORM,      _, F(0.718, 0.18, 1.0, 2.0), RRRR(0x3E02C2C0) },
-   { PIPE_FORMAT_B5G5R5A1_UNORM,    _, F(0.718, 0.18, 1.0, 2.0), RRRR(0xBE0302C0) },
-   { PIPE_FORMAT_B10G10R10A2_UNORM, _, F(0.718, 0.18, 1.0, 2.0), RRRR(0xFFF2E2DF) },
-   { PIPE_FORMAT_B8G8R8A8_SRGB,     _, F(0.718, 0.18, 1.0, 2.0), RRRR(0xFFFF76DC) },
+   {PIPE_FORMAT_B4G4R4A4_UNORM, D, F(0.127, 2.4, -1.0, 0.5), RRRR(0x7800F01E)},
+   {PIPE_FORMAT_B5G5R5A1_UNORM, D, F(0.127, 2.4, -1.0, 0.5), RRRR(0x400F807E)},
+   {PIPE_FORMAT_B5G6R5_UNORM, D, F(0.127, 2.4, -1.0, 0.5), RRRR(0x000FC07E)},
+   {PIPE_FORMAT_B10G10R10A2_UNORM, D, F(0.127, 2.4, -1.0, 0.5),
+    RRRR(0x800FFC82)},
+   {PIPE_FORMAT_B8G8R8A8_SRGB, D, F(0.127, 2.4, -1.0, 0.5), RRRR(0x8000FF64)},
+
+   {PIPE_FORMAT_B4G4R4A4_UNORM, D, F(0.718, 0.18, 1.0, 2.0), RRRR(0xF0F02BAC)},
+   {PIPE_FORMAT_B5G6R5_UNORM, D, F(0.718, 0.18, 1.0, 2.0), RRRR(0x3E02D6C8)},
+   {PIPE_FORMAT_B5G5R5A1_UNORM, D, F(0.718, 0.18, 1.0, 2.0), RRRR(0xBE02CEC8)},
+   {PIPE_FORMAT_B10G10R10A2_UNORM, D, F(0.718, 0.18, 1.0, 2.0),
+    RRRR(0xFFF2E2DF)},
+   {PIPE_FORMAT_B8G8R8A8_SRGB, D, F(0.718, 0.18, 1.0, 2.0), RRRR(0xFFFF76DC)},
+
+   {PIPE_FORMAT_B4G4R4A4_UNORM, _, F(0.718, 0.18, 1.0, 2.0), RRRR(0xF0F030B0)},
+   {PIPE_FORMAT_B5G6R5_UNORM, _, F(0.718, 0.18, 1.0, 2.0), RRRR(0x3E02C2C0)},
+   {PIPE_FORMAT_B5G5R5A1_UNORM, _, F(0.718, 0.18, 1.0, 2.0), RRRR(0xBE0302C0)},
+   {PIPE_FORMAT_B10G10R10A2_UNORM, _, F(0.718, 0.18, 1.0, 2.0),
+    RRRR(0xFFF2E2DF)},
+   {PIPE_FORMAT_B8G8R8A8_SRGB, _, F(0.718, 0.18, 1.0, 2.0), RRRR(0xFFFF76DC)},
 
    /* Check raw formats, which are not invariant under swizzling. Raw formats
     * cannot be dithered. */
 
-   { PIPE_FORMAT_R8G8B8A8_UINT,   D, UI(0xCA, 0xFE, 0xBA, 0xBE), RRRR(0xBEBAFECA) },
-   { PIPE_FORMAT_R8G8B8A8_UINT,   _, UI(0xCA, 0xFE, 0xBA, 0xBE), RRRR(0xBEBAFECA) },
+   {PIPE_FORMAT_R8G8B8A8_UINT, D, UI(0xCA, 0xFE, 0xBA, 0xBE), RRRR(0xBEBAFECA)},
+   {PIPE_FORMAT_R8G8B8A8_UINT, _, UI(0xCA, 0xFE, 0xBA, 0xBE), RRRR(0xBEBAFECA)},
 
-   { PIPE_FORMAT_B8G8R8A8_UINT,   D, UI(0xCA, 0xFE, 0xBA, 0xBE), RRRR(0xBECAFEBA) },
-   { PIPE_FORMAT_B8G8R8A8_UINT,   _, UI(0xCA, 0xFE, 0xBA, 0xBE), RRRR(0xBECAFEBA) },
+   {PIPE_FORMAT_B8G8R8A8_UINT, D, UI(0xCA, 0xFE, 0xBA, 0xBE), RRRR(0xBECAFEBA)},
+   {PIPE_FORMAT_B8G8R8A8_UINT, _, UI(0xCA, 0xFE, 0xBA, 0xBE), RRRR(0xBECAFEBA)},
 
    /* Check that larger raw formats are replicated correctly */
 
-   { PIPE_FORMAT_R16G16B16A16_UINT, D, UI(0xCAFE, 0xBABE, 0xABAD, 0x1DEA),
-                                       RGRG(0xBABECAFE, 0x1DEAABAD) },
+   {PIPE_FORMAT_R16G16B16A16_UINT, D, UI(0xCAFE, 0xBABE, 0xABAD, 0x1DEA),
+    RGRG(0xBABECAFE, 0x1DEAABAD)},
 
-   { PIPE_FORMAT_R16G16B16A16_UINT, _, UI(0xCAFE, 0xBABE, 0xABAD, 0x1DEA),
-                                       RGRG(0xBABECAFE, 0x1DEAABAD) },
+   {PIPE_FORMAT_R16G16B16A16_UINT, _, UI(0xCAFE, 0xBABE, 0xABAD, 0x1DEA),
+    RGRG(0xBABECAFE, 0x1DEAABAD)},
 
-   { PIPE_FORMAT_R32G32B32A32_UINT, D,
-      UI(0xCAFEBABE, 0xABAD1DEA, 0xDEADBEEF, 0xABCDEF01),
-      { 0xCAFEBABE, 0xABAD1DEA, 0xDEADBEEF, 0xABCDEF01 } },
-
-   { PIPE_FORMAT_R32G32B32A32_UINT, _,
-      UI(0xCAFEBABE, 0xABAD1DEA, 0xDEADBEEF, 0xABCDEF01),
-      { 0xCAFEBABE, 0xABAD1DEA, 0xDEADBEEF, 0xABCDEF01 } },
+   {PIPE_FORMAT_R32G32B32A32_UINT,
+    D,
+    UI(0xCAFEBABE, 0xABAD1DEA, 0xDEADBEEF, 0xABCDEF01),
+    {0xCAFEBABE, 0xABAD1DEA, 0xDEADBEEF, 0xABCDEF01}},
+
+   {PIPE_FORMAT_R32G32B32A32_UINT,
+    _,
+    UI(0xCAFEBABE, 0xABAD1DEA, 0xDEADBEEF, 0xABCDEF01),
+    {0xCAFEBABE, 0xABAD1DEA, 0xDEADBEEF, 0xABCDEF01}},
 };
-/* clang-format on */
 
 #define ASSERT_EQ(x, y)                                                                      \
    do {                                                                                      \
diff -urN mesa-23.0.0/src/panfrost/lib/tests/test-earlyzs.cpp mesa/src/panfrost/lib/tests/test-earlyzs.cpp
--- mesa-23.0.0/src/panfrost/lib/tests/test-earlyzs.cpp	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/tests/test-earlyzs.cpp	2023-03-06 17:54:35.856495086 +0100
@@ -21,8 +21,8 @@
  * SOFTWARE.
  */
 
-#include "util/pan_ir.h"
 #include "pan_earlyzs.h"
+#include "util/pan_ir.h"
 
 #include <gtest/gtest.h>
 
@@ -34,19 +34,18 @@
  * under test, only the external API. So we test only the composition.
  */
 
-#define ZS_WRITEMASK     BITFIELD_BIT(0)
-#define ALPHA2COV        BITFIELD_BIT(1)
+#define ZS_WRITEMASK BITFIELD_BIT(0)
+#define ALPHA2COV BITFIELD_BIT(1)
 #define ZS_ALWAYS_PASSES BITFIELD_BIT(2)
-#define DISCARD          BITFIELD_BIT(3)
-#define WRITES_Z         BITFIELD_BIT(4)
-#define WRITES_S         BITFIELD_BIT(5)
-#define WRITES_COV       BITFIELD_BIT(6)
-#define SIDEFX           BITFIELD_BIT(7)
-#define API_EARLY        BITFIELD_BIT(8)
+#define DISCARD BITFIELD_BIT(3)
+#define WRITES_Z BITFIELD_BIT(4)
+#define WRITES_S BITFIELD_BIT(5)
+#define WRITES_COV BITFIELD_BIT(6)
+#define SIDEFX BITFIELD_BIT(7)
+#define API_EARLY BITFIELD_BIT(8)
 
 static void
-test(enum pan_earlyzs expected_update, enum pan_earlyzs expected_kill,
-     uint32_t flags)
+test(enum pan_earlyzs expected_update, enum pan_earlyzs expected_kill, uint32_t flags)
 {
    struct pan_shader_info info = {};
    info.fs.can_discard = !!(flags & DISCARD);
@@ -57,15 +56,18 @@
    info.writes_global = !!(flags & SIDEFX);
 
    struct pan_earlyzs_state result =
-      pan_earlyzs_get(pan_earlyzs_analyze(&info), !!(flags & ZS_WRITEMASK),
-                      !!(flags & ALPHA2COV), !!(flags & ZS_ALWAYS_PASSES));
+      pan_earlyzs_get(pan_earlyzs_analyze(&info),
+                      !!(flags & ZS_WRITEMASK),
+                      !!(flags & ALPHA2COV),
+                      !!(flags & ZS_ALWAYS_PASSES));
 
    ASSERT_EQ(result.update, expected_update);
    ASSERT_EQ(result.kill, expected_kill);
 }
 
-#define CASE(expected_update, expected_kill, flags)                            \
-   test(PAN_EARLYZS_##expected_update, PAN_EARLYZS_##expected_kill, flags)
+
+#define CASE(expected_update, expected_kill, flags) \
+   test(PAN_EARLYZS_ ## expected_update, PAN_EARLYZS_ ## expected_kill, flags)
 
 TEST(EarlyZS, APIForceEarly)
 {
@@ -89,8 +91,7 @@
    CASE(FORCE_LATE, FORCE_EARLY, ZS_WRITEMASK | WRITES_COV);
    CASE(FORCE_LATE, FORCE_EARLY, ZS_WRITEMASK | DISCARD);
    CASE(FORCE_LATE, FORCE_EARLY, ZS_WRITEMASK | ALPHA2COV);
-   CASE(FORCE_LATE, FORCE_EARLY,
-        ZS_WRITEMASK | WRITES_COV | DISCARD | ALPHA2COV);
+   CASE(FORCE_LATE, FORCE_EARLY, ZS_WRITEMASK | WRITES_COV | DISCARD | ALPHA2COV);
 }
 
 TEST(EarlyZS, ModifiesCoverageWritesZSNoSideFXAlt)
@@ -98,8 +99,7 @@
    CASE(FORCE_LATE, WEAK_EARLY, ZS_ALWAYS_PASSES | ZS_WRITEMASK | WRITES_COV);
    CASE(FORCE_LATE, WEAK_EARLY, ZS_ALWAYS_PASSES | ZS_WRITEMASK | DISCARD);
    CASE(FORCE_LATE, WEAK_EARLY, ZS_ALWAYS_PASSES | ZS_WRITEMASK | ALPHA2COV);
-   CASE(FORCE_LATE, WEAK_EARLY,
-        ZS_ALWAYS_PASSES | ZS_WRITEMASK | WRITES_COV | DISCARD | ALPHA2COV);
+   CASE(FORCE_LATE, WEAK_EARLY, ZS_ALWAYS_PASSES | ZS_WRITEMASK | WRITES_COV | DISCARD | ALPHA2COV);
 }
 
 TEST(EarlyZS, ModifiesCoverageWritesZSSideFX)
@@ -107,8 +107,7 @@
    CASE(FORCE_LATE, FORCE_LATE, ZS_WRITEMASK | SIDEFX | WRITES_COV);
    CASE(FORCE_LATE, FORCE_LATE, ZS_WRITEMASK | SIDEFX | DISCARD);
    CASE(FORCE_LATE, FORCE_LATE, ZS_WRITEMASK | SIDEFX | ALPHA2COV);
-   CASE(FORCE_LATE, FORCE_LATE,
-        ZS_WRITEMASK | SIDEFX | WRITES_COV | DISCARD | ALPHA2COV);
+   CASE(FORCE_LATE, FORCE_LATE, ZS_WRITEMASK | SIDEFX | WRITES_COV | DISCARD | ALPHA2COV);
 }
 
 TEST(EarlyZS, SideFXNoShaderZS)
@@ -137,7 +136,6 @@
 TEST(EarlyZS, NoSideFXNoShaderZSAlt)
 {
    CASE(WEAK_EARLY, WEAK_EARLY, ZS_ALWAYS_PASSES);
-   CASE(WEAK_EARLY, WEAK_EARLY,
-        ZS_ALWAYS_PASSES | ALPHA2COV | DISCARD | WRITES_COV);
+   CASE(WEAK_EARLY, WEAK_EARLY, ZS_ALWAYS_PASSES | ALPHA2COV | DISCARD | WRITES_COV);
    CASE(WEAK_EARLY, WEAK_EARLY, ZS_ALWAYS_PASSES | ZS_WRITEMASK);
 }
diff -urN mesa-23.0.0/src/panfrost/lib/tests/test-layout.cpp mesa/src/panfrost/lib/tests/test-layout.cpp
--- mesa-23.0.0/src/panfrost/lib/tests/test-layout.cpp	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/tests/test-layout.cpp	2023-03-06 17:54:35.856495086 +0100
@@ -27,13 +27,15 @@
 
 TEST(BlockSize, Linear)
 {
-   enum pipe_format format[] = {PIPE_FORMAT_R32G32B32_FLOAT,
-                                PIPE_FORMAT_R8G8B8_UNORM, PIPE_FORMAT_ETC2_RGB8,
-                                PIPE_FORMAT_ASTC_5x5};
+   enum pipe_format format[] = {
+      PIPE_FORMAT_R32G32B32_FLOAT,
+      PIPE_FORMAT_R8G8B8_UNORM,
+      PIPE_FORMAT_ETC2_RGB8,
+      PIPE_FORMAT_ASTC_5x5
+   };
 
    for (unsigned i = 0; i < ARRAY_SIZE(format); ++i) {
-      struct pan_block_size blk =
-         panfrost_block_size(DRM_FORMAT_MOD_LINEAR, format[i]);
+      struct pan_block_size blk = panfrost_block_size(DRM_FORMAT_MOD_LINEAR, format[i]);
 
       EXPECT_EQ(blk.width, 1);
       EXPECT_EQ(blk.height, 1);
@@ -48,8 +50,7 @@
    };
 
    for (unsigned i = 0; i < ARRAY_SIZE(format); ++i) {
-      struct pan_block_size blk = panfrost_block_size(
-         DRM_FORMAT_MOD_ARM_16X16_BLOCK_U_INTERLEAVED, format[i]);
+      struct pan_block_size blk = panfrost_block_size(DRM_FORMAT_MOD_ARM_16X16_BLOCK_U_INTERLEAVED, format[i]);
 
       EXPECT_EQ(blk.width, 16);
       EXPECT_EQ(blk.height, 16);
@@ -58,11 +59,13 @@
 
 TEST(BlockSize, UInterleavedBlockCompressed)
 {
-   enum pipe_format format[] = {PIPE_FORMAT_ETC2_RGB8, PIPE_FORMAT_ASTC_5x5};
+   enum pipe_format format[] = {
+      PIPE_FORMAT_ETC2_RGB8,
+      PIPE_FORMAT_ASTC_5x5
+   };
 
    for (unsigned i = 0; i < ARRAY_SIZE(format); ++i) {
-      struct pan_block_size blk = panfrost_block_size(
-         DRM_FORMAT_MOD_ARM_16X16_BLOCK_U_INTERLEAVED, format[i]);
+      struct pan_block_size blk = panfrost_block_size(DRM_FORMAT_MOD_ARM_16X16_BLOCK_U_INTERLEAVED, format[i]);
 
       EXPECT_EQ(blk.width, 4);
       EXPECT_EQ(blk.height, 4);
@@ -71,13 +74,17 @@
 
 TEST(BlockSize, AFBCFormatInvariant16x16)
 {
-   enum pipe_format format[] = {PIPE_FORMAT_R32G32B32_FLOAT,
-                                PIPE_FORMAT_R8G8B8_UNORM, PIPE_FORMAT_ETC2_RGB8,
-                                PIPE_FORMAT_ASTC_5x5};
+   enum pipe_format format[] = {
+      PIPE_FORMAT_R32G32B32_FLOAT,
+      PIPE_FORMAT_R8G8B8_UNORM,
+      PIPE_FORMAT_ETC2_RGB8,
+      PIPE_FORMAT_ASTC_5x5
+   };
 
-   uint64_t modifier =
-      DRM_FORMAT_MOD_ARM_AFBC(AFBC_FORMAT_MOD_BLOCK_SIZE_16x16 |
-                              AFBC_FORMAT_MOD_SPARSE | AFBC_FORMAT_MOD_YTR);
+   uint64_t modifier = DRM_FORMAT_MOD_ARM_AFBC(
+                AFBC_FORMAT_MOD_BLOCK_SIZE_16x16 |
+                AFBC_FORMAT_MOD_SPARSE |
+                AFBC_FORMAT_MOD_YTR);
 
    for (unsigned i = 0; i < ARRAY_SIZE(format); ++i) {
       struct pan_block_size blk = panfrost_block_size(modifier, format[i]);
@@ -89,13 +96,17 @@
 
 TEST(BlockSize, AFBCFormatInvariant32x8)
 {
-   enum pipe_format format[] = {PIPE_FORMAT_R32G32B32_FLOAT,
-                                PIPE_FORMAT_R8G8B8_UNORM, PIPE_FORMAT_ETC2_RGB8,
-                                PIPE_FORMAT_ASTC_5x5};
+   enum pipe_format format[] = {
+      PIPE_FORMAT_R32G32B32_FLOAT,
+      PIPE_FORMAT_R8G8B8_UNORM,
+      PIPE_FORMAT_ETC2_RGB8,
+      PIPE_FORMAT_ASTC_5x5
+   };
 
-   uint64_t modifier =
-      DRM_FORMAT_MOD_ARM_AFBC(AFBC_FORMAT_MOD_BLOCK_SIZE_32x8 |
-                              AFBC_FORMAT_MOD_SPARSE | AFBC_FORMAT_MOD_YTR);
+   uint64_t modifier = DRM_FORMAT_MOD_ARM_AFBC(
+                AFBC_FORMAT_MOD_BLOCK_SIZE_32x8 |
+                AFBC_FORMAT_MOD_SPARSE |
+                AFBC_FORMAT_MOD_YTR);
 
    for (unsigned i = 0; i < ARRAY_SIZE(format); ++i) {
       struct pan_block_size blk = panfrost_block_size(modifier, format[i]);
@@ -107,9 +118,10 @@
 
 TEST(BlockSize, AFBCSuperblock16x16)
 {
-   uint64_t modifier =
-      DRM_FORMAT_MOD_ARM_AFBC(AFBC_FORMAT_MOD_BLOCK_SIZE_16x16 |
-                              AFBC_FORMAT_MOD_SPARSE | AFBC_FORMAT_MOD_YTR);
+   uint64_t modifier = DRM_FORMAT_MOD_ARM_AFBC(
+                AFBC_FORMAT_MOD_BLOCK_SIZE_16x16 |
+                AFBC_FORMAT_MOD_SPARSE |
+                AFBC_FORMAT_MOD_YTR);
 
    EXPECT_EQ(panfrost_afbc_superblock_size(modifier).width, 16);
    EXPECT_EQ(panfrost_afbc_superblock_width(modifier), 16);
@@ -122,8 +134,9 @@
 
 TEST(BlockSize, AFBCSuperblock32x8)
 {
-   uint64_t modifier = DRM_FORMAT_MOD_ARM_AFBC(AFBC_FORMAT_MOD_BLOCK_SIZE_32x8 |
-                                               AFBC_FORMAT_MOD_SPARSE);
+   uint64_t modifier = DRM_FORMAT_MOD_ARM_AFBC(
+                AFBC_FORMAT_MOD_BLOCK_SIZE_32x8 |
+                AFBC_FORMAT_MOD_SPARSE);
 
    EXPECT_EQ(panfrost_afbc_superblock_size(modifier).width, 32);
    EXPECT_EQ(panfrost_afbc_superblock_width(modifier), 32);
@@ -136,8 +149,9 @@
 
 TEST(BlockSize, AFBCSuperblock64x4)
 {
-   uint64_t modifier = DRM_FORMAT_MOD_ARM_AFBC(AFBC_FORMAT_MOD_BLOCK_SIZE_64x4 |
-                                               AFBC_FORMAT_MOD_SPARSE);
+   uint64_t modifier = DRM_FORMAT_MOD_ARM_AFBC(
+                AFBC_FORMAT_MOD_BLOCK_SIZE_64x4 |
+                AFBC_FORMAT_MOD_SPARSE);
 
    EXPECT_EQ(panfrost_afbc_superblock_size(modifier).width, 64);
    EXPECT_EQ(panfrost_afbc_superblock_width(modifier), 64);
@@ -151,11 +165,9 @@
 /* Calculate Bifrost line stride, since we have reference formulas for Bifrost
  * stride calculations.
  */
-static uint32_t
-pan_afbc_line_stride(uint64_t modifier, uint32_t width)
+static uint32_t pan_afbc_line_stride(uint64_t modifier, uint32_t width)
 {
-   return pan_afbc_stride_blocks(modifier,
-                                 pan_afbc_row_stride(modifier, width));
+   return pan_afbc_stride_blocks(modifier, pan_afbc_row_stride(modifier, width));
 }
 
 /* Which form of the stride we specify is hardware specific (row stride for
@@ -177,16 +189,16 @@
       uint64_t modifier = modifiers[m];
 
       uint32_t sw = panfrost_afbc_superblock_width(modifier);
-      uint32_t cases[] = {1, 4, 17, 39};
+      uint32_t cases[] = { 1, 4, 17, 39 };
 
       for (unsigned i = 0; i < ARRAY_SIZE(cases); ++i) {
          uint32_t width = sw * cases[i];
 
          EXPECT_EQ(pan_afbc_row_stride(modifier, width),
-                   16 * DIV_ROUND_UP(width, sw));
+               16 * DIV_ROUND_UP(width, sw));
 
          EXPECT_EQ(pan_afbc_line_stride(modifier, width),
-                   DIV_ROUND_UP(width, sw));
+               DIV_ROUND_UP(width, sw));
       }
    }
 }
@@ -195,73 +207,63 @@
 {
    uint64_t modifiers[] = {
       DRM_FORMAT_MOD_ARM_AFBC(AFBC_FORMAT_MOD_BLOCK_SIZE_16x16 |
-                              AFBC_FORMAT_MOD_TILED | AFBC_FORMAT_MOD_SPARSE),
+                              AFBC_FORMAT_MOD_TILED |
+                              AFBC_FORMAT_MOD_SPARSE),
       DRM_FORMAT_MOD_ARM_AFBC(AFBC_FORMAT_MOD_BLOCK_SIZE_32x8 |
-                              AFBC_FORMAT_MOD_TILED | AFBC_FORMAT_MOD_SPARSE),
+                              AFBC_FORMAT_MOD_TILED |
+                              AFBC_FORMAT_MOD_SPARSE),
       DRM_FORMAT_MOD_ARM_AFBC(AFBC_FORMAT_MOD_BLOCK_SIZE_64x4 |
-                              AFBC_FORMAT_MOD_TILED | AFBC_FORMAT_MOD_SPARSE),
+                              AFBC_FORMAT_MOD_TILED |
+                              AFBC_FORMAT_MOD_SPARSE),
    };
 
    for (unsigned m = 0; m < ARRAY_SIZE(modifiers); ++m) {
       uint64_t modifier = modifiers[m];
 
       uint32_t sw = panfrost_afbc_superblock_width(modifier);
-      uint32_t cases[] = {1, 4, 17, 39};
+      uint32_t cases[] = { 1, 4, 17, 39 };
 
       for (unsigned i = 0; i < ARRAY_SIZE(cases); ++i) {
          uint32_t width = sw * 8 * cases[i];
 
          EXPECT_EQ(pan_afbc_row_stride(modifier, width),
-                   16 * DIV_ROUND_UP(width, (sw * 8)) * 8 * 8);
+               16 * DIV_ROUND_UP(width, (sw * 8)) * 8 * 8);
 
          EXPECT_EQ(pan_afbc_line_stride(modifier, width),
-                   DIV_ROUND_UP(width, sw * 8) * 8);
+               DIV_ROUND_UP(width, sw * 8) * 8);
       }
    }
 }
 
 TEST(LegacyStride, FromLegacyLinear)
 {
-   EXPECT_EQ(panfrost_from_legacy_stride(1920 * 4, PIPE_FORMAT_R8G8B8A8_UINT,
-                                         DRM_FORMAT_MOD_LINEAR),
-             1920 * 4);
-   EXPECT_EQ(panfrost_from_legacy_stride(53, PIPE_FORMAT_R8_SNORM,
-                                         DRM_FORMAT_MOD_LINEAR),
-             53);
-   EXPECT_EQ(panfrost_from_legacy_stride(60, PIPE_FORMAT_ETC2_RGB8,
-                                         DRM_FORMAT_MOD_LINEAR),
-             60);
+   EXPECT_EQ(panfrost_from_legacy_stride(1920 * 4, PIPE_FORMAT_R8G8B8A8_UINT, DRM_FORMAT_MOD_LINEAR), 1920 * 4);
+   EXPECT_EQ(panfrost_from_legacy_stride(53, PIPE_FORMAT_R8_SNORM, DRM_FORMAT_MOD_LINEAR), 53);
+   EXPECT_EQ(panfrost_from_legacy_stride(60, PIPE_FORMAT_ETC2_RGB8, DRM_FORMAT_MOD_LINEAR), 60);
 }
 
 TEST(LegacyStride, FromLegacyInterleaved)
 {
-   EXPECT_EQ(
-      panfrost_from_legacy_stride(1920 * 4, PIPE_FORMAT_R8G8B8A8_UINT,
-                                  DRM_FORMAT_MOD_ARM_16X16_BLOCK_U_INTERLEAVED),
-      1920 * 4 * 16);
-
-   EXPECT_EQ(
-      panfrost_from_legacy_stride(53, PIPE_FORMAT_R8_SNORM,
-                                  DRM_FORMAT_MOD_ARM_16X16_BLOCK_U_INTERLEAVED),
-      53 * 16);
-
-   EXPECT_EQ(
-      panfrost_from_legacy_stride(60, PIPE_FORMAT_ETC2_RGB8,
-                                  DRM_FORMAT_MOD_ARM_16X16_BLOCK_U_INTERLEAVED),
-      60 * 4);
+   EXPECT_EQ(panfrost_from_legacy_stride(1920 * 4, PIPE_FORMAT_R8G8B8A8_UINT,
+            DRM_FORMAT_MOD_ARM_16X16_BLOCK_U_INTERLEAVED),
+            1920 * 4 * 16);
+
+   EXPECT_EQ(panfrost_from_legacy_stride(53, PIPE_FORMAT_R8_SNORM,
+            DRM_FORMAT_MOD_ARM_16X16_BLOCK_U_INTERLEAVED), 53 * 16);
+
+   EXPECT_EQ(panfrost_from_legacy_stride(60, PIPE_FORMAT_ETC2_RGB8,
+            DRM_FORMAT_MOD_ARM_16X16_BLOCK_U_INTERLEAVED), 60 * 4);
 }
 
 TEST(LegacyStride, FromLegacyAFBC)
 {
-   uint64_t modifier =
-      DRM_FORMAT_MOD_ARM_AFBC(AFBC_FORMAT_MOD_BLOCK_SIZE_32x8 |
-                              AFBC_FORMAT_MOD_SPARSE | AFBC_FORMAT_MOD_YTR);
+   uint64_t modifier = DRM_FORMAT_MOD_ARM_AFBC(
+                AFBC_FORMAT_MOD_BLOCK_SIZE_32x8 |
+                AFBC_FORMAT_MOD_SPARSE |
+                AFBC_FORMAT_MOD_YTR);
 
-   EXPECT_EQ(panfrost_from_legacy_stride(1920 * 4, PIPE_FORMAT_R8G8B8A8_UINT,
-                                         modifier),
-             60 * 16);
-   EXPECT_EQ(panfrost_from_legacy_stride(64, PIPE_FORMAT_R8_SNORM, modifier),
-             2 * 16);
+   EXPECT_EQ(panfrost_from_legacy_stride(1920 * 4, PIPE_FORMAT_R8G8B8A8_UINT, modifier), 60 * 16);
+   EXPECT_EQ(panfrost_from_legacy_stride(64, PIPE_FORMAT_R8_SNORM, modifier), 2 * 16);
 }
 
 /* dEQP-GLES3.functional.texture.format.compressed.etc1_2d_pot */
@@ -275,10 +277,12 @@
       .depth = 1,
       .nr_samples = 1,
       .dim = MALI_TEXTURE_DIMENSION_2D,
-      .nr_slices = 8};
+      .nr_slices = 8
+   };
 
-   unsigned offsets[9] = {0,     8192,  10240, 10752, 10880,
-                          11008, 11136, 11264, 11392};
+   unsigned offsets[9] = {
+      0, 8192, 10240, 10752, 10880, 11008, 11136, 11264, 11392
+   };
 
    ASSERT_TRUE(pan_image_layout_init(&l, NULL));
 
@@ -303,7 +307,8 @@
       .depth = 1,
       .nr_samples = 1,
       .dim = MALI_TEXTURE_DIMENSION_2D,
-      .nr_slices = 1};
+      .nr_slices = 1
+   };
 
    ASSERT_TRUE(pan_image_layout_init(&l, NULL));
 
@@ -321,14 +326,16 @@
 
 TEST(Layout, ImplicitLayoutLinearASTC5x5)
 {
-   struct pan_image_layout l = {.modifier = DRM_FORMAT_MOD_LINEAR,
-                                .format = PIPE_FORMAT_ASTC_5x5,
-                                .width = 50,
-                                .height = 50,
-                                .depth = 1,
-                                .nr_samples = 1,
-                                .dim = MALI_TEXTURE_DIMENSION_2D,
-                                .nr_slices = 1};
+   struct pan_image_layout l = {
+      .modifier = DRM_FORMAT_MOD_LINEAR,
+      .format = PIPE_FORMAT_ASTC_5x5,
+      .width = 50,
+      .height = 50,
+      .depth = 1,
+      .nr_samples = 1,
+      .dim = MALI_TEXTURE_DIMENSION_2D,
+      .nr_slices = 1
+   };
 
    ASSERT_TRUE(pan_image_layout_init(&l, NULL));
 
@@ -346,23 +353,25 @@
 /* dEQP-GLES3.functional.texture.format.unsized.rgba_unsigned_byte_3d_pot */
 TEST(AFBCLayout, Linear3D)
 {
-   uint64_t modifier = DRM_FORMAT_MOD_ARM_AFBC(
-      AFBC_FORMAT_MOD_BLOCK_SIZE_16x16 | AFBC_FORMAT_MOD_SPARSE);
+   uint64_t modifier = DRM_FORMAT_MOD_ARM_AFBC(AFBC_FORMAT_MOD_BLOCK_SIZE_16x16 |
+                        AFBC_FORMAT_MOD_SPARSE);
 
-   struct pan_image_layout l = {.modifier = modifier,
-                                .format = PIPE_FORMAT_R8G8B8A8_UNORM,
-                                .width = 8,
-                                .height = 32,
-                                .depth = 16,
-                                .nr_samples = 1,
-                                .dim = MALI_TEXTURE_DIMENSION_3D,
-                                .nr_slices = 1};
+   struct pan_image_layout l = {
+      .modifier = modifier,
+      .format = PIPE_FORMAT_R8G8B8A8_UNORM,
+      .width = 8,
+      .height = 32,
+      .depth = 16,
+      .nr_samples = 1,
+      .dim = MALI_TEXTURE_DIMENSION_3D,
+      .nr_slices = 1
+   };
 
    ASSERT_TRUE(pan_image_layout_init(&l, NULL));
 
    /* AFBC Surface stride is bytes between consecutive surface headers, which is
-    * the header size since this is a 3D texture. At superblock size 16x16, the
-    * 8x32 layer has 1x2 superblocks, so the header size is 2 * 16 = 32 bytes,
+    * the header size since this is a 3D texture. At superblock size 16x16, the 8x32
+    * layer has 1x2 superblocks, so the header size is 2 * 16 = 32 bytes,
     * rounded up to cache line 64.
     *
     * There is only 1 superblock per row, so the row stride is the bytes per 1
@@ -384,18 +393,20 @@
 
 TEST(AFBCLayout, Tiled16x16)
 {
-   uint64_t modifier =
-      DRM_FORMAT_MOD_ARM_AFBC(AFBC_FORMAT_MOD_BLOCK_SIZE_16x16 |
-                              AFBC_FORMAT_MOD_TILED | AFBC_FORMAT_MOD_SPARSE);
+   uint64_t modifier = DRM_FORMAT_MOD_ARM_AFBC(AFBC_FORMAT_MOD_BLOCK_SIZE_16x16 |
+                        AFBC_FORMAT_MOD_TILED |
+                        AFBC_FORMAT_MOD_SPARSE);
 
-   struct pan_image_layout l = {.modifier = modifier,
-                                .format = PIPE_FORMAT_R8G8B8A8_UNORM,
-                                .width = 917,
-                                .height = 417,
-                                .depth = 1,
-                                .nr_samples = 1,
-                                .dim = MALI_TEXTURE_DIMENSION_2D,
-                                .nr_slices = 1};
+   struct pan_image_layout l = {
+      .modifier = modifier,
+      .format = PIPE_FORMAT_R8G8B8A8_UNORM,
+      .width = 917,
+      .height = 417,
+      .depth = 1,
+      .nr_samples = 1,
+      .dim = MALI_TEXTURE_DIMENSION_2D,
+      .nr_slices = 1
+   };
 
    ASSERT_TRUE(pan_image_layout_init(&l, NULL));
 
@@ -421,17 +432,19 @@
 
 TEST(AFBCLayout, Linear16x16Minimal)
 {
-   uint64_t modifier = DRM_FORMAT_MOD_ARM_AFBC(
-      AFBC_FORMAT_MOD_BLOCK_SIZE_16x16 | AFBC_FORMAT_MOD_SPARSE);
+   uint64_t modifier = DRM_FORMAT_MOD_ARM_AFBC(AFBC_FORMAT_MOD_BLOCK_SIZE_16x16 |
+                        AFBC_FORMAT_MOD_SPARSE);
 
-   struct pan_image_layout l = {.modifier = modifier,
-                                .format = PIPE_FORMAT_R8_UNORM,
-                                .width = 1,
-                                .height = 1,
-                                .depth = 1,
-                                .nr_samples = 1,
-                                .dim = MALI_TEXTURE_DIMENSION_2D,
-                                .nr_slices = 1};
+   struct pan_image_layout l = {
+      .modifier = modifier,
+      .format = PIPE_FORMAT_R8_UNORM,
+      .width = 1,
+      .height = 1,
+      .depth = 1,
+      .nr_samples = 1,
+      .dim = MALI_TEXTURE_DIMENSION_2D,
+      .nr_slices = 1
+   };
 
    ASSERT_TRUE(pan_image_layout_init(&l, NULL));
 
@@ -446,18 +459,20 @@
 
 TEST(AFBCLayout, Tiled16x16Minimal)
 {
-   uint64_t modifier =
-      DRM_FORMAT_MOD_ARM_AFBC(AFBC_FORMAT_MOD_BLOCK_SIZE_16x16 |
-                              AFBC_FORMAT_MOD_TILED | AFBC_FORMAT_MOD_SPARSE);
+   uint64_t modifier = DRM_FORMAT_MOD_ARM_AFBC(AFBC_FORMAT_MOD_BLOCK_SIZE_16x16 |
+                        AFBC_FORMAT_MOD_TILED |
+                        AFBC_FORMAT_MOD_SPARSE);
 
-   struct pan_image_layout l = {.modifier = modifier,
-                                .format = PIPE_FORMAT_R8_UNORM,
-                                .width = 1,
-                                .height = 1,
-                                .depth = 1,
-                                .nr_samples = 1,
-                                .dim = MALI_TEXTURE_DIMENSION_2D,
-                                .nr_slices = 1};
+   struct pan_image_layout l = {
+      .modifier = modifier,
+      .format = PIPE_FORMAT_R8_UNORM,
+      .width = 1,
+      .height = 1,
+      .depth = 1,
+      .nr_samples = 1,
+      .dim = MALI_TEXTURE_DIMENSION_2D,
+      .nr_slices = 1
+   };
 
    ASSERT_TRUE(pan_image_layout_init(&l, NULL));
 
diff -urN mesa-23.0.0/src/panfrost/lib/wrap.h mesa/src/panfrost/lib/wrap.h
--- mesa-23.0.0/src/panfrost/lib/wrap.h	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/lib/wrap.h	2023-03-06 19:19:32.125304989 +0100
@@ -46,6 +46,8 @@
 
 void pandecode_next_frame(void);
 
+void pandecode_dump_file_close(void);
+
 void pandecode_close(void);
 
 void pandecode_inject_mmap(uint64_t gpu_va, void *cpu, unsigned sz,
@@ -55,6 +57,10 @@
 
 void pandecode_jc(uint64_t jc_gpu_va, unsigned gpu_id);
 
+void pandecode_cs(uint64_t cs_gpu_va, unsigned cs_size, unsigned gpu_id);
+
+void pandecode_dump_mappings(void);
+
 void pandecode_abort_on_fault(uint64_t jc_gpu_va, unsigned gpu_id);
 
 #endif /* __MMAP_TRACE_H__ */
diff -urN mesa-23.0.0/src/panfrost/meson.build mesa/src/panfrost/meson.build
--- mesa-23.0.0/src/panfrost/meson.build	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/meson.build	2023-03-06 19:17:26.735474895 +0100
@@ -20,7 +20,7 @@
 # SOFTWARE.
 
 inc_panfrost_hw = include_directories([
-   'include'
+   'include', 'base'
 ])
 
 inc_panfrost = include_directories([
@@ -34,7 +34,9 @@
 subdir('shared')
 subdir('util')
 subdir('midgard')
-subdir('compiler')
+subdir('compiler')
+
+subdir('base')
 
 if with_gallium_panfrost or with_panfrost_vk
    subdir('lib')
@@ -65,11 +67,52 @@
     dep_libdrm,
   ],
   link_with : [
+    libglsl_standalone,
     libpanfrost_bifrost,
   ],
   build_by_default : with_tools.contains('panfrost')
 )
 
+csf_test = executable(
+  'csf_test',
+  ['csf_test/test.c'],
+  include_directories : [
+    inc_mapi,
+    inc_mesa,
+    inc_gallium,
+    inc_gallium_aux,
+    inc_include,
+    inc_src,
+    inc_panfrost,
+    inc_panfrost_hw,
+  ],
+  dependencies : [
+    idep_nir,
+    idep_mesautil,
+    idep_bi_opcodes_h,
+    dep_libdrm,
+    libpanfrost_dep,
+  ],
+  build_by_default : true
+)
+
+custom_target(
+  'panfrost_panloader',
+  output: ['panfrost_panloader.txt'],
+  depends : [
+    libpanfrost_lib,
+    libpanfrost_util,
+    _libmesa_util,
+    libpanfrost_decode,
+    libpanfrost_decode_per_arch,
+    libpanfrost_midgard_disasm,
+    libpanfrost_bifrost_disasm,
+    libpanfrost_valhall_disasm,
+  ],
+  command: ['touch', '@OUTPUT@'],
+  build_by_default : false,
+)
+
 if with_panfrost_vk
   subdir('vulkan')
 endif
diff -urN mesa-23.0.0/src/panfrost/midgard/compiler.h mesa/src/panfrost/midgard/compiler.h
--- mesa-23.0.0/src/panfrost/midgard/compiler.h	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/midgard/compiler.h	2023-03-06 19:19:32.134305048 +0100
@@ -553,17 +553,15 @@
 static inline midgard_instruction
 v_mov(unsigned src, unsigned dest)
 {
-   midgard_instruction ins = {
-      .type = TAG_ALU_4,
-      .mask = 0xF,
-      .src = {~0, src, ~0, ~0},
-      .src_types = {0, nir_type_uint32},
-      .swizzle = SWIZZLE_IDENTITY,
-      .dest = dest,
-      .dest_type = nir_type_uint32,
-      .op = midgard_alu_op_imov,
-      .outmod = midgard_outmod_keeplo,
-   };
+   midgard_instruction ins = {.type = TAG_ALU_4,
+                              .mask = 0xF,
+                              .src = {~0, src, ~0, ~0},
+                              .src_types = {0, nir_type_uint32},
+                              .swizzle = SWIZZLE_IDENTITY,
+                              .dest = dest,
+                              .dest_type = nir_type_uint32,
+                              .op = midgard_alu_op_imov,
+                              .outmod = midgard_outmod_keeplo};
 
    return ins;
 }
@@ -604,8 +602,7 @@
          },
 
       /* If we spill an unspill, RA goes into an infinite loop */
-      .no_spill = (1 << REG_CLASS_WORK),
-   };
+      .no_spill = (1 << REG_CLASS_WORK)};
 
    ins.constants.u32[0] = byte;
 
diff -urN mesa-23.0.0/src/panfrost/midgard/disassemble.c mesa/src/panfrost/midgard/disassemble.c
--- mesa-23.0.0/src/panfrost/midgard/disassemble.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/midgard/disassemble.c	2023-03-06 19:19:32.153305173 +0100
@@ -1254,7 +1254,9 @@
 UNUSED static void
 print_varying_parameters(FILE *fp, midgard_load_store_word *word)
 {
-   midgard_varying_params p = midgard_unpack_varying_params(*word);
+   unsigned params = word->signed_offset & 0x1FF;
+   midgard_varying_params p;
+   memcpy(&p, &params, sizeof(p));
 
    /* If a varying, there are qualifiers */
    if (p.flat_shading)
@@ -1285,6 +1287,38 @@
    }
 }
 
+static bool
+is_op_varying(unsigned op)
+{
+   switch (op) {
+   case midgard_op_st_vary_16:
+   case midgard_op_st_vary_32:
+   case midgard_op_st_vary_32i:
+   case midgard_op_st_vary_32u:
+   case midgard_op_ld_vary_16:
+   case midgard_op_ld_vary_32:
+   case midgard_op_ld_vary_32i:
+   case midgard_op_ld_vary_32u:
+      return true;
+   }
+
+   return false;
+}
+
+static bool
+is_op_attribute(unsigned op)
+{
+   switch (op) {
+   case midgard_op_ld_attr_16:
+   case midgard_op_ld_attr_32:
+   case midgard_op_ld_attr_32i:
+   case midgard_op_ld_attr_32u:
+      return true;
+   }
+
+   return false;
+}
+
 /* Helper to print integer well-formatted, but only when non-zero. */
 static void
 midgard_print_sint(FILE *fp, int n)
@@ -1531,6 +1565,18 @@
    }
 }
 
+static bool
+midgard_op_has_helpers(unsigned op)
+{
+   switch (op) {
+   case midgard_tex_op_normal:
+   case midgard_tex_op_derivative:
+      return true;
+   default:
+      return false;
+   }
+}
+
 static void
 print_texture_op(FILE *fp, unsigned op)
 {
diff -urN mesa-23.0.0/src/panfrost/midgard/midgard_compile.c mesa/src/panfrost/midgard/midgard_compile.c
--- mesa-23.0.0/src/panfrost/midgard/midgard_compile.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/midgard/midgard_compile.c	2023-03-06 19:19:32.274305972 +0100
@@ -104,18 +104,13 @@
 #define M_LOAD_STORE(name, store, T)                                           \
    static midgard_instruction m_##name(unsigned ssa, unsigned address)         \
    {                                                                           \
-      midgard_instruction i = {                                                \
-         .type = TAG_LOAD_STORE_4,                                             \
-         .mask = 0xF,                                                          \
-         .dest = ~0,                                                           \
-         .src = {~0, ~0, ~0, ~0},                                              \
-         .swizzle = SWIZZLE_IDENTITY_4,                                        \
-         .op = midgard_op_##name,                                              \
-         .load_store =                                                         \
-            {                                                                  \
-               .signed_offset = address,                                       \
-            },                                                                 \
-      };                                                                       \
+      midgard_instruction i = {.type = TAG_LOAD_STORE_4,                       \
+                               .mask = 0xF,                                    \
+                               .dest = ~0,                                     \
+                               .src = {~0, ~0, ~0, ~0},                        \
+                               .swizzle = SWIZZLE_IDENTITY_4,                  \
+                               .op = midgard_op_##name,                        \
+                               .load_store = {.signed_offset = address}};      \
                                                                                \
       if (store) {                                                             \
          i.src[0] = ssa;                                                       \
@@ -192,11 +187,7 @@
       .type = TAG_ALU_4,
       .unit = ALU_ENAB_BRANCH,
       .compact_branch = true,
-      .branch =
-         {
-            .conditional = conditional,
-            .invert_conditional = invert,
-         },
+      .branch = {.conditional = conditional, .invert_conditional = invert},
       .dest = ~0,
       .src = {~0, ~0, ~0, ~0},
    };
@@ -1380,20 +1372,8 @@
    midgard_instruction ins = {.type = TAG_LOAD_STORE_4,
                               .mask = 0xF,
                               .dest = dest,
-                              .src =
-                                 {
-                                    ~0,
-                                    ~0,
-                                    ~0,
-                                    val,
-                                 },
-                              .src_types =
-                                 {
-                                    0,
-                                    0,
-                                    0,
-                                    type | bitsize,
-                                 },
+                              .src = {~0, ~0, ~0, val},
+                              .src_types = {0, 0, 0, type | bitsize},
                               .op = op};
 
    nir_src *src_offset = nir_get_io_offset_src(instr);
@@ -2446,12 +2426,10 @@
       .swizzle = SWIZZLE_IDENTITY_4,
       .outmod = outmod,
       .op = midgard_texop,
-      .texture = {
-         .format = midgard_tex_format(instr->sampler_dim),
-         .texture_handle = texture_index,
-         .sampler_handle = sampler_index,
-         .mode = mdg_texture_mode(instr),
-      }};
+      .texture = {.format = midgard_tex_format(instr->sampler_dim),
+                  .texture_handle = texture_index,
+                  .sampler_handle = sampler_index,
+                  .mode = mdg_texture_mode(instr)}};
 
    if (instr->is_shadow && !instr->is_new_style_shadow &&
        instr->op != nir_texop_tg4)
diff -urN mesa-23.0.0/src/panfrost/midgard/midgard_derivatives.c mesa/src/panfrost/midgard/midgard_derivatives.c
--- mesa-23.0.0/src/panfrost/midgard/midgard_derivatives.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/midgard/midgard_derivatives.c	2023-03-06 19:19:32.279306006 +0100
@@ -103,29 +103,17 @@
       .mask = mask_of(nr_components),
       .dest = nir_dest_index(&instr->dest.dest),
       .dest_type = nir_type_float32,
-      .src =
-         {
-            ~0,
-            nir_src_index(ctx, &instr->src[0].src),
-            ~0,
-            ~0,
-         },
+      .src = {~0, nir_src_index(ctx, &instr->src[0].src), ~0, ~0},
       .swizzle = SWIZZLE_IDENTITY_4,
-      .src_types =
-         {
-            nir_type_float32,
-            nir_type_float32,
-         },
+      .src_types = {nir_type_float32, nir_type_float32},
       .op = midgard_tex_op_derivative,
-      .texture =
-         {
-            .mode = mir_derivative_mode(instr->op),
-            .format = 2,
-            .in_reg_full = 1,
-            .out_full = 1,
-            .sampler_type = MALI_SAMPLER_FLOAT,
-         },
-   };
+      .texture = {
+         .mode = mir_derivative_mode(instr->op),
+         .format = 2,
+         .in_reg_full = 1,
+         .out_full = 1,
+         .sampler_type = MALI_SAMPLER_FLOAT,
+      }};
 
    if (!instr->dest.dest.is_ssa)
       ins.mask &= instr->dest.write_mask;
diff -urN mesa-23.0.0/src/panfrost/midgard/midgard_emit.c mesa/src/panfrost/midgard/midgard_emit.c
--- mesa-23.0.0/src/panfrost/midgard/midgard_emit.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/midgard/midgard_emit.c	2023-03-06 19:19:32.173305305 +0100
@@ -109,11 +109,9 @@
 static unsigned
 mir_pack_scalar_source(unsigned mod, bool is_full, unsigned component)
 {
-   midgard_scalar_alu_src s = {
-      .mod = mod,
-      .full = is_full,
-      .component = component << (is_full ? 1 : 0),
-   };
+   midgard_scalar_alu_src s = {.mod = mod,
+                               .full = is_full,
+                               .component = component << (is_full ? 1 : 0)};
 
    unsigned o;
    memcpy(&o, &s, sizeof(s));
@@ -137,14 +135,12 @@
                              ins->swizzle[1][comp])};
 
    /* The output component is from the mask */
-   midgard_scalar_alu s = {
-      .op = v.op,
-      .src1 = packed_src[0],
-      .src2 = packed_src[1],
-      .outmod = v.outmod,
-      .output_full = is_full,
-      .output_component = comp,
-   };
+   midgard_scalar_alu s = {.op = v.op,
+                           .src1 = packed_src[0],
+                           .src2 = packed_src[1],
+                           .outmod = v.outmod,
+                           .output_full = is_full,
+                           .output_component = comp};
 
    /* Full components are physically spaced out */
    if (is_full) {
@@ -340,11 +336,9 @@
       unsigned swizzle = mir_pack_swizzle(ins->mask, ins->swizzle[i], sz,
                                           base_size, channeled, &expand_mode);
 
-      midgard_vector_alu_src pack = {
-         .mod = mir_pack_mod(ins, i, false),
-         .expand_mode = expand_mode,
-         .swizzle = swizzle,
-      };
+      midgard_vector_alu_src pack = {.mod = mir_pack_mod(ins, i, false),
+                                     .expand_mode = expand_mode,
+                                     .swizzle = swizzle};
 
       unsigned p = vector_alu_srco_unsigned(pack);
 
@@ -636,8 +630,7 @@
       midgard_tex_register_select sel = {
          .select = SSA_REG_FROM_FIXED(ins->src[2]) & 1,
          .full = 1,
-         .component = ins->swizzle[2][0],
-      };
+         .component = ins->swizzle[2][0]};
       uint8_t packed;
       memcpy(&packed, &sel, sizeof(packed));
       tex.bias = packed;
@@ -669,8 +662,7 @@
    midgard_vector_alu alu = {
       .op = ins->op,
       .outmod = ins->outmod,
-      .reg_mode = reg_mode_for_bitsize(max_bitsize_for_alu(ins)),
-   };
+      .reg_mode = reg_mode_for_bitsize(max_bitsize_for_alu(ins))};
 
    if (ins->has_inline_constant) {
       /* Encode inline 16-bit constant. See disassembler for
@@ -699,12 +691,10 @@
                               (cond << 8) | (cond << 6) | (cond << 4) |
                               (cond << 2) | (cond << 0);
 
-   midgard_branch_extended branch = {
-      .op = op,
-      .dest_tag = dest_tag,
-      .offset = quadword_offset,
-      .cond = duplicated_cond,
-   };
+   midgard_branch_extended branch = {.op = op,
+                                     .dest_tag = dest_tag,
+                                     .offset = quadword_offset,
+                                     .cond = duplicated_cond};
 
    return branch;
 }
@@ -785,12 +775,10 @@
       unsigned size = sizeof(midgard_branch_cond);
 
       if (is_conditional || is_special) {
-         midgard_branch_cond branch = {
-            .op = op,
-            .dest_tag = dest_tag,
-            .offset = quadword_offset,
-            .cond = cond,
-         };
+         midgard_branch_cond branch = {.op = op,
+                                       .dest_tag = dest_tag,
+                                       .offset = quadword_offset,
+                                       .cond = cond};
          memcpy(util_dynarray_grow_bytes(emission, size, 1), &branch, size);
       } else {
          assert(op == midgard_jmp_writeout_op_branch_uncond);
@@ -798,8 +786,7 @@
             .op = op,
             .dest_tag = dest_tag,
             .offset = quadword_offset,
-            .call_mode = midgard_call_mode_default,
-         };
+            .call_mode = midgard_call_mode_default};
          assert(branch.offset == quadword_offset);
          memcpy(util_dynarray_grow_bytes(emission, size, 1), &branch, size);
       }
@@ -974,12 +961,10 @@
          memcpy(&next64, &ldst1, sizeof(next64));
       }
 
-      midgard_load_store instruction = {
-         .type = bundle->tag,
-         .next_type = next_tag,
-         .word1 = current64,
-         .word2 = next64,
-      };
+      midgard_load_store instruction = {.type = bundle->tag,
+                                        .next_type = next_tag,
+                                        .word1 = current64,
+                                        .word2 = next64};
 
       util_dynarray_append(emission, midgard_load_store, instruction);
 
diff -urN mesa-23.0.0/src/panfrost/midgard/midgard_ops.c mesa/src/panfrost/midgard/midgard_ops.c
--- mesa-23.0.0/src/panfrost/midgard/midgard_ops.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/midgard/midgard_ops.c	2023-03-06 19:19:32.376306646 +0100
@@ -30,91 +30,96 @@
 
 #include "midgard_ops.h"
 
-/*
- * Table of mapping opcodes to accompanying properties. This is used for both
+/* Table of mapping opcodes to accompanying properties. This is used for both
  * the disassembler and the compiler. It is placed in a .c file like this to
- * avoid duplications in the binary.
- */
+ * avoid duplications in the binary */
 
-/* clang-format off */
 struct mir_op_props alu_opcode_props[256] = {
-   [midgard_alu_op_fadd]            = {"FADD", UNITS_ADD | OP_COMMUTES},
-   [midgard_alu_op_fadd_rtz]        = {"FADD.rtz", UNITS_ADD | OP_COMMUTES},
-   [midgard_alu_op_fadd_rtn]        = {"FADD.rtn", UNITS_ADD | OP_COMMUTES},
-   [midgard_alu_op_fadd_rtp]        = {"FADD.rtp", UNITS_ADD | OP_COMMUTES},
-   [midgard_alu_op_fmul]            = {"FMUL", UNITS_MUL | UNIT_VLUT | OP_COMMUTES},
-   [midgard_alu_op_fmul_rtz]        = {"FMUL.rtz", UNITS_MUL | UNIT_VLUT | OP_COMMUTES},
-   [midgard_alu_op_fmul_rtn]        = {"FMUL.rtn", UNITS_MUL | UNIT_VLUT | OP_COMMUTES},
-   [midgard_alu_op_fmul_rtp]        = {"FMUL.rtp", UNITS_MUL | UNIT_VLUT | OP_COMMUTES},
-   [midgard_alu_op_fmin]            = {"FMIN", UNITS_MOST | OP_COMMUTES},
-   [midgard_alu_op_fmin_nan]        = {"FMIN.nan", UNITS_MOST | OP_COMMUTES},
-   [midgard_alu_op_fabsmin]         = {"FABSMIN", UNITS_MOST | OP_COMMUTES},
-   [midgard_alu_op_fabsmin_nan]     = {"FABSMIN.nan", UNITS_MOST | OP_COMMUTES},
-   [midgard_alu_op_fmax]            = {"FMAX", UNITS_MOST | OP_COMMUTES},
-   [midgard_alu_op_fmax_nan]        = {"FMAX.nan", UNITS_MOST | OP_COMMUTES},
-   [midgard_alu_op_fabsmax]         = {"FABSMAX", UNITS_MOST | OP_COMMUTES},
-   [midgard_alu_op_fabsmax_nan]     = {"FABSMAX.nan", UNITS_MOST | OP_COMMUTES},
-   [midgard_alu_op_imin]            = {"MIN", UNITS_MOST | OP_COMMUTES},
-   [midgard_alu_op_imax]            = {"MAX", UNITS_MOST | OP_COMMUTES},
-   [midgard_alu_op_umin]            = {"MIN", UNITS_MOST | OP_COMMUTES},
-   [midgard_alu_op_umax]            = {"MAX", UNITS_MOST | OP_COMMUTES},
-   [midgard_alu_op_iavg]            = {"AVG.rtz", UNITS_ADD | OP_COMMUTES},
-   [midgard_alu_op_uavg]            = {"AVG.rtz", UNITS_ADD | OP_COMMUTES},
-   [midgard_alu_op_iravg]           = {"AVG.round", UNITS_ADD | OP_COMMUTES},
-   [midgard_alu_op_uravg]           = {"AVG.round", UNITS_ADD | OP_COMMUTES},
-
-   [midgard_alu_op_fmov]            = {"FMOV", UNITS_ALL | QUIRK_FLIPPED_R24},
-   [midgard_alu_op_fmov_rtz]        = {"FMOV.rtz", UNITS_ALL | QUIRK_FLIPPED_R24},
-   [midgard_alu_op_fmov_rtn]        = {"FMOV.rtn", UNITS_ALL | QUIRK_FLIPPED_R24},
-   [midgard_alu_op_fmov_rtp]        = {"FMOV.rtp", UNITS_ALL | QUIRK_FLIPPED_R24},
-   [midgard_alu_op_froundaway]      = {"FROUNDAWAY", UNITS_ADD},
-   [midgard_alu_op_froundeven]      = {"FROUNDEVEN", UNITS_ADD},
-   [midgard_alu_op_ftrunc]          = {"FTRUNC", UNITS_ADD},
-   [midgard_alu_op_ffloor]          = {"FFLOOR", UNITS_ADD},
-   [midgard_alu_op_fceil]           = {"FCEIL", UNITS_ADD},
+   [midgard_alu_op_fadd] = {"FADD", UNITS_ADD | OP_COMMUTES},
+   [midgard_alu_op_fadd_rtz] = {"FADD.rtz", UNITS_ADD | OP_COMMUTES},
+   [midgard_alu_op_fadd_rtn] = {"FADD.rtn", UNITS_ADD | OP_COMMUTES},
+   [midgard_alu_op_fadd_rtp] = {"FADD.rtp", UNITS_ADD | OP_COMMUTES},
+   [midgard_alu_op_fmul] = {"FMUL", UNITS_MUL | UNIT_VLUT | OP_COMMUTES},
+   [midgard_alu_op_fmul_rtz] = {"FMUL.rtz",
+                                UNITS_MUL | UNIT_VLUT | OP_COMMUTES},
+   [midgard_alu_op_fmul_rtn] = {"FMUL.rtn",
+                                UNITS_MUL | UNIT_VLUT | OP_COMMUTES},
+   [midgard_alu_op_fmul_rtp] = {"FMUL.rtp",
+                                UNITS_MUL | UNIT_VLUT | OP_COMMUTES},
+   [midgard_alu_op_fmin] = {"FMIN", UNITS_MOST | OP_COMMUTES},
+   [midgard_alu_op_fmin_nan] = {"FMIN.nan", UNITS_MOST | OP_COMMUTES},
+   [midgard_alu_op_fabsmin] = {"FABSMIN", UNITS_MOST | OP_COMMUTES},
+   [midgard_alu_op_fabsmin_nan] = {"FABSMIN.nan", UNITS_MOST | OP_COMMUTES},
+   [midgard_alu_op_fmax] = {"FMAX", UNITS_MOST | OP_COMMUTES},
+   [midgard_alu_op_fmax_nan] = {"FMAX.nan", UNITS_MOST | OP_COMMUTES},
+   [midgard_alu_op_fabsmax] = {"FABSMAX", UNITS_MOST | OP_COMMUTES},
+   [midgard_alu_op_fabsmax_nan] = {"FABSMAX.nan", UNITS_MOST | OP_COMMUTES},
+   [midgard_alu_op_imin] = {"MIN", UNITS_MOST | OP_COMMUTES},
+   [midgard_alu_op_imax] = {"MAX", UNITS_MOST | OP_COMMUTES},
+   [midgard_alu_op_umin] = {"MIN", UNITS_MOST | OP_COMMUTES},
+   [midgard_alu_op_umax] = {"MAX", UNITS_MOST | OP_COMMUTES},
+   [midgard_alu_op_iavg] = {"AVG.rtz", UNITS_ADD | OP_COMMUTES},
+   [midgard_alu_op_uavg] = {"AVG.rtz", UNITS_ADD | OP_COMMUTES},
+   [midgard_alu_op_iravg] = {"AVG.round", UNITS_ADD | OP_COMMUTES},
+   [midgard_alu_op_uravg] = {"AVG.round", UNITS_ADD | OP_COMMUTES},
+
+   [midgard_alu_op_fmov] = {"FMOV", UNITS_ALL | QUIRK_FLIPPED_R24},
+   [midgard_alu_op_fmov_rtz] = {"FMOV.rtz", UNITS_ALL | QUIRK_FLIPPED_R24},
+   [midgard_alu_op_fmov_rtn] = {"FMOV.rtn", UNITS_ALL | QUIRK_FLIPPED_R24},
+   [midgard_alu_op_fmov_rtp] = {"FMOV.rtp", UNITS_ALL | QUIRK_FLIPPED_R24},
+   [midgard_alu_op_froundaway] = {"FROUNDAWAY", UNITS_ADD},
+   [midgard_alu_op_froundeven] = {"FROUNDEVEN", UNITS_ADD},
+   [midgard_alu_op_ftrunc] = {"FTRUNC", UNITS_ADD},
+   [midgard_alu_op_ffloor] = {"FFLOOR", UNITS_ADD},
+   [midgard_alu_op_fceil] = {"FCEIL", UNITS_ADD},
 
    /* Multiplies the X/Y components of the first arg and adds the second
     * arg. Like other LUTs, it must be scalarized. */
-   [midgard_alu_op_ffma]            = {"FMA", UNIT_VLUT},
-   [midgard_alu_op_ffma_rtz]        = {"FMA.rtz", UNIT_VLUT},
-   [midgard_alu_op_ffma_rtn]        = {"FMA.rtn", UNIT_VLUT},
-   [midgard_alu_op_ffma_rtp]        = {"FMA.rtp", UNIT_VLUT},
+   [midgard_alu_op_ffma] = {"FMA", UNIT_VLUT},
+   [midgard_alu_op_ffma_rtz] = {"FMA.rtz", UNIT_VLUT},
+   [midgard_alu_op_ffma_rtn] = {"FMA.rtn", UNIT_VLUT},
+   [midgard_alu_op_ffma_rtp] = {"FMA.rtp", UNIT_VLUT},
 
    /* Though they output a scalar, they need to run on a vector unit
     * since they process vectors */
-   [midgard_alu_op_fdot3]           = {"FDOT3", UNIT_VMUL | OP_CHANNEL_COUNT(3) | OP_COMMUTES},
-   [midgard_alu_op_fdot3r]          = {"FDOT3R", UNIT_VMUL | OP_CHANNEL_COUNT(3) | OP_COMMUTES},
-   [midgard_alu_op_fdot4]           = {"FDOT4", UNIT_VMUL | OP_CHANNEL_COUNT(4) | OP_COMMUTES},
+   [midgard_alu_op_fdot3] = {"FDOT3",
+                             UNIT_VMUL | OP_CHANNEL_COUNT(3) | OP_COMMUTES},
+   [midgard_alu_op_fdot3r] = {"FDOT3R",
+                              UNIT_VMUL | OP_CHANNEL_COUNT(3) | OP_COMMUTES},
+   [midgard_alu_op_fdot4] = {"FDOT4",
+                             UNIT_VMUL | OP_CHANNEL_COUNT(4) | OP_COMMUTES},
 
    /* Incredibly, iadd can run on vmul, etc */
-   [midgard_alu_op_iadd]            = {"ADD", UNITS_MOST | OP_COMMUTES},
-   [midgard_alu_op_ishladd]         = {"ADD", UNITS_MUL},
-   [midgard_alu_op_iaddsat]         = {"ADDSAT", UNITS_ADD | OP_COMMUTES},
-   [midgard_alu_op_uaddsat]         = {"ADDSAT", UNITS_ADD | OP_COMMUTES},
-   [midgard_alu_op_uabsdiff]        = {"ABSDIFF", UNITS_ADD},
-   [midgard_alu_op_iabsdiff]        = {"ABSDIFF", UNITS_ADD},
-   [midgard_alu_op_ichoose]         = {"CHOOSE", UNITS_ADD},
-   [midgard_alu_op_isub]            = {"SUB", UNITS_MOST},
-   [midgard_alu_op_ishlsub]         = {"SUB", UNITS_MUL},
-   [midgard_alu_op_isubsat]         = {"SUBSAT", UNITS_ADD},
-   [midgard_alu_op_usubsat]         = {"SUBSAT", UNITS_ADD},
-   [midgard_alu_op_imul]            = {"MUL", UNITS_MUL | OP_COMMUTES},
-   [midgard_alu_op_iwmul]           = {"WMUL.s", UNIT_VMUL | OP_COMMUTES},
-   [midgard_alu_op_uwmul]           = {"WMUL.u", UNIT_VMUL | OP_COMMUTES},
-   [midgard_alu_op_iuwmul]          = {"WMUL.su", UNIT_VMUL | OP_COMMUTES},
-   [midgard_alu_op_imov]            = {"MOV", UNITS_ALL | QUIRK_FLIPPED_R24},
+   [midgard_alu_op_iadd] = {"ADD", UNITS_MOST | OP_COMMUTES},
+   [midgard_alu_op_ishladd] = {"ADD", UNITS_MUL},
+   [midgard_alu_op_iaddsat] = {"ADDSAT", UNITS_ADD | OP_COMMUTES},
+   [midgard_alu_op_uaddsat] = {"ADDSAT", UNITS_ADD | OP_COMMUTES},
+   [midgard_alu_op_uabsdiff] = {"ABSDIFF", UNITS_ADD},
+   [midgard_alu_op_iabsdiff] = {"ABSDIFF", UNITS_ADD},
+   [midgard_alu_op_ichoose] = {"CHOOSE", UNITS_ADD},
+   [midgard_alu_op_isub] = {"SUB", UNITS_MOST},
+   [midgard_alu_op_ishlsub] = {"SUB", UNITS_MUL},
+   [midgard_alu_op_isubsat] = {"SUBSAT", UNITS_ADD},
+   [midgard_alu_op_usubsat] = {"SUBSAT", UNITS_ADD},
+   [midgard_alu_op_imul] = {"MUL", UNITS_MUL | OP_COMMUTES},
+   [midgard_alu_op_iwmul] = {"WMUL.s", UNIT_VMUL | OP_COMMUTES},
+   [midgard_alu_op_uwmul] = {"WMUL.u", UNIT_VMUL | OP_COMMUTES},
+   [midgard_alu_op_iuwmul] = {"WMUL.su", UNIT_VMUL | OP_COMMUTES},
+   [midgard_alu_op_imov] = {"MOV", UNITS_ALL | QUIRK_FLIPPED_R24},
 
    /* For vector comparisons, use ball etc */
-   [midgard_alu_op_feq]             = {"FCMP.eq", UNITS_MOST | OP_TYPE_CONVERT | OP_COMMUTES},
-   [midgard_alu_op_fne]             = {"FCMP.ne", UNITS_MOST | OP_TYPE_CONVERT | OP_COMMUTES},
-   [midgard_alu_op_fle]             = {"FCMP.le", UNITS_MOST | OP_TYPE_CONVERT},
-   [midgard_alu_op_flt]             = {"FCMP.lt", UNITS_MOST | OP_TYPE_CONVERT},
-   [midgard_alu_op_ieq]             = {"CMP.eq", UNITS_MOST | OP_COMMUTES},
-   [midgard_alu_op_ine]             = {"CMP.ne", UNITS_MOST | OP_COMMUTES},
-   [midgard_alu_op_ilt]             = {"CMP.lt", UNITS_MOST},
-   [midgard_alu_op_ile]             = {"CMP.le", UNITS_MOST},
-   [midgard_alu_op_ult]             = {"CMP.lt", UNITS_MOST},
-   [midgard_alu_op_ule]             = {"CMP.le", UNITS_MOST},
+   [midgard_alu_op_feq] = {"FCMP.eq",
+                           UNITS_MOST | OP_TYPE_CONVERT | OP_COMMUTES},
+   [midgard_alu_op_fne] = {"FCMP.ne",
+                           UNITS_MOST | OP_TYPE_CONVERT | OP_COMMUTES},
+   [midgard_alu_op_fle] = {"FCMP.le", UNITS_MOST | OP_TYPE_CONVERT},
+   [midgard_alu_op_flt] = {"FCMP.lt", UNITS_MOST | OP_TYPE_CONVERT},
+   [midgard_alu_op_ieq] = {"CMP.eq", UNITS_MOST | OP_COMMUTES},
+   [midgard_alu_op_ine] = {"CMP.ne", UNITS_MOST | OP_COMMUTES},
+   [midgard_alu_op_ilt] = {"CMP.lt", UNITS_MOST},
+   [midgard_alu_op_ile] = {"CMP.le", UNITS_MOST},
+   [midgard_alu_op_ult] = {"CMP.lt", UNITS_MOST},
+   [midgard_alu_op_ule] = {"CMP.le", UNITS_MOST},
 
    /* CSEL (MUX) runs in the second pipeline stage, sourcing its selector
     * the previous scalar or vector stage as indicated in the opcode. It
@@ -125,86 +130,128 @@
     * isn't usually useful. Our scheduler does not handle that case, so
     * don't try to and fall over.
     */
-   [midgard_alu_op_icsel]           = {"CSEL.scalar", UNIT_VADD | UNIT_SMUL},
-   [midgard_alu_op_icsel_v]         = {"CSEL.vector", UNIT_VADD},
-   [midgard_alu_op_fcsel_v]         = {"FCSEL.vector", UNIT_VADD},
-   [midgard_alu_op_fcsel]           = {"FCSEL.scalar", UNIT_VADD | UNIT_SMUL},
-
-   [midgard_alu_op_frcp]            = {"FRCP", UNIT_VLUT},
-   [midgard_alu_op_frsqrt]          = {"FRSQRT", UNIT_VLUT},
-   [midgard_alu_op_fsqrt]           = {"FSQRT", UNIT_VLUT},
-   [midgard_alu_op_fpow_pt1]        = {"FPOW_PT1", UNIT_VLUT},
-   [midgard_alu_op_fpown_pt1]       = {"FPOWN_PT1", UNIT_VLUT},
-   [midgard_alu_op_fpowr_pt1]       = {"FPOWR_PT1", UNIT_VLUT},
-   [midgard_alu_op_fexp2]           = {"FEXP2", UNIT_VLUT},
-   [midgard_alu_op_flog2]           = {"FLOG2", UNIT_VLUT},
-
-   [midgard_alu_op_f2i_rte]         = {"F2I", UNITS_ADD | OP_TYPE_CONVERT | MIDGARD_ROUNDS},
-   [midgard_alu_op_f2i_rtz]         = {"F2I.rtz", UNITS_ADD | OP_TYPE_CONVERT},
-   [midgard_alu_op_f2i_rtn]         = {"F2I.rtn", UNITS_ADD | OP_TYPE_CONVERT},
-   [midgard_alu_op_f2i_rtp]         = {"F2I.rtp", UNITS_ADD | OP_TYPE_CONVERT},
-   [midgard_alu_op_f2u_rte]         = {"F2U", UNITS_ADD | OP_TYPE_CONVERT | MIDGARD_ROUNDS},
-   [midgard_alu_op_f2u_rtz]         = {"F2U.rtz", UNITS_ADD | OP_TYPE_CONVERT},
-   [midgard_alu_op_f2u_rtn]         = {"F2U.rtn", UNITS_ADD | OP_TYPE_CONVERT},
-   [midgard_alu_op_f2u_rtp]         = {"F2U.rtp", UNITS_ADD | OP_TYPE_CONVERT},
-   [midgard_alu_op_i2f_rte]         = {"I2F", UNITS_ADD | OP_TYPE_CONVERT},
-   [midgard_alu_op_i2f_rtz]         = {"I2F.rtz", UNITS_ADD | OP_TYPE_CONVERT},
-   [midgard_alu_op_i2f_rtn]         = {"I2F.rtn", UNITS_ADD | OP_TYPE_CONVERT},
-   [midgard_alu_op_i2f_rtp]         = {"I2F.rtp", UNITS_ADD | OP_TYPE_CONVERT},
-   [midgard_alu_op_u2f_rte]         = {"U2F", UNITS_ADD | OP_TYPE_CONVERT},
-   [midgard_alu_op_u2f_rtz]         = {"U2F.rtz", UNITS_ADD | OP_TYPE_CONVERT},
-   [midgard_alu_op_u2f_rtn]         = {"U2F.rtn", UNITS_ADD | OP_TYPE_CONVERT},
-   [midgard_alu_op_u2f_rtp]         = {"U2F.rtp", UNITS_ADD | OP_TYPE_CONVERT},
-
-   [midgard_alu_op_fsinpi]          = {"FSINPI", UNIT_VLUT},
-   [midgard_alu_op_fcospi]          = {"FCOSPI", UNIT_VLUT},
-
-   [midgard_alu_op_iand]            = {"AND", UNITS_MOST | OP_COMMUTES},
-   [midgard_alu_op_iandnot]         = {"ANDNOT", UNITS_MOST},
-
-   [midgard_alu_op_ior]             = {"OR", UNITS_MOST | OP_COMMUTES},
-   [midgard_alu_op_iornot]          = {"ORNOT", UNITS_MOST | OP_COMMUTES},
-   [midgard_alu_op_inor]            = {"NOR", UNITS_MOST | OP_COMMUTES},
-   [midgard_alu_op_ixor]            = {"XOR", UNITS_MOST | OP_COMMUTES},
-   [midgard_alu_op_inxor]           = {"NXOR", UNITS_MOST | OP_COMMUTES},
-   [midgard_alu_op_iclz]            = {"CLZ", UNITS_ADD},
-   [midgard_alu_op_ipopcnt]         = {"POPCNT", UNIT_VADD},
-   [midgard_alu_op_inand]           = {"NAND", UNITS_MOST},
-   [midgard_alu_op_ishl]            = {"SHL", UNITS_ADD},
-   [midgard_alu_op_ishlsat]         = {"SHL.sat", UNITS_ADD},
-   [midgard_alu_op_ushlsat]         = {"SHL.sat", UNITS_ADD},
-   [midgard_alu_op_iasr]            = {"ASR", UNITS_ADD},
-   [midgard_alu_op_ilsr]            = {"LSR", UNITS_ADD},
-
-   [midgard_alu_op_fball_eq]        = {"FCMP.all.eq",  UNITS_VECTOR | OP_CHANNEL_COUNT(4) | OP_COMMUTES | OP_TYPE_CONVERT},
-   [midgard_alu_op_fball_neq]       = {"FCMP.all.ne", UNITS_VECTOR | OP_CHANNEL_COUNT(4) | OP_COMMUTES | OP_TYPE_CONVERT},
-   [midgard_alu_op_fball_lt]        = {"FCMP.all.lt",  UNITS_VECTOR | OP_CHANNEL_COUNT(4) | OP_COMMUTES | OP_TYPE_CONVERT},
-   [midgard_alu_op_fball_lte]       = {"FCMP.all.le", UNITS_VECTOR | OP_CHANNEL_COUNT(4) | OP_COMMUTES | OP_TYPE_CONVERT},
-
-   [midgard_alu_op_fbany_eq]        = {"FCMP.any.eq",  UNITS_VECTOR | OP_CHANNEL_COUNT(4) | OP_COMMUTES | OP_TYPE_CONVERT},
-   [midgard_alu_op_fbany_neq]       = {"FCMP.any.ne", UNITS_VECTOR | OP_CHANNEL_COUNT(4) | OP_COMMUTES | OP_TYPE_CONVERT},
-   [midgard_alu_op_fbany_lt]        = {"FCMP.any.lt",  UNITS_VECTOR | OP_CHANNEL_COUNT(4) | OP_COMMUTES | OP_TYPE_CONVERT},
-   [midgard_alu_op_fbany_lte]       = {"FCMP.any.le", UNITS_VECTOR | OP_CHANNEL_COUNT(4) | OP_COMMUTES | OP_TYPE_CONVERT},
-
-   [midgard_alu_op_iball_eq]        = {"CMP.all.eq",  UNITS_VECTOR | OP_CHANNEL_COUNT(4) | OP_COMMUTES},
-   [midgard_alu_op_iball_neq]       = {"CMP.all.ne", UNITS_VECTOR | OP_CHANNEL_COUNT(4) | OP_COMMUTES},
-   [midgard_alu_op_iball_lt]        = {"CMP.all.lt",  UNITS_VECTOR | OP_CHANNEL_COUNT(4) | OP_COMMUTES},
-   [midgard_alu_op_iball_lte]       = {"CMP.all.le", UNITS_VECTOR | OP_CHANNEL_COUNT(4) | OP_COMMUTES},
-   [midgard_alu_op_uball_lt]        = {"CMP.all.lt",  UNITS_VECTOR | OP_CHANNEL_COUNT(4) | OP_COMMUTES},
-   [midgard_alu_op_uball_lte]       = {"CMP.all.le", UNITS_VECTOR | OP_CHANNEL_COUNT(4) | OP_COMMUTES},
-
-   [midgard_alu_op_ibany_eq]        = {"CMP.any.eq",  UNITS_VECTOR | OP_CHANNEL_COUNT(4) | OP_COMMUTES},
-   [midgard_alu_op_ibany_neq]       = {"CMP.any.ne", UNITS_VECTOR | OP_CHANNEL_COUNT(4) | OP_COMMUTES},
-   [midgard_alu_op_ibany_lt]        = {"CMP.any.lt",  UNITS_VECTOR | OP_CHANNEL_COUNT(4) | OP_COMMUTES},
-   [midgard_alu_op_ibany_lte]       = {"CMP.any.le", UNITS_VECTOR | OP_CHANNEL_COUNT(4) | OP_COMMUTES},
-   [midgard_alu_op_ubany_lt]        = {"CMP.any.lt",  UNITS_VECTOR | OP_CHANNEL_COUNT(4) | OP_COMMUTES},
-   [midgard_alu_op_ubany_lte]       = {"CMP.any.le", UNITS_VECTOR | OP_CHANNEL_COUNT(4) | OP_COMMUTES},
+   [midgard_alu_op_icsel] = {"CSEL.scalar", UNIT_VADD | UNIT_SMUL},
+   [midgard_alu_op_icsel_v] = {"CSEL.vector", UNIT_VADD},
+   [midgard_alu_op_fcsel_v] = {"FCSEL.vector", UNIT_VADD},
+   [midgard_alu_op_fcsel] = {"FCSEL.scalar", UNIT_VADD | UNIT_SMUL},
+
+   [midgard_alu_op_frcp] = {"FRCP", UNIT_VLUT},
+   [midgard_alu_op_frsqrt] = {"FRSQRT", UNIT_VLUT},
+   [midgard_alu_op_fsqrt] = {"FSQRT", UNIT_VLUT},
+   [midgard_alu_op_fpow_pt1] = {"FPOW_PT1", UNIT_VLUT},
+   [midgard_alu_op_fpown_pt1] = {"FPOWN_PT1", UNIT_VLUT},
+   [midgard_alu_op_fpowr_pt1] = {"FPOWR_PT1", UNIT_VLUT},
+   [midgard_alu_op_fexp2] = {"FEXP2", UNIT_VLUT},
+   [midgard_alu_op_flog2] = {"FLOG2", UNIT_VLUT},
+
+   [midgard_alu_op_f2i_rte] = {"F2I",
+                               UNITS_ADD | OP_TYPE_CONVERT | MIDGARD_ROUNDS},
+   [midgard_alu_op_f2i_rtz] = {"F2I.rtz", UNITS_ADD | OP_TYPE_CONVERT},
+   [midgard_alu_op_f2i_rtn] = {"F2I.rtn", UNITS_ADD | OP_TYPE_CONVERT},
+   [midgard_alu_op_f2i_rtp] = {"F2I.rtp", UNITS_ADD | OP_TYPE_CONVERT},
+   [midgard_alu_op_f2u_rte] = {"F2U",
+                               UNITS_ADD | OP_TYPE_CONVERT | MIDGARD_ROUNDS},
+   [midgard_alu_op_f2u_rtz] = {"F2U.rtz", UNITS_ADD | OP_TYPE_CONVERT},
+   [midgard_alu_op_f2u_rtn] = {"F2U.rtn", UNITS_ADD | OP_TYPE_CONVERT},
+   [midgard_alu_op_f2u_rtp] = {"F2U.rtp", UNITS_ADD | OP_TYPE_CONVERT},
+   [midgard_alu_op_i2f_rte] = {"I2F", UNITS_ADD | OP_TYPE_CONVERT},
+   [midgard_alu_op_i2f_rtz] = {"I2F.rtz", UNITS_ADD | OP_TYPE_CONVERT},
+   [midgard_alu_op_i2f_rtn] = {"I2F.rtn", UNITS_ADD | OP_TYPE_CONVERT},
+   [midgard_alu_op_i2f_rtp] = {"I2F.rtp", UNITS_ADD | OP_TYPE_CONVERT},
+   [midgard_alu_op_u2f_rte] = {"U2F", UNITS_ADD | OP_TYPE_CONVERT},
+   [midgard_alu_op_u2f_rtz] = {"U2F.rtz", UNITS_ADD | OP_TYPE_CONVERT},
+   [midgard_alu_op_u2f_rtn] = {"U2F.rtn", UNITS_ADD | OP_TYPE_CONVERT},
+   [midgard_alu_op_u2f_rtp] = {"U2F.rtp", UNITS_ADD | OP_TYPE_CONVERT},
+
+   [midgard_alu_op_fsinpi] = {"FSINPI", UNIT_VLUT},
+   [midgard_alu_op_fcospi] = {"FCOSPI", UNIT_VLUT},
+
+   [midgard_alu_op_iand] = {"AND", UNITS_MOST | OP_COMMUTES},
+   [midgard_alu_op_iandnot] = {"ANDNOT", UNITS_MOST},
+
+   [midgard_alu_op_ior] = {"OR", UNITS_MOST | OP_COMMUTES},
+   [midgard_alu_op_iornot] = {"ORNOT", UNITS_MOST | OP_COMMUTES},
+   [midgard_alu_op_inor] = {"NOR", UNITS_MOST | OP_COMMUTES},
+   [midgard_alu_op_ixor] = {"XOR", UNITS_MOST | OP_COMMUTES},
+   [midgard_alu_op_inxor] = {"NXOR", UNITS_MOST | OP_COMMUTES},
+   [midgard_alu_op_iclz] = {"CLZ", UNITS_ADD},
+   [midgard_alu_op_ipopcnt] = {"POPCNT", UNIT_VADD},
+   [midgard_alu_op_inand] = {"NAND", UNITS_MOST},
+   [midgard_alu_op_ishl] = {"SHL", UNITS_ADD},
+   [midgard_alu_op_ishlsat] = {"SHL.sat", UNITS_ADD},
+   [midgard_alu_op_ushlsat] = {"SHL.sat", UNITS_ADD},
+   [midgard_alu_op_iasr] = {"ASR", UNITS_ADD},
+   [midgard_alu_op_ilsr] = {"LSR", UNITS_ADD},
+
+   [midgard_alu_op_fball_eq] = {"FCMP.all.eq",
+                                UNITS_VECTOR | OP_CHANNEL_COUNT(4) |
+                                   OP_COMMUTES | OP_TYPE_CONVERT},
+   [midgard_alu_op_fball_neq] = {"FCMP.all.ne",
+                                 UNITS_VECTOR | OP_CHANNEL_COUNT(4) |
+                                    OP_COMMUTES | OP_TYPE_CONVERT},
+   [midgard_alu_op_fball_lt] = {"FCMP.all.lt",
+                                UNITS_VECTOR | OP_CHANNEL_COUNT(4) |
+                                   OP_COMMUTES | OP_TYPE_CONVERT},
+   [midgard_alu_op_fball_lte] = {"FCMP.all.le",
+                                 UNITS_VECTOR | OP_CHANNEL_COUNT(4) |
+                                    OP_COMMUTES | OP_TYPE_CONVERT},
+
+   [midgard_alu_op_fbany_eq] = {"FCMP.any.eq",
+                                UNITS_VECTOR | OP_CHANNEL_COUNT(4) |
+                                   OP_COMMUTES | OP_TYPE_CONVERT},
+   [midgard_alu_op_fbany_neq] = {"FCMP.any.ne",
+                                 UNITS_VECTOR | OP_CHANNEL_COUNT(4) |
+                                    OP_COMMUTES | OP_TYPE_CONVERT},
+   [midgard_alu_op_fbany_lt] = {"FCMP.any.lt",
+                                UNITS_VECTOR | OP_CHANNEL_COUNT(4) |
+                                   OP_COMMUTES | OP_TYPE_CONVERT},
+   [midgard_alu_op_fbany_lte] = {"FCMP.any.le",
+                                 UNITS_VECTOR | OP_CHANNEL_COUNT(4) |
+                                    OP_COMMUTES | OP_TYPE_CONVERT},
+
+   [midgard_alu_op_iball_eq] = {"CMP.all.eq", UNITS_VECTOR |
+                                                 OP_CHANNEL_COUNT(4) |
+                                                 OP_COMMUTES},
+   [midgard_alu_op_iball_neq] = {"CMP.all.ne", UNITS_VECTOR |
+                                                  OP_CHANNEL_COUNT(4) |
+                                                  OP_COMMUTES},
+   [midgard_alu_op_iball_lt] = {"CMP.all.lt", UNITS_VECTOR |
+                                                 OP_CHANNEL_COUNT(4) |
+                                                 OP_COMMUTES},
+   [midgard_alu_op_iball_lte] = {"CMP.all.le", UNITS_VECTOR |
+                                                  OP_CHANNEL_COUNT(4) |
+                                                  OP_COMMUTES},
+   [midgard_alu_op_uball_lt] = {"CMP.all.lt", UNITS_VECTOR |
+                                                 OP_CHANNEL_COUNT(4) |
+                                                 OP_COMMUTES},
+   [midgard_alu_op_uball_lte] = {"CMP.all.le", UNITS_VECTOR |
+                                                  OP_CHANNEL_COUNT(4) |
+                                                  OP_COMMUTES},
+
+   [midgard_alu_op_ibany_eq] = {"CMP.any.eq", UNITS_VECTOR |
+                                                 OP_CHANNEL_COUNT(4) |
+                                                 OP_COMMUTES},
+   [midgard_alu_op_ibany_neq] = {"CMP.any.ne", UNITS_VECTOR |
+                                                  OP_CHANNEL_COUNT(4) |
+                                                  OP_COMMUTES},
+   [midgard_alu_op_ibany_lt] = {"CMP.any.lt", UNITS_VECTOR |
+                                                 OP_CHANNEL_COUNT(4) |
+                                                 OP_COMMUTES},
+   [midgard_alu_op_ibany_lte] = {"CMP.any.le", UNITS_VECTOR |
+                                                  OP_CHANNEL_COUNT(4) |
+                                                  OP_COMMUTES},
+   [midgard_alu_op_ubany_lt] = {"CMP.any.lt", UNITS_VECTOR |
+                                                 OP_CHANNEL_COUNT(4) |
+                                                 OP_COMMUTES},
+   [midgard_alu_op_ubany_lte] = {"CMP.any.le", UNITS_VECTOR |
+                                                  OP_CHANNEL_COUNT(4) |
+                                                  OP_COMMUTES},
 
-   [midgard_alu_op_fatan2_pt1]      = {"FATAN2_PT1", UNIT_VLUT},
-   [midgard_alu_op_fatan2_pt2]      = {"FATAN2_PT2", UNIT_VLUT},
+   [midgard_alu_op_fatan2_pt1] = {"FATAN2_PT1", UNIT_VLUT},
+   [midgard_alu_op_fatan2_pt2] = {"FATAN2_PT2", UNIT_VLUT},
 
    /* Haven't seen in a while */
-   [midgard_alu_op_freduce]         = {"FREDUCE", 0},
+   [midgard_alu_op_freduce] = {"FREDUCE", 0},
 };
 
 /* Define shorthands */
@@ -223,83 +270,126 @@
    [midgard_op_pack_colour_f16] = {"PACK.f16", M32},
    [midgard_op_pack_colour_u32] = {"PACK.u32", M32},
    [midgard_op_pack_colour_s32] = {"PACK.s32", M32},
-   [midgard_op_lea] = {"LEA", M32 | LDST_ADDRESS },
-   [midgard_op_lea_image] = {"LEA_IMAGE", M32 | LDST_ATTRIB },
+   [midgard_op_lea] = {"LEA", M32 | LDST_ADDRESS},
+   [midgard_op_lea_image] = {"LEA_IMAGE", M32 | LDST_ATTRIB},
    [midgard_op_ld_cubemap_coords] = {"CUBEMAP", M32},
    [midgard_op_ldst_mov] = {"LDST_MOV", M32},
    [midgard_op_ldst_perspective_div_y] = {"LDST_PERSPECTIVE_DIV_Y", M32},
    [midgard_op_ldst_perspective_div_z] = {"LDST_PERSPECTIVE_DIV_Z", M32},
    [midgard_op_ldst_perspective_div_w] = {"LDST_PERSPECTIVE_DIV_W", M32},
 
-   [midgard_op_atomic_add]     = {"AADD.32",    M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_and]     = {"AAND.32",    M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_or]      = {"AOR.32",     M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_xor]     = {"AXOR.32",    M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_imin]    = {"AMIN.s32",   M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_umin]    = {"AMIN.u32",   M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_imax]    = {"AMAX.s32",   M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_umax]    = {"AMAX.u32",   M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_xchg]    = {"XCHG.32",    M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_cmpxchg] = {"CMPXCHG.32", M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-
-   [midgard_op_atomic_add64]     = {"AADD.64",    M64 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_and64]     = {"AAND.64",    M64 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_or64]      = {"AOR.64",     M64 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_xor64]     = {"AXOR.64",    M64 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_imin64]    = {"AMIN.s64",   M64 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_umin64]    = {"AMIN.u64",   M64 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_imax64]    = {"AMAX.s64",   M64 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_umax64]    = {"AMAX.u64",   M64 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_xchg64]    = {"XCHG.64",    M64 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_cmpxchg64] = {"CMPXCHG.64", M64 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-
-   [midgard_op_atomic_add_be]     = {"AADD.32.be",    M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_and_be]     = {"AAND.32.be",    M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_or_be]      = {"AOR.32.be",     M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_xor_be]     = {"AXOR.32.be",    M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_imin_be]    = {"AMIN.s32.be",   M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_umin_be]    = {"AMIN.u32.be",   M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_imax_be]    = {"AMAX.s32.be",   M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_umax_be]    = {"AMAX.u32.be",   M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_xchg_be]    = {"XCHG.32.be",    M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_cmpxchg_be] = {"CMPXCHG.32.be", M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-
-   [midgard_op_atomic_add64]     = {"AADD.64.be",    M64 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_and64]     = {"AAND.64.be",    M64 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_or64]      = {"AOR.64.be",     M64 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_xor64]     = {"AXOR.64.be",    M64 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_imin64]    = {"AMIN.s64.be",   M64 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_umin64]    = {"AMIN.u64.be",   M64 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_imax64]    = {"AMAX.s64.be",   M64 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_umax64]    = {"AMAX.u64.be",   M64 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_xchg64]    = {"XCHG.64.be",    M64 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-   [midgard_op_atomic_cmpxchg64] = {"CMPXCHG.64.be", M64 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
-
-   [midgard_op_ld_u8]         = {"LD.u8",         M32 | LDST_ADDRESS},
-   [midgard_op_ld_i8]         = {"LD.s8",         M32 | LDST_ADDRESS},
-   [midgard_op_ld_u16]        = {"LD.u16",        M32 | LDST_ADDRESS},
-   [midgard_op_ld_i16]        = {"LD.s16",        M32 | LDST_ADDRESS},
-   [midgard_op_ld_u16_be]     = {"LD.u16.be",     M32 | LDST_ADDRESS},
-   [midgard_op_ld_i16_be]     = {"LD.s16.be",     M32 | LDST_ADDRESS},
-   [midgard_op_ld_32]         = {"LD.32",         M32 | LDST_ADDRESS},
-   [midgard_op_ld_32_bswap2]  = {"LD.32.bswap2",  M32 | LDST_ADDRESS},
-   [midgard_op_ld_32_bswap4]  = {"LD.32.bswap4",  M32 | LDST_ADDRESS},
-   [midgard_op_ld_64]         = {"LD.64",         M32 | LDST_ADDRESS},
-   [midgard_op_ld_64_bswap2]  = {"LD.64.bswap2",  M32 | LDST_ADDRESS},
-   [midgard_op_ld_64_bswap4]  = {"LD.64.bswap4",  M32 | LDST_ADDRESS},
-   [midgard_op_ld_64_bswap8]  = {"LD.64.bswap8",  M32 | LDST_ADDRESS},
-   [midgard_op_ld_128]        = {"LD.128",        M32 | LDST_ADDRESS},
+   [midgard_op_atomic_add] = {"AADD.32",
+                              M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_and] = {"AAND.32",
+                              M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_or] = {"AOR.32",
+                             M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_xor] = {"AXOR.32",
+                              M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_imin] = {"AMIN.s32",
+                               M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_umin] = {"AMIN.u32",
+                               M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_imax] = {"AMAX.s32",
+                               M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_umax] = {"AMAX.u32",
+                               M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_xchg] = {"XCHG.32",
+                               M32 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_cmpxchg] = {"CMPXCHG.32", M32 | LDST_SIDE_FX |
+                                                   LDST_ADDRESS | LDST_ATOMIC},
+
+   [midgard_op_atomic_add64] = {"AADD.64", M64 | LDST_SIDE_FX | LDST_ADDRESS |
+                                              LDST_ATOMIC},
+   [midgard_op_atomic_and64] = {"AAND.64", M64 | LDST_SIDE_FX | LDST_ADDRESS |
+                                              LDST_ATOMIC},
+   [midgard_op_atomic_or64] = {"AOR.64",
+                               M64 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_xor64] = {"AXOR.64", M64 | LDST_SIDE_FX | LDST_ADDRESS |
+                                              LDST_ATOMIC},
+   [midgard_op_atomic_imin64] = {"AMIN.s64", M64 | LDST_SIDE_FX | LDST_ADDRESS |
+                                                LDST_ATOMIC},
+   [midgard_op_atomic_umin64] = {"AMIN.u64", M64 | LDST_SIDE_FX | LDST_ADDRESS |
+                                                LDST_ATOMIC},
+   [midgard_op_atomic_imax64] = {"AMAX.s64", M64 | LDST_SIDE_FX | LDST_ADDRESS |
+                                                LDST_ATOMIC},
+   [midgard_op_atomic_umax64] = {"AMAX.u64", M64 | LDST_SIDE_FX | LDST_ADDRESS |
+                                                LDST_ATOMIC},
+   [midgard_op_atomic_xchg64] = {"XCHG.64", M64 | LDST_SIDE_FX | LDST_ADDRESS |
+                                               LDST_ATOMIC},
+   [midgard_op_atomic_cmpxchg64] = {"CMPXCHG.64", M64 | LDST_SIDE_FX |
+                                                     LDST_ADDRESS |
+                                                     LDST_ATOMIC},
+
+   [midgard_op_atomic_add_be] = {"AADD.32.be", M32 | LDST_SIDE_FX |
+                                                  LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_and_be] = {"AAND.32.be", M32 | LDST_SIDE_FX |
+                                                  LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_or_be] = {"AOR.32.be", M32 | LDST_SIDE_FX | LDST_ADDRESS |
+                                                LDST_ATOMIC},
+   [midgard_op_atomic_xor_be] = {"AXOR.32.be", M32 | LDST_SIDE_FX |
+                                                  LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_imin_be] = {"AMIN.s32.be", M32 | LDST_SIDE_FX |
+                                                    LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_umin_be] = {"AMIN.u32.be", M32 | LDST_SIDE_FX |
+                                                    LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_imax_be] = {"AMAX.s32.be", M32 | LDST_SIDE_FX |
+                                                    LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_umax_be] = {"AMAX.u32.be", M32 | LDST_SIDE_FX |
+                                                    LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_xchg_be] = {"XCHG.32.be", M32 | LDST_SIDE_FX |
+                                                   LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_cmpxchg_be] = {"CMPXCHG.32.be", M32 | LDST_SIDE_FX |
+                                                         LDST_ADDRESS |
+                                                         LDST_ATOMIC},
+
+   [midgard_op_atomic_add64] = {"AADD.64.be", M64 | LDST_SIDE_FX |
+                                                 LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_and64] = {"AAND.64.be", M64 | LDST_SIDE_FX |
+                                                 LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_or64] = {"AOR.64.be",
+                               M64 | LDST_SIDE_FX | LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_xor64] = {"AXOR.64.be", M64 | LDST_SIDE_FX |
+                                                 LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_imin64] = {"AMIN.s64.be", M64 | LDST_SIDE_FX |
+                                                   LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_umin64] = {"AMIN.u64.be", M64 | LDST_SIDE_FX |
+                                                   LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_imax64] = {"AMAX.s64.be", M64 | LDST_SIDE_FX |
+                                                   LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_umax64] = {"AMAX.u64.be", M64 | LDST_SIDE_FX |
+                                                   LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_xchg64] = {"XCHG.64.be", M64 | LDST_SIDE_FX |
+                                                  LDST_ADDRESS | LDST_ATOMIC},
+   [midgard_op_atomic_cmpxchg64] = {"CMPXCHG.64.be", M64 | LDST_SIDE_FX |
+                                                        LDST_ADDRESS |
+                                                        LDST_ATOMIC},
+
+   [midgard_op_ld_u8] = {"LD.u8", M32 | LDST_ADDRESS},
+   [midgard_op_ld_i8] = {"LD.s8", M32 | LDST_ADDRESS},
+   [midgard_op_ld_u16] = {"LD.u16", M32 | LDST_ADDRESS},
+   [midgard_op_ld_i16] = {"LD.s16", M32 | LDST_ADDRESS},
+   [midgard_op_ld_u16_be] = {"LD.u16.be", M32 | LDST_ADDRESS},
+   [midgard_op_ld_i16_be] = {"LD.s16.be", M32 | LDST_ADDRESS},
+   [midgard_op_ld_32] = {"LD.32", M32 | LDST_ADDRESS},
+   [midgard_op_ld_32_bswap2] = {"LD.32.bswap2", M32 | LDST_ADDRESS},
+   [midgard_op_ld_32_bswap4] = {"LD.32.bswap4", M32 | LDST_ADDRESS},
+   [midgard_op_ld_64] = {"LD.64", M32 | LDST_ADDRESS},
+   [midgard_op_ld_64_bswap2] = {"LD.64.bswap2", M32 | LDST_ADDRESS},
+   [midgard_op_ld_64_bswap4] = {"LD.64.bswap4", M32 | LDST_ADDRESS},
+   [midgard_op_ld_64_bswap8] = {"LD.64.bswap8", M32 | LDST_ADDRESS},
+   [midgard_op_ld_128] = {"LD.128", M32 | LDST_ADDRESS},
    [midgard_op_ld_128_bswap2] = {"LD.128.bswap2", M32 | LDST_ADDRESS},
    [midgard_op_ld_128_bswap4] = {"LD.128.bswap4", M32 | LDST_ADDRESS},
    [midgard_op_ld_128_bswap8] = {"LD.128.bswap8", M32 | LDST_ADDRESS},
 
-   [midgard_op_ld_attr_32]  = {"LD_ATTR.f32", M32 | LDST_ATTRIB},
+   [midgard_op_ld_attr_32] = {"LD_ATTR.f32", M32 | LDST_ATTRIB},
    [midgard_op_ld_attr_32i] = {"LD_ATTR.s32", M32 | LDST_ATTRIB},
    [midgard_op_ld_attr_32u] = {"LD_ATTR.u32", M32 | LDST_ATTRIB},
-   [midgard_op_ld_attr_16]  = {"LD_ATTR.f16", M32 | LDST_ATTRIB},
+   [midgard_op_ld_attr_16] = {"LD_ATTR.f16", M32 | LDST_ATTRIB},
 
-   [midgard_op_ld_vary_32]  = {"LD_VARY.f32", M32 | LDST_ATTRIB},
-   [midgard_op_ld_vary_16]  = {"LD_VARY.f16", M32 | LDST_ATTRIB},
+   [midgard_op_ld_vary_32] = {"LD_VARY.f32", M32 | LDST_ATTRIB},
+   [midgard_op_ld_vary_16] = {"LD_VARY.f16", M32 | LDST_ATTRIB},
    [midgard_op_ld_vary_32i] = {"LD_VARY.s32", M32 | LDST_ATTRIB},
    [midgard_op_ld_vary_32u] = {"LD_VARY.u32", M32 | LDST_ATTRIB},
 
@@ -312,20 +402,20 @@
    [midgard_op_ld_tilebuffer_16f] = {"LD_TILEBUFFER.f16", M16},
    [midgard_op_ld_tilebuffer_raw] = {"LD_TILEBUFFER.raw", M32},
 
-   [midgard_op_ld_ubo_u8]         = {"LD_UBO.u8",         M32},
-   [midgard_op_ld_ubo_i8]         = {"LD_UBO.s8",         M32},
-   [midgard_op_ld_ubo_u16]        = {"LD_UBO.u16",        M16},
-   [midgard_op_ld_ubo_i16]        = {"LD_UBO.s16",        M16},
-   [midgard_op_ld_ubo_u16_be]     = {"LD_UBO.u16.be",     M16},
-   [midgard_op_ld_ubo_i16_be]     = {"LD_UBO.s16.be",     M16},
-   [midgard_op_ld_ubo_32]         = {"LD_UBO.32",         M32},
-   [midgard_op_ld_ubo_32_bswap2]  = {"LD_UBO.32.bswap2",  M32},
-   [midgard_op_ld_ubo_32_bswap4]  = {"LD_UBO.32.bswap4",  M32},
-   [midgard_op_ld_ubo_64]         = {"LD_UBO.64",         M32},
-   [midgard_op_ld_ubo_64_bswap2]  = {"LD_UBO.64.bswap2",  M32},
-   [midgard_op_ld_ubo_64_bswap4]  = {"LD_UBO.64.bswap4",  M32},
-   [midgard_op_ld_ubo_64_bswap8]  = {"LD_UBO.64.bswap8",  M32},
-   [midgard_op_ld_ubo_128]        = {"LD_UBO.128",        M32},
+   [midgard_op_ld_ubo_u8] = {"LD_UBO.u8", M32},
+   [midgard_op_ld_ubo_i8] = {"LD_UBO.s8", M32},
+   [midgard_op_ld_ubo_u16] = {"LD_UBO.u16", M16},
+   [midgard_op_ld_ubo_i16] = {"LD_UBO.s16", M16},
+   [midgard_op_ld_ubo_u16_be] = {"LD_UBO.u16.be", M16},
+   [midgard_op_ld_ubo_i16_be] = {"LD_UBO.s16.be", M16},
+   [midgard_op_ld_ubo_32] = {"LD_UBO.32", M32},
+   [midgard_op_ld_ubo_32_bswap2] = {"LD_UBO.32.bswap2", M32},
+   [midgard_op_ld_ubo_32_bswap4] = {"LD_UBO.32.bswap4", M32},
+   [midgard_op_ld_ubo_64] = {"LD_UBO.64", M32},
+   [midgard_op_ld_ubo_64_bswap2] = {"LD_UBO.64.bswap2", M32},
+   [midgard_op_ld_ubo_64_bswap4] = {"LD_UBO.64.bswap4", M32},
+   [midgard_op_ld_ubo_64_bswap8] = {"LD_UBO.64.bswap8", M32},
+   [midgard_op_ld_ubo_128] = {"LD_UBO.128", M32},
    [midgard_op_ld_ubo_128_bswap2] = {"LD_UBO.128.bswap2", M32},
    [midgard_op_ld_ubo_128_bswap4] = {"LD_UBO.128.bswap4", M32},
    [midgard_op_ld_ubo_128_bswap8] = {"LD_UBO.128.bswap8", M32},
@@ -335,28 +425,36 @@
    [midgard_op_ld_image_32i] = {"LD_IMAGE.s32", M32 | LDST_ATTRIB},
    [midgard_op_ld_image_32u] = {"LD_IMAGE.u32", M32 | LDST_ATTRIB},
 
-   [midgard_op_st_u8]         = {"ST.u8",         M32 | LDST_STORE | LDST_ADDRESS},
-   [midgard_op_st_i8]         = {"ST.s8",         M32 | LDST_STORE | LDST_ADDRESS},
-   [midgard_op_st_u16]        = {"ST.u16",        M32 | LDST_STORE | LDST_ADDRESS},
-   [midgard_op_st_i16]        = {"ST.s16",        M32 | LDST_STORE | LDST_ADDRESS},
-   [midgard_op_st_u16_be]     = {"ST.u16.be",     M32 | LDST_STORE | LDST_ADDRESS},
-   [midgard_op_st_i16_be]     = {"ST.s16.be",     M32 | LDST_STORE | LDST_ADDRESS},
-   [midgard_op_st_32]         = {"ST.32",         M32 | LDST_STORE | LDST_ADDRESS},
-   [midgard_op_st_32_bswap2]  = {"ST.32.bswap2",  M32 | LDST_STORE | LDST_ADDRESS},
-   [midgard_op_st_32_bswap4]  = {"ST.32.bswap4",  M32 | LDST_STORE | LDST_ADDRESS},
-   [midgard_op_st_64]         = {"ST.64",         M32 | LDST_STORE | LDST_ADDRESS},
-   [midgard_op_st_64_bswap2]  = {"ST.64.bswap2",  M32 | LDST_STORE | LDST_ADDRESS},
-   [midgard_op_st_64_bswap4]  = {"ST.64.bswap4",  M32 | LDST_STORE | LDST_ADDRESS},
-   [midgard_op_st_64_bswap8]  = {"ST.64.bswap8",  M32 | LDST_STORE | LDST_ADDRESS},
-   [midgard_op_st_128]        = {"ST.128",        M32 | LDST_STORE | LDST_ADDRESS},
-   [midgard_op_st_128_bswap2] = {"ST.128.bswap2", M32 | LDST_STORE | LDST_ADDRESS},
-   [midgard_op_st_128_bswap4] = {"ST.128.bswap4", M32 | LDST_STORE | LDST_ADDRESS},
-   [midgard_op_st_128_bswap8] = {"ST.128.bswap8", M32 | LDST_STORE | LDST_ADDRESS},
+   [midgard_op_st_u8] = {"ST.u8", M32 | LDST_STORE | LDST_ADDRESS},
+   [midgard_op_st_i8] = {"ST.s8", M32 | LDST_STORE | LDST_ADDRESS},
+   [midgard_op_st_u16] = {"ST.u16", M32 | LDST_STORE | LDST_ADDRESS},
+   [midgard_op_st_i16] = {"ST.s16", M32 | LDST_STORE | LDST_ADDRESS},
+   [midgard_op_st_u16_be] = {"ST.u16.be", M32 | LDST_STORE | LDST_ADDRESS},
+   [midgard_op_st_i16_be] = {"ST.s16.be", M32 | LDST_STORE | LDST_ADDRESS},
+   [midgard_op_st_32] = {"ST.32", M32 | LDST_STORE | LDST_ADDRESS},
+   [midgard_op_st_32_bswap2] = {"ST.32.bswap2",
+                                M32 | LDST_STORE | LDST_ADDRESS},
+   [midgard_op_st_32_bswap4] = {"ST.32.bswap4",
+                                M32 | LDST_STORE | LDST_ADDRESS},
+   [midgard_op_st_64] = {"ST.64", M32 | LDST_STORE | LDST_ADDRESS},
+   [midgard_op_st_64_bswap2] = {"ST.64.bswap2",
+                                M32 | LDST_STORE | LDST_ADDRESS},
+   [midgard_op_st_64_bswap4] = {"ST.64.bswap4",
+                                M32 | LDST_STORE | LDST_ADDRESS},
+   [midgard_op_st_64_bswap8] = {"ST.64.bswap8",
+                                M32 | LDST_STORE | LDST_ADDRESS},
+   [midgard_op_st_128] = {"ST.128", M32 | LDST_STORE | LDST_ADDRESS},
+   [midgard_op_st_128_bswap2] = {"ST.128.bswap2",
+                                 M32 | LDST_STORE | LDST_ADDRESS},
+   [midgard_op_st_128_bswap4] = {"ST.128.bswap4",
+                                 M32 | LDST_STORE | LDST_ADDRESS},
+   [midgard_op_st_128_bswap8] = {"ST.128.bswap8",
+                                 M32 | LDST_STORE | LDST_ADDRESS},
 
-   [midgard_op_st_vary_32]  = {"ST_VARY.f32", M32 | LDST_STORE | LDST_ATTRIB},
+   [midgard_op_st_vary_32] = {"ST_VARY.f32", M32 | LDST_STORE | LDST_ATTRIB},
    [midgard_op_st_vary_32i] = {"ST_VARY.s32", M32 | LDST_STORE | LDST_ATTRIB},
    [midgard_op_st_vary_32u] = {"ST_VARY.u32", M32 | LDST_STORE | LDST_ATTRIB},
-   [midgard_op_st_vary_16]  = {"ST_VARY.f16", M16 | LDST_STORE | LDST_ATTRIB},
+   [midgard_op_st_vary_16] = {"ST_VARY.f16", M16 | LDST_STORE | LDST_ATTRIB},
 
    [midgard_op_st_image_32f] = {"ST_IMAGE.f32", M32 | LDST_STORE | LDST_ATTRIB},
    [midgard_op_st_image_16f] = {"ST_IMAGE.f16", M16 | LDST_STORE | LDST_ATTRIB},
@@ -374,15 +472,14 @@
 };
 
 struct mir_tex_op_props tex_opcode_props[16] = {
-   [midgard_tex_op_normal] =                 {"TEX", M32},
-   [midgard_tex_op_gradient] =               {"TEX_GRAD", M32},
-   [midgard_tex_op_fetch] =                  {"TEX_FETCH", M32},
-   [midgard_tex_op_grad_from_derivative] =   {"DER_TO_GRAD", M32},
-   [midgard_tex_op_grad_from_coords] =       {"COORDS_TO_GRAD", M32},
-   [midgard_tex_op_mov] =                    {"MOV", M32},
-   [midgard_tex_op_barrier] =                {"BARRIER", M32},
-   [midgard_tex_op_derivative] =             {"DERIVATIVE", M32}
-};
+   [midgard_tex_op_normal] = {"TEX", M32},
+   [midgard_tex_op_gradient] = {"TEX_GRAD", M32},
+   [midgard_tex_op_fetch] = {"TEX_FETCH", M32},
+   [midgard_tex_op_grad_from_derivative] = {"DER_TO_GRAD", M32},
+   [midgard_tex_op_grad_from_coords] = {"COORDS_TO_GRAD", M32},
+   [midgard_tex_op_mov] = {"MOV", M32},
+   [midgard_tex_op_barrier] = {"BARRIER", M32},
+   [midgard_tex_op_derivative] = {"DERIVATIVE", M32}};
 
 #undef M8
 #undef M16
@@ -390,21 +487,19 @@
 #undef M64
 
 struct mir_tag_props midgard_tag_props[16] = {
-   [TAG_INVALID]           = {"invalid", 0},
-   [TAG_BREAK]             = {"break", 0},
-   [TAG_TEXTURE_4_VTX]     = {"tex/vt", 1},
-   [TAG_TEXTURE_4]         = {"tex", 1},
+   [TAG_INVALID] = {"invalid", 0},
+   [TAG_BREAK] = {"break", 0},
+   [TAG_TEXTURE_4_VTX] = {"tex/vt", 1},
+   [TAG_TEXTURE_4] = {"tex", 1},
    [TAG_TEXTURE_4_BARRIER] = {"tex/bar", 1},
-   [TAG_LOAD_STORE_4]      = {"ldst", 1},
-   [TAG_UNKNOWN_1]         = {"unk1", 1},
-   [TAG_UNKNOWN_2]         = {"unk2", 1},
-   [TAG_ALU_4]             = {"alu/4", 1},
-   [TAG_ALU_8]             = {"alu/8", 2},
-   [TAG_ALU_12]            = {"alu/12", 3},
-   [TAG_ALU_16]            = {"alu/16", 4},
-   [TAG_ALU_4_WRITEOUT]    = {"aluw/4", 1},
-   [TAG_ALU_8_WRITEOUT]    = {"aluw/8", 2},
-   [TAG_ALU_12_WRITEOUT]   = {"aluw/12", 3},
-   [TAG_ALU_16_WRITEOUT]   = {"aluw/16", 4}
-};
-/* clang-format on */
+   [TAG_LOAD_STORE_4] = {"ldst", 1},
+   [TAG_UNKNOWN_1] = {"unk1", 1},
+   [TAG_UNKNOWN_2] = {"unk2", 1},
+   [TAG_ALU_4] = {"alu/4", 1},
+   [TAG_ALU_8] = {"alu/8", 2},
+   [TAG_ALU_12] = {"alu/12", 3},
+   [TAG_ALU_16] = {"alu/16", 4},
+   [TAG_ALU_4_WRITEOUT] = {"aluw/4", 1},
+   [TAG_ALU_8_WRITEOUT] = {"aluw/8", 2},
+   [TAG_ALU_12_WRITEOUT] = {"aluw/12", 3},
+   [TAG_ALU_16_WRITEOUT] = {"aluw/16", 4}};
diff -urN mesa-23.0.0/src/panfrost/midgard/midgard_opt_perspective.c mesa/src/panfrost/midgard/midgard_opt_perspective.c
--- mesa-23.0.0/src/panfrost/midgard/midgard_opt_perspective.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/midgard/midgard_opt_perspective.c	2023-03-06 19:19:32.190305418 +0100
@@ -127,31 +127,19 @@
 
       /* Nice, we got the form spot on. Let's convert! */
 
-      midgard_instruction accel = {
-         .type = TAG_LOAD_STORE_4,
-         .mask = ins->mask,
-         .dest = to,
-         .dest_type = nir_type_float32,
-         .src =
-            {
-               frcp_from,
-               ~0,
-               ~0,
-               ~0,
-            },
-         .src_types =
-            {
-               nir_type_float32,
-            },
-         .swizzle = SWIZZLE_IDENTITY_4,
-         .op = frcp_component == COMPONENT_W
-                  ? midgard_op_ldst_perspective_div_w
-                  : midgard_op_ldst_perspective_div_z,
-         .load_store =
-            {
-               .bitsize_toggle = true,
-            },
-      };
+      midgard_instruction accel = {.type = TAG_LOAD_STORE_4,
+                                   .mask = ins->mask,
+                                   .dest = to,
+                                   .dest_type = nir_type_float32,
+                                   .src = {frcp_from, ~0, ~0, ~0},
+                                   .src_types = {nir_type_float32},
+                                   .swizzle = SWIZZLE_IDENTITY_4,
+                                   .op = frcp_component == COMPONENT_W
+                                            ? midgard_op_ldst_perspective_div_w
+                                            : midgard_op_ldst_perspective_div_z,
+                                   .load_store = {
+                                      .bitsize_toggle = true,
+                                   }};
 
       mir_insert_instruction_before(ctx, ins, accel);
       mir_remove_instruction(ins);
diff -urN mesa-23.0.0/src/panfrost/midgard/midgard_ra.c mesa/src/panfrost/midgard/midgard_ra.c
--- mesa-23.0.0/src/panfrost/midgard/midgard_ra.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/midgard/midgard_ra.c	2023-03-06 19:19:32.208305536 +0100
@@ -68,11 +68,7 @@
 static struct phys_reg
 default_phys_reg(int reg, unsigned shift)
 {
-   struct phys_reg r = {
-      .reg = reg,
-      .offset = 0,
-      .shift = shift,
-   };
+   struct phys_reg r = {.reg = reg, .offset = 0, .shift = shift};
 
    return r;
 }
@@ -92,11 +88,9 @@
    else if (!l)
       return default_phys_reg(REGISTER_UNUSED, shift);
 
-   struct phys_reg r = {
-      .reg = l->solutions[reg] / 16,
-      .offset = l->solutions[reg] & 0xF,
-      .shift = shift,
-   };
+   struct phys_reg r = {.reg = l->solutions[reg] / 16,
+                        .offset = l->solutions[reg] & 0xF,
+                        .shift = shift};
 
    /* Report that we actually use this register, and return it */
 
@@ -1189,8 +1183,7 @@
                   {
                      .index_reg = REGISTER_LDST_ZERO,
                   },
-               .constants.u32[0] = ctx->info->push.words[idx].offset,
-            };
+               .constants.u32[0] = ctx->info->push.words[idx].offset};
 
             midgard_pack_ubo_index_imm(&ld.load_store,
                                        ctx->info->push.words[idx].ubo);
diff -urN mesa-23.0.0/src/panfrost/midgard/midgard_schedule.c mesa/src/panfrost/midgard/midgard_schedule.c
--- mesa-23.0.0/src/panfrost/midgard/midgard_schedule.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/midgard/midgard_schedule.c	2023-03-06 19:19:32.225305649 +0100
@@ -830,12 +830,10 @@
     * best instruction, regardless of what else could be scheduled
     * alongside it. This is not optimal but it works okay for in-order */
 
-   struct midgard_predicate predicate = {
-      .tag = ~0,
-      .unit = ~0,
-      .destructive = false,
-      .exclude = ~0,
-   };
+   struct midgard_predicate predicate = {.tag = ~0,
+                                         .unit = ~0,
+                                         .destructive = false,
+                                         .exclude = ~0};
 
    midgard_instruction *chosen = mir_choose_instruction(
       instructions, liveness, worklist, count, &predicate);
@@ -1047,11 +1045,9 @@
 mir_schedule_texture(midgard_instruction **instructions, uint16_t *liveness,
                      BITSET_WORD *worklist, unsigned len, bool is_vertex)
 {
-   struct midgard_predicate predicate = {
-      .tag = TAG_TEXTURE_4,
-      .destructive = true,
-      .exclude = ~0,
-   };
+   struct midgard_predicate predicate = {.tag = TAG_TEXTURE_4,
+                                         .destructive = true,
+                                         .exclude = ~0};
 
    midgard_instruction *ins =
       mir_choose_instruction(instructions, liveness, worklist, len, &predicate);
@@ -1064,8 +1060,7 @@
                 ? TAG_TEXTURE_4_VTX
                 : TAG_TEXTURE_4,
       .instruction_count = 1,
-      .instructions = {ins},
-   };
+      .instructions = {ins}};
 
    return out;
 }
@@ -1074,11 +1069,9 @@
 mir_schedule_ldst(midgard_instruction **instructions, uint16_t *liveness,
                   BITSET_WORD *worklist, unsigned len, unsigned *num_ldst)
 {
-   struct midgard_predicate predicate = {
-      .tag = TAG_LOAD_STORE_4,
-      .destructive = true,
-      .exclude = ~0,
-   };
+   struct midgard_predicate predicate = {.tag = TAG_LOAD_STORE_4,
+                                         .destructive = true,
+                                         .exclude = ~0};
 
    /* Try to pick two load/store ops. Second not gauranteed to exist */
 
@@ -1090,11 +1083,9 @@
 
    assert(ins != NULL);
 
-   struct midgard_bundle out = {
-      .tag = TAG_LOAD_STORE_4,
-      .instruction_count = pair ? 2 : 1,
-      .instructions = {ins, pair},
-   };
+   struct midgard_bundle out = {.tag = TAG_LOAD_STORE_4,
+                                .instruction_count = pair ? 2 : 1,
+                                .instructions = {ins, pair}};
 
    *num_ldst -= out.instruction_count;
 
@@ -1180,12 +1171,10 @@
 
    unsigned bytes_emitted = sizeof(bundle.control);
 
-   struct midgard_predicate predicate = {
-      .tag = TAG_ALU_4,
-      .destructive = true,
-      .exclude = ~0,
-      .constants = &bundle.constants,
-   };
+   struct midgard_predicate predicate = {.tag = TAG_ALU_4,
+                                         .destructive = true,
+                                         .exclude = ~0,
+                                         .constants = &bundle.constants};
 
    midgard_instruction *vmul = NULL;
    midgard_instruction *vadd = NULL;
diff -urN mesa-23.0.0/src/panfrost/midgard/mir_promote_uniforms.c mesa/src/panfrost/midgard/mir_promote_uniforms.c
--- mesa-23.0.0/src/panfrost/midgard/mir_promote_uniforms.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/midgard/mir_promote_uniforms.c	2023-03-06 19:19:32.228305669 +0100
@@ -111,8 +111,7 @@
          for (unsigned offs = 0; offs < 4; ++offs) {
             struct panfrost_ubo_word word = {
                .ubo = ubo,
-               .offset = (vec4 * 16) + (offs * 4),
-            };
+               .offset = (vec4 * 16) + (offs * 4)};
 
             push->words[push->count++] = word;
          }
diff -urN mesa-23.0.0/src/panfrost/shared/pan_tiling.c mesa/src/panfrost/shared/pan_tiling.c
--- mesa-23.0.0/src/panfrost/shared/pan_tiling.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/shared/pan_tiling.c	2023-03-06 19:19:32.391306745 +0100
@@ -52,56 +52,23 @@
  *
  */
 
-/*
- * Given the lower 4-bits of the Y coordinate, we would like to
+/* Given the lower 4-bits of the Y coordinate, we would like to
  * duplicate every bit over. So instead of 0b1010, we would like
  * 0b11001100. The idea is that for the bits in the solely Y place, we
- * get a Y place, and the bits in the XOR place *also* get a Y.
- */
-/* clang-format off */
+ * get a Y place, and the bits in the XOR place *also* get a Y. */
+
 const uint32_t bit_duplication[16] = {
-   0b00000000,
-   0b00000011,
-   0b00001100,
-   0b00001111,
-   0b00110000,
-   0b00110011,
-   0b00111100,
-   0b00111111,
-   0b11000000,
-   0b11000011,
-   0b11001100,
-   0b11001111,
-   0b11110000,
-   0b11110011,
-   0b11111100,
-   0b11111111,
+   0b00000000, 0b00000011, 0b00001100, 0b00001111, 0b00110000, 0b00110011,
+   0b00111100, 0b00111111, 0b11000000, 0b11000011, 0b11001100, 0b11001111,
+   0b11110000, 0b11110011, 0b11111100, 0b11111111,
 };
-/* clang-format on */
 
-/*
- * Space the bits out of a 4-bit nibble
- */
-/* clang-format off */
-const unsigned space_4[16] = {
-   0b0000000,
-   0b0000001,
-   0b0000100,
-   0b0000101,
-   0b0010000,
-   0b0010001,
-   0b0010100,
-   0b0010101,
-   0b1000000,
-   0b1000001,
-   0b1000100,
-   0b1000101,
-   0b1010000,
-   0b1010001,
-   0b1010100,
-   0b1010101
-};
-/* clang-format on */
+/* Space the bits out of a 4-bit nibble */
+
+const unsigned space_4[16] = {0b0000000, 0b0000001, 0b0000100, 0b0000101,
+                              0b0010000, 0b0010001, 0b0010100, 0b0010101,
+                              0b1000000, 0b1000001, 0b1000100, 0b1000101,
+                              0b1010000, 0b1010001, 0b1010100, 0b1010101};
 
 /* The scheme uses 16x16 tiles */
 
diff -urN mesa-23.0.0/src/panfrost/shared/test/test-tiling.cpp mesa/src/panfrost/shared/test/test-tiling.cpp
--- mesa-23.0.0/src/panfrost/shared/test/test-tiling.cpp	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/shared/test/test-tiling.cpp	2023-03-06 17:54:35.858495099 +0100
@@ -45,14 +45,13 @@
    unsigned y2 = (y & 4) ? 1 : 0;
    unsigned y3 = (y & 8) ? 1 : 0;
 
-   return (xy0 << 0) | (y0 << 1) | (xy1 << 2) | (y1 << 3) | (xy2 << 4) |
-          (y2 << 5) | (xy3 << 6) | (y3 << 7);
+   return (xy0 << 0) | (y0 << 1) | (xy1 << 2) | (y1 << 3) |
+          (xy2 << 4) | (y2 << 5) | (xy3 << 6) | (y3 << 7);
 }
 
 /* x/y are in blocks */
 static unsigned
-tiled_offset(unsigned x, unsigned y, unsigned stride, unsigned tilesize,
-             unsigned blocksize)
+tiled_offset(unsigned x, unsigned y, unsigned stride, unsigned tilesize, unsigned blocksize)
 {
    unsigned tile_x = x / tilesize;
    unsigned tile_y = y / tilesize;
@@ -76,13 +75,15 @@
 }
 
 static void
-ref_access_tiled(void *dst, const void *src, unsigned region_x,
-                 unsigned region_y, unsigned w, unsigned h, uint32_t dst_stride,
-                 uint32_t src_stride, enum pipe_format format,
+ref_access_tiled(void *dst, const void *src,
+                 unsigned region_x, unsigned region_y,
+                 unsigned w, unsigned h,
+                 uint32_t dst_stride,
+                 uint32_t src_stride,
+                 enum pipe_format format,
                  bool dst_is_tiled)
 {
-   const struct util_format_description *desc = util_format_description(format);
-   ;
+   const struct util_format_description *desc = util_format_description(format);;
 
    unsigned tilesize = (desc->block.width > 1) ? 4 : 16;
    unsigned blocksize = (desc->block.bits / 8);
@@ -93,10 +94,8 @@
    unsigned region_x_block = region_x / desc->block.width;
    unsigned region_y_block = region_y / desc->block.height;
 
-   for (unsigned linear_y_block = 0; linear_y_block < h_block;
-        ++linear_y_block) {
-      for (unsigned linear_x_block = 0; linear_x_block < w_block;
-           ++linear_x_block) {
+   for (unsigned linear_y_block = 0; linear_y_block < h_block; ++linear_y_block) {
+      for (unsigned linear_x_block = 0; linear_x_block < w_block; ++linear_x_block) {
 
          unsigned tiled_x_block = region_x_block + linear_x_block;
          unsigned tiled_y_block = region_y_block + linear_y_block;
@@ -104,18 +103,15 @@
          unsigned dst_offset, src_offset;
 
          if (dst_is_tiled) {
-            dst_offset = tiled_offset(tiled_x_block, tiled_y_block, dst_stride,
-                                      tilesize, blocksize);
-            src_offset = linear_offset(linear_x_block, linear_y_block,
-                                       src_stride, blocksize);
+            dst_offset = tiled_offset(tiled_x_block, tiled_y_block, dst_stride, tilesize, blocksize);
+            src_offset = linear_offset(linear_x_block, linear_y_block, src_stride, blocksize);
          } else {
-            dst_offset = linear_offset(linear_x_block, linear_y_block,
-                                       dst_stride, blocksize);
-            src_offset = tiled_offset(tiled_x_block, tiled_y_block, src_stride,
-                                      tilesize, blocksize);
+            dst_offset = linear_offset(linear_x_block, linear_y_block, dst_stride, blocksize);
+            src_offset = tiled_offset(tiled_x_block, tiled_y_block, src_stride, tilesize, blocksize);
          }
 
-         memcpy((uint8_t *)dst + dst_offset, (const uint8_t *)src + src_offset,
+         memcpy((uint8_t *) dst + dst_offset,
+                (const uint8_t *) src + src_offset,
                 desc->block.bits / 8);
       }
    }
@@ -127,13 +123,14 @@
  * production.
  */
 static void
-test(unsigned width, unsigned height, unsigned rx, unsigned ry, unsigned rw,
-     unsigned rh, unsigned linear_stride, enum pipe_format format, bool store)
+test(unsigned width, unsigned height, unsigned rx, unsigned ry,
+     unsigned rw, unsigned rh, unsigned linear_stride,
+     enum pipe_format format, bool store)
 {
    unsigned bpp = util_format_get_blocksize(format);
    unsigned tile_height = util_format_is_compressed(format) ? 4 : 16;
 
-   unsigned tiled_width = ALIGN_POT(width, 16);
+   unsigned tiled_width  = ALIGN_POT(width, 16);
    unsigned tiled_height = ALIGN_POT(height, 16);
    unsigned tiled_stride = tiled_width * tile_height * bpp;
 
@@ -142,27 +139,26 @@
 
    void *tiled = calloc(bpp, tiled_width * tiled_height);
    void *linear = calloc(bpp, rw * linear_stride);
-   void *ref =
-      calloc(bpp, store ? (tiled_width * tiled_height) : (rw * linear_stride));
+   void *ref = calloc(bpp, store ? (tiled_width * tiled_height) : (rw * linear_stride));
 
    if (store) {
       for (unsigned i = 0; i < bpp * rw * linear_stride; ++i) {
-         ((uint8_t *)linear)[i] = (i & 0xFF);
+         ((uint8_t *) linear)[i] = (i & 0xFF);
       }
 
-      panfrost_store_tiled_image(tiled, linear, rx, ry, rw, rh, dst_stride,
-                                 src_stride, format);
+      panfrost_store_tiled_image(tiled, linear, rx, ry, rw, rh,
+                                 dst_stride, src_stride, format);
    } else {
       for (unsigned i = 0; i < bpp * tiled_width * tiled_height; ++i) {
-         ((uint8_t *)tiled)[i] = (i & 0xFF);
+         ((uint8_t *) tiled)[i] = (i & 0xFF);
       }
 
-      panfrost_load_tiled_image(linear, tiled, rx, ry, rw, rh, dst_stride,
-                                src_stride, format);
+      panfrost_load_tiled_image(linear, tiled, rx, ry, rw, rh,
+                                dst_stride, src_stride, format);
    }
 
-   ref_access_tiled(ref, store ? linear : tiled, rx, ry, rw, rh, dst_stride,
-                    src_stride, format, store);
+   ref_access_tiled(ref, store ? linear : tiled, rx, ry, rw, rh,
+                    dst_stride, src_stride, format, store);
 
    if (store)
       EXPECT_EQ(memcmp(ref, tiled, bpp * tiled_width * tiled_height), 0);
@@ -277,7 +273,7 @@
 TEST(UInterleavedTiling, PartialASTC)
 {
    /* Block alignment assumed */
-   test_ldst(40, 40, 4, 4, 16, 8, 512, PIPE_FORMAT_ASTC_4x4);
-   test_ldst(50, 40, 5, 4, 10, 8, 512, PIPE_FORMAT_ASTC_5x4);
+   test_ldst(40, 40, 4, 4, 16,  8, 512, PIPE_FORMAT_ASTC_4x4);
+   test_ldst(50, 40, 5, 4, 10,  8, 512, PIPE_FORMAT_ASTC_5x4);
    test_ldst(50, 50, 5, 5, 10, 10, 512, PIPE_FORMAT_ASTC_5x5);
 }
diff -urN mesa-23.0.0/src/panfrost/tiler/tiler-hex-read mesa/src/panfrost/tiler/tiler-hex-read
--- mesa-23.0.0/src/panfrost/tiler/tiler-hex-read	1970-01-01 01:00:00.000000000 +0100
+++ mesa/src/panfrost/tiler/tiler-hex-read	2023-03-06 17:54:35.858495099 +0100
@@ -0,0 +1,400 @@
+#!/usr/bin/env python3
+
+import sys
+import struct
+
+FLIP_Y = False
+
+data = b''
+
+fb_width = 160
+fb_height = 160
+hierarchy_mask = 0xffff
+
+HEAP_OFS = 0x8000
+
+base_ptr = 0
+heap_ptr = 0
+midgard = False
+bifrost = True
+valhall = False
+size = None
+
+bak_data = b''
+
+cur_data = b''
+
+# TODO: More robust looping..
+for line in sys.stdin.read().split("\n"):
+    print(line)
+    split = line.split(" ")
+    if not len(split) or split[0] == "":
+        continue
+    if split[0] == "width":
+        fb_width = int(split[1])
+        continue
+    if split[0] == "height":
+        fb_height = int(split[1])
+        continue
+    if split[0] == "mask":
+        hierarchy_mask = int(split[1], 0)
+        continue
+    if split[0] == "vaheap":
+        base_ptr = int(split[1], 16)
+        bifrost = False
+        valhall = True
+        continue
+    if split[0] == "addr":
+        base_ptr = int(split[1], 16)
+        bifrost = False
+        midgard = True
+        HEAP_OFS = 0x40
+        continue
+    if split[0] == "heap":
+        heap_ptr = int(split[1], 16)
+        data += cur_data
+        cur_data = b''
+        bak_data = data
+        data = b''
+        continue
+    if split[0] == "size":
+        size = int(split[1], 0)
+        continue
+    offset = int(split[0], 16)
+    if offset > len(data):
+        data += cur_data
+        cur_data = b''
+        data += b'\0' * (offset - len(data))
+    for d in split[1:]:
+        if d == "" or d == "*":
+            continue
+        cur_data += bytes([int(d, 16)])
+
+data += cur_data
+
+if heap_ptr:
+    data, heap_data = bak_data, data
+
+if size == None:
+    size = len(data)
+
+def int7(val, signed=True):
+    val = val & 0x7f
+    if signed and val >= 0x40:
+        return val - 0x80
+    else:
+        return val
+
+def int8(val, signed=True):
+    val = val & 0xff
+    if signed and val >= 0x80:
+        return val - 0x100
+    else:
+        return val
+
+def fetch(ptr, size):
+    if midgard:
+        if ptr >= base_ptr and ptr < base_ptr + len(data):
+            base = ptr - base_ptr
+            return data[base:base+size]
+        elif ptr >= heap_ptr and ptr < heap_ptr + len(heap_data):
+            base = ptr - heap_ptr
+            return heap_data[base:base+size]
+    else:
+        if valhall:
+            ptr -= base_ptr
+        if ptr < 0:
+            return b""
+        return data[ptr:ptr+size]
+
+def print_draw(ptr):
+    draw = fetch(ptr, 128)
+    if len(draw) < 128:
+        print(" couldn't fetch draw struct")
+        return
+    decoded = struct.unpack("=16Q", draw)
+    coverage = [0 for x in decoded]
+
+    fields = (
+        ("Allow forward pixel to kill", 1, "0:0", "bool"),
+        ("Allow forward pixel to be killed", 1, "0:1", "bool"),
+        ("Pixel kill operation", 2, "0:2", "Pixel Kill"),
+        ("ZS update operation", 2, "0:4", "Pixel Kill"),
+        ("Allow primitive reorder", 1, "0:6", "bool"),
+        ("Overdraw alpha0", 1, "0:7", "bool"),
+        ("Overdraw alpha1", 1, "0:8", "bool"),
+        ("Clean Fragment Write", 1, "0:9", "bool"),
+        ("Primitive Barrier", 1, "0:10", "bool"),
+        ("Evaluate per-sample", 1, "0:11", "bool"),
+        ("Single-sampled lines", 1, "0:13", "bool"),
+        ("Occlusion query", 2, "0:14", "Occlusion Mode"),
+        ("Front face CCW", 1, "0:16", "bool"),
+        ("Cull front face", 1, "0:17", "bool"),
+        ("Cull back face", 1, "0:18", "bool"),
+        ("Multisample enable", 1, "0:19", "bool"),
+        ("Shader modifies coverage", 1, "0:20", "bool"),
+        ("Alpha-to-coverage Invert", 1, "0:21", "bool"),
+        ("Alpha-to-coverage", 1, "0:22", "bool"),
+        ("Scissor to bounding box", 1, "0:23", "bool"),
+        ("Sample mask", 16, "1:0", "uint"),
+        ("Render target mask", 8, "1:16", "hex"),
+
+        ("Packet", 1, "2:0", "bool"),
+        # TODO: shr modifier
+        ("Vertex array", 64, "2:0", "address"),
+        ("Vertex packet stride", 16, "4:0", "uint"),
+        ("Vertex attribute stride", 16, "4:16", "uint"),
+        ("Unk", 16, "5:0", "uint"),
+
+        ("Minimum Z", 32, "6:0", "float"),
+        ("Maximum Z", 32, "7:0", "float"),
+        ("Depth/stencil", 64, "10:0", "address"),
+        ("Blend count", 4, "12:0", "uint"),
+        ("Blend", 60, "12:4", "address"),
+        ("Occlusion", 64, "14:0", "address"),
+
+        ("Attribute offset", 32, "16:0", "uint"),
+        ("FAU count", 8, "17:0", "uint"),
+        ("Resources", 48, "24:0", "address"),
+        ("Shader", 48, "26:0", "address"),
+        ("Thread storage", 48, "28:0", "address"),
+        ("FAU", 64, "30:0", "address"),
+    )
+
+    for f in fields:
+        name, size, start, type = f
+        word, bit = [int(x) for x in start.split(":")]
+        if word & 1:
+            bit += 32
+        word >>= 1
+
+        mask = (1 << size) - 1
+        data = (decoded[word] >> bit) & mask
+        coverage[word] |= mask << bit
+        if type == "float":
+            data = struct.unpack("=f", struct.pack("=I", data))[0]
+        else:
+            data = hex(data)
+        print(f"   {name}: {data}")
+
+    for i, (d, c) in enumerate(zip(decoded, coverage)):
+        ci = c ^ ((1 << 64) - 1)
+        if d & ci:
+            print(f"    unk at 64-bit word {i}: {hex(d)} (known mask {hex(c)})")
+
+def print_vertex(ptr, positions):
+    for p in positions:
+        addr = ptr + p * 16
+        data = fetch(addr, 16)
+        if len(data) < 16:
+            print(f"        <no data : {hex(addr)}>")
+            continue
+        x, y, z, w = struct.unpack("=4f", data)
+        print(f"       <{x} {y} {z} {w}>")
+
+DRAW_TYPES = [
+    "unk",
+    "points",
+    "lines",
+    "tris",
+]
+
+def heap_interpret(start, end):
+    print(f"interpreting from {hex(start)} to {hex(end)}")
+
+    struct_count = 0
+
+    signed = True
+
+    base = 0
+    a = 0
+    b = 0
+    c = 0
+
+    num_vert = 3
+
+    draw_ptr = 0
+    pos_ptr = 0
+
+    while start != end:
+        if midgard and start & 0x1ff == 0x1f8:
+            jump = struct.unpack("=Q", fetch(start, 8))[0]
+            print(f"jump mdg: {hex(jump)}")
+            start = jump
+            continue
+
+        dat = fetch(start, 4)
+        if dat[3] & 0xe0 == 0x80:
+            struct_count += 1
+
+        print(f"{struct_count}:", " ".join([f"{hex(x)[2:].upper():>02}" for x in dat]), end="  ")
+
+        masked_op = dat[3] & ~3
+
+        up = struct.unpack("=I", dat)[0]
+
+        if valhall:
+            tri0 = tri0_7 = int7(up >> 15, signed)
+            tri1 = int7(up >> 8, signed)
+            tri2 = int7(up >> 1, signed)
+        else:
+            tri0 = int8(up >> 14, signed)
+            tri0_7 = int7(up >> 14, signed)
+            tri1 = int7(up >> 7, signed)
+            tri2 = int7(up, signed)
+
+        signed = True
+
+        if dat[3] & 0xe0 == 0x80:
+            res = ""
+            if valhall:
+                address = (up & 0x7ffffff) * 32
+                num_vert = (dat[3] >> 3) & 0x3
+            else:
+                address = (up & 0xffffff) * 64
+                num_vert = (dat[3] >> 2) & 0x3
+                if dat[3] & 0x10:
+                    a = 0
+                    res = " reset"
+            draw_ptr = address
+            if valhall:
+                pos_ptr = address + 128
+            print(f"draw {DRAW_TYPES[num_vert]}{res}: {hex(address)}")
+        elif valhall and dat[3] >> 4 == 12:
+            unk1 = up & 0x3f
+            address = (up >> 6) & 0xffff
+            unk2 = up >> 22
+            draw_ptr += address << 32
+            pos_ptr += address << 32
+            print(f"draw offset: {hex(address)}, unk {hex(unk1)}, {hex(unk2)}")
+
+            print_draw(draw_ptr)
+        elif dat[3] >> 6 == 1:
+            # TODO: handle two of these in a row
+            res = ""
+            if valhall:
+                # TOOD: Is the mask correct?
+                pf = (up >> 22) & 0x7f
+                shift = 7
+                if dat[3] & 0x20:
+                    a = 0
+                    res = " reset"
+            else:
+                pf = (up >> 21) & 0x7f
+                shift = 8
+
+            a += tri0_7 << shift
+            b += tri1 << 7
+            c += tri2 << 7
+            print(f"primitive offset{res}: {hex(pf << 4)} | +{tri0_7 << shift} {tri1 << 7} {tri2 << 7}")
+            signed = False
+        # TODO: Jumps are located based on position, not opcode
+        elif dat[3] == 0xff:
+            up64 = struct.unpack("=Q", fetch(start, 8))[0]
+            assert((up64 & 3) == 3)
+            print(f"jump (from {hex(start+8)}-8): {hex(up64 - 3)}")
+            start = up64 - 7
+        elif dat[3] == 0x00:
+            assert((up & 3) == 3)
+            print(f"jump (from {hex(start+4)}-4): {hex(up - 3)}, {hex(HEAP_OFS + up - 3)}")
+            start = HEAP_OFS + up - 7
+        elif (masked_op & 0xc0) == 0:
+            mode = hex(dat[3] >> 2)
+
+            pre_offset = (up >> 22) & 0xf
+
+            unk = ""
+            if valhall and up & 1:
+                unk = ", unk 1"
+
+            a += base + tri0
+            b += a + tri1
+            c += a + tri2
+            base = a
+
+            print(f"{mode} draw: {hex(pre_offset)} | +{tri0} {tri1} {tri2}{unk}")
+
+            print_vertex(pos_ptr, [a, b, c][:num_vert])
+
+            a = b = c = 0
+
+        else:
+            print(f"Unknown opcode {hex(dat[3])}")
+
+        start += 4
+
+def level_list():
+    levels = []
+    size = 16
+    anylevel = False
+
+    # TODO: Does this miss the largest level?
+    while anylevel == False or size // 2 < min(fb_width, fb_height):
+        if (hierarchy_mask << 4) & size != 0:
+            anylevel = True
+            levels.append(size)
+
+        size *= 2
+
+    return levels
+
+def div_round_up(x, y):
+    return (x + y - 1) // y
+
+def align(x, y):
+    return div_round_up(x, y) * y
+
+def tile_count(alignment=4):
+    return sum(align(div_round_up(fb_width, size) * div_round_up(fb_height, size), 4)
+               for size in level_list())
+
+if midgard:
+    unpacked_header = list(struct.unpack("=16i", data[0:64]))
+    # Is this really big endian?
+    unpacked_header[5:7] = struct.unpack(">2i", data[20:28])
+    print(f"header: {' '.join([str(x) for x in unpacked_header])}")
+
+    # Extra is because of HEAP_OFS
+    header_size = align(tile_count() + 8, 64)
+elif valhall:
+    # TODO: Does this figure need alignment?
+    HEAP_STRIDE = tile_count() * 8
+    HEAP_OFS = size - HEAP_STRIDE * 2
+
+pos = base_ptr + HEAP_OFS
+
+for size in level_list():
+    for y in range((fb_height + size - 1) // size):
+        for x in range((fb_width + size - 1) // size):
+            header = fetch(pos, 8)
+            if len(header) == 0:
+                break
+
+            if midgard:
+                end = struct.unpack("=Q", header)[0]
+                use = bool(end)
+                end += 4
+                start = base_ptr + header_size * 8 + (pos - base_ptr - HEAP_OFS) * 64
+            elif bifrost:
+                end, start = struct.unpack("=II", header)
+                use = bool(end)
+                start += HEAP_OFS
+                end += HEAP_OFS + 4
+                end &= ~3
+            else:
+                footer = fetch(pos + HEAP_STRIDE, 8)
+                if len(footer) == 0:
+                    break
+                start, end = struct.unpack("=QQ", header + footer)
+                use = bool(end)
+                # The upper bits are used for jump metadata
+                end &= (1 << 48) - 1
+                end += 4
+            if use:
+                if FLIP_Y:
+                    print([x * size, fb_height - (y + 1) * size], ((x + 1) * size, fb_height - y * size))
+                else:
+                    print([x * size, y * size], ((x + 1) * size, (y + 1) * size))
+                heap_interpret(start, end)
+
+            pos += 8
diff -urN mesa-23.0.0/src/panfrost/tools/panfrostdump.c mesa/src/panfrost/tools/panfrostdump.c
--- mesa-23.0.0/src/panfrost/tools/panfrostdump.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/tools/panfrostdump.c	2023-03-06 19:19:32.397306784 +0100
@@ -224,14 +224,11 @@
       return EXIT_FAILURE;
    }
 
-   /* clang-format off */
    const struct option longopts[] = {
-      { "addr", no_argument, (int *) &print_addr, true },
-      { "regs", no_argument, (int *) &print_reg, true },
-      { "help", no_argument, NULL, 'h' },
-      { NULL, 0, NULL, 0 }
-   };
-   /* clang-format on */
+      {"addr", no_argument, (int *)&print_addr, true},
+      {"regs", no_argument, (int *)&print_reg, true},
+      {"help", no_argument, NULL, 'h'},
+      {NULL, 0, NULL, 0}};
 
    while ((c = getopt_long(argc, argv, "arh", longopts, NULL)) != -1) {
       switch (c) {
diff -urN mesa-23.0.0/src/panfrost/util/pan_lower_framebuffer.c mesa/src/panfrost/util/pan_lower_framebuffer.c
--- mesa-23.0.0/src/panfrost/util/pan_lower_framebuffer.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/util/pan_lower_framebuffer.c	2023-03-06 19:19:32.421306943 +0100
@@ -256,6 +256,13 @@
    return pan_pack_norm(b, v, x, y, z, w, false);
 }
 
+static nir_ssa_def *
+pan_pack_snorm(nir_builder *b, nir_ssa_def *v, unsigned x, unsigned y,
+               unsigned z, unsigned w)
+{
+   return pan_pack_norm(b, v, x, y, z, w, true);
+}
+
 /* RGB10_A2 is packed in the tilebuffer as the bottom 3 bytes being the top
  * 8-bits of RGB and the top byte being RGBA as 2-bits packed. As imirkin
  * pointed out, this means free conversion to RGBX8 */
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_cmd_buffer.c mesa/src/panfrost/vulkan/panvk_cmd_buffer.c
--- mesa-23.0.0/src/panfrost/vulkan/panvk_cmd_buffer.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_cmd_buffer.c	2023-03-06 19:19:32.444307095 +0100
@@ -34,10 +34,8 @@
 #include "vk_format.h"
 
 void
-panvk_CmdBindVertexBuffers(VkCommandBuffer commandBuffer,
-                           uint32_t firstBinding,
-                           uint32_t bindingCount,
-                           const VkBuffer *pBuffers,
+panvk_CmdBindVertexBuffers(VkCommandBuffer commandBuffer, uint32_t firstBinding,
+                           uint32_t bindingCount, const VkBuffer *pBuffers,
                            const VkDeviceSize *pOffsets)
 {
    VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
@@ -55,15 +53,14 @@
          panvk_buffer_range(buffer, pOffsets[i], VK_WHOLE_SIZE);
    }
 
-   cmdbuf->state.vb.count = MAX2(cmdbuf->state.vb.count, firstBinding + bindingCount);
+   cmdbuf->state.vb.count =
+      MAX2(cmdbuf->state.vb.count, firstBinding + bindingCount);
    desc_state->vs_attrib_bufs = desc_state->vs_attribs = 0;
 }
 
 void
-panvk_CmdBindIndexBuffer(VkCommandBuffer commandBuffer,
-                         VkBuffer buffer,
-                         VkDeviceSize offset,
-                         VkIndexType indexType)
+panvk_CmdBindIndexBuffer(VkCommandBuffer commandBuffer, VkBuffer buffer,
+                         VkDeviceSize offset, VkIndexType indexType)
 {
    VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
    VK_FROM_HANDLE(panvk_buffer, buf, buffer);
@@ -99,7 +96,7 @@
       const struct panvk_buffer_desc *ssbo =
          &desc_state->dyn.ssbos[dyn_ssbo_offset + i];
 
-      sysvals->dyn_ssbos[dyn_ssbo_offset + i] = (struct panvk_ssbo_addr) {
+      sysvals->dyn_ssbos[dyn_ssbo_offset + i] = (struct panvk_ssbo_addr){
          .base_addr = panvk_buffer_gpu_ptr(ssbo->buffer, ssbo->offset),
          .size = panvk_buffer_range(ssbo->buffer, ssbo->offset, ssbo->size),
       };
@@ -111,8 +108,7 @@
 void
 panvk_CmdBindDescriptorSets(VkCommandBuffer commandBuffer,
                             VkPipelineBindPoint pipelineBindPoint,
-                            VkPipelineLayout layout,
-                            uint32_t firstSet,
+                            VkPipelineLayout layout, uint32_t firstSet,
                             uint32_t descriptorSetCount,
                             const VkDescriptorSet *pDescriptorSets,
                             uint32_t dynamicOffsetCount,
@@ -139,12 +135,16 @@
             for (unsigned e = 0; e < set->layout->bindings[b].array_size; e++) {
                struct panvk_buffer_desc *bdesc = NULL;
 
-               if (set->layout->bindings[b].type == VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC) {
+               if (set->layout->bindings[b].type ==
+                   VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC) {
                   bdesc = &descriptors_state->dyn.ubos[dyn_ubo_offset++];
-                  *bdesc = set->dyn_ubos[set->layout->bindings[b].dyn_ubo_idx + e];
-               } else if (set->layout->bindings[b].type == VK_DESCRIPTOR_TYPE_STORAGE_BUFFER_DYNAMIC) {
+                  *bdesc =
+                     set->dyn_ubos[set->layout->bindings[b].dyn_ubo_idx + e];
+               } else if (set->layout->bindings[b].type ==
+                          VK_DESCRIPTOR_TYPE_STORAGE_BUFFER_DYNAMIC) {
                   bdesc = &descriptors_state->dyn.ssbos[dyn_ssbo_offset++];
-                  *bdesc = set->dyn_ssbos[set->layout->bindings[b].dyn_ssbo_idx + e];
+                  *bdesc =
+                     set->dyn_ssbos[set->layout->bindings[b].dyn_ssbo_idx + e];
                }
 
                if (bdesc) {
@@ -156,8 +156,7 @@
 
       if (set->layout->num_dyn_ssbos) {
          panvk_set_dyn_ssbo_pointers(descriptors_state,
-                                     playout->sets[idx].dyn_ssbo_offset,
-                                     set);
+                                     playout->sets[idx].dyn_ssbo_offset, set);
       }
 
       if (set->layout->num_dyn_ssbos)
@@ -174,7 +173,8 @@
          descriptors_state->samplers = 0;
 
       if (set->layout->num_imgs) {
-         descriptors_state->vs_attrib_bufs = descriptors_state->non_vs_attrib_bufs = 0;
+         descriptors_state->vs_attrib_bufs =
+            descriptors_state->non_vs_attrib_bufs = 0;
          descriptors_state->vs_attribs = descriptors_state->non_vs_attribs = 0;
       }
    }
@@ -183,12 +183,9 @@
 }
 
 void
-panvk_CmdPushConstants(VkCommandBuffer commandBuffer,
-                       VkPipelineLayout layout,
-                       VkShaderStageFlags stageFlags,
-                       uint32_t offset,
-                       uint32_t size,
-                       const void *pValues)
+panvk_CmdPushConstants(VkCommandBuffer commandBuffer, VkPipelineLayout layout,
+                       VkShaderStageFlags stageFlags, uint32_t offset,
+                       uint32_t size, const void *pValues)
 {
    VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
 
@@ -225,11 +222,13 @@
    if (pipelineBindPoint == VK_PIPELINE_BIND_POINT_GRAPHICS) {
       cmdbuf->state.varyings = pipeline->varyings;
 
-      if (!(pipeline->dynamic_state_mask & BITFIELD_BIT(VK_DYNAMIC_STATE_VIEWPORT))) {
+      if (!(pipeline->dynamic_state_mask &
+            BITFIELD_BIT(VK_DYNAMIC_STATE_VIEWPORT))) {
          cmdbuf->state.viewport = pipeline->viewport;
          cmdbuf->state.dirty |= PANVK_DYNAMIC_VIEWPORT;
       }
-      if (!(pipeline->dynamic_state_mask & BITFIELD_BIT(VK_DYNAMIC_STATE_SCISSOR))) {
+      if (!(pipeline->dynamic_state_mask &
+            BITFIELD_BIT(VK_DYNAMIC_STATE_SCISSOR))) {
          cmdbuf->state.scissor = pipeline->scissor;
          cmdbuf->state.dirty |= PANVK_DYNAMIC_SCISSOR;
       }
@@ -242,10 +241,8 @@
 }
 
 void
-panvk_CmdSetViewport(VkCommandBuffer commandBuffer,
-                     uint32_t firstViewport,
-                     uint32_t viewportCount,
-                     const VkViewport *pViewports)
+panvk_CmdSetViewport(VkCommandBuffer commandBuffer, uint32_t firstViewport,
+                     uint32_t viewportCount, const VkViewport *pViewports)
 {
    VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
    assert(viewportCount == 1);
@@ -257,10 +254,8 @@
 }
 
 void
-panvk_CmdSetScissor(VkCommandBuffer commandBuffer,
-                    uint32_t firstScissor,
-                    uint32_t scissorCount,
-                    const VkRect2D *pScissors)
+panvk_CmdSetScissor(VkCommandBuffer commandBuffer, uint32_t firstScissor,
+                    uint32_t scissorCount, const VkRect2D *pScissors)
 {
    VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
    assert(scissorCount == 1);
@@ -282,8 +277,7 @@
 
 void
 panvk_CmdSetDepthBias(VkCommandBuffer commandBuffer,
-                      float depthBiasConstantFactor,
-                      float depthBiasClamp,
+                      float depthBiasConstantFactor, float depthBiasClamp,
                       float depthBiasSlopeFactor)
 {
    VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
@@ -309,8 +303,7 @@
 }
 
 void
-panvk_CmdSetDepthBounds(VkCommandBuffer commandBuffer,
-                        float minDepthBounds,
+panvk_CmdSetDepthBounds(VkCommandBuffer commandBuffer, float minDepthBounds,
                         float maxDepthBounds)
 {
    panvk_stub();
@@ -335,8 +328,7 @@
 
 void
 panvk_CmdSetStencilWriteMask(VkCommandBuffer commandBuffer,
-                             VkStencilFaceFlags faceMask,
-                             uint32_t writeMask)
+                             VkStencilFaceFlags faceMask, uint32_t writeMask)
 {
    VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
 
@@ -352,8 +344,7 @@
 
 void
 panvk_CmdSetStencilReference(VkCommandBuffer commandBuffer,
-                             VkStencilFaceFlags faceMask,
-                             uint32_t reference)
+                             VkStencilFaceFlags faceMask, uint32_t reference)
 {
    VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
 
@@ -381,8 +372,8 @@
    if (pool == NULL)
       return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
 
-   VkResult result = vk_command_pool_init(&device->vk, &pool->vk,
-                                          pCreateInfo, pAllocator);
+   VkResult result =
+      vk_command_pool_init(&device->vk, &pool->vk, pCreateInfo, pAllocator);
    if (result != VK_SUCCESS) {
       vk_free2(&device->vk.alloc, pAllocator, pool);
       return result;
@@ -400,27 +391,29 @@
                                const VkClearValue *in)
 {
    for (unsigned i = 0; i < cmdbuf->state.pass->attachment_count; i++) {
-       const struct panvk_render_pass_attachment *attachment =
-          &cmdbuf->state.pass->attachments[i];
-       enum pipe_format fmt = attachment->format;
-
-       if (util_format_is_depth_or_stencil(fmt)) {
-          if (attachment->load_op == VK_ATTACHMENT_LOAD_OP_CLEAR ||
-              attachment->stencil_load_op == VK_ATTACHMENT_LOAD_OP_CLEAR) {
-             cmdbuf->state.clear[i].depth = in[i].depthStencil.depth;
-             cmdbuf->state.clear[i].stencil = in[i].depthStencil.stencil;
-          } else {
-             cmdbuf->state.clear[i].depth = 0;
-             cmdbuf->state.clear[i].stencil = 0;
-          }
-       } else {
-          if (attachment->load_op == VK_ATTACHMENT_LOAD_OP_CLEAR) {
-             union pipe_color_union *col = (union pipe_color_union *) &in[i].color;
-             pan_pack_color(cmdbuf->state.clear[i].color, col, fmt, false);
-          } else {
-             memset(cmdbuf->state.clear[i].color, 0, sizeof(cmdbuf->state.clear[0].color));
-          }
-       }
+      const struct panvk_render_pass_attachment *attachment =
+         &cmdbuf->state.pass->attachments[i];
+      enum pipe_format fmt = attachment->format;
+
+      if (util_format_is_depth_or_stencil(fmt)) {
+         if (attachment->load_op == VK_ATTACHMENT_LOAD_OP_CLEAR ||
+             attachment->stencil_load_op == VK_ATTACHMENT_LOAD_OP_CLEAR) {
+            cmdbuf->state.clear[i].depth = in[i].depthStencil.depth;
+            cmdbuf->state.clear[i].stencil = in[i].depthStencil.stencil;
+         } else {
+            cmdbuf->state.clear[i].depth = 0;
+            cmdbuf->state.clear[i].stencil = 0;
+         }
+      } else {
+         if (attachment->load_op == VK_ATTACHMENT_LOAD_OP_CLEAR) {
+            union pipe_color_union *col =
+               (union pipe_color_union *)&in[i].color;
+            pan_pack_color(cmdbuf->state.clear[i].color, col, fmt, false);
+         } else {
+            memset(cmdbuf->state.clear[i].color, 0,
+                   sizeof(cmdbuf->state.clear[0].color));
+         }
+      }
    }
 }
 
@@ -435,12 +428,12 @@
 
    fbinfo->nr_samples = 1;
    fbinfo->rt_count = subpass->color_count;
-   memset(&fbinfo->bifrost.pre_post.dcds, 0, sizeof(fbinfo->bifrost.pre_post.dcds));
+   memset(&fbinfo->bifrost.pre_post.dcds, 0,
+          sizeof(fbinfo->bifrost.pre_post.dcds));
 
    for (unsigned cb = 0; cb < subpass->color_count; cb++) {
       int idx = subpass->color_attachments[cb].idx;
-      view = idx != VK_ATTACHMENT_UNUSED ?
-             fb->attachments[idx].iview : NULL;
+      view = idx != VK_ATTACHMENT_UNUSED ? fb->attachments[idx].iview : NULL;
       if (!view)
          continue;
       fbinfo->rts[cb].view = &view->pview;
@@ -464,13 +457,15 @@
 
       if (util_format_has_depth(fdesc)) {
          fbinfo->zs.clear.z = subpass->zs_attachment.clear;
-         fbinfo->zs.clear_value.depth = clears[subpass->zs_attachment.idx].depth;
+         fbinfo->zs.clear_value.depth =
+            clears[subpass->zs_attachment.idx].depth;
          fbinfo->zs.view.zs = &view->pview;
       }
 
       if (util_format_has_stencil(fdesc)) {
          fbinfo->zs.clear.s = subpass->zs_attachment.clear;
-         fbinfo->zs.clear_value.stencil = clears[subpass->zs_attachment.idx].stencil;
+         fbinfo->zs.clear_value.stencil =
+            clears[subpass->zs_attachment.idx].stencil;
          if (!fbinfo->zs.view.zs)
             fbinfo->zs.view.s = &view->pview;
       }
@@ -485,7 +480,7 @@
 
    memset(cmdbuf->state.fb.crc_valid, 0, sizeof(cmdbuf->state.fb.crc_valid));
 
-   *fbinfo = (struct pan_fb_info) {
+   *fbinfo = (struct pan_fb_info){
       .width = fb->width,
       .height = fb->height,
       .extent.maxx = fb->width - 1,
@@ -506,16 +501,16 @@
    cmdbuf->state.subpass = pass->subpasses;
    cmdbuf->state.framebuffer = fb;
    cmdbuf->state.render_area = pRenderPassBegin->renderArea;
-   cmdbuf->state.batch = vk_zalloc(&cmdbuf->vk.pool->alloc,
-                                   sizeof(*cmdbuf->state.batch), 8,
-                                   VK_SYSTEM_ALLOCATION_SCOPE_COMMAND);
+   cmdbuf->state.batch =
+      vk_zalloc(&cmdbuf->vk.pool->alloc, sizeof(*cmdbuf->state.batch), 8,
+                VK_SYSTEM_ALLOCATION_SCOPE_COMMAND);
    util_dynarray_init(&cmdbuf->state.batch->jobs, NULL);
    util_dynarray_init(&cmdbuf->state.batch->event_ops, NULL);
    assert(pRenderPassBegin->clearValueCount <= pass->attachment_count);
    cmdbuf->state.clear =
       vk_zalloc(&cmdbuf->vk.pool->alloc,
-                sizeof(*cmdbuf->state.clear) * pass->attachment_count,
-                8, VK_SYSTEM_ALLOCATION_SCOPE_COMMAND);
+                sizeof(*cmdbuf->state.clear) * pass->attachment_count, 8,
+                VK_SYSTEM_ALLOCATION_SCOPE_COMMAND);
    panvk_cmd_prepare_clear_values(cmdbuf, pRenderPassBegin->pClearValues);
    panvk_cmd_fb_info_init(cmdbuf);
    panvk_cmd_fb_info_set_subpass(cmdbuf);
@@ -538,7 +533,8 @@
 
    if (cmdbuf->state.fb.info.zs.view.s ||
        (cmdbuf->state.fb.info.zs.view.zs &&
-        util_format_is_depth_and_stencil(cmdbuf->state.fb.info.zs.view.zs->format))) {
+        util_format_is_depth_and_stencil(
+           cmdbuf->state.fb.info.zs.view.zs->format))) {
       cmdbuf->state.fb.info.zs.clear.s = false;
       cmdbuf->state.fb.info.zs.preload.s = true;
    }
@@ -548,49 +544,45 @@
 panvk_cmd_open_batch(struct panvk_cmd_buffer *cmdbuf)
 {
    assert(!cmdbuf->state.batch);
-   cmdbuf->state.batch = vk_zalloc(&cmdbuf->vk.pool->alloc,
-                                   sizeof(*cmdbuf->state.batch), 8,
-                                   VK_SYSTEM_ALLOCATION_SCOPE_COMMAND);
+   cmdbuf->state.batch =
+      vk_zalloc(&cmdbuf->vk.pool->alloc, sizeof(*cmdbuf->state.batch), 8,
+                VK_SYSTEM_ALLOCATION_SCOPE_COMMAND);
    assert(cmdbuf->state.batch);
    return cmdbuf->state.batch;
 }
 
 void
-panvk_CmdDrawIndirect(VkCommandBuffer commandBuffer,
-                      VkBuffer _buffer,
-                      VkDeviceSize offset,
-                      uint32_t drawCount,
-                      uint32_t stride)
+panvk_CmdDrawIndirect(VkCommandBuffer commandBuffer, VkBuffer _buffer,
+                      VkDeviceSize offset, uint32_t drawCount, uint32_t stride)
 {
    panvk_stub();
 }
 
 void
-panvk_CmdDrawIndexedIndirect(VkCommandBuffer commandBuffer,
-                             VkBuffer _buffer,
-                             VkDeviceSize offset,
-                             uint32_t drawCount,
+panvk_CmdDrawIndexedIndirect(VkCommandBuffer commandBuffer, VkBuffer _buffer,
+                             VkDeviceSize offset, uint32_t drawCount,
                              uint32_t stride)
 {
    panvk_stub();
 }
 
 void
-panvk_CmdDispatchBase(VkCommandBuffer commandBuffer,
-                      uint32_t base_x,
-                      uint32_t base_y,
-                      uint32_t base_z,
-                      uint32_t x,
-                      uint32_t y,
+panvk_CmdDispatchBase(VkCommandBuffer commandBuffer, uint32_t base_x,
+                      uint32_t base_y, uint32_t base_z, uint32_t x, uint32_t y,
                       uint32_t z)
 {
    panvk_stub();
 }
 
 void
-panvk_CmdDispatchIndirect(VkCommandBuffer commandBuffer,
-                          VkBuffer _buffer,
+panvk_CmdDispatchIndirect(VkCommandBuffer commandBuffer, VkBuffer _buffer,
                           VkDeviceSize offset)
 {
    panvk_stub();
 }
+
+void
+panvk_CmdSetDeviceMask(VkCommandBuffer commandBuffer, uint32_t deviceMask)
+{
+   panvk_stub();
+}
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_cs.c mesa/src/panfrost/vulkan/panvk_cs.c
--- mesa-23.0.0/src/panfrost/vulkan/panvk_cs.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_cs.c	2023-03-06 19:19:32.444307095 +0100
@@ -21,8 +21,8 @@
  * DEALINGS IN THE SOFTWARE.
  */
 
-#include "util/macros.h"
 #include "compiler/shader_enums.h"
+#include "util/macros.h"
 
 #include "pan_cs.h"
 #include "pan_pool.h"
@@ -31,9 +31,9 @@
 #include "panvk_private.h"
 
 /*
- * Upload the viewport scale. Defined as (px/2, py/2, pz) at the start of section
- * 24.5 ("Controlling the Viewport") of the Vulkan spec. At the end of the
- * section, the spec defines:
+ * Upload the viewport scale. Defined as (px/2, py/2, pz) at the start of
+ * section 24.5 ("Controlling the Viewport") of the Vulkan spec. At the end of
+ * the section, the spec defines:
  *
  * px = width
  * py = height
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_cs.h mesa/src/panfrost/vulkan/panvk_cs.h
--- mesa-23.0.0/src/panfrost/vulkan/panvk_cs.h	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_cs.h	2023-03-06 19:19:32.446307108 +0100
@@ -29,8 +29,8 @@
 #include <vulkan/vulkan.h>
 
 #include "compiler/shader_enums.h"
-#include "panfrost-job.h"
 #include "pan_cs.h"
+#include "panfrost-job.h"
 
 #include "vk_util.h"
 
@@ -66,29 +66,30 @@
    STATIC_ASSERT(VK_COMPARE_OP_LESS_OR_EQUAL == (VkCompareOp)MALI_FUNC_LEQUAL);
    STATIC_ASSERT(VK_COMPARE_OP_GREATER == (VkCompareOp)MALI_FUNC_GREATER);
    STATIC_ASSERT(VK_COMPARE_OP_NOT_EQUAL == (VkCompareOp)MALI_FUNC_NOT_EQUAL);
-   STATIC_ASSERT(VK_COMPARE_OP_GREATER_OR_EQUAL == (VkCompareOp)MALI_FUNC_GEQUAL);
+   STATIC_ASSERT(VK_COMPARE_OP_GREATER_OR_EQUAL ==
+                 (VkCompareOp)MALI_FUNC_GEQUAL);
    STATIC_ASSERT(VK_COMPARE_OP_ALWAYS == (VkCompareOp)MALI_FUNC_ALWAYS);
 
    return (enum mali_func)comp;
 }
 
 static inline enum mali_func
-panvk_per_arch(translate_sampler_compare_func)(const VkSamplerCreateInfo *pCreateInfo)
+panvk_per_arch(translate_sampler_compare_func)(
+   const VkSamplerCreateInfo *pCreateInfo)
 {
    if (!pCreateInfo->compareEnable)
       return MALI_FUNC_NEVER;
 
-   enum mali_func f = panvk_per_arch(translate_compare_func)(pCreateInfo->compareOp);
+   enum mali_func f =
+      panvk_per_arch(translate_compare_func)(pCreateInfo->compareOp);
    return panfrost_flip_compare_func(f);
 }
 #endif
 
-void
-panvk_sysval_upload_viewport_scale(const VkViewport *viewport,
-                                   union panvk_sysval_vec4 *data);
-
-void
-panvk_sysval_upload_viewport_offset(const VkViewport *viewport,
-                                    union panvk_sysval_vec4 *data);
+void panvk_sysval_upload_viewport_scale(const VkViewport *viewport,
+                                        union panvk_sysval_vec4 *data);
+
+void panvk_sysval_upload_viewport_offset(const VkViewport *viewport,
+                                         union panvk_sysval_vec4 *data);
 
 #endif
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_descriptor_set.c mesa/src/panfrost/vulkan/panvk_descriptor_set.c
--- mesa-23.0.0/src/panfrost/vulkan/panvk_descriptor_set.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_descriptor_set.c	2023-03-06 19:19:32.449307128 +0100
@@ -39,33 +39,30 @@
 #include "pan_bo.h"
 
 /* FIXME: make sure those values are correct */
-#define PANVK_MAX_TEXTURES     (1 << 16)
-#define PANVK_MAX_IMAGES       (1 << 8)
-#define PANVK_MAX_SAMPLERS     (1 << 16)
-#define PANVK_MAX_UBOS         255
+#define PANVK_MAX_TEXTURES (1 << 16)
+#define PANVK_MAX_IMAGES   (1 << 8)
+#define PANVK_MAX_SAMPLERS (1 << 16)
+#define PANVK_MAX_UBOS     255
 
 void
-panvk_GetDescriptorSetLayoutSupport(VkDevice _device,
-                                    const VkDescriptorSetLayoutCreateInfo *pCreateInfo,
-                                    VkDescriptorSetLayoutSupport *pSupport)
+panvk_GetDescriptorSetLayoutSupport(
+   VkDevice _device, const VkDescriptorSetLayoutCreateInfo *pCreateInfo,
+   VkDescriptorSetLayoutSupport *pSupport)
 {
    VK_FROM_HANDLE(panvk_device, device, _device);
 
    pSupport->supported = false;
 
    VkDescriptorSetLayoutBinding *bindings;
-   VkResult result =
-      vk_create_sorted_bindings(pCreateInfo->pBindings,
-                                pCreateInfo->bindingCount,
-                                &bindings);
+   VkResult result = vk_create_sorted_bindings(
+      pCreateInfo->pBindings, pCreateInfo->bindingCount, &bindings);
    if (result != VK_SUCCESS) {
       vk_error(device, result);
       return;
    }
 
    unsigned sampler_idx = 0, tex_idx = 0, ubo_idx = 0;
-   unsigned img_idx = 0;
-   UNUSED unsigned dynoffset_idx = 0;
+   unsigned dynoffset_idx = 0, img_idx = 0;
 
    for (unsigned i = 0; i < pCreateInfo->bindingCount; i++) {
       const VkDescriptorSetLayoutBinding *binding = &bindings[i];
@@ -130,8 +127,8 @@
    struct panvk_pipeline_layout *layout;
    struct mesa_sha1 ctx;
 
-   layout = vk_pipeline_layout_zalloc(&device->vk, sizeof(*layout),
-                                      pCreateInfo);
+   layout =
+      vk_pipeline_layout_zalloc(&device->vk, sizeof(*layout), pCreateInfo);
    if (layout == NULL)
       return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
 
@@ -162,21 +159,26 @@
 
          if (binding_layout->immutable_samplers) {
             for (unsigned s = 0; s < binding_layout->array_size; s++) {
-               struct panvk_sampler *sampler = binding_layout->immutable_samplers[s];
+               struct panvk_sampler *sampler =
+                  binding_layout->immutable_samplers[s];
 
                _mesa_sha1_update(&ctx, &sampler->desc, sizeof(sampler->desc));
             }
          }
-         _mesa_sha1_update(&ctx, &binding_layout->type, sizeof(binding_layout->type));
-         _mesa_sha1_update(&ctx, &binding_layout->array_size, sizeof(binding_layout->array_size));
-         _mesa_sha1_update(&ctx, &binding_layout->shader_stages, sizeof(binding_layout->shader_stages));
+         _mesa_sha1_update(&ctx, &binding_layout->type,
+                           sizeof(binding_layout->type));
+         _mesa_sha1_update(&ctx, &binding_layout->array_size,
+                           sizeof(binding_layout->array_size));
+         _mesa_sha1_update(&ctx, &binding_layout->shader_stages,
+                           sizeof(binding_layout->shader_stages));
       }
    }
 
-   for (unsigned range = 0; range < pCreateInfo->pushConstantRangeCount; range++) {
+   for (unsigned range = 0; range < pCreateInfo->pushConstantRangeCount;
+        range++) {
       layout->push_constants.size =
          MAX2(pCreateInfo->pPushConstantRanges[range].offset +
-              pCreateInfo->pPushConstantRanges[range].size,
+                 pCreateInfo->pPushConstantRanges[range].size,
               layout->push_constants.size);
    }
 
@@ -222,7 +224,7 @@
    for (unsigned i = 0; i < pCreateInfo->poolSizeCount; ++i) {
       unsigned desc_count = pCreateInfo->pPoolSizes[i].descriptorCount;
 
-      switch(pCreateInfo->pPoolSizes[i].type) {
+      switch (pCreateInfo->pPoolSizes[i].type) {
       case VK_DESCRIPTOR_TYPE_SAMPLER:
          pool->max.samplers += desc_count;
          break;
@@ -266,8 +268,7 @@
 }
 
 void
-panvk_DestroyDescriptorPool(VkDevice _device,
-                            VkDescriptorPool _pool,
+panvk_DestroyDescriptorPool(VkDevice _device, VkDescriptorPool _pool,
                             const VkAllocationCallbacks *pAllocator)
 {
    VK_FROM_HANDLE(panvk_device, device, _device);
@@ -278,8 +279,7 @@
 }
 
 VkResult
-panvk_ResetDescriptorPool(VkDevice _device,
-                          VkDescriptorPool _pool,
+panvk_ResetDescriptorPool(VkDevice _device, VkDescriptorPool _pool,
                           VkDescriptorPoolResetFlags flags)
 {
    VK_FROM_HANDLE(panvk_descriptor_pool, pool, _pool);
@@ -305,10 +305,8 @@
 }
 
 VkResult
-panvk_FreeDescriptorSets(VkDevice _device,
-                         VkDescriptorPool descriptorPool,
-                         uint32_t count,
-                         const VkDescriptorSet *pDescriptorSets)
+panvk_FreeDescriptorSets(VkDevice _device, VkDescriptorPool descriptorPool,
+                         uint32_t count, const VkDescriptorSet *pDescriptorSets)
 {
    VK_FROM_HANDLE(panvk_device, device, _device);
    VK_FROM_HANDLE(panvk_descriptor_pool, pool, descriptorPool);
@@ -323,10 +321,10 @@
 }
 
 VkResult
-panvk_CreateSamplerYcbcrConversion(VkDevice device,
-                                   const VkSamplerYcbcrConversionCreateInfo *pCreateInfo,
-                                   const VkAllocationCallbacks *pAllocator,
-                                   VkSamplerYcbcrConversion *pYcbcrConversion)
+panvk_CreateSamplerYcbcrConversion(
+   VkDevice device, const VkSamplerYcbcrConversionCreateInfo *pCreateInfo,
+   const VkAllocationCallbacks *pAllocator,
+   VkSamplerYcbcrConversion *pYcbcrConversion)
 {
    panvk_stub();
    return VK_SUCCESS;
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_device.c mesa/src/panfrost/vulkan/panvk_device.c
--- mesa-23.0.0/src/panfrost/vulkan/panvk_device.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_device.c	2023-03-06 19:19:32.466307240 +0100
@@ -31,37 +31,36 @@
 #include "pan_bo.h"
 #include "pan_encoder.h"
 #include "pan_util.h"
-#include "vk_common_entrypoints.h"
 #include "vk_cmd_enqueue_entrypoints.h"
+#include "vk_common_entrypoints.h"
 
 #include <fcntl.h>
 #include <libsync.h>
 #include <stdbool.h>
 #include <string.h>
-#include <sys/mman.h>
-#include <sys/sysinfo.h>
 #include <unistd.h>
 #include <xf86drm.h>
+#include <sys/mman.h>
+#include <sys/sysinfo.h>
 
 #include "drm-uapi/panfrost_drm.h"
 
-#include "util/u_debug.h"
 #include "util/disk_cache.h"
 #include "util/strtod.h"
-#include "vk_format.h"
+#include "util/u_debug.h"
 #include "vk_drm_syncobj.h"
+#include "vk_format.h"
 #include "vk_util.h"
 
 #ifdef VK_USE_PLATFORM_WAYLAND_KHR
-#include <wayland-client.h>
 #include "wayland-drm-client-protocol.h"
+#include <wayland-client.h>
 #endif
 
 #include "panvk_cs.h"
 
 VkResult
-_panvk_device_set_lost(struct panvk_device *device,
-                       const char *file, int line,
+_panvk_device_set_lost(struct panvk_device *device, const char *file, int line,
                        const char *msg, ...)
 {
    /* Set the flag indicating that waits should return in finite time even
@@ -94,8 +93,8 @@
 
    memset(uuid, 0, VK_UUID_SIZE);
    memcpy(uuid, &mesa_timestamp, 4);
-   memcpy((char *) uuid + 4, &f, 2);
-   snprintf((char *) uuid + 6, VK_UUID_SIZE - 10, "pan");
+   memcpy((char *)uuid + 4, &f, 2);
+   snprintf((char *)uuid + 6, VK_UUID_SIZE - 10, "pan");
    return 0;
 }
 
@@ -113,15 +112,10 @@
 }
 
 static const struct debug_control panvk_debug_options[] = {
-   { "startup", PANVK_DEBUG_STARTUP },
-   { "nir", PANVK_DEBUG_NIR },
-   { "trace", PANVK_DEBUG_TRACE },
-   { "sync", PANVK_DEBUG_SYNC },
-   { "afbc", PANVK_DEBUG_AFBC },
-   { "linear", PANVK_DEBUG_LINEAR },
-   { "dump", PANVK_DEBUG_DUMP },
-   { NULL, 0 }
-};
+   {"startup", PANVK_DEBUG_STARTUP}, {"nir", PANVK_DEBUG_NIR},
+   {"trace", PANVK_DEBUG_TRACE},     {"sync", PANVK_DEBUG_SYNC},
+   {"afbc", PANVK_DEBUG_AFBC},       {"linear", PANVK_DEBUG_LINEAR},
+   {"dump", PANVK_DEBUG_DUMP},       {NULL, 0}};
 
 #if defined(VK_USE_PLATFORM_WAYLAND_KHR)
 #define PANVK_USE_WSI_PLATFORM
@@ -132,8 +126,8 @@
 VkResult
 panvk_EnumerateInstanceVersion(uint32_t *pApiVersion)
 {
-    *pApiVersion = PANVK_API_VERSION;
-    return VK_SUCCESS;
+   *pApiVersion = PANVK_API_VERSION;
+   return VK_SUCCESS;
 }
 
 static const struct vk_instance_extension_table panvk_instance_extensions = {
@@ -153,7 +147,7 @@
 panvk_get_device_extensions(const struct panvk_physical_device *device,
                             struct vk_device_extension_table *ext)
 {
-   *ext = (struct vk_device_extension_table) {
+   *ext = (struct vk_device_extension_table){
       .KHR_copy_commands2 = true,
       .KHR_storage_buffer_storage_class = true,
       .KHR_descriptor_update_template = true,
@@ -202,7 +196,7 @@
 
    assert(pCreateInfo->sType == VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO);
 
-   pAllocator = pAllocator ? : vk_default_allocator();
+   pAllocator = pAllocator ?: vk_default_allocator();
    instance = vk_zalloc(pAllocator, sizeof(*instance), 8,
                         VK_SYSTEM_ALLOCATION_SCOPE_INSTANCE);
    if (!instance)
@@ -210,17 +204,12 @@
 
    struct vk_instance_dispatch_table dispatch_table;
 
-   vk_instance_dispatch_table_from_entrypoints(&dispatch_table,
-                                               &panvk_instance_entrypoints,
-                                               true);
-   vk_instance_dispatch_table_from_entrypoints(&dispatch_table,
-                                               &wsi_instance_entrypoints,
-                                               false);
-   result = vk_instance_init(&instance->vk,
-                             &panvk_instance_extensions,
-                             &dispatch_table,
-                             pCreateInfo,
-                             pAllocator);
+   vk_instance_dispatch_table_from_entrypoints(
+      &dispatch_table, &panvk_instance_entrypoints, true);
+   vk_instance_dispatch_table_from_entrypoints(
+      &dispatch_table, &wsi_instance_entrypoints, false);
+   result = vk_instance_init(&instance->vk, &panvk_instance_extensions,
+                             &dispatch_table, pCreateInfo, pAllocator);
    if (result != VK_SUCCESS) {
       vk_free(pAllocator, instance);
       return vk_error(NULL, result);
@@ -230,8 +219,8 @@
       panvk_physical_device_try_create;
    instance->vk.physical_devices.destroy = panvk_destroy_physical_device;
 
-   instance->debug_flags = parse_debug_string(getenv("PANVK_DEBUG"),
-                                              panvk_debug_options);
+   instance->debug_flags =
+      parse_debug_string(getenv("PANVK_DEBUG"), panvk_debug_options);
 
    if (instance->debug_flags & PANVK_DEBUG_STARTUP)
       panvk_logi("Created an instance");
@@ -268,9 +257,10 @@
    int master_fd = -1;
 
    if (!getenv("PAN_I_WANT_A_BROKEN_VULKAN_DRIVER")) {
-      return vk_errorf(instance, VK_ERROR_INCOMPATIBLE_DRIVER,
-                       "WARNING: panvk is not a conformant vulkan implementation, "
-                       "pass PAN_I_WANT_A_BROKEN_VULKAN_DRIVER=1 if you know what you're doing.");
+      return vk_errorf(
+         instance, VK_ERROR_INCOMPATIBLE_DRIVER,
+         "WARNING: panvk is not a conformant vulkan implementation, "
+         "pass PAN_I_WANT_A_BROKEN_VULKAN_DRIVER=1 if you know what you're doing.");
    }
 
    fd = open(path, O_RDWR | O_CLOEXEC);
@@ -291,7 +281,8 @@
       drmFreeVersion(version);
       close(fd);
       return vk_errorf(instance, VK_ERROR_INCOMPATIBLE_DRIVER,
-                       "device %s does not use the panfrost kernel driver", path);
+                       "device %s does not use the panfrost kernel driver",
+                       path);
    }
 
    drmFreeVersion(version);
@@ -303,16 +294,13 @@
    panvk_get_device_extensions(device, &supported_extensions);
 
    struct vk_physical_device_dispatch_table dispatch_table;
-   vk_physical_device_dispatch_table_from_entrypoints(&dispatch_table,
-                                                      &panvk_physical_device_entrypoints,
-                                                      true);
-   vk_physical_device_dispatch_table_from_entrypoints(&dispatch_table,
-                                                      &wsi_physical_device_entrypoints,
-                                                      false);
+   vk_physical_device_dispatch_table_from_entrypoints(
+      &dispatch_table, &panvk_physical_device_entrypoints, true);
+   vk_physical_device_dispatch_table_from_entrypoints(
+      &dispatch_table, &wsi_physical_device_entrypoints, false);
 
    result = vk_physical_device_init(&device->vk, &instance->vk,
-                                    &supported_extensions,
-                                    &dispatch_table);
+                                    &supported_extensions, &dispatch_table);
 
    if (result != VK_SUCCESS) {
       vk_error(instance, result);
@@ -340,8 +328,7 @@
 
    if (device->pdev.arch <= 5) {
       result = vk_errorf(instance, VK_ERROR_INCOMPATIBLE_DRIVER,
-                         "%s not supported",
-                         device->pdev.model->name);
+                         "%s not supported", device->pdev.model->name);
       goto fail;
    }
 
@@ -407,7 +394,7 @@
                 VK_SYSTEM_ALLOCATION_SCOPE_INSTANCE);
    if (!device)
       return vk_error(instance, VK_ERROR_OUT_OF_HOST_MEMORY);
-   
+
    VkResult result = panvk_physical_device_init(device, instance, drm_device);
    if (result != VK_SUCCESS) {
       vk_free(&instance->vk.alloc, device);
@@ -422,7 +409,7 @@
 panvk_GetPhysicalDeviceFeatures2(VkPhysicalDevice physicalDevice,
                                  VkPhysicalDeviceFeatures2 *pFeatures)
 {
-   pFeatures->features = (VkPhysicalDeviceFeatures) {
+   pFeatures->features = (VkPhysicalDeviceFeatures){
       .robustBufferAccess = true,
       .fullDrawIndexUint32 = true,
       .independentBlend = true,
@@ -439,90 +426,90 @@
 
    const VkPhysicalDeviceVulkan11Features core_1_1 = {
       .sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_1_1_FEATURES,
-      .storageBuffer16BitAccess           = false,
+      .storageBuffer16BitAccess = false,
       .uniformAndStorageBuffer16BitAccess = false,
-      .storagePushConstant16              = false,
-      .storageInputOutput16               = false,
-      .multiview                          = false,
-      .multiviewGeometryShader            = false,
-      .multiviewTessellationShader        = false,
-      .variablePointersStorageBuffer      = true,
-      .variablePointers                   = true,
-      .protectedMemory                    = false,
-      .samplerYcbcrConversion             = false,
-      .shaderDrawParameters               = false,
+      .storagePushConstant16 = false,
+      .storageInputOutput16 = false,
+      .multiview = false,
+      .multiviewGeometryShader = false,
+      .multiviewTessellationShader = false,
+      .variablePointersStorageBuffer = true,
+      .variablePointers = true,
+      .protectedMemory = false,
+      .samplerYcbcrConversion = false,
+      .shaderDrawParameters = false,
    };
 
    const VkPhysicalDeviceVulkan12Features core_1_2 = {
       .sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_1_2_FEATURES,
-      .samplerMirrorClampToEdge           = false,
-      .drawIndirectCount                  = false,
-      .storageBuffer8BitAccess            = false,
-      .uniformAndStorageBuffer8BitAccess  = false,
-      .storagePushConstant8               = false,
-      .shaderBufferInt64Atomics           = false,
-      .shaderSharedInt64Atomics           = false,
-      .shaderFloat16                      = false,
-      .shaderInt8                         = false,
-
-      .descriptorIndexing                                   = false,
-      .shaderInputAttachmentArrayDynamicIndexing            = false,
-      .shaderUniformTexelBufferArrayDynamicIndexing         = false,
-      .shaderStorageTexelBufferArrayDynamicIndexing         = false,
-      .shaderUniformBufferArrayNonUniformIndexing           = false,
-      .shaderSampledImageArrayNonUniformIndexing            = false,
-      .shaderStorageBufferArrayNonUniformIndexing           = false,
-      .shaderStorageImageArrayNonUniformIndexing            = false,
-      .shaderInputAttachmentArrayNonUniformIndexing         = false,
-      .shaderUniformTexelBufferArrayNonUniformIndexing      = false,
-      .shaderStorageTexelBufferArrayNonUniformIndexing      = false,
-      .descriptorBindingUniformBufferUpdateAfterBind        = false,
-      .descriptorBindingSampledImageUpdateAfterBind         = false,
-      .descriptorBindingStorageImageUpdateAfterBind         = false,
-      .descriptorBindingStorageBufferUpdateAfterBind        = false,
-      .descriptorBindingUniformTexelBufferUpdateAfterBind   = false,
-      .descriptorBindingStorageTexelBufferUpdateAfterBind   = false,
-      .descriptorBindingUpdateUnusedWhilePending            = false,
-      .descriptorBindingPartiallyBound                      = false,
-      .descriptorBindingVariableDescriptorCount             = false,
-      .runtimeDescriptorArray                               = false,
-
-      .samplerFilterMinmax                = false,
-      .scalarBlockLayout                  = false,
-      .imagelessFramebuffer               = false,
-      .uniformBufferStandardLayout        = false,
-      .shaderSubgroupExtendedTypes        = false,
-      .separateDepthStencilLayouts        = false,
-      .hostQueryReset                     = false,
-      .timelineSemaphore                  = false,
-      .bufferDeviceAddress                = false,
-      .bufferDeviceAddressCaptureReplay   = false,
-      .bufferDeviceAddressMultiDevice     = false,
-      .vulkanMemoryModel                  = false,
-      .vulkanMemoryModelDeviceScope       = false,
+      .samplerMirrorClampToEdge = false,
+      .drawIndirectCount = false,
+      .storageBuffer8BitAccess = false,
+      .uniformAndStorageBuffer8BitAccess = false,
+      .storagePushConstant8 = false,
+      .shaderBufferInt64Atomics = false,
+      .shaderSharedInt64Atomics = false,
+      .shaderFloat16 = false,
+      .shaderInt8 = false,
+
+      .descriptorIndexing = false,
+      .shaderInputAttachmentArrayDynamicIndexing = false,
+      .shaderUniformTexelBufferArrayDynamicIndexing = false,
+      .shaderStorageTexelBufferArrayDynamicIndexing = false,
+      .shaderUniformBufferArrayNonUniformIndexing = false,
+      .shaderSampledImageArrayNonUniformIndexing = false,
+      .shaderStorageBufferArrayNonUniformIndexing = false,
+      .shaderStorageImageArrayNonUniformIndexing = false,
+      .shaderInputAttachmentArrayNonUniformIndexing = false,
+      .shaderUniformTexelBufferArrayNonUniformIndexing = false,
+      .shaderStorageTexelBufferArrayNonUniformIndexing = false,
+      .descriptorBindingUniformBufferUpdateAfterBind = false,
+      .descriptorBindingSampledImageUpdateAfterBind = false,
+      .descriptorBindingStorageImageUpdateAfterBind = false,
+      .descriptorBindingStorageBufferUpdateAfterBind = false,
+      .descriptorBindingUniformTexelBufferUpdateAfterBind = false,
+      .descriptorBindingStorageTexelBufferUpdateAfterBind = false,
+      .descriptorBindingUpdateUnusedWhilePending = false,
+      .descriptorBindingPartiallyBound = false,
+      .descriptorBindingVariableDescriptorCount = false,
+      .runtimeDescriptorArray = false,
+
+      .samplerFilterMinmax = false,
+      .scalarBlockLayout = false,
+      .imagelessFramebuffer = false,
+      .uniformBufferStandardLayout = false,
+      .shaderSubgroupExtendedTypes = false,
+      .separateDepthStencilLayouts = false,
+      .hostQueryReset = false,
+      .timelineSemaphore = false,
+      .bufferDeviceAddress = false,
+      .bufferDeviceAddressCaptureReplay = false,
+      .bufferDeviceAddressMultiDevice = false,
+      .vulkanMemoryModel = false,
+      .vulkanMemoryModelDeviceScope = false,
       .vulkanMemoryModelAvailabilityVisibilityChains = false,
-      .shaderOutputViewportIndex          = false,
-      .shaderOutputLayer                  = false,
-      .subgroupBroadcastDynamicId         = false,
+      .shaderOutputViewportIndex = false,
+      .shaderOutputLayer = false,
+      .subgroupBroadcastDynamicId = false,
    };
 
    const VkPhysicalDeviceVulkan13Features core_1_3 = {
       .sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_1_3_FEATURES,
-      .robustImageAccess                  = false,
-      .inlineUniformBlock                 = false,
+      .robustImageAccess = false,
+      .inlineUniformBlock = false,
       .descriptorBindingInlineUniformBlockUpdateAfterBind = false,
-      .pipelineCreationCacheControl       = false,
-      .privateData                        = true,
-      .shaderDemoteToHelperInvocation     = false,
-      .shaderTerminateInvocation          = false,
-      .subgroupSizeControl                = false,
-      .computeFullSubgroups               = false,
-      .synchronization2                   = true,
-      .textureCompressionASTC_HDR         = false,
+      .pipelineCreationCacheControl = false,
+      .privateData = true,
+      .shaderDemoteToHelperInvocation = false,
+      .shaderTerminateInvocation = false,
+      .subgroupSizeControl = false,
+      .computeFullSubgroups = false,
+      .synchronization2 = true,
+      .textureCompressionASTC_HDR = false,
       .shaderZeroInitializeWorkgroupMemory = false,
-      .dynamicRendering                   = false,
-      .shaderIntegerDotProduct            = false,
-      .maintenance4                       = false,
+      .dynamicRendering = false,
+      .shaderIntegerDotProduct = false,
+      .maintenance4 = false,
    };
 
    vk_foreach_struct(ext, pFeatures->pNext)
@@ -536,14 +523,14 @@
       switch (ext->sType) {
       case VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_CONDITIONAL_RENDERING_FEATURES_EXT: {
          VkPhysicalDeviceConditionalRenderingFeaturesEXT *features =
-            (VkPhysicalDeviceConditionalRenderingFeaturesEXT *) ext;
+            (VkPhysicalDeviceConditionalRenderingFeaturesEXT *)ext;
          features->conditionalRendering = false;
          features->inheritedConditionalRendering = false;
          break;
       }
       case VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_TRANSFORM_FEEDBACK_FEATURES_EXT: {
          VkPhysicalDeviceTransformFeedbackFeaturesEXT *features =
-            (VkPhysicalDeviceTransformFeedbackFeaturesEXT *) ext;
+            (VkPhysicalDeviceTransformFeedbackFeaturesEXT *)ext;
          features->transformFeedback = false;
          features->geometryStreams = false;
          break;
@@ -574,7 +561,7 @@
          break;
       }
       case VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_CUSTOM_BORDER_COLOR_FEATURES_EXT: {
-         VkPhysicalDeviceCustomBorderColorFeaturesEXT *features = (void *) ext;
+         VkPhysicalDeviceCustomBorderColorFeaturesEXT *features = (void *)ext;
          features->customBorderColors = true;
          features->customBorderColorWithoutFormat = true;
          break;
@@ -658,11 +645,12 @@
       .maxFragmentInputComponents = 128,
       .maxFragmentOutputAttachments = 8,
       .maxFragmentDualSrcAttachments = 1,
-      .maxFragmentCombinedOutputResources = MAX_RTS + max_descriptor_set_size * 2,
+      .maxFragmentCombinedOutputResources =
+         MAX_RTS + max_descriptor_set_size * 2,
       .maxComputeSharedMemorySize = 32768,
-      .maxComputeWorkGroupCount = { 65535, 65535, 65535 },
+      .maxComputeWorkGroupCount = {65535, 65535, 65535},
       .maxComputeWorkGroupInvocations = 2048,
-      .maxComputeWorkGroupSize = { 2048, 2048, 2048 },
+      .maxComputeWorkGroupSize = {2048, 2048, 2048},
       .subPixelPrecisionBits = 4 /* FIXME */,
       .subTexelPrecisionBits = 4 /* FIXME */,
       .mipmapPrecisionBits = 4 /* FIXME */,
@@ -671,8 +659,8 @@
       .maxSamplerLodBias = 16,
       .maxSamplerAnisotropy = 16,
       .maxViewports = MAX_VIEWPORTS,
-      .maxViewportDimensions = { (1 << 14), (1 << 14) },
-      .viewportBoundsRange = { INT16_MIN, INT16_MAX },
+      .maxViewportDimensions = {(1 << 14), (1 << 14)},
+      .viewportBoundsRange = {INT16_MIN, INT16_MAX},
       .viewportSubPixelBits = 8,
       .minMemoryMapAlignment = 4096, /* A page */
       .minTexelBufferOffsetAlignment = 64,
@@ -705,8 +693,8 @@
       .maxCullDistances = 8,
       .maxCombinedClipAndCullDistances = 8,
       .discreteQueuePriorities = 1,
-      .pointSizeRange = { 0.125, 4095.9375 },
-      .lineWidthRange = { 0.0, 7.9921875 },
+      .pointSizeRange = {0.125, 4095.9375},
+      .lineWidthRange = {0.0, 7.9921875},
       .pointSizeGranularity = (1.0 / 16.0),
       .lineWidthGranularity = (1.0 / 128.0),
       .strictLines = false, /* FINISHME */
@@ -716,31 +704,32 @@
       .nonCoherentAtomSize = 64,
    };
 
-   pProperties->properties = (VkPhysicalDeviceProperties) {
+   pProperties->properties = (VkPhysicalDeviceProperties){
       .apiVersion = PANVK_API_VERSION,
       .driverVersion = vk_get_driver_version(),
       .vendorID = 0, /* TODO */
       .deviceID = 0,
       .deviceType = VK_PHYSICAL_DEVICE_TYPE_INTEGRATED_GPU,
       .limits = limits,
-      .sparseProperties = { 0 },
+      .sparseProperties = {0},
    };
 
    strcpy(pProperties->properties.deviceName, pdevice->name);
-   memcpy(pProperties->properties.pipelineCacheUUID, pdevice->cache_uuid, VK_UUID_SIZE);
+   memcpy(pProperties->properties.pipelineCacheUUID, pdevice->cache_uuid,
+          VK_UUID_SIZE);
 
    VkPhysicalDeviceVulkan11Properties core_1_1 = {
       .sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_1_1_PROPERTIES,
-      .deviceLUIDValid                       = false,
-      .pointClippingBehavior                 = VK_POINT_CLIPPING_BEHAVIOR_ALL_CLIP_PLANES,
-      .maxMultiviewViewCount                 = 0,
-      .maxMultiviewInstanceIndex             = 0,
-      .protectedNoFault                      = false,
+      .deviceLUIDValid = false,
+      .pointClippingBehavior = VK_POINT_CLIPPING_BEHAVIOR_ALL_CLIP_PLANES,
+      .maxMultiviewViewCount = 0,
+      .maxMultiviewInstanceIndex = 0,
+      .protectedNoFault = false,
       /* Make sure everything is addressable by a signed 32-bit int, and
        * our largest descriptors are 96 bytes. */
-      .maxPerSetDescriptors                  = (1ull << 31) / 96,
+      .maxPerSetDescriptors = (1ull << 31) / 96,
       /* Our buffer size fields allow only this much */
-      .maxMemoryAllocationSize               = 0xFFFFFFFFull,
+      .maxMemoryAllocationSize = 0xFFFFFFFFull,
    };
    memcpy(core_1_1.driverUUID, pdevice->driver_uuid, VK_UUID_SIZE);
    memcpy(core_1_1.deviceUUID, pdevice->device_uuid, VK_UUID_SIZE);
@@ -764,7 +753,8 @@
 
       switch (ext->sType) {
       case VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_PUSH_DESCRIPTOR_PROPERTIES_KHR: {
-         VkPhysicalDevicePushDescriptorPropertiesKHR *properties = (VkPhysicalDevicePushDescriptorPropertiesKHR *)ext;
+         VkPhysicalDevicePushDescriptorPropertiesKHR *properties =
+            (VkPhysicalDevicePushDescriptorPropertiesKHR *)ext;
          properties->maxPushDescriptors = MAX_PUSH_DESCRIPTORS;
          break;
       }
@@ -782,19 +772,19 @@
 }
 
 static const VkQueueFamilyProperties panvk_queue_family_properties = {
-   .queueFlags = VK_QUEUE_GRAPHICS_BIT | VK_QUEUE_COMPUTE_BIT | VK_QUEUE_TRANSFER_BIT,
+   .queueFlags =
+      VK_QUEUE_GRAPHICS_BIT | VK_QUEUE_COMPUTE_BIT | VK_QUEUE_TRANSFER_BIT,
    .queueCount = 1,
    .timestampValidBits = 64,
-   .minImageTransferGranularity = { 1, 1, 1 },
+   .minImageTransferGranularity = {1, 1, 1},
 };
 
 void
-panvk_GetPhysicalDeviceQueueFamilyProperties2(VkPhysicalDevice physicalDevice,
-                                              uint32_t *pQueueFamilyPropertyCount,
-                                              VkQueueFamilyProperties2 *pQueueFamilyProperties)
+panvk_GetPhysicalDeviceQueueFamilyProperties2(
+   VkPhysicalDevice physicalDevice, uint32_t *pQueueFamilyPropertyCount,
+   VkQueueFamilyProperties2 *pQueueFamilyProperties)
 {
-   VK_OUTARRAY_MAKE_TYPED(VkQueueFamilyProperties2, out,
-                          pQueueFamilyProperties,
+   VK_OUTARRAY_MAKE_TYPED(VkQueueFamilyProperties2, out, pQueueFamilyProperties,
                           pQueueFamilyPropertyCount);
 
    vk_outarray_append_typed(VkQueueFamilyProperties2, &out, p)
@@ -824,10 +814,11 @@
 }
 
 void
-panvk_GetPhysicalDeviceMemoryProperties2(VkPhysicalDevice physicalDevice,
-                                         VkPhysicalDeviceMemoryProperties2 *pMemoryProperties)
+panvk_GetPhysicalDeviceMemoryProperties2(
+   VkPhysicalDevice physicalDevice,
+   VkPhysicalDeviceMemoryProperties2 *pMemoryProperties)
 {
-   pMemoryProperties->memoryProperties = (VkPhysicalDeviceMemoryProperties) {
+   pMemoryProperties->memoryProperties = (VkPhysicalDeviceMemoryProperties){
       .memoryHeapCount = 1,
       .memoryHeaps[0].size = panvk_get_system_heap_size(),
       .memoryHeaps[0].flags = VK_MEMORY_HEAP_DEVICE_LOCAL_BIT,
@@ -840,10 +831,8 @@
 }
 
 static VkResult
-panvk_queue_init(struct panvk_device *device,
-                 struct panvk_queue *queue,
-                 int idx,
-                 const VkDeviceQueueCreateInfo *create_info)
+panvk_queue_init(struct panvk_device *device, struct panvk_queue *queue,
+                 int idx, const VkDeviceQueueCreateInfo *create_info)
 {
    const struct panfrost_device *pdev = &device->physical_device->pdev;
 
@@ -863,9 +852,14 @@
    }
 
    switch (pdev->arch) {
-   case 6: queue->vk.driver_submit = panvk_v6_queue_submit; break;
-   case 7: queue->vk.driver_submit = panvk_v7_queue_submit; break;
-   default: unreachable("Invalid arch");
+   case 6:
+      queue->vk.driver_submit = panvk_v6_queue_submit;
+      break;
+   case 7:
+      queue->vk.driver_submit = panvk_v7_queue_submit;
+      break;
+   default:
+      unreachable("Invalid arch");
    }
 
    queue->sync = create.handle;
@@ -881,8 +875,7 @@
 VkResult
 panvk_CreateDevice(VkPhysicalDevice physicalDevice,
                    const VkDeviceCreateInfo *pCreateInfo,
-                   const VkAllocationCallbacks *pAllocator,
-                   VkDevice *pDevice)
+                   const VkAllocationCallbacks *pAllocator, VkDevice *pDevice)
 {
    VK_FROM_HANDLE(panvk_physical_device, physical_device, physicalDevice);
    VkResult result;
@@ -914,30 +907,23 @@
     * in the main device-level dispatch table with
     * vk_cmd_enqueue_unless_primary_Cmd*.
     */
-   vk_device_dispatch_table_from_entrypoints(&dispatch_table,
-                                             &vk_cmd_enqueue_unless_primary_device_entrypoints,
-                                             true);
+   vk_device_dispatch_table_from_entrypoints(
+      &dispatch_table, &vk_cmd_enqueue_unless_primary_device_entrypoints, true);
 
-   vk_device_dispatch_table_from_entrypoints(&dispatch_table,
-                                             dev_entrypoints,
+   vk_device_dispatch_table_from_entrypoints(&dispatch_table, dev_entrypoints,
                                              false);
    vk_device_dispatch_table_from_entrypoints(&dispatch_table,
-                                             &panvk_device_entrypoints,
-                                             false);
+                                             &panvk_device_entrypoints, false);
    vk_device_dispatch_table_from_entrypoints(&dispatch_table,
-                                             &wsi_device_entrypoints,
-                                             false);
+                                             &wsi_device_entrypoints, false);
 
    /* Populate our primary cmd_dispatch table. */
    vk_device_dispatch_table_from_entrypoints(&device->cmd_dispatch,
-                                             dev_entrypoints,
-                                             true);
+                                             dev_entrypoints, true);
    vk_device_dispatch_table_from_entrypoints(&device->cmd_dispatch,
-                                             &panvk_device_entrypoints,
-                                             false);
-   vk_device_dispatch_table_from_entrypoints(&device->cmd_dispatch,
-                                             &vk_common_device_entrypoints,
-                                             false);
+                                             &panvk_device_entrypoints, false);
+   vk_device_dispatch_table_from_entrypoints(
+      &device->cmd_dispatch, &vk_common_device_entrypoints, false);
 
    result = vk_device_init(&device->vk, &physical_device->vk, &dispatch_table,
                            pCreateInfo, pAllocator);
@@ -964,8 +950,8 @@
       uint32_t qfi = queue_create->queueFamilyIndex;
       device->queues[qfi] =
          vk_alloc(&device->vk.alloc,
-                  queue_create->queueCount * sizeof(struct panvk_queue),
-                  8, VK_SYSTEM_ALLOCATION_SCOPE_DEVICE);
+                  queue_create->queueCount * sizeof(struct panvk_queue), 8,
+                  VK_SYSTEM_ALLOCATION_SCOPE_DEVICE);
       if (!device->queues[qfi]) {
          result = VK_ERROR_OUT_OF_HOST_MEMORY;
          goto fail;
@@ -977,8 +963,8 @@
       device->queue_count[qfi] = queue_create->queueCount;
 
       for (unsigned q = 0; q < queue_create->queueCount; q++) {
-         result = panvk_queue_init(device, &device->queues[qfi][q], q,
-                                   queue_create);
+         result =
+            panvk_queue_init(device, &device->queues[qfi][q], q, queue_create);
          if (result != VK_SUCCESS)
             goto fail;
       }
@@ -1035,7 +1021,7 @@
 
    const struct panfrost_device *pdev = &queue->device->physical_device->pdev;
    struct drm_syncobj_wait wait = {
-      .handles = (uint64_t) (uintptr_t)(&queue->sync),
+      .handles = (uint64_t)(uintptr_t)(&queue->sync),
       .count_handles = 1,
       .timeout_nsec = INT64_MAX,
       .flags = DRM_SYNCOBJ_WAIT_FLAGS_WAIT_ALL,
@@ -1056,16 +1042,15 @@
    if (pLayerName)
       return vk_error(NULL, VK_ERROR_LAYER_NOT_PRESENT);
 
-   return vk_enumerate_instance_extension_properties(&panvk_instance_extensions,
-                                                     pPropertyCount, pProperties);
+   return vk_enumerate_instance_extension_properties(
+      &panvk_instance_extensions, pPropertyCount, pProperties);
 }
 
 PFN_vkVoidFunction
 panvk_GetInstanceProcAddr(VkInstance _instance, const char *pName)
 {
    VK_FROM_HANDLE(panvk_instance, instance, _instance);
-   return vk_instance_get_proc_addr(&instance->vk,
-                                    &panvk_instance_entrypoints,
+   return vk_instance_get_proc_addr(&instance->vk, &panvk_instance_entrypoints,
                                     pName);
 }
 
@@ -1088,12 +1073,10 @@
  */
 PUBLIC
 VKAPI_ATTR PFN_vkVoidFunction VKAPI_CALL
-vk_icdGetPhysicalDeviceProcAddr(VkInstance  _instance,
-                                const char* pName);
+vk_icdGetPhysicalDeviceProcAddr(VkInstance _instance, const char *pName);
 
 PFN_vkVoidFunction
-vk_icdGetPhysicalDeviceProcAddr(VkInstance  _instance,
-                                const char* pName)
+vk_icdGetPhysicalDeviceProcAddr(VkInstance _instance, const char *pName)
 {
    VK_FROM_HANDLE(panvk_instance, instance, _instance);
 
@@ -1123,17 +1106,15 @@
       return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
 
    const VkImportMemoryFdInfoKHR *fd_info =
-      vk_find_struct_const(pAllocateInfo->pNext,
-                           IMPORT_MEMORY_FD_INFO_KHR);
+      vk_find_struct_const(pAllocateInfo->pNext, IMPORT_MEMORY_FD_INFO_KHR);
 
    if (fd_info && !fd_info->handleType)
       fd_info = NULL;
 
    if (fd_info) {
-      assert(fd_info->handleType ==
-                VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD_BIT ||
-             fd_info->handleType ==
-                VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT);
+      assert(
+         fd_info->handleType == VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD_BIT ||
+         fd_info->handleType == VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT);
 
       /*
        * TODO Importing the same fd twice gives us the same handle without
@@ -1157,8 +1138,7 @@
 }
 
 void
-panvk_FreeMemory(VkDevice _device,
-                 VkDeviceMemory _mem,
+panvk_FreeMemory(VkDevice _device, VkDeviceMemory _mem,
                  const VkAllocationCallbacks *pAllocator)
 {
    VK_FROM_HANDLE(panvk_device, device, _device);
@@ -1172,12 +1152,8 @@
 }
 
 VkResult
-panvk_MapMemory(VkDevice _device,
-                VkDeviceMemory _memory,
-                VkDeviceSize offset,
-                VkDeviceSize size,
-                VkMemoryMapFlags flags,
-                void **ppData)
+panvk_MapMemory(VkDevice _device, VkDeviceMemory _memory, VkDeviceSize offset,
+                VkDeviceSize size, VkMemoryMapFlags flags, void **ppData)
 {
    VK_FROM_HANDLE(panvk_device, device, _device);
    VK_FROM_HANDLE(panvk_device_memory, mem, _memory);
@@ -1206,16 +1182,14 @@
 }
 
 VkResult
-panvk_FlushMappedMemoryRanges(VkDevice _device,
-                              uint32_t memoryRangeCount,
+panvk_FlushMappedMemoryRanges(VkDevice _device, uint32_t memoryRangeCount,
                               const VkMappedMemoryRange *pMemoryRanges)
 {
    return VK_SUCCESS;
 }
 
 VkResult
-panvk_InvalidateMappedMemoryRanges(VkDevice _device,
-                                   uint32_t memoryRangeCount,
+panvk_InvalidateMappedMemoryRanges(VkDevice _device, uint32_t memoryRangeCount,
                                    const VkMappedMemoryRange *pMemoryRanges)
 {
    return VK_SUCCESS;
@@ -1238,8 +1212,8 @@
 
 void
 panvk_GetImageMemoryRequirements2(VkDevice device,
-                                 const VkImageMemoryRequirementsInfo2 *pInfo,
-                                 VkMemoryRequirements2 *pMemoryRequirements)
+                                  const VkImageMemoryRequirementsInfo2 *pInfo,
+                                  VkMemoryRequirements2 *pMemoryRequirements)
 {
    VK_FROM_HANDLE(panvk_image, image, pInfo->image);
 
@@ -1252,25 +1226,23 @@
 }
 
 void
-panvk_GetImageSparseMemoryRequirements2(VkDevice device,
-                                        const VkImageSparseMemoryRequirementsInfo2 *pInfo,
-                                        uint32_t *pSparseMemoryRequirementCount,
-                                        VkSparseImageMemoryRequirements2 *pSparseMemoryRequirements)
+panvk_GetImageSparseMemoryRequirements2(
+   VkDevice device, const VkImageSparseMemoryRequirementsInfo2 *pInfo,
+   uint32_t *pSparseMemoryRequirementCount,
+   VkSparseImageMemoryRequirements2 *pSparseMemoryRequirements)
 {
    panvk_stub();
 }
 
 void
-panvk_GetDeviceMemoryCommitment(VkDevice device,
-                                VkDeviceMemory memory,
+panvk_GetDeviceMemoryCommitment(VkDevice device, VkDeviceMemory memory,
                                 VkDeviceSize *pCommittedMemoryInBytes)
 {
    *pCommittedMemoryInBytes = 0;
 }
 
 VkResult
-panvk_BindBufferMemory2(VkDevice device,
-                        uint32_t bindInfoCount,
+panvk_BindBufferMemory2(VkDevice device, uint32_t bindInfoCount,
                         const VkBindBufferMemoryInfo *pBindInfos)
 {
    for (uint32_t i = 0; i < bindInfoCount; ++i) {
@@ -1288,8 +1260,7 @@
 }
 
 VkResult
-panvk_BindImageMemory2(VkDevice device,
-                       uint32_t bindInfoCount,
+panvk_BindImageMemory2(VkDevice device, uint32_t bindInfoCount,
                        const VkBindImageMemoryInfo *pBindInfos)
 {
    for (uint32_t i = 0; i < bindInfoCount; ++i) {
@@ -1301,14 +1272,18 @@
          image->pimage.data.offset = pBindInfos[i].memoryOffset;
          /* Reset the AFBC headers */
          if (drm_is_afbc(image->pimage.layout.modifier)) {
-            void *base = image->pimage.data.bo->ptr.cpu + image->pimage.data.offset;
+            void *base =
+               image->pimage.data.bo->ptr.cpu + image->pimage.data.offset;
 
-            for (unsigned layer = 0; layer < image->pimage.layout.array_size; layer++) {
-               for (unsigned level = 0; level < image->pimage.layout.nr_slices; level++) {
+            for (unsigned layer = 0; layer < image->pimage.layout.array_size;
+                 layer++) {
+               for (unsigned level = 0; level < image->pimage.layout.nr_slices;
+                    level++) {
                   void *header = base +
                                  (layer * image->pimage.layout.array_stride) +
                                  image->pimage.layout.slices[level].offset;
-                  memset(header, 0, image->pimage.layout.slices[level].afbc.header_size);
+                  memset(header, 0,
+                         image->pimage.layout.slices[level].afbc.header_size);
                }
             }
          }
@@ -1322,16 +1297,13 @@
 }
 
 VkResult
-panvk_CreateEvent(VkDevice _device,
-                  const VkEventCreateInfo *pCreateInfo,
-                  const VkAllocationCallbacks *pAllocator,
-                  VkEvent *pEvent)
+panvk_CreateEvent(VkDevice _device, const VkEventCreateInfo *pCreateInfo,
+                  const VkAllocationCallbacks *pAllocator, VkEvent *pEvent)
 {
    VK_FROM_HANDLE(panvk_device, device, _device);
    const struct panfrost_device *pdev = &device->physical_device->pdev;
-   struct panvk_event *event =
-      vk_object_zalloc(&device->vk, pAllocator, sizeof(*event),
-                       VK_OBJECT_TYPE_EVENT);
+   struct panvk_event *event = vk_object_zalloc(
+      &device->vk, pAllocator, sizeof(*event), VK_OBJECT_TYPE_EVENT);
    if (!event)
       return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
 
@@ -1350,8 +1322,7 @@
 }
 
 void
-panvk_DestroyEvent(VkDevice _device,
-                   VkEvent _event,
+panvk_DestroyEvent(VkDevice _device, VkEvent _event,
                    const VkAllocationCallbacks *pAllocator)
 {
    VK_FROM_HANDLE(panvk_device, device, _device);
@@ -1361,7 +1332,7 @@
    if (!event)
       return;
 
-   struct drm_syncobj_destroy destroy = { .handle = event->syncobj };
+   struct drm_syncobj_destroy destroy = {.handle = event->syncobj};
    drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_DESTROY, &destroy);
 
    vk_object_free(&device->vk, pAllocator, event);
@@ -1376,7 +1347,7 @@
    bool signaled;
 
    struct drm_syncobj_wait wait = {
-      .handles = (uintptr_t) &event->syncobj,
+      .handles = (uintptr_t)&event->syncobj,
       .count_handles = 1,
       .timeout_nsec = 0,
       .flags = DRM_SYNCOBJ_WAIT_FLAGS_WAIT_FOR_SUBMIT,
@@ -1404,9 +1375,8 @@
    const struct panfrost_device *pdev = &device->physical_device->pdev;
 
    struct drm_syncobj_array objs = {
-      .handles = (uint64_t) (uintptr_t) &event->syncobj,
-      .count_handles = 1
-   };
+      .handles = (uint64_t)(uintptr_t)&event->syncobj,
+      .count_handles = 1};
 
    /* This is going to just replace the fence for this syncobj with one that
     * is already in signaled state. This won't be a problem because the spec
@@ -1417,7 +1387,7 @@
    if (drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_SIGNAL, &objs))
       return VK_ERROR_DEVICE_LOST;
 
-  return VK_SUCCESS;
+   return VK_SUCCESS;
 }
 
 VkResult
@@ -1428,29 +1398,26 @@
    const struct panfrost_device *pdev = &device->physical_device->pdev;
 
    struct drm_syncobj_array objs = {
-      .handles = (uint64_t) (uintptr_t) &event->syncobj,
-      .count_handles = 1
-   };
+      .handles = (uint64_t)(uintptr_t)&event->syncobj,
+      .count_handles = 1};
 
    if (drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_RESET, &objs))
       return VK_ERROR_DEVICE_LOST;
 
-  return VK_SUCCESS;
+   return VK_SUCCESS;
 }
 
 VkResult
-panvk_CreateBuffer(VkDevice _device,
-                   const VkBufferCreateInfo *pCreateInfo,
-                   const VkAllocationCallbacks *pAllocator,
-                   VkBuffer *pBuffer)
+panvk_CreateBuffer(VkDevice _device, const VkBufferCreateInfo *pCreateInfo,
+                   const VkAllocationCallbacks *pAllocator, VkBuffer *pBuffer)
 {
    VK_FROM_HANDLE(panvk_device, device, _device);
    struct panvk_buffer *buffer;
 
    assert(pCreateInfo->sType == VK_STRUCTURE_TYPE_BUFFER_CREATE_INFO);
 
-   buffer = vk_buffer_create(&device->vk, pCreateInfo,
-                             pAllocator, sizeof(*buffer));
+   buffer =
+      vk_buffer_create(&device->vk, pCreateInfo, pAllocator, sizeof(*buffer));
    if (buffer == NULL)
       return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
 
@@ -1460,8 +1427,7 @@
 }
 
 void
-panvk_DestroyBuffer(VkDevice _device,
-                    VkBuffer _buffer,
+panvk_DestroyBuffer(VkDevice _device, VkBuffer _buffer,
                     const VkAllocationCallbacks *pAllocator)
 {
    VK_FROM_HANDLE(panvk_device, device, _device);
@@ -1506,8 +1472,7 @@
 }
 
 void
-panvk_DestroyFramebuffer(VkDevice _device,
-                         VkFramebuffer _fb,
+panvk_DestroyFramebuffer(VkDevice _device, VkFramebuffer _fb,
                          const VkAllocationCallbacks *pAllocator)
 {
    VK_FROM_HANDLE(panvk_device, device, _device);
@@ -1518,8 +1483,7 @@
 }
 
 void
-panvk_DestroySampler(VkDevice _device,
-                     VkSampler _sampler,
+panvk_DestroySampler(VkDevice _device, VkSampler _sampler,
                      const VkAllocationCallbacks *pAllocator)
 {
    VK_FROM_HANDLE(panvk_device, device, _device);
@@ -1574,9 +1538,9 @@
     *
     *    - Loader interface v4 differs from v3 in:
     *        - The ICD must implement vk_icdGetPhysicalDeviceProcAddr().
-    * 
+    *
     *    - Loader interface v5 differs from v4 in:
-    *        - The ICD must support 1.1 and must not return 
+    *        - The ICD must support 1.1 and must not return
     *          VK_ERROR_INCOMPATIBLE_DRIVER from vkCreateInstance() unless a
     *          Vulkan Loader with interface v4 or smaller is being used and the
     *          application provides an API version that is greater than 1.0.
@@ -1586,8 +1550,7 @@
 }
 
 VkResult
-panvk_GetMemoryFdKHR(VkDevice _device,
-                     const VkMemoryGetFdInfoKHR *pGetFdInfo,
+panvk_GetMemoryFdKHR(VkDevice _device, const VkMemoryGetFdInfoKHR *pGetFdInfo,
                      int *pFd)
 {
    VK_FROM_HANDLE(panvk_device, device, _device);
@@ -1596,8 +1559,9 @@
    assert(pGetFdInfo->sType == VK_STRUCTURE_TYPE_MEMORY_GET_FD_INFO_KHR);
 
    /* At the moment, we support only the below handle types. */
-   assert(pGetFdInfo->handleType == VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD_BIT ||
-          pGetFdInfo->handleType == VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT);
+   assert(
+      pGetFdInfo->handleType == VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD_BIT ||
+      pGetFdInfo->handleType == VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT);
 
    int prime_fd = panfrost_bo_export(memory->bo);
    if (prime_fd < 0)
@@ -1619,12 +1583,15 @@
 }
 
 void
-panvk_GetPhysicalDeviceExternalSemaphoreProperties(VkPhysicalDevice physicalDevice,
-                                                   const VkPhysicalDeviceExternalSemaphoreInfo *pExternalSemaphoreInfo,
-                                                   VkExternalSemaphoreProperties *pExternalSemaphoreProperties)
-{
-   if ((pExternalSemaphoreInfo->handleType == VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_FD_BIT ||
-        pExternalSemaphoreInfo->handleType == VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_SYNC_FD_BIT)) {
+panvk_GetPhysicalDeviceExternalSemaphoreProperties(
+   VkPhysicalDevice physicalDevice,
+   const VkPhysicalDeviceExternalSemaphoreInfo *pExternalSemaphoreInfo,
+   VkExternalSemaphoreProperties *pExternalSemaphoreProperties)
+{
+   if ((pExternalSemaphoreInfo->handleType ==
+           VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_FD_BIT ||
+        pExternalSemaphoreInfo->handleType ==
+           VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_SYNC_FD_BIT)) {
       pExternalSemaphoreProperties->exportFromImportedHandleTypes =
          VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_FD_BIT |
          VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_SYNC_FD_BIT;
@@ -1642,11 +1609,25 @@
 }
 
 void
-panvk_GetPhysicalDeviceExternalFenceProperties(VkPhysicalDevice physicalDevice,
-                                               const VkPhysicalDeviceExternalFenceInfo *pExternalFenceInfo,
-                                               VkExternalFenceProperties *pExternalFenceProperties)
+panvk_GetPhysicalDeviceExternalFenceProperties(
+   VkPhysicalDevice physicalDevice,
+   const VkPhysicalDeviceExternalFenceInfo *pExternalFenceInfo,
+   VkExternalFenceProperties *pExternalFenceProperties)
 {
    pExternalFenceProperties->exportFromImportedHandleTypes = 0;
    pExternalFenceProperties->compatibleHandleTypes = 0;
    pExternalFenceProperties->externalFenceFeatures = 0;
 }
+
+void
+panvk_GetDeviceGroupPeerMemoryFeatures(
+   VkDevice device, uint32_t heapIndex, uint32_t localDeviceIndex,
+   uint32_t remoteDeviceIndex, VkPeerMemoryFeatureFlags *pPeerMemoryFeatures)
+{
+   assert(localDeviceIndex == remoteDeviceIndex);
+
+   *pPeerMemoryFeatures = VK_PEER_MEMORY_FEATURE_COPY_SRC_BIT |
+                          VK_PEER_MEMORY_FEATURE_COPY_DST_BIT |
+                          VK_PEER_MEMORY_FEATURE_GENERIC_SRC_BIT |
+                          VK_PEER_MEMORY_FEATURE_GENERIC_DST_BIT;
+}
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_formats.c mesa/src/panfrost/vulkan/panvk_formats.c
--- mesa-23.0.0/src/panfrost/vulkan/panvk_formats.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_formats.c	2023-03-06 19:19:32.470307266 +0100
@@ -27,18 +27,17 @@
 
 #include "panvk_private.h"
 
+#include "panfrost/lib/pan_texture.h"
 #include "util/format_r11g11b10f.h"
 #include "util/format_srgb.h"
 #include "util/half_float.h"
 #include "vulkan/util/vk_format.h"
 #include "vk_format.h"
 #include "vk_util.h"
-#include "panfrost/lib/pan_texture.h"
 
 static void
 get_format_properties(struct panvk_physical_device *physical_device,
-                      VkFormat format,
-                      VkFormatProperties *out_properties)
+                      VkFormat format, VkFormatProperties *out_properties)
 {
    struct panfrost_device *pdev = &physical_device->pdev;
    VkFormatFeatureFlags tex = 0, buffer = 0;
@@ -59,8 +58,8 @@
    if (util_format_is_compressed(pfmt))
       goto end;
 
-   buffer |= VK_FORMAT_FEATURE_TRANSFER_SRC_BIT |
-             VK_FORMAT_FEATURE_TRANSFER_DST_BIT;
+   buffer |=
+      VK_FORMAT_FEATURE_TRANSFER_SRC_BIT | VK_FORMAT_FEATURE_TRANSFER_DST_BIT;
 
    if (fmt.bind & PIPE_BIND_VERTEX_BUFFER)
       buffer |= VK_FORMAT_FEATURE_VERTEX_BUFFER_BIT;
@@ -73,8 +72,7 @@
              VK_FORMAT_FEATURE_MIDPOINT_CHROMA_SAMPLES_BIT;
 
       /* Integer formats only support nearest filtering */
-      if (!util_format_is_scaled(pfmt) &&
-          !util_format_is_pure_integer(pfmt))
+      if (!util_format_is_scaled(pfmt) && !util_format_is_pure_integer(pfmt))
          tex |= VK_FORMAT_FEATURE_SAMPLED_IMAGE_FILTER_LINEAR_BIT;
 
       buffer |= VK_FORMAT_FEATURE_UNIFORM_TEXEL_BUFFER_BIT;
@@ -94,7 +92,7 @@
    }
 
    if (fmt.bind & PIPE_BIND_DEPTH_STENCIL)
-         tex |= VK_FORMAT_FEATURE_DEPTH_STENCIL_ATTACHMENT_BIT;
+      tex |= VK_FORMAT_FEATURE_DEPTH_STENCIL_ATTACHMENT_BIT;
 
 end:
    out_properties->linearTilingFeatures = tex;
@@ -104,8 +102,8 @@
 
 void
 panvk_GetPhysicalDeviceFormatProperties(VkPhysicalDevice physicalDevice,
-                                      VkFormat format,
-                                      VkFormatProperties *pFormatProperties)
+                                        VkFormat format,
+                                        VkFormatProperties *pFormatProperties)
 {
    VK_FROM_HANDLE(panvk_physical_device, physical_device, physicalDevice);
 
@@ -122,14 +120,16 @@
    get_format_properties(physical_device, format,
                          &pFormatProperties->formatProperties);
 
-   VkDrmFormatModifierPropertiesListEXT *list =
-      vk_find_struct(pFormatProperties->pNext, DRM_FORMAT_MODIFIER_PROPERTIES_LIST_EXT);
+   VkDrmFormatModifierPropertiesListEXT *list = vk_find_struct(
+      pFormatProperties->pNext, DRM_FORMAT_MODIFIER_PROPERTIES_LIST_EXT);
    if (list) {
       VK_OUTARRAY_MAKE_TYPED(VkDrmFormatModifierPropertiesEXT, out,
                              list->pDrmFormatModifierProperties,
                              &list->drmFormatModifierCount);
 
-      vk_outarray_append_typed(VkDrmFormatModifierProperties2EXT, &out, mod_props) {
+      vk_outarray_append_typed(VkDrmFormatModifierProperties2EXT, &out,
+                               mod_props)
+      {
          mod_props->drmFormatModifier = DRM_FORMAT_MOD_LINEAR;
          mod_props->drmFormatModifierPlaneCount = 1;
       }
@@ -169,7 +169,8 @@
       if (util_format_is_depth_or_stencil(format))
          goto unsupported;
 
-      assert(format_props.optimalTilingFeatures == format_props.linearTilingFeatures);
+      assert(format_props.optimalTilingFeatures ==
+             format_props.linearTilingFeatures);
       FALLTHROUGH;
    case VK_IMAGE_TILING_OPTIMAL:
       format_feature_flags = format_props.optimalTilingFeatures;
@@ -246,7 +247,7 @@
       }
    }
 
-   *pImageFormatProperties = (VkImageFormatProperties) {
+   *pImageFormatProperties = (VkImageFormatProperties){
       .maxExtent = maxExtent,
       .maxMipLevels = maxMipLevels,
       .maxArrayLayers = maxArraySize,
@@ -263,8 +264,8 @@
 
    return VK_SUCCESS;
 unsupported:
-   *pImageFormatProperties = (VkImageFormatProperties) {
-      .maxExtent = { 0, 0, 0 },
+   *pImageFormatProperties = (VkImageFormatProperties){
+      .maxExtent = {0, 0, 0},
       .maxMipLevels = 0,
       .maxArrayLayers = 0,
       .sampleCounts = 0,
@@ -274,15 +275,12 @@
    return VK_ERROR_FORMAT_NOT_SUPPORTED;
 }
 
-
 VkResult
-panvk_GetPhysicalDeviceImageFormatProperties(VkPhysicalDevice physicalDevice,
-                                            VkFormat format,
-                                            VkImageType type,
-                                            VkImageTiling tiling,
-                                            VkImageUsageFlags usage,
-                                            VkImageCreateFlags createFlags,
-                                            VkImageFormatProperties *pImageFormatProperties)
+panvk_GetPhysicalDeviceImageFormatProperties(
+   VkPhysicalDevice physicalDevice, VkFormat format, VkImageType type,
+   VkImageTiling tiling, VkImageUsageFlags usage,
+   VkImageCreateFlags createFlags,
+   VkImageFormatProperties *pImageFormatProperties)
 {
    VK_FROM_HANDLE(panvk_physical_device, physical_device, physicalDevice);
 
@@ -301,10 +299,11 @@
 }
 
 static VkResult
-panvk_get_external_image_format_properties(const struct panvk_physical_device *physical_device,
-                                           const VkPhysicalDeviceImageFormatInfo2 *pImageFormatInfo,
-                                           VkExternalMemoryHandleTypeFlagBits handleType,
-                                           VkExternalMemoryProperties *external_properties)
+panvk_get_external_image_format_properties(
+   const struct panvk_physical_device *physical_device,
+   const VkPhysicalDeviceImageFormatInfo2 *pImageFormatInfo,
+   VkExternalMemoryHandleTypeFlagBits handleType,
+   VkExternalMemoryProperties *external_properties)
 {
    VkExternalMemoryFeatureFlagBits flags = 0;
    VkExternalMemoryHandleTypeFlags export_flags = 0;
@@ -330,9 +329,10 @@
             VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT;
          break;
       default:
-         return vk_errorf(physical_device, VK_ERROR_FORMAT_NOT_SUPPORTED,
-                          "VkExternalMemoryTypeFlagBits(0x%x) unsupported for VkImageType(%d)",
-                          handleType, pImageFormatInfo->type);
+         return vk_errorf(
+            physical_device, VK_ERROR_FORMAT_NOT_SUPPORTED,
+            "VkExternalMemoryTypeFlagBits(0x%x) unsupported for VkImageType(%d)",
+            handleType, pImageFormatInfo->type);
       }
       break;
    case VK_EXTERNAL_MEMORY_HANDLE_TYPE_HOST_ALLOCATION_BIT_EXT:
@@ -345,7 +345,7 @@
                        handleType);
    }
 
-   *external_properties = (VkExternalMemoryProperties) {
+   *external_properties = (VkExternalMemoryProperties){
       .externalMemoryFeatures = flags,
       .exportFromImportedHandleTypes = export_flags,
       .compatibleHandleTypes = compat_flags,
@@ -355,9 +355,10 @@
 }
 
 VkResult
-panvk_GetPhysicalDeviceImageFormatProperties2(VkPhysicalDevice physicalDevice,
-                                              const VkPhysicalDeviceImageFormatInfo2 *base_info,
-                                              VkImageFormatProperties2 *base_props)
+panvk_GetPhysicalDeviceImageFormatProperties2(
+   VkPhysicalDevice physicalDevice,
+   const VkPhysicalDeviceImageFormatInfo2 *base_info,
+   VkImageFormatProperties2 *base_props)
 {
    VK_FROM_HANDLE(panvk_physical_device, physical_device, physicalDevice);
    const VkPhysicalDeviceExternalImageFormatInfo *external_info = NULL;
@@ -379,10 +380,10 @@
    {
       switch (s->sType) {
       case VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_EXTERNAL_IMAGE_FORMAT_INFO:
-         external_info = (const void *) s;
+         external_info = (const void *)s;
          break;
       case VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_IMAGE_VIEW_IMAGE_FORMAT_INFO_EXT:
-         image_view_info = (const void *) s;
+         image_view_info = (const void *)s;
          break;
       default:
          break;
@@ -394,13 +395,13 @@
    {
       switch (s->sType) {
       case VK_STRUCTURE_TYPE_EXTERNAL_IMAGE_FORMAT_PROPERTIES:
-         external_props = (void *) s;
+         external_props = (void *)s;
          break;
       case VK_STRUCTURE_TYPE_FILTER_CUBIC_IMAGE_VIEW_IMAGE_FORMAT_PROPERTIES_EXT:
-         cubic_props = (void *) s;
+         cubic_props = (void *)s;
          break;
       case VK_STRUCTURE_TYPE_SAMPLER_YCBCR_CONVERSION_IMAGE_FORMAT_PROPERTIES:
-         ycbcr_props = (void *) s;
+         ycbcr_props = (void *)s;
          break;
       default:
          break;
@@ -414,10 +415,9 @@
     *    present and VkExternalImageFormatProperties will be ignored.
     */
    if (external_info && external_info->handleType != 0) {
-      result = panvk_get_external_image_format_properties(physical_device,
-                                                          base_info,
-                                                          external_info->handleType,
-                                                          &external_props->externalMemoryProperties);
+      result = panvk_get_external_image_format_properties(
+         physical_device, base_info, external_info->handleType,
+         &external_props->externalMemoryProperties);
       if (result != VK_SUCCESS)
          goto fail;
    }
@@ -428,7 +428,8 @@
        */
       if ((image_view_info->imageViewType == VK_IMAGE_VIEW_TYPE_2D ||
            image_view_info->imageViewType == VK_IMAGE_VIEW_TYPE_2D_ARRAY) &&
-          (format_feature_flags & VK_FORMAT_FEATURE_SAMPLED_IMAGE_FILTER_CUBIC_BIT_EXT)) {
+          (format_feature_flags &
+           VK_FORMAT_FEATURE_SAMPLED_IMAGE_FILTER_CUBIC_BIT_EXT)) {
          cubic_props->filterCubic = true;
          cubic_props->filterCubicMinmax = true;
       } else {
@@ -451,40 +452,37 @@
        *    the implementation for use in vkCreateImage, then all members of
        *    imageFormatProperties will be filled with zero.
        */
-      base_props->imageFormatProperties = (VkImageFormatProperties) {};
+      base_props->imageFormatProperties = (VkImageFormatProperties){};
    }
 
    return result;
 }
 
 void
-panvk_GetPhysicalDeviceSparseImageFormatProperties(VkPhysicalDevice physicalDevice,
-                                                   VkFormat format,
-                                                   VkImageType type,
-                                                   uint32_t samples,
-                                                   VkImageUsageFlags usage,
-                                                   VkImageTiling tiling,
-                                                   uint32_t *pNumProperties,
-                                                   VkSparseImageFormatProperties *pProperties)
+panvk_GetPhysicalDeviceSparseImageFormatProperties(
+   VkPhysicalDevice physicalDevice, VkFormat format, VkImageType type,
+   uint32_t samples, VkImageUsageFlags usage, VkImageTiling tiling,
+   uint32_t *pNumProperties, VkSparseImageFormatProperties *pProperties)
 {
    /* Sparse images are not yet supported. */
    *pNumProperties = 0;
 }
 
 void
-panvk_GetPhysicalDeviceSparseImageFormatProperties2(VkPhysicalDevice physicalDevice,
-                                                    const VkPhysicalDeviceSparseImageFormatInfo2 *pFormatInfo,
-                                                    uint32_t *pPropertyCount,
-                                                    VkSparseImageFormatProperties2 *pProperties)
+panvk_GetPhysicalDeviceSparseImageFormatProperties2(
+   VkPhysicalDevice physicalDevice,
+   const VkPhysicalDeviceSparseImageFormatInfo2 *pFormatInfo,
+   uint32_t *pPropertyCount, VkSparseImageFormatProperties2 *pProperties)
 {
    /* Sparse images are not yet supported. */
    *pPropertyCount = 0;
 }
 
 void
-panvk_GetPhysicalDeviceExternalBufferProperties(VkPhysicalDevice physicalDevice,
-                                                const VkPhysicalDeviceExternalBufferInfo *pExternalBufferInfo,
-                                                VkExternalBufferProperties *pExternalBufferProperties)
+panvk_GetPhysicalDeviceExternalBufferProperties(
+   VkPhysicalDevice physicalDevice,
+   const VkPhysicalDeviceExternalBufferInfo *pExternalBufferInfo,
+   VkExternalBufferProperties *pExternalBufferProperties)
 {
    panvk_stub();
 }
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_image.c mesa/src/panfrost/vulkan/panvk_image.c
--- mesa-23.0.0/src/panfrost/vulkan/panvk_image.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_image.c	2023-03-06 19:19:32.473307286 +0100
@@ -28,12 +28,12 @@
 
 #include "panvk_private.h"
 
-#include "util/u_debug.h"
+#include "drm-uapi/drm_fourcc.h"
 #include "util/u_atomic.h"
+#include "util/u_debug.h"
 #include "vk_format.h"
 #include "vk_object.h"
 #include "vk_util.h"
-#include "drm-uapi/drm_fourcc.h"
 
 unsigned
 panvk_image_get_plane_size(const struct panvk_image *image, unsigned plane)
@@ -53,20 +53,21 @@
 panvk_image_type_to_mali_tex_dim(VkImageType type)
 {
    switch (type) {
-   case VK_IMAGE_TYPE_1D: return MALI_TEXTURE_DIMENSION_1D;
-   case VK_IMAGE_TYPE_2D: return MALI_TEXTURE_DIMENSION_2D;
-   case VK_IMAGE_TYPE_3D: return MALI_TEXTURE_DIMENSION_3D;
-   default: unreachable("Invalid image type");
+   case VK_IMAGE_TYPE_1D:
+      return MALI_TEXTURE_DIMENSION_1D;
+   case VK_IMAGE_TYPE_2D:
+      return MALI_TEXTURE_DIMENSION_2D;
+   case VK_IMAGE_TYPE_3D:
+      return MALI_TEXTURE_DIMENSION_3D;
+   default:
+      unreachable("Invalid image type");
    }
 }
 
 static VkResult
-panvk_image_create(VkDevice _device,
-                   const VkImageCreateInfo *pCreateInfo,
-                   const VkAllocationCallbacks *alloc,
-                   VkImage *pImage,
-                   uint64_t modifier,
-                   const VkSubresourceLayout *plane_layouts)
+panvk_image_create(VkDevice _device, const VkImageCreateInfo *pCreateInfo,
+                   const VkAllocationCallbacks *alloc, VkImage *pImage,
+                   uint64_t modifier, const VkSubresourceLayout *plane_layouts)
 {
    VK_FROM_HANDLE(panvk_device, device, _device);
    struct panvk_image *image = NULL;
@@ -75,7 +76,7 @@
    if (!image)
       return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
 
-   image->pimage.layout = (struct pan_image_layout) {
+   image->pimage.layout = (struct pan_image_layout){
       .modifier = modifier,
       .format = vk_format_to_pipe_format(image->vk.format),
       .dim = panvk_image_type_to_mali_tex_dim(image->vk.image_type),
@@ -94,15 +95,16 @@
 }
 
 static uint64_t
-panvk_image_select_mod(VkDevice _device,
-                       const VkImageCreateInfo *pCreateInfo,
+panvk_image_select_mod(VkDevice _device, const VkImageCreateInfo *pCreateInfo,
                        const VkSubresourceLayout **plane_layouts)
 {
    VK_FROM_HANDLE(panvk_device, device, _device);
    const struct panfrost_device *pdev = &device->physical_device->pdev;
    enum pipe_format fmt = vk_format_to_pipe_format(pCreateInfo->format);
-   bool noafbc = !(device->physical_device->instance->debug_flags & PANVK_DEBUG_AFBC);
-   bool linear = device->physical_device->instance->debug_flags & PANVK_DEBUG_LINEAR;
+   bool noafbc =
+      !(device->physical_device->instance->debug_flags & PANVK_DEBUG_AFBC);
+   bool linear =
+      device->physical_device->instance->debug_flags & PANVK_DEBUG_LINEAR;
 
    *plane_layouts = NULL;
 
@@ -114,8 +116,9 @@
          vk_find_struct_const(pCreateInfo->pNext,
                               IMAGE_DRM_FORMAT_MODIFIER_LIST_CREATE_INFO_EXT);
       const VkImageDrmFormatModifierExplicitCreateInfoEXT *drm_explicit_info =
-         vk_find_struct_const(pCreateInfo->pNext,
-                              IMAGE_DRM_FORMAT_MODIFIER_EXPLICIT_CREATE_INFO_EXT);
+         vk_find_struct_const(
+            pCreateInfo->pNext,
+            IMAGE_DRM_FORMAT_MODIFIER_EXPLICIT_CREATE_INFO_EXT);
 
       assert(mod_info || drm_explicit_info);
 
@@ -178,8 +181,8 @@
    if (noafbc)
       return DRM_FORMAT_MOD_ARM_16X16_BLOCK_U_INTERLEAVED;
 
-   uint64_t afbc_type = AFBC_FORMAT_MOD_BLOCK_SIZE_16x16 |
-                        AFBC_FORMAT_MOD_SPARSE;
+   uint64_t afbc_type =
+      AFBC_FORMAT_MOD_BLOCK_SIZE_16x16 | AFBC_FORMAT_MOD_SPARSE;
 
    if (panfrost_afbc_can_ytr(fmt))
       afbc_type |= AFBC_FORMAT_MOD_YTR;
@@ -188,20 +191,19 @@
 }
 
 VkResult
-panvk_CreateImage(VkDevice device,
-                  const VkImageCreateInfo *pCreateInfo,
-                  const VkAllocationCallbacks *pAllocator,
-                  VkImage *pImage)
+panvk_CreateImage(VkDevice device, const VkImageCreateInfo *pCreateInfo,
+                  const VkAllocationCallbacks *pAllocator, VkImage *pImage)
 {
    const VkSubresourceLayout *plane_layouts;
-   uint64_t modifier = panvk_image_select_mod(device, pCreateInfo, &plane_layouts);
+   uint64_t modifier =
+      panvk_image_select_mod(device, pCreateInfo, &plane_layouts);
 
-   return panvk_image_create(device, pCreateInfo, pAllocator, pImage, modifier, plane_layouts);
+   return panvk_image_create(device, pCreateInfo, pAllocator, pImage, modifier,
+                             plane_layouts);
 }
 
 void
-panvk_DestroyImage(VkDevice _device,
-                   VkImage _image,
+panvk_DestroyImage(VkDevice _device, VkImage _image,
                    const VkAllocationCallbacks *pAllocator)
 {
    VK_FROM_HANDLE(panvk_device, device, _device);
@@ -229,22 +231,21 @@
 }
 
 void
-panvk_GetImageSubresourceLayout(VkDevice _device,
-                                VkImage _image,
+panvk_GetImageSubresourceLayout(VkDevice _device, VkImage _image,
                                 const VkImageSubresource *pSubresource,
                                 VkSubresourceLayout *pLayout)
 {
    VK_FROM_HANDLE(panvk_image, image, _image);
 
-   unsigned plane = panvk_plane_index(image->vk.format, pSubresource->aspectMask);
+   unsigned plane =
+      panvk_plane_index(image->vk.format, pSubresource->aspectMask);
    assert(plane < PANVK_MAX_PLANES);
 
    const struct pan_image_slice_layout *slice_layout =
       &image->pimage.layout.slices[pSubresource->mipLevel];
 
-   pLayout->offset = slice_layout->offset +
-                     (pSubresource->arrayLayer *
-                      image->pimage.layout.array_stride);
+   pLayout->offset = slice_layout->offset + (pSubresource->arrayLayer *
+                                             image->pimage.layout.array_stride);
    pLayout->size = slice_layout->size;
    pLayout->rowPitch = slice_layout->row_stride;
    pLayout->arrayPitch = image->pimage.layout.array_stride;
@@ -252,8 +253,7 @@
 }
 
 void
-panvk_DestroyImageView(VkDevice _device,
-                       VkImageView _view,
+panvk_DestroyImageView(VkDevice _device, VkImageView _view,
                        const VkAllocationCallbacks *pAllocator)
 {
    VK_FROM_HANDLE(panvk_device, device, _device);
@@ -267,8 +267,7 @@
 }
 
 void
-panvk_DestroyBufferView(VkDevice _device,
-                        VkBufferView bufferView,
+panvk_DestroyBufferView(VkDevice _device, VkBufferView bufferView,
                         const VkAllocationCallbacks *pAllocator)
 {
    VK_FROM_HANDLE(panvk_device, device, _device);
@@ -282,13 +281,14 @@
 }
 
 VkResult
-panvk_GetImageDrmFormatModifierPropertiesEXT(VkDevice device,
-                                             VkImage _image,
-                                             VkImageDrmFormatModifierPropertiesEXT *pProperties)
+panvk_GetImageDrmFormatModifierPropertiesEXT(
+   VkDevice device, VkImage _image,
+   VkImageDrmFormatModifierPropertiesEXT *pProperties)
 {
    VK_FROM_HANDLE(panvk_image, image, _image);
 
-   assert(pProperties->sType == VK_STRUCTURE_TYPE_IMAGE_DRM_FORMAT_MODIFIER_PROPERTIES_EXT);
+   assert(pProperties->sType ==
+          VK_STRUCTURE_TYPE_IMAGE_DRM_FORMAT_MODIFIER_PROPERTIES_EXT);
 
    pProperties->drmFormatModifier = image->pimage.layout.modifier;
    return VK_SUCCESS;
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_mempool.c mesa/src/panfrost/vulkan/panvk_mempool.c
--- mesa-23.0.0/src/panfrost/vulkan/panvk_mempool.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_mempool.c	2023-03-06 19:19:32.475307299 +0100
@@ -23,8 +23,8 @@
  *
  */
 
-#include "pan_device.h"
 #include "panvk_mempool.h"
+#include "pan_device.h"
 
 /* Knockoff u_upload_mgr. Uploads wherever we left off, allocating new entries
  * when needed.
@@ -47,7 +47,8 @@
 
    /* If there's a free BO in our BO pool, let's pick it. */
    if (pool->bo_pool && bo_sz == pool->base.slab_size &&
-       util_dynarray_num_elements(&pool->bo_pool->free_bos, struct panfrost_bo *)) {
+       util_dynarray_num_elements(&pool->bo_pool->free_bos,
+                                  struct panfrost_bo *)) {
       bo = util_dynarray_pop(&pool->bo_pool->free_bos, struct panfrost_bo *);
    } else {
       /* We don't know what the BO will be used for, so let's flag it
@@ -56,8 +57,7 @@
        * flags to this function and keep the read/write,
        * fragment/vertex+tiler pools separate.
        */
-      bo = panfrost_bo_create(pool->base.dev, bo_sz,
-                              pool->base.create_flags,
+      bo = panfrost_bo_create(pool->base.dev, bo_sz, pool->base.create_flags,
                               pool->base.label);
    }
 
@@ -82,9 +82,8 @@
 
    /* If we don't fit, allocate a new backing */
    if (unlikely(bo == NULL || (offset + sz) >= pool->base.slab_size)) {
-      bo = panvk_pool_alloc_backing(pool,
-                                    ALIGN_POT(MAX2(pool->base.slab_size, sz),
-                                    4096));
+      bo = panvk_pool_alloc_backing(
+         pool, ALIGN_POT(MAX2(pool->base.slab_size, sz), 4096));
       offset = 0;
    }
 
@@ -100,10 +99,9 @@
 PAN_POOL_ALLOCATOR(struct panvk_pool, panvk_pool_alloc_aligned)
 
 void
-panvk_pool_init(struct panvk_pool *pool,
-                struct panfrost_device *dev, struct panvk_bo_pool *bo_pool,
-                unsigned create_flags, size_t slab_size, const char *label,
-                bool prealloc)
+panvk_pool_init(struct panvk_pool *pool, struct panfrost_device *dev,
+                struct panvk_bo_pool *bo_pool, unsigned create_flags,
+                size_t slab_size, const char *label, bool prealloc)
 {
    memset(pool, 0, sizeof(*pool));
    pan_pool_init(&pool->base, dev, create_flags, slab_size, label);
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_mempool.h mesa/src/panfrost/vulkan/panvk_mempool.h
--- mesa-23.0.0/src/panfrost/vulkan/panvk_mempool.h	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_mempool.h	2023-03-06 19:19:32.476307306 +0100
@@ -31,12 +31,14 @@
    struct util_dynarray free_bos;
 };
 
-static inline void panvk_bo_pool_init(struct panvk_bo_pool *bo_pool)
+static inline void
+panvk_bo_pool_init(struct panvk_bo_pool *bo_pool)
 {
    util_dynarray_init(&bo_pool->free_bos, NULL);
 }
 
-static inline void panvk_bo_pool_cleanup(struct panvk_bo_pool *bo_pool)
+static inline void
+panvk_bo_pool_cleanup(struct panvk_bo_pool *bo_pool)
 {
    util_dynarray_foreach(&bo_pool->free_bos, struct panfrost_bo *, bo)
       panfrost_bo_unreference(*bo);
@@ -72,16 +74,13 @@
    return container_of(pool, struct panvk_pool, base);
 }
 
-void
-panvk_pool_init(struct panvk_pool *pool, struct panfrost_device *dev,
-                struct panvk_bo_pool *bo_pool, unsigned create_flags,
-                size_t slab_size, const char *label, bool prealloc);
+void panvk_pool_init(struct panvk_pool *pool, struct panfrost_device *dev,
+                     struct panvk_bo_pool *bo_pool, unsigned create_flags,
+                     size_t slab_size, const char *label, bool prealloc);
 
-void
-panvk_pool_reset(struct panvk_pool *pool);
+void panvk_pool_reset(struct panvk_pool *pool);
 
-void
-panvk_pool_cleanup(struct panvk_pool *pool);
+void panvk_pool_cleanup(struct panvk_pool *pool);
 
 static inline unsigned
 panvk_pool_num_bos(struct panvk_pool *pool)
@@ -89,7 +88,6 @@
    return util_dynarray_num_elements(&pool->bos, struct panfrost_bo *);
 }
 
-void
-panvk_pool_get_bo_handles(struct panvk_pool *pool, uint32_t *handles);
+void panvk_pool_get_bo_handles(struct panvk_pool *pool, uint32_t *handles);
 
 #endif
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_pass.c mesa/src/panfrost/vulkan/panvk_pass.c
--- mesa-23.0.0/src/panfrost/vulkan/panvk_pass.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_pass.c	2023-03-06 19:19:32.478307319 +0100
@@ -56,12 +56,13 @@
 
    pass->attachment_count = pCreateInfo->attachmentCount;
    pass->subpass_count = pCreateInfo->subpassCount;
-   pass->attachments = (void *) pass + attachments_offset;
+   pass->attachments = (void *)pass + attachments_offset;
 
-   vk_foreach_struct_const(ext, pCreateInfo->pNext) {
+   vk_foreach_struct_const(ext, pCreateInfo->pNext)
+   {
       switch (ext->sType) {
       case VK_STRUCTURE_TYPE_RENDER_PASS_MULTIVIEW_CREATE_INFO:
-         multiview_info = (VkRenderPassMultiviewCreateInfo *) ext;
+         multiview_info = (VkRenderPassMultiviewCreateInfo *)ext;
          break;
       default:
          break;
@@ -71,7 +72,8 @@
    for (uint32_t i = 0; i < pCreateInfo->attachmentCount; i++) {
       struct panvk_render_pass_attachment *att = &pass->attachments[i];
 
-      att->format = vk_format_to_pipe_format(pCreateInfo->pAttachments[i].format);
+      att->format =
+         vk_format_to_pipe_format(pCreateInfo->pAttachments[i].format);
       att->samples = pCreateInfo->pAttachments[i].samples;
       att->load_op = pCreateInfo->pAttachments[i].loadOp;
       att->stencil_load_op = pCreateInfo->pAttachments[i].stencilLoadOp;
@@ -94,11 +96,10 @@
    }
 
    if (subpass_attachment_count) {
-      pass->subpass_attachments =
-         vk_alloc2(&device->vk.alloc, pAllocator,
-                   subpass_attachment_count *
-                   sizeof(struct panvk_subpass_attachment),
-                   8, VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
+      pass->subpass_attachments = vk_alloc2(
+         &device->vk.alloc, pAllocator,
+         subpass_attachment_count * sizeof(struct panvk_subpass_attachment), 8,
+         VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
       if (pass->subpass_attachments == NULL) {
          vk_object_free(&device->vk, pAllocator, pass);
          return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
@@ -120,7 +121,7 @@
          p += desc->inputAttachmentCount;
 
          for (uint32_t j = 0; j < desc->inputAttachmentCount; j++) {
-            subpass->input_attachments[j] = (struct panvk_subpass_attachment) {
+            subpass->input_attachments[j] = (struct panvk_subpass_attachment){
                .idx = desc->pInputAttachments[j].attachment,
                .layout = desc->pInputAttachments[j].layout,
             };
@@ -137,7 +138,7 @@
          for (uint32_t j = 0; j < desc->colorAttachmentCount; j++) {
             uint32_t idx = desc->pColorAttachments[j].attachment;
 
-            subpass->color_attachments[j] = (struct panvk_subpass_attachment) {
+            subpass->color_attachments[j] = (struct panvk_subpass_attachment){
                .idx = idx,
                .layout = desc->pColorAttachments[j].layout,
             };
@@ -146,9 +147,11 @@
                pass->attachments[idx].view_mask |= subpass->view_mask;
                if (pass->attachments[idx].first_used_in_subpass == ~0) {
                   pass->attachments[idx].first_used_in_subpass = i;
-                  if (pass->attachments[idx].load_op == VK_ATTACHMENT_LOAD_OP_CLEAR)
+                  if (pass->attachments[idx].load_op ==
+                      VK_ATTACHMENT_LOAD_OP_CLEAR)
                      subpass->color_attachments[j].clear = true;
-                  else if (pass->attachments[idx].load_op == VK_ATTACHMENT_LOAD_OP_LOAD)
+                  else if (pass->attachments[idx].load_op ==
+                           VK_ATTACHMENT_LOAD_OP_LOAD)
                      subpass->color_attachments[j].preload = true;
                } else {
                   subpass->color_attachments[j].preload = true;
@@ -164,7 +167,7 @@
          for (uint32_t j = 0; j < desc->colorAttachmentCount; j++) {
             uint32_t idx = desc->pResolveAttachments[j].attachment;
 
-            subpass->resolve_attachments[j] = (struct panvk_subpass_attachment) {
+            subpass->resolve_attachments[j] = (struct panvk_subpass_attachment){
                .idx = idx,
                .layout = desc->pResolveAttachments[j].layout,
             };
@@ -174,9 +177,9 @@
          }
       }
 
-      unsigned idx = desc->pDepthStencilAttachment ?
-                     desc->pDepthStencilAttachment->attachment :
-                     VK_ATTACHMENT_UNUSED;
+      unsigned idx = desc->pDepthStencilAttachment
+                        ? desc->pDepthStencilAttachment->attachment
+                        : VK_ATTACHMENT_UNUSED;
       subpass->zs_attachment.idx = idx;
       if (idx != VK_ATTACHMENT_UNUSED) {
          subpass->zs_attachment.layout = desc->pDepthStencilAttachment->layout;
@@ -186,7 +189,8 @@
             pass->attachments[idx].first_used_in_subpass = i;
             if (pass->attachments[idx].load_op == VK_ATTACHMENT_LOAD_OP_CLEAR)
                subpass->zs_attachment.clear = true;
-            else if (pass->attachments[idx].load_op == VK_ATTACHMENT_LOAD_OP_LOAD)
+            else if (pass->attachments[idx].load_op ==
+                     VK_ATTACHMENT_LOAD_OP_LOAD)
                subpass->zs_attachment.preload = true;
          } else {
             subpass->zs_attachment.preload = true;
@@ -199,8 +203,7 @@
 }
 
 void
-panvk_DestroyRenderPass(VkDevice _device,
-                        VkRenderPass _pass,
+panvk_DestroyRenderPass(VkDevice _device, VkRenderPass _pass,
                         const VkAllocationCallbacks *pAllocator)
 {
    VK_FROM_HANDLE(panvk_device, device, _device);
@@ -214,10 +217,9 @@
 }
 
 void
-panvk_GetRenderAreaGranularity(VkDevice _device,
-                               VkRenderPass renderPass,
+panvk_GetRenderAreaGranularity(VkDevice _device, VkRenderPass renderPass,
                                VkExtent2D *pGranularity)
 {
    /* TODO: Return the actual tile size for the render pass? */
-   *pGranularity = (VkExtent2D) { 1, 1 };
+   *pGranularity = (VkExtent2D){1, 1};
 }
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_pipeline.c mesa/src/panfrost/vulkan/panvk_pipeline.c
--- mesa-23.0.0/src/panfrost/vulkan/panvk_pipeline.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_pipeline.c	2023-03-06 19:19:32.479307326 +0100
@@ -34,16 +34,14 @@
 #include "nir/nir.h"
 #include "nir/nir_builder.h"
 #include "spirv/nir_spirv.h"
-#include "util/u_debug.h"
 #include "util/mesa-sha1.h"
 #include "util/u_atomic.h"
+#include "util/u_debug.h"
 #include "vk_format.h"
 #include "vk_util.h"
 
-
 void
-panvk_DestroyPipeline(VkDevice _device,
-                      VkPipeline _pipeline,
+panvk_DestroyPipeline(VkDevice _device, VkPipeline _pipeline,
                       const VkAllocationCallbacks *pAllocator)
 {
    VK_FROM_HANDLE(panvk_device, device, _device);
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_private.h mesa/src/panfrost/vulkan/panvk_private.h
--- mesa-23.0.0/src/panfrost/vulkan/panvk_private.h	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_private.h	2023-03-06 19:19:32.499307458 +0100
@@ -69,11 +69,11 @@
 #include "pan_blend.h"
 #include "pan_cs.h"
 #include "pan_device.h"
-#include "panvk_mempool.h"
-#include "pan_texture.h"
 #include "pan_scoreboard.h"
-#include "vk_extensions.h"
+#include "pan_texture.h"
+#include "panvk_mempool.h"
 #include "panvk_varyings.h"
+#include "vk_extensions.h"
 
 /* Pre-declarations needed for WSI entrypoints */
 struct wl_surface;
@@ -88,43 +88,41 @@
 
 #include "panvk_entrypoints.h"
 
-#define MAX_BIND_POINTS 2 /* compute + graphics */
-#define MAX_VBS 16
-#define MAX_VERTEX_ATTRIBS 16
-#define MAX_RTS 8
-#define MAX_VSC_PIPES 32
-#define MAX_VIEWPORTS 1
-#define MAX_SCISSORS 16
-#define MAX_DISCARD_RECTANGLES 4
-#define MAX_PUSH_CONSTANTS_SIZE 128
-#define MAX_PUSH_DESCRIPTORS 32
+#define MAX_BIND_POINTS             2 /* compute + graphics */
+#define MAX_VBS                     16
+#define MAX_VERTEX_ATTRIBS          16
+#define MAX_RTS                     8
+#define MAX_VSC_PIPES               32
+#define MAX_VIEWPORTS               1
+#define MAX_SCISSORS                16
+#define MAX_DISCARD_RECTANGLES      4
+#define MAX_PUSH_CONSTANTS_SIZE     128
+#define MAX_PUSH_DESCRIPTORS        32
 #define MAX_DYNAMIC_UNIFORM_BUFFERS 16
 #define MAX_DYNAMIC_STORAGE_BUFFERS 8
-#define MAX_DYNAMIC_BUFFERS                                                  \
+#define MAX_DYNAMIC_BUFFERS                                                    \
    (MAX_DYNAMIC_UNIFORM_BUFFERS + MAX_DYNAMIC_STORAGE_BUFFERS)
 #define MAX_SAMPLES_LOG2 4
 #define NUM_META_FS_KEYS 13
-#define MAX_VIEWS 8
+#define MAX_VIEWS        8
 
 #define NUM_DEPTH_CLEAR_PIPELINES 3
 
-#define PANVK_SYSVAL_UBO_INDEX 0
+#define PANVK_SYSVAL_UBO_INDEX     0
 #define PANVK_PUSH_CONST_UBO_INDEX 1
-#define PANVK_NUM_BUILTIN_UBOS 2
+#define PANVK_NUM_BUILTIN_UBOS     2
 
 #define panvk_printflike(a, b) __attribute__((__format__(__printf__, a, b)))
 
-void
-panvk_logi(const char *format, ...) panvk_printflike(1, 2);
-void
-panvk_logi_v(const char *format, va_list va);
+void panvk_logi(const char *format, ...) panvk_printflike(1, 2);
+void panvk_logi_v(const char *format, va_list va);
 
 #define panvk_stub() assert(!"stub")
 
-#define PANVK_META_COPY_BUF2IMG_NUM_FORMATS 12
-#define PANVK_META_COPY_IMG2BUF_NUM_FORMATS 12
-#define PANVK_META_COPY_IMG2IMG_NUM_FORMATS 14
-#define PANVK_META_COPY_NUM_TEX_TYPES 5
+#define PANVK_META_COPY_BUF2IMG_NUM_FORMATS  12
+#define PANVK_META_COPY_IMG2BUF_NUM_FORMATS  12
+#define PANVK_META_COPY_IMG2IMG_NUM_FORMATS  14
+#define PANVK_META_COPY_NUM_TEX_TYPES        5
 #define PANVK_META_COPY_BUF2BUF_NUM_BLKSIZES 5
 
 static inline unsigned
@@ -161,10 +159,12 @@
       } buf2img[PANVK_META_COPY_BUF2IMG_NUM_FORMATS];
       struct {
          mali_ptr rsd;
-      } img2buf[PANVK_META_COPY_NUM_TEX_TYPES][PANVK_META_COPY_IMG2BUF_NUM_FORMATS];
+      } img2buf[PANVK_META_COPY_NUM_TEX_TYPES]
+               [PANVK_META_COPY_IMG2BUF_NUM_FORMATS];
       struct {
          mali_ptr rsd;
-      } img2img[2][PANVK_META_COPY_NUM_TEX_TYPES][PANVK_META_COPY_IMG2IMG_NUM_FORMATS];
+      } img2img[2][PANVK_META_COPY_NUM_TEX_TYPES]
+               [PANVK_META_COPY_IMG2IMG_NUM_FORMATS];
       struct {
          mali_ptr rsd;
       } buf2buf[PANVK_META_COPY_BUF2BUF_NUM_BLKSIZES];
@@ -215,18 +215,14 @@
    enum panvk_debug_flags debug_flags;
 };
 
-VkResult
-panvk_wsi_init(struct panvk_physical_device *physical_device);
-void
-panvk_wsi_finish(struct panvk_physical_device *physical_device);
+VkResult panvk_wsi_init(struct panvk_physical_device *physical_device);
+void panvk_wsi_finish(struct panvk_physical_device *physical_device);
 
-bool
-panvk_instance_extension_supported(const char *name);
-uint32_t
-panvk_physical_device_api_version(struct panvk_physical_device *dev);
+bool panvk_instance_extension_supported(const char *name);
+uint32_t panvk_physical_device_api_version(struct panvk_physical_device *dev);
 bool
 panvk_physical_device_extension_supported(struct panvk_physical_device *dev,
-                                       const char *name);
+                                          const char *name);
 
 struct panvk_pipeline_cache {
    struct vk_object_base base;
@@ -255,10 +251,10 @@
    int _lost;
 };
 
-VkResult _panvk_device_set_lost(struct panvk_device *device,
-                                const char *file, int line,
-                                const char *msg, ...) PRINTFLIKE(4, 5);
-#define panvk_device_set_lost(dev, ...) \
+VkResult _panvk_device_set_lost(struct panvk_device *device, const char *file,
+                                int line, const char *msg, ...)
+   PRINTFLIKE(4, 5);
+#define panvk_device_set_lost(dev, ...)                                        \
    _panvk_device_set_lost(dev, __FILE__, __LINE__, __VA_ARGS__)
 
 static inline bool
@@ -267,7 +263,7 @@
    return unlikely(p_atomic_read(&device->_lost));
 }
 
-#define TILER_DESC_WORDS 56 
+#define TILER_DESC_WORDS 56
 
 struct panvk_batch {
    struct list_head node;
@@ -430,8 +426,7 @@
    const struct panvk_descriptor_set_layout *set_layout =
       vk_to_panvk_descriptor_set_layout(layout->vk.set_layouts[set]);
 
-   unsigned offset = PANVK_NUM_BUILTIN_UBOS +
-                     layout->sets[set].ubo_offset +
+   unsigned offset = PANVK_NUM_BUILTIN_UBOS + layout->sets[set].ubo_offset +
                      layout->sets[set].dyn_ubo_offset;
 
    if (is_dynamic)
@@ -452,11 +447,11 @@
 
    const bool is_dynamic =
       binding_layout->type == VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC;
-   const uint32_t ubo_idx = is_dynamic ? binding_layout->dyn_ubo_idx :
-                                         binding_layout->ubo_idx;
+   const uint32_t ubo_idx =
+      is_dynamic ? binding_layout->dyn_ubo_idx : binding_layout->ubo_idx;
 
-   return panvk_pipeline_layout_ubo_start(layout, set, is_dynamic) +
-          ubo_idx + array_index;
+   return panvk_pipeline_layout_ubo_start(layout, set, is_dynamic) + ubo_idx +
+          array_index;
 }
 
 struct panvk_desc_pool_counters {
@@ -498,8 +493,8 @@
 }
 
 static inline uint64_t
-panvk_buffer_range(const struct panvk_buffer *buffer,
-                   uint64_t offset, uint64_t range)
+panvk_buffer_range(const struct panvk_buffer *buffer, uint64_t offset,
+                   uint64_t range)
 {
    if (buffer->bo == NULL)
       return 0;
@@ -758,31 +753,25 @@
    struct panvk_cmd_bind_point_state bind_points[MAX_BIND_POINTS];
 };
 
-#define panvk_cmd_get_bind_point_state(cmdbuf, bindpoint) \
-        &(cmdbuf)->bind_points[VK_PIPELINE_BIND_POINT_ ## bindpoint]
+#define panvk_cmd_get_bind_point_state(cmdbuf, bindpoint)                      \
+   &(cmdbuf)->bind_points[VK_PIPELINE_BIND_POINT_##bindpoint]
 
-#define panvk_cmd_get_pipeline(cmdbuf, bindpoint) \
-        (cmdbuf)->bind_points[VK_PIPELINE_BIND_POINT_ ## bindpoint].pipeline
+#define panvk_cmd_get_pipeline(cmdbuf, bindpoint)                              \
+   (cmdbuf)->bind_points[VK_PIPELINE_BIND_POINT_##bindpoint].pipeline
 
-#define panvk_cmd_get_desc_state(cmdbuf, bindpoint) \
-        &(cmdbuf)->bind_points[VK_PIPELINE_BIND_POINT_ ## bindpoint].desc_state
+#define panvk_cmd_get_desc_state(cmdbuf, bindpoint)                            \
+   &(cmdbuf)->bind_points[VK_PIPELINE_BIND_POINT_##bindpoint].desc_state
 
-struct panvk_batch *
-panvk_cmd_open_batch(struct panvk_cmd_buffer *cmdbuf);
+struct panvk_batch *panvk_cmd_open_batch(struct panvk_cmd_buffer *cmdbuf);
 
-void
-panvk_cmd_fb_info_set_subpass(struct panvk_cmd_buffer *cmdbuf);
+void panvk_cmd_fb_info_set_subpass(struct panvk_cmd_buffer *cmdbuf);
 
-void
-panvk_cmd_fb_info_init(struct panvk_cmd_buffer *cmdbuf);
+void panvk_cmd_fb_info_init(struct panvk_cmd_buffer *cmdbuf);
 
-void
-panvk_cmd_preload_fb_after_batch_split(struct panvk_cmd_buffer *cmdbuf);
+void panvk_cmd_preload_fb_after_batch_split(struct panvk_cmd_buffer *cmdbuf);
 
-void
-panvk_pack_color(struct panvk_clear_value *out,
-                 const VkClearColorValue *in,
-                 enum pipe_format format);
+void panvk_pack_color(struct panvk_clear_value *out,
+                      const VkClearColorValue *in, enum pipe_format format);
 
 struct panvk_event {
    struct vk_object_base base;
@@ -798,21 +787,17 @@
 };
 
 struct panvk_shader *
-panvk_shader_create(struct panvk_device *dev,
-                    gl_shader_stage stage,
+panvk_shader_create(struct panvk_device *dev, gl_shader_stage stage,
                     const VkPipelineShaderStageCreateInfo *stage_info,
                     const struct panvk_pipeline_layout *layout,
-                    unsigned sysval_ubo,
-                    struct pan_blend_state *blend_state,
+                    unsigned sysval_ubo, struct pan_blend_state *blend_state,
                     bool static_blend_constants,
                     const VkAllocationCallbacks *alloc);
 
-void
-panvk_shader_destroy(struct panvk_device *dev,
-                     struct panvk_shader *shader,
-                     const VkAllocationCallbacks *alloc);
+void panvk_shader_destroy(struct panvk_device *dev, struct panvk_shader *shader,
+                          const VkAllocationCallbacks *alloc);
 
-#define RSD_WORDS 16
+#define RSD_WORDS        16
 #define BLEND_DESC_WORDS 4
 
 struct panvk_pipeline {
@@ -926,13 +911,12 @@
    struct pan_image pimage;
 };
 
-unsigned
-panvk_image_get_plane_size(const struct panvk_image *image, unsigned plane);
+unsigned panvk_image_get_plane_size(const struct panvk_image *image,
+                                    unsigned plane);
 
-unsigned
-panvk_image_get_total_size(const struct panvk_image *image);
+unsigned panvk_image_get_total_size(const struct panvk_image *image);
 
-#define TEXTURE_DESC_WORDS 8
+#define TEXTURE_DESC_WORDS    8
 #define ATTRIB_BUF_DESC_WORDS 4
 
 struct panvk_image_view {
@@ -1033,40 +1017,63 @@
    struct panvk_subpass subpasses[0];
 };
 
-VK_DEFINE_HANDLE_CASTS(panvk_cmd_buffer, vk.base, VkCommandBuffer, VK_OBJECT_TYPE_COMMAND_BUFFER)
+VK_DEFINE_HANDLE_CASTS(panvk_cmd_buffer, vk.base, VkCommandBuffer,
+                       VK_OBJECT_TYPE_COMMAND_BUFFER)
 VK_DEFINE_HANDLE_CASTS(panvk_device, vk.base, VkDevice, VK_OBJECT_TYPE_DEVICE)
-VK_DEFINE_HANDLE_CASTS(panvk_instance, vk.base, VkInstance, VK_OBJECT_TYPE_INSTANCE)
-VK_DEFINE_HANDLE_CASTS(panvk_physical_device, vk.base, VkPhysicalDevice, VK_OBJECT_TYPE_PHYSICAL_DEVICE)
+VK_DEFINE_HANDLE_CASTS(panvk_instance, vk.base, VkInstance,
+                       VK_OBJECT_TYPE_INSTANCE)
+VK_DEFINE_HANDLE_CASTS(panvk_physical_device, vk.base, VkPhysicalDevice,
+                       VK_OBJECT_TYPE_PHYSICAL_DEVICE)
 VK_DEFINE_HANDLE_CASTS(panvk_queue, vk.base, VkQueue, VK_OBJECT_TYPE_QUEUE)
 
-VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_cmd_pool, vk.base, VkCommandPool, VK_OBJECT_TYPE_COMMAND_POOL)
-VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_buffer, vk.base, VkBuffer, VK_OBJECT_TYPE_BUFFER)
-VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_buffer_view, base, VkBufferView, VK_OBJECT_TYPE_BUFFER_VIEW)
-VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_descriptor_pool, base, VkDescriptorPool, VK_OBJECT_TYPE_DESCRIPTOR_POOL)
-VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_descriptor_set, base, VkDescriptorSet, VK_OBJECT_TYPE_DESCRIPTOR_SET)
+VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_cmd_pool, vk.base, VkCommandPool,
+                               VK_OBJECT_TYPE_COMMAND_POOL)
+VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_buffer, vk.base, VkBuffer,
+                               VK_OBJECT_TYPE_BUFFER)
+VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_buffer_view, base, VkBufferView,
+                               VK_OBJECT_TYPE_BUFFER_VIEW)
+VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_descriptor_pool, base, VkDescriptorPool,
+                               VK_OBJECT_TYPE_DESCRIPTOR_POOL)
+VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_descriptor_set, base, VkDescriptorSet,
+                               VK_OBJECT_TYPE_DESCRIPTOR_SET)
 VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_descriptor_set_layout, vk.base,
-                               VkDescriptorSetLayout, VK_OBJECT_TYPE_DESCRIPTOR_SET_LAYOUT)
-VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_device_memory, base, VkDeviceMemory, VK_OBJECT_TYPE_DEVICE_MEMORY)
+                               VkDescriptorSetLayout,
+                               VK_OBJECT_TYPE_DESCRIPTOR_SET_LAYOUT)
+VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_device_memory, base, VkDeviceMemory,
+                               VK_OBJECT_TYPE_DEVICE_MEMORY)
 VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_event, base, VkEvent, VK_OBJECT_TYPE_EVENT)
-VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_framebuffer, base, VkFramebuffer, VK_OBJECT_TYPE_FRAMEBUFFER)
-VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_image, vk.base, VkImage, VK_OBJECT_TYPE_IMAGE)
-VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_image_view, vk.base, VkImageView, VK_OBJECT_TYPE_IMAGE_VIEW);
-VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_pipeline_cache, base, VkPipelineCache, VK_OBJECT_TYPE_PIPELINE_CACHE)
-VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_pipeline, base, VkPipeline, VK_OBJECT_TYPE_PIPELINE)
-VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_pipeline_layout, vk.base, VkPipelineLayout, VK_OBJECT_TYPE_PIPELINE_LAYOUT)
-VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_render_pass, base, VkRenderPass, VK_OBJECT_TYPE_RENDER_PASS)
-VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_sampler, base, VkSampler, VK_OBJECT_TYPE_SAMPLER)
-
-#define panvk_arch_name(name, version) panvk_## version ## _ ## name
-
-#define panvk_arch_dispatch(arch, name, ...) \
-do { \
-   switch (arch) { \
-   case 6: panvk_arch_name(name, v6)(__VA_ARGS__); break; \
-   case 7: panvk_arch_name(name, v7)(__VA_ARGS__); break; \
-   default: unreachable("Invalid arch"); \
-   } \
-} while (0)
+VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_framebuffer, base, VkFramebuffer,
+                               VK_OBJECT_TYPE_FRAMEBUFFER)
+VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_image, vk.base, VkImage,
+                               VK_OBJECT_TYPE_IMAGE)
+VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_image_view, vk.base, VkImageView,
+                               VK_OBJECT_TYPE_IMAGE_VIEW);
+VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_pipeline_cache, base, VkPipelineCache,
+                               VK_OBJECT_TYPE_PIPELINE_CACHE)
+VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_pipeline, base, VkPipeline,
+                               VK_OBJECT_TYPE_PIPELINE)
+VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_pipeline_layout, vk.base, VkPipelineLayout,
+                               VK_OBJECT_TYPE_PIPELINE_LAYOUT)
+VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_render_pass, base, VkRenderPass,
+                               VK_OBJECT_TYPE_RENDER_PASS)
+VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_sampler, base, VkSampler,
+                               VK_OBJECT_TYPE_SAMPLER)
+
+#define panvk_arch_name(name, version) panvk_##version##_##name
+
+#define panvk_arch_dispatch(arch, name, ...)                                   \
+   do {                                                                        \
+      switch (arch) {                                                          \
+      case 6:                                                                  \
+         panvk_arch_name(name, v6)(__VA_ARGS__);                               \
+         break;                                                                \
+      case 7:                                                                  \
+         panvk_arch_name(name, v7)(__VA_ARGS__);                               \
+         break;                                                                \
+      default:                                                                 \
+         unreachable("Invalid arch");                                          \
+      }                                                                        \
+   } while (0)
 
 #ifdef PAN_ARCH
 #if PAN_ARCH == 6
@@ -1079,7 +1086,7 @@
 #include "panvk_vX_device.h"
 #include "panvk_vX_meta.h"
 #else
-#define PAN_ARCH 6
+#define PAN_ARCH             6
 #define panvk_per_arch(name) panvk_arch_name(name, v6)
 #include "panvk_vX_cmd_buffer.h"
 #include "panvk_vX_cs.h"
@@ -1087,7 +1094,7 @@
 #include "panvk_vX_meta.h"
 #undef PAN_ARCH
 #undef panvk_per_arch
-#define PAN_ARCH 7
+#define PAN_ARCH             7
 #define panvk_per_arch(name) panvk_arch_name(name, v7)
 #include "panvk_vX_cmd_buffer.h"
 #include "panvk_vX_cs.h"
@@ -1098,27 +1105,21 @@
 #endif
 
 #ifdef PAN_ARCH
-bool
-panvk_per_arch(blend_needs_lowering)(const struct panfrost_device *dev,
-                                     const struct pan_blend_state *state,
-                                     unsigned rt);
-
-struct panvk_shader *
-panvk_per_arch(shader_create)(struct panvk_device *dev,
-                              gl_shader_stage stage,
-                              const VkPipelineShaderStageCreateInfo *stage_info,
-                              const struct panvk_pipeline_layout *layout,
-                              unsigned sysval_ubo,
-                              struct pan_blend_state *blend_state,
-                              bool static_blend_constants,
-                              const VkAllocationCallbacks *alloc);
+bool panvk_per_arch(blend_needs_lowering)(const struct panfrost_device *dev,
+                                          const struct pan_blend_state *state,
+                                          unsigned rt);
+
+struct panvk_shader *panvk_per_arch(shader_create)(
+   struct panvk_device *dev, gl_shader_stage stage,
+   const VkPipelineShaderStageCreateInfo *stage_info,
+   const struct panvk_pipeline_layout *layout, unsigned sysval_ubo,
+   struct pan_blend_state *blend_state, bool static_blend_constants,
+   const VkAllocationCallbacks *alloc);
 struct nir_shader;
 
-bool
-panvk_per_arch(nir_lower_descriptors)(struct nir_shader *nir,
-                                      struct panvk_device *dev,
-                                      const struct panvk_pipeline_layout *layout,
-                                      bool *has_img_access_out);
+bool panvk_per_arch(nir_lower_descriptors)(
+   struct nir_shader *nir, struct panvk_device *dev,
+   const struct panvk_pipeline_layout *layout, bool *has_img_access_out);
 #endif
 
 #endif /* PANVK_PRIVATE_H */
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_query.c mesa/src/panfrost/vulkan/panvk_query.c
--- mesa-23.0.0/src/panfrost/vulkan/panvk_query.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_query.c	2023-03-06 19:19:32.500307464 +0100
@@ -34,21 +34,16 @@
 }
 
 void
-panvk_DestroyQueryPool(VkDevice _device,
-                       VkQueryPool _pool,
+panvk_DestroyQueryPool(VkDevice _device, VkQueryPool _pool,
                        const VkAllocationCallbacks *pAllocator)
 {
    panvk_stub();
 }
 
 VkResult
-panvk_GetQueryPoolResults(VkDevice _device,
-                          VkQueryPool queryPool,
-                          uint32_t firstQuery,
-                          uint32_t queryCount,
-                          size_t dataSize,
-                          void *pData,
-                          VkDeviceSize stride,
+panvk_GetQueryPoolResults(VkDevice _device, VkQueryPool queryPool,
+                          uint32_t firstQuery, uint32_t queryCount,
+                          size_t dataSize, void *pData, VkDeviceSize stride,
                           VkQueryResultFlags flags)
 {
    panvk_stub();
@@ -57,38 +52,30 @@
 
 void
 panvk_CmdCopyQueryPoolResults(VkCommandBuffer commandBuffer,
-                              VkQueryPool queryPool,
-                              uint32_t firstQuery,
-                              uint32_t queryCount,
-                              VkBuffer dstBuffer,
-                              VkDeviceSize dstOffset,
-                              VkDeviceSize stride,
+                              VkQueryPool queryPool, uint32_t firstQuery,
+                              uint32_t queryCount, VkBuffer dstBuffer,
+                              VkDeviceSize dstOffset, VkDeviceSize stride,
                               VkQueryResultFlags flags)
 {
    panvk_stub();
 }
 
 void
-panvk_CmdResetQueryPool(VkCommandBuffer commandBuffer,
-                        VkQueryPool queryPool,
-                        uint32_t firstQuery,
-                        uint32_t queryCount)
+panvk_CmdResetQueryPool(VkCommandBuffer commandBuffer, VkQueryPool queryPool,
+                        uint32_t firstQuery, uint32_t queryCount)
 {
    panvk_stub();
 }
 
 void
-panvk_CmdBeginQuery(VkCommandBuffer commandBuffer,
-                    VkQueryPool queryPool,
-                    uint32_t query,
-                    VkQueryControlFlags flags)
+panvk_CmdBeginQuery(VkCommandBuffer commandBuffer, VkQueryPool queryPool,
+                    uint32_t query, VkQueryControlFlags flags)
 {
    panvk_stub();
 }
 
 void
-panvk_CmdEndQuery(VkCommandBuffer commandBuffer,
-                  VkQueryPool queryPool,
+panvk_CmdEndQuery(VkCommandBuffer commandBuffer, VkQueryPool queryPool,
                   uint32_t query)
 {
    panvk_stub();
@@ -96,8 +83,7 @@
 
 void
 panvk_CmdWriteTimestamp2(VkCommandBuffer commandBuffer,
-                         VkPipelineStageFlags2 stage,
-                         VkQueryPool queryPool,
+                         VkPipelineStageFlags2 stage, VkQueryPool queryPool,
                          uint32_t query)
 {
    panvk_stub();
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_shader.c mesa/src/panfrost/vulkan/panvk_shader.c
--- mesa-23.0.0/src/panfrost/vulkan/panvk_shader.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_shader.c	2023-03-06 19:19:32.500307464 +0100
@@ -31,8 +31,7 @@
 #include "vk_util.h"
 
 void
-panvk_shader_destroy(struct panvk_device *dev,
-                     struct panvk_shader *shader,
+panvk_shader_destroy(struct panvk_device *dev, struct panvk_shader *shader,
                      const VkAllocationCallbacks *alloc)
 {
    util_dynarray_fini(&shader->binary);
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_util.c mesa/src/panfrost/vulkan/panvk_util.c
--- mesa-23.0.0/src/panfrost/vulkan/panvk_util.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_util.c	2023-03-06 19:19:32.501307471 +0100
@@ -37,8 +37,7 @@
 #include "vk_enum_to_str.h"
 
 /** Log an error message.  */
-void panvk_printflike(1, 2)
-panvk_logi(const char *format, ...)
+void panvk_printflike(1, 2) panvk_logi(const char *format, ...)
 {
    va_list va;
 
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_varyings.h mesa/src/panfrost/vulkan/panvk_varyings.h
--- mesa-23.0.0/src/panfrost/vulkan/panvk_varyings.h	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_varyings.h	2023-03-06 19:19:32.436307042 +0100
@@ -110,11 +110,11 @@
 
 static inline void
 panvk_varyings_alloc(struct panvk_varyings_info *varyings,
-                     struct pan_pool *varying_mem_pool,
-                     unsigned vertex_count)
+                     struct pan_pool *varying_mem_pool, unsigned vertex_count)
 {
    for (unsigned i = 0; i < PANVK_VARY_BUF_MAX; i++) {
-      if (!(varyings->buf_mask & (1 << i))) continue;
+      if (!(varyings->buf_mask & (1 << i)))
+         continue;
 
       unsigned buf_idx = panvk_varying_buf_index(varyings, i);
       unsigned size = varyings->buf[buf_idx].stride * vertex_count;
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_vX_cmd_buffer.c mesa/src/panfrost/vulkan/panvk_vX_cmd_buffer.c
--- mesa-23.0.0/src/panfrost/vulkan/panvk_vX_cmd_buffer.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_vX_cmd_buffer.c	2023-03-06 19:19:32.515307563 +0100
@@ -41,10 +41,9 @@
 
 static uint32_t
 panvk_debug_adjust_bo_flags(const struct panvk_device *device,
-                      uint32_t bo_flags)
+                            uint32_t bo_flags)
 {
-   uint32_t debug_flags =
-      device->physical_device->instance->debug_flags;
+   uint32_t debug_flags = device->physical_device->instance->debug_flags;
 
    if (debug_flags & PANVK_DEBUG_DUMP)
       bo_flags &= ~PAN_BO_INVISIBLE;
@@ -60,8 +59,8 @@
    struct panfrost_ptr job_ptr =
       pan_pool_alloc_desc(&cmdbuf->desc_pool.base, FRAGMENT_JOB);
 
-   GENX(pan_emit_fragment_job)(fbinfo, batch->fb.desc.gpu, job_ptr.cpu),
-   batch->fragment_job = job_ptr.gpu;
+   GENX(pan_emit_fragment_job)
+   (fbinfo, batch->fb.desc.gpu, job_ptr.cpu), batch->fragment_job = job_ptr.gpu;
    util_dynarray_append(&batch->jobs, void *, job_ptr.cpu);
 }
 
@@ -82,19 +81,19 @@
       clear |= fbinfo->rts[i].clear;
 
    if (!clear && !batch->scoreboard.first_job) {
-      if (util_dynarray_num_elements(&batch->event_ops, struct panvk_event_op) == 0) {
+      if (util_dynarray_num_elements(&batch->event_ops,
+                                     struct panvk_event_op) == 0) {
          /* Content-less batch, let's drop it */
          vk_free(&cmdbuf->vk.pool->alloc, batch);
       } else {
          /* Batch has no jobs but is needed for synchronization, let's add a
           * NULL job so the SUBMIT ioctl doesn't choke on it.
           */
-         struct panfrost_ptr ptr = pan_pool_alloc_desc(&cmdbuf->desc_pool.base,
-                                                       JOB_HEADER);
+         struct panfrost_ptr ptr =
+            pan_pool_alloc_desc(&cmdbuf->desc_pool.base, JOB_HEADER);
          util_dynarray_append(&batch->jobs, void *, ptr.cpu);
          panfrost_add_job(&cmdbuf->desc_pool.base, &batch->scoreboard,
-                          MALI_JOB_TYPE_NULL, false, false, 0, 0,
-                          &ptr, false);
+                          MALI_JOB_TYPE_NULL, false, false, 0, 0, &ptr, false);
          list_addtail(&batch->node, &cmdbuf->batches);
       }
       cmdbuf->state.batch = NULL;
@@ -107,18 +106,16 @@
 
    if (batch->scoreboard.first_tiler) {
       struct panfrost_ptr preload_jobs[2];
-      unsigned num_preload_jobs =
-         GENX(pan_preload_fb)(&cmdbuf->desc_pool.base, &batch->scoreboard,
-                              &cmdbuf->state.fb.info, batch->tls.gpu,
-                              batch->tiler.descs.gpu, preload_jobs);
+      unsigned num_preload_jobs = GENX(pan_preload_fb)(
+         &cmdbuf->desc_pool.base, &batch->scoreboard, &cmdbuf->state.fb.info,
+         batch->tls.gpu, batch->tiler.descs.gpu, preload_jobs);
       for (unsigned i = 0; i < num_preload_jobs; i++)
          util_dynarray_append(&batch->jobs, void *, preload_jobs[i].cpu);
    }
 
    if (batch->tlsinfo.tls.size) {
-      unsigned size = panfrost_get_total_stack_size(batch->tlsinfo.tls.size,
-                                                    pdev->thread_tls_alloc,
-                                                    pdev->core_id_range);
+      unsigned size = panfrost_get_total_stack_size(
+         batch->tlsinfo.tls.size, pdev->thread_tls_alloc, pdev->core_id_range);
       batch->tlsinfo.tls.ptr =
          pan_pool_alloc_aligned(&cmdbuf->tls_pool.base, size, 4096).gpu;
    }
@@ -126,7 +123,9 @@
    if (batch->tlsinfo.wls.size) {
       assert(batch->wls_total_size);
       batch->tlsinfo.wls.ptr =
-         pan_pool_alloc_aligned(&cmdbuf->tls_pool.base, batch->wls_total_size, 4096).gpu;
+         pan_pool_alloc_aligned(&cmdbuf->tls_pool.base, batch->wls_total_size,
+                                4096)
+            .gpu;
    }
 
    if (batch->tls.cpu)
@@ -160,10 +159,8 @@
 void
 panvk_per_arch(CmdNextSubpass)(VkCommandBuffer cmd, VkSubpassContents contents)
 {
-   VkSubpassBeginInfo binfo = {
-      .sType = VK_STRUCTURE_TYPE_SUBPASS_BEGIN_INFO,
-      .contents = contents
-   };
+   VkSubpassBeginInfo binfo = {.sType = VK_STRUCTURE_TYPE_SUBPASS_BEGIN_INFO,
+                               .contents = contents};
    VkSubpassEndInfo einfo = {
       .sType = VK_STRUCTURE_TYPE_SUBPASS_END_INFO,
    };
@@ -184,11 +181,10 @@
    unsigned tags = MALI_FBD_TAG_IS_MFBD;
 
    batch->fb.info = cmdbuf->state.framebuffer;
-   batch->fb.desc =
-      pan_pool_alloc_desc_aggregate(&cmdbuf->desc_pool.base,
-                                    PAN_DESC(FRAMEBUFFER),
-                                    PAN_DESC_ARRAY(has_zs_ext ? 1 : 0, ZS_CRC_EXTENSION),
-                                    PAN_DESC_ARRAY(MAX2(fbinfo->rt_count, 1), RENDER_TARGET));
+   batch->fb.desc = pan_pool_alloc_desc_aggregate(
+      &cmdbuf->desc_pool.base, PAN_DESC(FRAMEBUFFER),
+      PAN_DESC_ARRAY(has_zs_ext ? 1 : 0, ZS_CRC_EXTENSION),
+      PAN_DESC_ARRAY(MAX2(fbinfo->rt_count, 1), RENDER_TARGET));
 
    /* Tag the pointer */
    batch->fb.desc.gpu |= tags;
@@ -204,15 +200,15 @@
 
    assert(batch);
    if (!batch->tls.gpu) {
-      batch->tls =
-         pan_pool_alloc_desc(&cmdbuf->desc_pool.base, LOCAL_STORAGE);
+      batch->tls = pan_pool_alloc_desc(&cmdbuf->desc_pool.base, LOCAL_STORAGE);
    }
 }
 
 static void
-panvk_cmd_prepare_draw_sysvals(struct panvk_cmd_buffer *cmdbuf,
-                               struct panvk_cmd_bind_point_state *bind_point_state,
-                               struct panvk_draw_info *draw)
+panvk_cmd_prepare_draw_sysvals(
+   struct panvk_cmd_buffer *cmdbuf,
+   struct panvk_cmd_bind_point_state *bind_point_state,
+   struct panvk_draw_info *draw)
 {
    struct panvk_sysvals *sysvals = &bind_point_state->desc_state.sysvals;
 
@@ -250,16 +246,16 @@
    if (desc_state->sysvals_ptr)
       return;
 
-   struct panfrost_ptr sysvals =
-      pan_pool_alloc_aligned(&cmdbuf->desc_pool.base,
-                             sizeof(desc_state->sysvals), 16);
+   struct panfrost_ptr sysvals = pan_pool_alloc_aligned(
+      &cmdbuf->desc_pool.base, sizeof(desc_state->sysvals), 16);
    memcpy(sysvals.cpu, &desc_state->sysvals, sizeof(desc_state->sysvals));
    desc_state->sysvals_ptr = sysvals.gpu;
 }
 
 static void
-panvk_cmd_prepare_push_constants(struct panvk_cmd_buffer *cmdbuf,
-                                 struct panvk_cmd_bind_point_state *bind_point_state)
+panvk_cmd_prepare_push_constants(
+   struct panvk_cmd_buffer *cmdbuf,
+   struct panvk_cmd_bind_point_state *bind_point_state)
 {
    struct panvk_descriptor_state *desc_state = &bind_point_state->desc_state;
    const struct panvk_pipeline *pipeline = bind_point_state->pipeline;
@@ -267,10 +263,9 @@
    if (!pipeline->layout->push_constants.size || desc_state->push_constants)
       return;
 
-   struct panfrost_ptr push_constants =
-      pan_pool_alloc_aligned(&cmdbuf->desc_pool.base,
-                             ALIGN_POT(pipeline->layout->push_constants.size, 16),
-                             16);
+   struct panfrost_ptr push_constants = pan_pool_alloc_aligned(
+      &cmdbuf->desc_pool.base,
+      ALIGN_POT(pipeline->layout->push_constants.size, 16), 16);
 
    memcpy(push_constants.cpu, cmdbuf->push_constants,
           pipeline->layout->push_constants.size);
@@ -290,10 +285,8 @@
    panvk_cmd_prepare_sysvals(cmdbuf, bind_point_state);
    panvk_cmd_prepare_push_constants(cmdbuf, bind_point_state);
 
-   struct panfrost_ptr ubos =
-      pan_pool_alloc_desc_array(&cmdbuf->desc_pool.base,
-                                pipeline->num_ubos,
-                                UNIFORM_BUFFER);
+   struct panfrost_ptr ubos = pan_pool_alloc_desc_array(
+      &cmdbuf->desc_pool.base, pipeline->num_ubos, UNIFORM_BUFFER);
 
    panvk_per_arch(emit_ubos)(pipeline, desc_state, ubos.cpu);
 
@@ -311,23 +304,20 @@
    if (!num_textures || desc_state->textures)
       return;
 
-   struct panfrost_ptr textures =
-      pan_pool_alloc_aligned(&cmdbuf->desc_pool.base,
-                             num_textures * pan_size(TEXTURE),
-                             pan_size(TEXTURE));
+   struct panfrost_ptr textures = pan_pool_alloc_aligned(
+      &cmdbuf->desc_pool.base, num_textures * pan_size(TEXTURE),
+      pan_size(TEXTURE));
 
    void *texture = textures.cpu;
 
    for (unsigned i = 0; i < ARRAY_SIZE(desc_state->sets); i++) {
-      if (!desc_state->sets[i]) continue;
+      if (!desc_state->sets[i])
+         continue;
 
-      memcpy(texture,
-             desc_state->sets[i]->textures,
-             desc_state->sets[i]->layout->num_textures *
-             pan_size(TEXTURE));
+      memcpy(texture, desc_state->sets[i]->textures,
+             desc_state->sets[i]->layout->num_textures * pan_size(TEXTURE));
 
-      texture += desc_state->sets[i]->layout->num_textures *
-                 pan_size(TEXTURE);
+      texture += desc_state->sets[i]->layout->num_textures * pan_size(TEXTURE);
    }
 
    desc_state->textures = textures.gpu;
@@ -345,9 +335,7 @@
       return;
 
    struct panfrost_ptr samplers =
-      pan_pool_alloc_desc_array(&cmdbuf->desc_pool.base,
-                                num_samplers,
-                                SAMPLER);
+      pan_pool_alloc_desc_array(&cmdbuf->desc_pool.base, num_samplers, SAMPLER);
 
    void *sampler = samplers.cpu;
 
@@ -362,15 +350,13 @@
    sampler += pan_size(SAMPLER);
 
    for (unsigned i = 0; i < ARRAY_SIZE(desc_state->sets); i++) {
-      if (!desc_state->sets[i]) continue;
+      if (!desc_state->sets[i])
+         continue;
 
-      memcpy(sampler,
-             desc_state->sets[i]->samplers,
-             desc_state->sets[i]->layout->num_samplers *
-             pan_size(SAMPLER));
+      memcpy(sampler, desc_state->sets[i]->samplers,
+             desc_state->sets[i]->layout->num_samplers * pan_size(SAMPLER));
 
-      sampler += desc_state->sets[i]->layout->num_samplers *
-                 pan_size(SAMPLER);
+      sampler += desc_state->sets[i]->layout->num_samplers * pan_size(SAMPLER);
    }
 
    desc_state->samplers = samplers.gpu;
@@ -389,11 +375,9 @@
    }
 
    if (!cmdbuf->state.fs_rsd) {
-      struct panfrost_ptr rsd =
-         pan_pool_alloc_desc_aggregate(&cmdbuf->desc_pool.base,
-                                       PAN_DESC(RENDERER_STATE),
-                                       PAN_DESC_ARRAY(pipeline->blend.state.rt_count,
-                                                      BLEND));
+      struct panfrost_ptr rsd = pan_pool_alloc_desc_aggregate(
+         &cmdbuf->desc_pool.base, PAN_DESC(RENDERER_STATE),
+         PAN_DESC_ARRAY(pipeline->blend.state.rt_count, BLEND));
 
       struct mali_renderer_state_packed rsd_dyn;
       struct mali_renderer_state_packed *rsd_templ =
@@ -412,7 +396,8 @@
             struct mali_blend_packed *bd_templ =
                (struct mali_blend_packed *)&pipeline->blend.bd_template[i];
 
-            STATIC_ASSERT(sizeof(pipeline->blend.bd_template[0]) >= sizeof(*bd_templ));
+            STATIC_ASSERT(sizeof(pipeline->blend.bd_template[0]) >=
+                          sizeof(*bd_templ));
             panvk_per_arch(emit_blend_constant)(cmdbuf->device, pipeline, i,
                                                 cmdbuf->state.blend.constants,
                                                 &bd_dyn);
@@ -437,10 +422,8 @@
    if (batch->tiler.descs.cpu)
       return;
 
-   batch->tiler.descs =
-      pan_pool_alloc_desc_aggregate(&cmdbuf->desc_pool.base,
-                                    PAN_DESC(TILER_CONTEXT),
-                                    PAN_DESC(TILER_HEAP));
+   batch->tiler.descs = pan_pool_alloc_desc_aggregate(
+      &cmdbuf->desc_pool.base, PAN_DESC(TILER_CONTEXT), PAN_DESC(TILER_HEAP));
    STATIC_ASSERT(sizeof(batch->tiler.templ) >=
                  pan_size(TILER_CONTEXT) + pan_size(TILER_HEAP));
 
@@ -460,9 +443,7 @@
 {
    const struct pan_fb_info *fbinfo = &cmdbuf->state.fb.info;
 
-   panvk_per_arch(cmd_get_tiler_context)(cmdbuf,
-                                         fbinfo->width,
-                                         fbinfo->height);
+   panvk_per_arch(cmd_get_tiler_context)(cmdbuf, fbinfo->width, fbinfo->height);
 }
 
 static void
@@ -479,17 +460,16 @@
 panvk_draw_prepare_varyings(struct panvk_cmd_buffer *cmdbuf,
                             struct panvk_draw_info *draw)
 {
-   const struct panvk_pipeline *pipeline = panvk_cmd_get_pipeline(cmdbuf, GRAPHICS);
+   const struct panvk_pipeline *pipeline =
+      panvk_cmd_get_pipeline(cmdbuf, GRAPHICS);
    struct panvk_varyings_info *varyings = &cmdbuf->state.varyings;
 
    panvk_varyings_alloc(varyings, &cmdbuf->varying_pool.base,
                         draw->padded_vertex_count * draw->instance_count);
 
    unsigned buf_count = panvk_varyings_buf_count(varyings);
-   struct panfrost_ptr bufs =
-      pan_pool_alloc_desc_array(&cmdbuf->desc_pool.base,
-                                buf_count + 1,
-                                ATTRIBUTE_BUFFER);
+   struct panfrost_ptr bufs = pan_pool_alloc_desc_array(
+      &cmdbuf->desc_pool.base, buf_count + 1, ATTRIBUTE_BUFFER);
 
    panvk_per_arch(emit_varying_bufs)(varyings, bufs.cpu);
 
@@ -498,30 +478,32 @@
           pan_size(ATTRIBUTE_BUFFER));
 
    if (BITSET_TEST(varyings->active, VARYING_SLOT_POS)) {
-      draw->position = varyings->buf[varyings->varying[VARYING_SLOT_POS].buf].address +
-                       varyings->varying[VARYING_SLOT_POS].offset;
+      draw->position =
+         varyings->buf[varyings->varying[VARYING_SLOT_POS].buf].address +
+         varyings->varying[VARYING_SLOT_POS].offset;
    }
 
    if (pipeline->ia.writes_point_size) {
-      draw->psiz = varyings->buf[varyings->varying[VARYING_SLOT_PSIZ].buf].address +
-                       varyings->varying[VARYING_SLOT_POS].offset;
+      draw->psiz =
+         varyings->buf[varyings->varying[VARYING_SLOT_PSIZ].buf].address +
+         varyings->varying[VARYING_SLOT_POS].offset;
    } else if (pipeline->ia.topology == MALI_DRAW_MODE_LINES ||
               pipeline->ia.topology == MALI_DRAW_MODE_LINE_STRIP ||
               pipeline->ia.topology == MALI_DRAW_MODE_LINE_LOOP) {
-      draw->line_width = pipeline->dynamic_state_mask & PANVK_DYNAMIC_LINE_WIDTH ?
-                         cmdbuf->state.rast.line_width : pipeline->rast.line_width;
+      draw->line_width = pipeline->dynamic_state_mask & PANVK_DYNAMIC_LINE_WIDTH
+                            ? cmdbuf->state.rast.line_width
+                            : pipeline->rast.line_width;
    } else {
       draw->line_width = 1.0f;
    }
    draw->varying_bufs = bufs.gpu;
 
    for (unsigned s = 0; s < MESA_SHADER_STAGES; s++) {
-      if (!varyings->stage[s].count) continue;
+      if (!varyings->stage[s].count)
+         continue;
 
-      struct panfrost_ptr attribs =
-         pan_pool_alloc_desc_array(&cmdbuf->desc_pool.base,
-                                   varyings->stage[s].count,
-                                   ATTRIBUTE);
+      struct panfrost_ptr attribs = pan_pool_alloc_desc_array(
+         &cmdbuf->desc_pool.base, varyings->stage[s].count, ATTRIBUTE);
 
       panvk_per_arch(emit_varyings)(cmdbuf->device, varyings, s, attribs.cpu);
       draw->stages[s].varyings = attribs.gpu;
@@ -531,8 +513,7 @@
 static void
 panvk_fill_non_vs_attribs(struct panvk_cmd_buffer *cmdbuf,
                           struct panvk_cmd_bind_point_state *bind_point_state,
-                          void *attrib_bufs, void *attribs,
-                          unsigned first_buf)
+                          void *attrib_bufs, void *attribs, unsigned first_buf)
 {
    struct panvk_descriptor_state *desc_state = &bind_point_state->desc_state;
    const struct panvk_pipeline *pipeline = bind_point_state->pipeline;
@@ -540,7 +521,8 @@
    for (unsigned s = 0; s < pipeline->layout->num_sets; s++) {
       const struct panvk_descriptor_set *set = desc_state->sets[s];
 
-      if (!set) continue;
+      if (!set)
+         continue;
 
       const struct panvk_descriptor_set_layout *layout = set->layout;
       unsigned img_idx = pipeline->layout->sets[s].img_offset;
@@ -572,15 +554,13 @@
 
    unsigned attrib_count = pipeline->layout->num_imgs;
    unsigned attrib_buf_count = (pipeline->layout->num_imgs * 2);
-   struct panfrost_ptr bufs =
-      pan_pool_alloc_desc_array(&cmdbuf->desc_pool.base,
-                                attrib_buf_count + 1,
-                                ATTRIBUTE_BUFFER);
-   struct panfrost_ptr attribs =
-      pan_pool_alloc_desc_array(&cmdbuf->desc_pool.base, attrib_count,
-                                ATTRIBUTE);
+   struct panfrost_ptr bufs = pan_pool_alloc_desc_array(
+      &cmdbuf->desc_pool.base, attrib_buf_count + 1, ATTRIBUTE_BUFFER);
+   struct panfrost_ptr attribs = pan_pool_alloc_desc_array(
+      &cmdbuf->desc_pool.base, attrib_count, ATTRIBUTE);
 
-   panvk_fill_non_vs_attribs(cmdbuf, bind_point_state, bufs.cpu, attribs.cpu, 0);
+   panvk_fill_non_vs_attribs(cmdbuf, bind_point_state, bufs.cpu, attribs.cpu,
+                             0);
 
    desc_state->non_vs_attrib_bufs = bufs.gpu;
    desc_state->non_vs_attribs = attribs.gpu;
@@ -595,8 +575,9 @@
    struct panvk_descriptor_state *desc_state = &bind_point_state->desc_state;
    const struct panvk_pipeline *pipeline = bind_point_state->pipeline;
    unsigned num_imgs =
-      pipeline->img_access_mask & BITFIELD_BIT(MESA_SHADER_VERTEX) ?
-      pipeline->layout->num_imgs : 0;
+      pipeline->img_access_mask & BITFIELD_BIT(MESA_SHADER_VERTEX)
+         ? pipeline->layout->num_imgs
+         : 0;
    unsigned attrib_count = pipeline->attribs.attrib_count + num_imgs;
 
    if (desc_state->vs_attribs || !attrib_count)
@@ -610,29 +591,26 @@
    }
 
    unsigned attrib_buf_count = pipeline->attribs.buf_count * 2;
-   struct panfrost_ptr bufs =
-      pan_pool_alloc_desc_array(&cmdbuf->desc_pool.base,
-                                attrib_buf_count + 1,
-                                ATTRIBUTE_BUFFER);
-   struct panfrost_ptr attribs =
-      pan_pool_alloc_desc_array(&cmdbuf->desc_pool.base, attrib_count,
-                                ATTRIBUTE);
-
-   panvk_per_arch(emit_attrib_bufs)(&pipeline->attribs,
-                                    cmdbuf->state.vb.bufs,
-                                    cmdbuf->state.vb.count,
-                                    draw, bufs.cpu);
+   struct panfrost_ptr bufs = pan_pool_alloc_desc_array(
+      &cmdbuf->desc_pool.base, attrib_buf_count + 1, ATTRIBUTE_BUFFER);
+   struct panfrost_ptr attribs = pan_pool_alloc_desc_array(
+      &cmdbuf->desc_pool.base, attrib_count, ATTRIBUTE);
+
+   panvk_per_arch(emit_attrib_bufs)(&pipeline->attribs, cmdbuf->state.vb.bufs,
+                                    cmdbuf->state.vb.count, draw, bufs.cpu);
    panvk_per_arch(emit_attribs)(cmdbuf->device, draw, &pipeline->attribs,
                                 cmdbuf->state.vb.bufs, cmdbuf->state.vb.count,
                                 attribs.cpu);
 
    if (attrib_count > pipeline->attribs.buf_count) {
-      unsigned bufs_offset = pipeline->attribs.buf_count * pan_size(ATTRIBUTE_BUFFER) * 2;
-      unsigned attribs_offset = pipeline->attribs.buf_count * pan_size(ATTRIBUTE);
-
-      panvk_fill_non_vs_attribs(cmdbuf, bind_point_state,
-                                bufs.cpu + bufs_offset, attribs.cpu + attribs_offset,
-                                pipeline->attribs.buf_count * 2);
+      unsigned bufs_offset =
+         pipeline->attribs.buf_count * pan_size(ATTRIBUTE_BUFFER) * 2;
+      unsigned attribs_offset =
+         pipeline->attribs.buf_count * pan_size(ATTRIBUTE);
+
+      panvk_fill_non_vs_attribs(
+         cmdbuf, bind_point_state, bufs.cpu + bufs_offset,
+         attribs.cpu + attribs_offset, pipeline->attribs.buf_count * 2);
    }
 
    /* A NULL entry is needed to stop prefecting on Bifrost */
@@ -669,7 +647,8 @@
 panvk_draw_prepare_viewport(struct panvk_cmd_buffer *cmdbuf,
                             struct panvk_draw_info *draw)
 {
-   const struct panvk_pipeline *pipeline = panvk_cmd_get_pipeline(cmdbuf, GRAPHICS);
+   const struct panvk_pipeline *pipeline =
+      panvk_cmd_get_pipeline(cmdbuf, GRAPHICS);
 
    if (pipeline->vpd) {
       draw->viewport = pipeline->vpd;
@@ -680,11 +659,13 @@
          pan_pool_alloc_desc(&cmdbuf->desc_pool.base, VIEWPORT);
 
       const VkViewport *viewport =
-         pipeline->dynamic_state_mask & PANVK_DYNAMIC_VIEWPORT ?
-         &cmdbuf->state.viewport : &pipeline->viewport;
+         pipeline->dynamic_state_mask & PANVK_DYNAMIC_VIEWPORT
+            ? &cmdbuf->state.viewport
+            : &pipeline->viewport;
       const VkRect2D *scissor =
-         pipeline->dynamic_state_mask & PANVK_DYNAMIC_SCISSOR ?
-         &cmdbuf->state.scissor : &pipeline->scissor;
+         pipeline->dynamic_state_mask & PANVK_DYNAMIC_SCISSOR
+            ? &cmdbuf->state.scissor
+            : &pipeline->scissor;
 
       panvk_per_arch(emit_viewport)(viewport, scissor, vp.cpu);
       draw->viewport = cmdbuf->state.vpd = vp.gpu;
@@ -695,7 +676,8 @@
 panvk_draw_prepare_vertex_job(struct panvk_cmd_buffer *cmdbuf,
                               struct panvk_draw_info *draw)
 {
-   const struct panvk_pipeline *pipeline = panvk_cmd_get_pipeline(cmdbuf, GRAPHICS);
+   const struct panvk_pipeline *pipeline =
+      panvk_cmd_get_pipeline(cmdbuf, GRAPHICS);
    struct panvk_batch *batch = cmdbuf->state.batch;
    struct panfrost_ptr ptr =
       pan_pool_alloc_desc(&cmdbuf->desc_pool.base, COMPUTE_JOB);
@@ -709,7 +691,8 @@
 panvk_draw_prepare_tiler_job(struct panvk_cmd_buffer *cmdbuf,
                              struct panvk_draw_info *draw)
 {
-   const struct panvk_pipeline *pipeline = panvk_cmd_get_pipeline(cmdbuf, GRAPHICS);
+   const struct panvk_pipeline *pipeline =
+      panvk_cmd_get_pipeline(cmdbuf, GRAPHICS);
    struct panvk_batch *batch = cmdbuf->state.batch;
    struct panfrost_ptr ptr =
       pan_pool_alloc_desc(&cmdbuf->desc_pool.base, TILER_JOB);
@@ -720,8 +703,7 @@
 }
 
 static void
-panvk_cmd_draw(struct panvk_cmd_buffer *cmdbuf,
-               struct panvk_draw_info *draw)
+panvk_cmd_draw(struct panvk_cmd_buffer *cmdbuf, struct panvk_draw_info *draw)
 {
    struct panvk_batch *batch = cmdbuf->state.batch;
    struct panvk_cmd_bind_point_state *bind_point_state =
@@ -758,10 +740,11 @@
    draw->textures = desc_state->textures;
    draw->samplers = desc_state->samplers;
 
-   STATIC_ASSERT(sizeof(draw->invocation) >= sizeof(struct mali_invocation_packed));
-   panfrost_pack_work_groups_compute((struct mali_invocation_packed *)&draw->invocation,
-                                      1, draw->vertex_range, draw->instance_count,
-                                      1, 1, 1, true, false);
+   STATIC_ASSERT(sizeof(draw->invocation) >=
+                 sizeof(struct mali_invocation_packed));
+   panfrost_pack_work_groups_compute(
+      (struct mali_invocation_packed *)&draw->invocation, 1, draw->vertex_range,
+      draw->instance_count, 1, 1, 1, true, false);
 
    panvk_draw_prepare_fs_rsd(cmdbuf, draw);
    panvk_draw_prepare_varyings(cmdbuf, draw);
@@ -773,10 +756,9 @@
    batch->tlsinfo.tls.size = MAX2(pipeline->tls_size, batch->tlsinfo.tls.size);
    assert(!pipeline->wls_size);
 
-   unsigned vjob_id =
-      panfrost_add_job(&cmdbuf->desc_pool.base, &batch->scoreboard,
-                       MALI_JOB_TYPE_VERTEX, false, false, 0, 0,
-                       &draw->jobs.vertex, false);
+   unsigned vjob_id = panfrost_add_job(
+      &cmdbuf->desc_pool.base, &batch->scoreboard, MALI_JOB_TYPE_VERTEX, false,
+      false, 0, 0, &draw->jobs.vertex, false);
 
    if (pipeline->rast.enable) {
       panfrost_add_job(&cmdbuf->desc_pool.base, &batch->scoreboard,
@@ -789,10 +771,8 @@
 }
 
 void
-panvk_per_arch(CmdDraw)(VkCommandBuffer commandBuffer,
-                        uint32_t vertexCount,
-                        uint32_t instanceCount,
-                        uint32_t firstVertex,
+panvk_per_arch(CmdDraw)(VkCommandBuffer commandBuffer, uint32_t vertexCount,
+                        uint32_t instanceCount, uint32_t firstVertex,
                         uint32_t firstInstance)
 {
    VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
@@ -806,9 +786,9 @@
       .vertex_range = vertexCount,
       .first_instance = firstInstance,
       .instance_count = instanceCount,
-      .padded_vertex_count = instanceCount > 1 ?
-                             panfrost_padded_vertex_count(vertexCount) :
-                             vertexCount,
+      .padded_vertex_count = instanceCount > 1
+                                ? panfrost_padded_vertex_count(vertexCount)
+                                : vertexCount,
       .offset_start = firstVertex,
    };
 
@@ -816,16 +796,16 @@
 }
 
 static void
-panvk_index_minmax_search(struct panvk_cmd_buffer *cmdbuf,
-                          uint32_t start, uint32_t count,
-                          bool restart,
-                          uint32_t *min, uint32_t *max)
+panvk_index_minmax_search(struct panvk_cmd_buffer *cmdbuf, uint32_t start,
+                          uint32_t count, bool restart, uint32_t *min,
+                          uint32_t *max)
 {
    void *ptr = cmdbuf->state.ib.buffer->bo->ptr.cpu +
-               cmdbuf->state.ib.buffer->bo_offset +
-               cmdbuf->state.ib.offset;
+               cmdbuf->state.ib.buffer->bo_offset + cmdbuf->state.ib.offset;
 
-   fprintf(stderr, "WARNING: Crawling index buffers from the CPU isn't valid in Vulkan\n");
+   fprintf(
+      stderr,
+      "WARNING: Crawling index buffers from the CPU isn't valid in Vulkan\n");
 
    assert(cmdbuf->state.ib.buffer);
    assert(cmdbuf->state.ib.buffer->bo);
@@ -838,20 +818,21 @@
     * mapping slowness.
     */
    switch (cmdbuf->state.ib.index_size) {
-#define MINMAX_SEARCH_CASE(sz) \
-   case sz: { \
-      uint ## sz ## _t *indices = ptr; \
-      *min = UINT ## sz ## _MAX; \
-      for (uint32_t i = 0; i < count; i++) { \
-         if (restart && indices[i + start] == UINT ## sz ##_MAX) continue; \
-         *min = MIN2(indices[i + start], *min); \
-         *max = MAX2(indices[i + start], *max); \
-      } \
-      break; \
-   }
-   MINMAX_SEARCH_CASE(32)
-   MINMAX_SEARCH_CASE(16)
-   MINMAX_SEARCH_CASE(8)
+#define MINMAX_SEARCH_CASE(sz)                                                 \
+   case sz: {                                                                  \
+      uint##sz##_t *indices = ptr;                                             \
+      *min = UINT##sz##_MAX;                                                   \
+      for (uint32_t i = 0; i < count; i++) {                                   \
+         if (restart && indices[i + start] == UINT##sz##_MAX)                  \
+            continue;                                                          \
+         *min = MIN2(indices[i + start], *min);                                \
+         *max = MAX2(indices[i + start], *max);                                \
+      }                                                                        \
+      break;                                                                   \
+   }
+      MINMAX_SEARCH_CASE(32)
+      MINMAX_SEARCH_CASE(16)
+      MINMAX_SEARCH_CASE(8)
 #undef MINMAX_SEARCH_CASE
    default:
       unreachable("Invalid index size");
@@ -860,10 +841,8 @@
 
 void
 panvk_per_arch(CmdDrawIndexed)(VkCommandBuffer commandBuffer,
-                               uint32_t indexCount,
-                               uint32_t instanceCount,
-                               uint32_t firstIndex,
-                               int32_t vertexOffset,
+                               uint32_t indexCount, uint32_t instanceCount,
+                               uint32_t firstIndex, int32_t vertexOffset,
                                uint32_t firstInstance)
 {
    VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
@@ -889,9 +868,9 @@
       .instance_count = instanceCount,
       .vertex_range = vertex_range,
       .vertex_count = indexCount + abs(vertexOffset),
-      .padded_vertex_count = instanceCount > 1 ?
-                             panfrost_padded_vertex_count(vertex_range) :
-                             vertex_range,
+      .padded_vertex_count = instanceCount > 1
+                                ? panfrost_padded_vertex_count(vertex_range)
+                                : vertex_range,
       .offset_start = min_vertex + vertexOffset,
       .indices = panvk_buffer_gpu_ptr(cmdbuf->state.ib.buffer,
                                       cmdbuf->state.ib.offset) +
@@ -936,7 +915,6 @@
    panvk_per_arch(CmdEndRenderPass2)(cmd, &einfo);
 }
 
-
 void
 panvk_per_arch(CmdPipelineBarrier2)(VkCommandBuffer commandBuffer,
                                     const VkDependencyInfo *pDependencyInfo)
@@ -972,16 +950,14 @@
        */
       panvk_cmd_open_batch(cmdbuf);
       util_dynarray_append(&cmdbuf->state.batch->event_ops,
-                           struct panvk_event_op,
-                           op);
+                           struct panvk_event_op, op);
       panvk_per_arch(cmd_close_batch)(cmdbuf);
    } else {
       /* Let's close the current batch so the operation executes before any
        * future commands.
        */
       util_dynarray_append(&cmdbuf->state.batch->event_ops,
-                           struct panvk_event_op,
-                           op);
+                           struct panvk_event_op, op);
       panvk_per_arch(cmd_close_batch)(cmdbuf);
       panvk_cmd_preload_fb_after_batch_split(cmdbuf);
       panvk_cmd_open_batch(cmdbuf);
@@ -1001,8 +977,7 @@
       /* No open batch, let's create a new one and have it wait for this event. */
       panvk_cmd_open_batch(cmdbuf);
       util_dynarray_append(&cmdbuf->state.batch->event_ops,
-                           struct panvk_event_op,
-                           op);
+                           struct panvk_event_op, op);
    } else {
       /* Let's close the current batch so any future commands wait on the
        * event signal operation.
@@ -1014,14 +989,12 @@
          panvk_cmd_open_batch(cmdbuf);
       }
       util_dynarray_append(&cmdbuf->state.batch->event_ops,
-                           struct panvk_event_op,
-                           op);
+                           struct panvk_event_op, op);
    }
 }
 
 void
-panvk_per_arch(CmdSetEvent2)(VkCommandBuffer commandBuffer,
-                             VkEvent _event,
+panvk_per_arch(CmdSetEvent2)(VkCommandBuffer commandBuffer, VkEvent _event,
                              const VkDependencyInfo *pDependencyInfo)
 {
    VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
@@ -1034,8 +1007,7 @@
 }
 
 void
-panvk_per_arch(CmdResetEvent2)(VkCommandBuffer commandBuffer,
-                               VkEvent _event,
+panvk_per_arch(CmdResetEvent2)(VkCommandBuffer commandBuffer, VkEvent _event,
                                VkPipelineStageFlags2 stageMask)
 {
    VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
@@ -1049,8 +1021,7 @@
 
 void
 panvk_per_arch(CmdWaitEvents2)(VkCommandBuffer commandBuffer,
-                               uint32_t eventCount,
-                               const VkEvent *pEvents,
+                               uint32_t eventCount, const VkEvent *pEvents,
                                const VkDependencyInfo *pDependencyInfos)
 {
    VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
@@ -1085,7 +1056,8 @@
    panvk_pool_reset(&cmdbuf->varying_pool);
 
    for (unsigned i = 0; i < MAX_BIND_POINTS; i++)
-      memset(&cmdbuf->bind_points[i].desc_state.sets, 0, sizeof(cmdbuf->bind_points[0].desc_state.sets));
+      memset(&cmdbuf->bind_points[i].desc_state.sets, 0,
+             sizeof(cmdbuf->bind_points[0].desc_state.sets));
 }
 
 static void
@@ -1120,13 +1092,12 @@
       container_of(vk_pool, struct panvk_cmd_pool, vk);
    struct panvk_cmd_buffer *cmdbuf;
 
-   cmdbuf = vk_zalloc(&device->vk.alloc, sizeof(*cmdbuf),
-                      8, VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
+   cmdbuf = vk_zalloc(&device->vk.alloc, sizeof(*cmdbuf), 8,
+                      VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
    if (!cmdbuf)
       return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
 
-   VkResult result = vk_command_buffer_init(&pool->vk, &cmdbuf->vk,
-                                            &panvk_per_arch(cmd_buffer_ops), 0);
+   VkResult result = vk_command_buffer_init(&pool->vk, &cmdbuf->vk, NULL, 0);
    if (result != VK_SUCCESS) {
       vk_free(&device->vk.alloc, cmdbuf);
       return result;
@@ -1170,8 +1141,7 @@
 }
 
 void
-panvk_per_arch(DestroyCommandPool)(VkDevice _device,
-                                   VkCommandPool commandPool,
+panvk_per_arch(DestroyCommandPool)(VkDevice _device, VkCommandPool commandPool,
                                    const VkAllocationCallbacks *pAllocator)
 {
    VK_FROM_HANDLE(panvk_device, device, _device);
@@ -1187,16 +1157,13 @@
 }
 
 void
-panvk_per_arch(CmdDispatch)(VkCommandBuffer commandBuffer,
-                            uint32_t x,
-                            uint32_t y,
-                            uint32_t z)
+panvk_per_arch(CmdDispatch)(VkCommandBuffer commandBuffer, uint32_t x,
+                            uint32_t y, uint32_t z)
 {
    VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
-   const struct panfrost_device *pdev =
-      &cmdbuf->device->physical_device->pdev;
+   const struct panfrost_device *pdev = &cmdbuf->device->physical_device->pdev;
    struct panvk_dispatch_info dispatch = {
-      .wg_count = { x, y, z },
+      .wg_count = {x, y, z},
    };
 
    panvk_per_arch(cmd_close_batch)(cmdbuf);
@@ -1236,8 +1203,7 @@
 
    panvk_per_arch(emit_compute_job)(pipeline, &dispatch, job.cpu);
    panfrost_add_job(&cmdbuf->desc_pool.base, &batch->scoreboard,
-                    MALI_JOB_TYPE_COMPUTE, false, false, 0, 0,
-                    &job, false);
+                    MALI_JOB_TYPE_COMPUTE, false, false, 0, 0, &job, false);
 
    batch->tlsinfo.tls.size = pipeline->tls_size;
    batch->tlsinfo.wls.size = pipeline->wls_size;
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_vX_cmd_buffer.h mesa/src/panfrost/vulkan/panvk_vX_cmd_buffer.h
--- mesa-23.0.0/src/panfrost/vulkan/panvk_vX_cmd_buffer.h	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_vX_cmd_buffer.h	2023-03-06 19:19:32.516307570 +0100
@@ -29,24 +29,19 @@
 #error "no arch"
 #endif
 
-#include <vulkan/vulkan.h>
 #include "compiler/shader_enums.h"
+#include <vulkan/vulkan.h>
 
 extern const struct vk_command_buffer_ops panvk_per_arch(cmd_buffer_ops);
 
-void
-panvk_per_arch(cmd_close_batch)(struct panvk_cmd_buffer *cmdbuf);
-
+void panvk_per_arch(cmd_close_batch)(struct panvk_cmd_buffer *cmdbuf);
 
-void
-panvk_per_arch(cmd_get_tiler_context)(struct panvk_cmd_buffer *cmdbuf,
-                                      unsigned width, unsigned height);
+void panvk_per_arch(cmd_get_tiler_context)(struct panvk_cmd_buffer *cmdbuf,
+                                           unsigned width, unsigned height);
 
-void
-panvk_per_arch(cmd_alloc_fb_desc)(struct panvk_cmd_buffer *cmdbuf);
+void panvk_per_arch(cmd_alloc_fb_desc)(struct panvk_cmd_buffer *cmdbuf);
 
-void
-panvk_per_arch(cmd_alloc_tls_desc)(struct panvk_cmd_buffer *cmdbuf, bool gfx);
+void panvk_per_arch(cmd_alloc_tls_desc)(struct panvk_cmd_buffer *cmdbuf,
+                                        bool gfx);
 
-void
-panvk_per_arch(cmd_prepare_tiler_context)(struct panvk_cmd_buffer *cmdbuf);
+void panvk_per_arch(cmd_prepare_tiler_context)(struct panvk_cmd_buffer *cmdbuf);
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_vX_cs.c mesa/src/panfrost/vulkan/panvk_vX_cs.c
--- mesa-23.0.0/src/panfrost/vulkan/panvk_vX_cs.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_vX_cs.c	2023-03-06 19:19:32.529307656 +0100
@@ -23,16 +23,16 @@
 
 #include "genxml/gen_macros.h"
 
-#include "util/macros.h"
 #include "compiler/shader_enums.h"
+#include "util/macros.h"
 
 #include "vk_util.h"
 
 #include "pan_cs.h"
+#include "pan_earlyzs.h"
 #include "pan_encoder.h"
 #include "pan_pool.h"
 #include "pan_shader.h"
-#include "pan_earlyzs.h"
 
 #include "panvk_cs.h"
 #include "panvk_private.h"
@@ -44,9 +44,12 @@
 panvk_translate_sampler_mipmap_mode(VkSamplerMipmapMode mode)
 {
    switch (mode) {
-   case VK_SAMPLER_MIPMAP_MODE_NEAREST: return MALI_MIPMAP_MODE_NEAREST;
-   case VK_SAMPLER_MIPMAP_MODE_LINEAR: return MALI_MIPMAP_MODE_TRILINEAR;
-   default: unreachable("Invalid mipmap mode");
+   case VK_SAMPLER_MIPMAP_MODE_NEAREST:
+      return MALI_MIPMAP_MODE_NEAREST;
+   case VK_SAMPLER_MIPMAP_MODE_LINEAR:
+      return MALI_MIPMAP_MODE_TRILINEAR;
+   default:
+      unreachable("Invalid mipmap mode");
    }
 }
 
@@ -54,12 +57,18 @@
 panvk_translate_sampler_address_mode(VkSamplerAddressMode mode)
 {
    switch (mode) {
-   case VK_SAMPLER_ADDRESS_MODE_REPEAT: return MALI_WRAP_MODE_REPEAT;
-   case VK_SAMPLER_ADDRESS_MODE_MIRRORED_REPEAT: return MALI_WRAP_MODE_MIRRORED_REPEAT;
-   case VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE: return MALI_WRAP_MODE_CLAMP_TO_EDGE;
-   case VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_BORDER: return MALI_WRAP_MODE_CLAMP_TO_BORDER;
-   case VK_SAMPLER_ADDRESS_MODE_MIRROR_CLAMP_TO_EDGE: return MALI_WRAP_MODE_MIRRORED_CLAMP_TO_EDGE;
-   default: unreachable("Invalid wrap");
+   case VK_SAMPLER_ADDRESS_MODE_REPEAT:
+      return MALI_WRAP_MODE_REPEAT;
+   case VK_SAMPLER_ADDRESS_MODE_MIRRORED_REPEAT:
+      return MALI_WRAP_MODE_MIRRORED_REPEAT;
+   case VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE:
+      return MALI_WRAP_MODE_CLAMP_TO_EDGE;
+   case VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_BORDER:
+      return MALI_WRAP_MODE_CLAMP_TO_BORDER;
+   case VK_SAMPLER_ADDRESS_MODE_MIRROR_CLAMP_TO_EDGE:
+      return MALI_WRAP_MODE_MIRRORED_CLAMP_TO_EDGE;
+   default:
+      unreachable("Invalid wrap");
    }
 }
 
@@ -99,8 +108,7 @@
 static void
 panvk_emit_varying(const struct panvk_device *dev,
                    const struct panvk_varyings_info *varyings,
-                   gl_shader_stage stage, unsigned idx,
-                   void *attrib)
+                   gl_shader_stage stage, unsigned idx, void *attrib)
 {
    gl_varying_slot loc = varyings->stage[stage].loc[idx];
 
@@ -114,8 +122,7 @@
 void
 panvk_per_arch(emit_varyings)(const struct panvk_device *dev,
                               const struct panvk_varyings_info *varyings,
-                              gl_shader_stage stage,
-                              void *descs)
+                              gl_shader_stage stage, void *descs)
 {
    struct mali_attribute_packed *attrib = descs;
 
@@ -153,8 +160,7 @@
 static void
 panvk_emit_attrib_buf(const struct panvk_attribs_info *info,
                       const struct panvk_draw_info *draw,
-                      const struct panvk_attrib_buf *bufs,
-                      unsigned buf_count,
+                      const struct panvk_attrib_buf *bufs, unsigned buf_count,
                       unsigned idx, void *desc)
 {
    const struct panvk_attrib_buf_info *buf_info = &info->buf[idx];
@@ -163,8 +169,7 @@
    const struct panvk_attrib_buf *buf = &bufs[idx];
    mali_ptr addr = buf->address & ~63ULL;
    unsigned size = buf->size + (buf->address & 63);
-   unsigned divisor =
-      draw->padded_vertex_count * buf_info->instance_divisor;
+   unsigned divisor = draw->padded_vertex_count * buf_info->instance_divisor;
 
    /* TODO: support instanced arrays */
    if (draw->instance_count <= 1) {
@@ -237,8 +242,7 @@
 }
 
 void
-panvk_per_arch(emit_sampler)(const VkSamplerCreateInfo *pCreateInfo,
-                             void *desc)
+panvk_per_arch(emit_sampler)(const VkSamplerCreateInfo *pCreateInfo, void *desc)
 {
    VkClearColorValue border_color =
       vk_sampler_border_color_value(pCreateInfo, NULL);
@@ -246,16 +250,21 @@
    pan_pack(desc, SAMPLER, cfg) {
       cfg.magnify_nearest = pCreateInfo->magFilter == VK_FILTER_NEAREST;
       cfg.minify_nearest = pCreateInfo->minFilter == VK_FILTER_NEAREST;
-      cfg.mipmap_mode = panvk_translate_sampler_mipmap_mode(pCreateInfo->mipmapMode);
+      cfg.mipmap_mode =
+         panvk_translate_sampler_mipmap_mode(pCreateInfo->mipmapMode);
       cfg.normalized_coordinates = !pCreateInfo->unnormalizedCoordinates;
 
       cfg.lod_bias = FIXED_16(pCreateInfo->mipLodBias, true);
       cfg.minimum_lod = FIXED_16(pCreateInfo->minLod, false);
       cfg.maximum_lod = FIXED_16(pCreateInfo->maxLod, false);
-      cfg.wrap_mode_s = panvk_translate_sampler_address_mode(pCreateInfo->addressModeU);
-      cfg.wrap_mode_t = panvk_translate_sampler_address_mode(pCreateInfo->addressModeV);
-      cfg.wrap_mode_r = panvk_translate_sampler_address_mode(pCreateInfo->addressModeW);
-      cfg.compare_function = panvk_per_arch(translate_sampler_compare_func)(pCreateInfo);
+      cfg.wrap_mode_s =
+         panvk_translate_sampler_address_mode(pCreateInfo->addressModeU);
+      cfg.wrap_mode_t =
+         panvk_translate_sampler_address_mode(pCreateInfo->addressModeV);
+      cfg.wrap_mode_r =
+         panvk_translate_sampler_address_mode(pCreateInfo->addressModeW);
+      cfg.compare_function =
+         panvk_per_arch(translate_sampler_compare_func)(pCreateInfo);
       cfg.border_color_r = border_color.uint32[0];
       cfg.border_color_g = border_color.uint32[1];
       cfg.border_color_b = border_color.uint32[2];
@@ -267,8 +276,7 @@
 panvk_emit_attrib(const struct panvk_device *dev,
                   const struct panvk_draw_info *draw,
                   const struct panvk_attribs_info *attribs,
-                  const struct panvk_attrib_buf *bufs,
-                  unsigned buf_count,
+                  const struct panvk_attrib_buf *bufs, unsigned buf_count,
                   unsigned idx, void *attrib)
 {
    const struct panfrost_device *pdev = &dev->physical_device->pdev;
@@ -277,8 +285,7 @@
 
    pan_pack(attrib, ATTRIBUTE, cfg) {
       cfg.buffer_index = buf_idx * 2;
-      cfg.offset = attribs->attrib[idx].offset +
-                   (bufs[buf_idx].address & 63);
+      cfg.offset = attribs->attrib[idx].offset + (bufs[buf_idx].address & 63);
 
       if (buf_info->per_instance)
          cfg.offset += draw->first_instance * buf_info->stride;
@@ -292,8 +299,7 @@
                              const struct panvk_draw_info *draw,
                              const struct panvk_attribs_info *attribs,
                              const struct panvk_attrib_buf *bufs,
-                             unsigned buf_count,
-                             void *descs)
+                             unsigned buf_count, void *descs)
 {
    struct mali_attribute_packed *attrib = descs;
 
@@ -302,7 +308,7 @@
 }
 
 void
-panvk_per_arch(emit_ubo)(mali_ptr address, size_t size,  void *desc)
+panvk_per_arch(emit_ubo)(mali_ptr address, size_t size, void *desc)
 {
    pan_pack(desc, UNIFORM_BUFFER, cfg) {
       cfg.pointer = address;
@@ -317,14 +323,14 @@
 {
    struct mali_uniform_buffer_packed *ubos = descs;
 
-   panvk_per_arch(emit_ubo)(state->sysvals_ptr,
-                            sizeof(state->sysvals),
+   panvk_per_arch(emit_ubo)(state->sysvals_ptr, sizeof(state->sysvals),
                             &ubos[PANVK_SYSVAL_UBO_INDEX]);
 
    if (pipeline->layout->push_constants.size) {
-      panvk_per_arch(emit_ubo)(state->push_constants,
-                               ALIGN_POT(pipeline->layout->push_constants.size, 16),
-                               &ubos[PANVK_PUSH_CONST_UBO_INDEX]);
+      panvk_per_arch(emit_ubo)(
+         state->push_constants,
+         ALIGN_POT(pipeline->layout->push_constants.size, 16),
+         &ubos[PANVK_PUSH_CONST_UBO_INDEX]);
    } else {
       memset(&ubos[PANVK_PUSH_CONST_UBO_INDEX], 0, sizeof(*ubos));
    }
@@ -351,10 +357,10 @@
             const struct panvk_buffer_desc *bdesc =
                &state->dyn.ubos[pipeline->layout->sets[s].dyn_ubo_offset + i];
 
-            mali_ptr address = panvk_buffer_gpu_ptr(bdesc->buffer,
-                                                    bdesc->offset);
-            size_t size = panvk_buffer_range(bdesc->buffer,
-                                             bdesc->offset, bdesc->size);
+            mali_ptr address =
+               panvk_buffer_gpu_ptr(bdesc->buffer, bdesc->offset);
+            size_t size =
+               panvk_buffer_range(bdesc->buffer, bdesc->offset, bdesc->size);
             if (size) {
                panvk_per_arch(emit_ubo)(address, size,
                                         &ubos[dyn_ubo_start + i]);
@@ -368,8 +374,7 @@
 
 void
 panvk_per_arch(emit_vertex_job)(const struct panvk_pipeline *pipeline,
-                                const struct panvk_draw_info *draw,
-                                void *job)
+                                const struct panvk_draw_info *draw, void *job)
 {
    void *section = pan_section_ptr(job, COMPUTE_JOB, INVOCATION);
 
@@ -387,8 +392,8 @@
       cfg.varying_buffers = draw->varying_bufs;
       cfg.thread_storage = draw->tls;
       cfg.offset_start = draw->offset_start;
-      cfg.instance_size = draw->instance_count > 1 ?
-                          draw->padded_vertex_count : 1;
+      cfg.instance_size =
+         draw->instance_count > 1 ? draw->padded_vertex_count : 1;
       cfg.uniform_buffers = draw->ubos;
       cfg.push_uniforms = draw->stages[PIPE_SHADER_VERTEX].push_constants;
       cfg.textures = draw->textures;
@@ -401,20 +406,15 @@
                                  const struct panvk_dispatch_info *dispatch,
                                  void *job)
 {
-   panfrost_pack_work_groups_compute(pan_section_ptr(job, COMPUTE_JOB, INVOCATION),
-                                     dispatch->wg_count.x,
-                                     dispatch->wg_count.y,
-                                     dispatch->wg_count.z,
-                                     pipeline->cs.local_size.x,
-                                     pipeline->cs.local_size.y,
-                                     pipeline->cs.local_size.z,
-                                     false, false);
+   panfrost_pack_work_groups_compute(
+      pan_section_ptr(job, COMPUTE_JOB, INVOCATION), dispatch->wg_count.x,
+      dispatch->wg_count.y, dispatch->wg_count.z, pipeline->cs.local_size.x,
+      pipeline->cs.local_size.y, pipeline->cs.local_size.z, false, false);
 
    pan_section_pack(job, COMPUTE_JOB, PARAMETERS, cfg) {
-      cfg.job_task_split =
-         util_logbase2_ceil(pipeline->cs.local_size.x + 1) +
-         util_logbase2_ceil(pipeline->cs.local_size.y + 1) +
-         util_logbase2_ceil(pipeline->cs.local_size.z + 1);
+      cfg.job_task_split = util_logbase2_ceil(pipeline->cs.local_size.x + 1) +
+                           util_logbase2_ceil(pipeline->cs.local_size.y + 1) +
+                           util_logbase2_ceil(pipeline->cs.local_size.z + 1);
    }
 
    pan_section_pack(job, COMPUTE_JOB, DRAW, cfg) {
@@ -431,8 +431,7 @@
 
 static void
 panvk_emit_tiler_primitive(const struct panvk_pipeline *pipeline,
-                           const struct panvk_draw_info *draw,
-                           void *prim)
+                           const struct panvk_draw_info *draw, void *prim)
 {
    pan_pack(prim, PRIMITIVE, cfg) {
       cfg.draw_mode = pipeline->ia.topology;
@@ -450,10 +449,17 @@
          cfg.base_vertex_offset = draw->vertex_offset - draw->offset_start;
 
          switch (draw->index_size) {
-         case 32: cfg.index_type = MALI_INDEX_TYPE_UINT32; break;
-         case 16: cfg.index_type = MALI_INDEX_TYPE_UINT16; break;
-         case 8: cfg.index_type = MALI_INDEX_TYPE_UINT8; break;
-         default: unreachable("Invalid index size");
+         case 32:
+            cfg.index_type = MALI_INDEX_TYPE_UINT32;
+            break;
+         case 16:
+            cfg.index_type = MALI_INDEX_TYPE_UINT16;
+            break;
+         case 8:
+            cfg.index_type = MALI_INDEX_TYPE_UINT8;
+            break;
+         default:
+            unreachable("Invalid index size");
          }
       } else {
          cfg.index_count = draw->vertex_count;
@@ -478,8 +484,7 @@
 
 static void
 panvk_emit_tiler_dcd(const struct panvk_pipeline *pipeline,
-                     const struct panvk_draw_info *draw,
-                     void *dcd)
+                     const struct panvk_draw_info *draw, void *dcd)
 {
    pan_pack(dcd, DRAW, cfg) {
       cfg.front_face_ccw = pipeline->rast.front_ccw;
@@ -505,8 +510,8 @@
       }
 
       cfg.offset_start = draw->offset_start;
-      cfg.instance_size = draw->instance_count > 1 ?
-                         draw->padded_vertex_count : 1;
+      cfg.instance_size =
+         draw->instance_count > 1 ? draw->padded_vertex_count : 1;
       cfg.uniform_buffers = draw->ubos;
       cfg.push_uniforms = draw->stages[PIPE_SHADER_FRAGMENT].push_constants;
       cfg.textures = draw->textures;
@@ -518,8 +523,7 @@
 
 void
 panvk_per_arch(emit_tiler_job)(const struct panvk_pipeline *pipeline,
-                               const struct panvk_draw_info *draw,
-                               void *job)
+                               const struct panvk_draw_info *draw, void *job)
 {
    void *section;
 
@@ -538,13 +542,13 @@
    pan_section_pack(job, TILER_JOB, TILER, cfg) {
       cfg.address = draw->tiler_ctx->bifrost;
    }
-   pan_section_pack(job, TILER_JOB, PADDING, padding);
+   pan_section_pack(job, TILER_JOB, PADDING, padding)
+      ;
 }
 
 void
 panvk_per_arch(emit_viewport)(const VkViewport *viewport,
-                              const VkRect2D *scissor,
-                              void *vpd)
+                              const VkRect2D *scissor, void *vpd)
 {
    /* The spec says "width must be greater than 0.0" */
    assert(viewport->x >= 0);
@@ -581,7 +585,7 @@
 static enum mali_register_file_format
 bifrost_blend_type_from_nir(nir_alu_type nir_type)
 {
-   switch(nir_type) {
+   switch (nir_type) {
    case 0: /* Render target not in use */
       return 0;
    case nir_type_float16:
@@ -603,8 +607,8 @@
 
 void
 panvk_per_arch(emit_blend)(const struct panvk_device *dev,
-                           const struct panvk_pipeline *pipeline,
-                           unsigned rt, void *bd)
+                           const struct panvk_pipeline *pipeline, unsigned rt,
+                           void *bd)
 {
    const struct pan_blend_state *blend = &pipeline->blend.state;
    const struct pan_blend_rt_state *rts = &blend->rts[rt];
@@ -632,9 +636,8 @@
                                            &cfg.equation);
 
       /* Fixed point constant */
-      float fconst =
-         pan_blend_get_constant(pan_blend_constant_mask(blend->rts[rt].equation),
-                                blend->constants);
+      float fconst = pan_blend_get_constant(
+         pan_blend_constant_mask(blend->rts[rt].equation), blend->constants);
       u16 constant = fconst * ((1 << chan_size) - 1);
       constant <<= 16 - chan_size;
       cfg.constant = constant;
@@ -645,9 +648,9 @@
          cfg.internal.mode = MALI_BLEND_MODE_FIXED_FUNCTION;
 
          cfg.internal.fixed_function.alpha_zero_nop =
-                 pan_blend_alpha_zero_nop(blend->rts[rt].equation);
+            pan_blend_alpha_zero_nop(blend->rts[rt].equation);
          cfg.internal.fixed_function.alpha_one_store =
-                 pan_blend_alpha_one_store(blend->rts[rt].equation);
+            pan_blend_alpha_one_store(blend->rts[rt].equation);
       }
 
       /* If we want the conversion to work properly,
@@ -678,8 +681,7 @@
 
 void
 panvk_per_arch(emit_dyn_fs_rsd)(const struct panvk_pipeline *pipeline,
-                                const struct panvk_cmd_state *state,
-                                void *rsd)
+                                const struct panvk_cmd_state *state, void *rsd)
 {
    pan_pack(rsd, RENDERER_STATE, cfg) {
       if (pipeline->dynamic_state_mask & (1 << VK_DYNAMIC_STATE_DEPTH_BIAS)) {
@@ -688,17 +690,21 @@
          cfg.depth_bias_clamp = state->rast.depth_bias.clamp;
       }
 
-      if (pipeline->dynamic_state_mask & (1 << VK_DYNAMIC_STATE_STENCIL_COMPARE_MASK)) {
+      if (pipeline->dynamic_state_mask &
+          (1 << VK_DYNAMIC_STATE_STENCIL_COMPARE_MASK)) {
          cfg.stencil_front.mask = state->zs.s_front.compare_mask;
          cfg.stencil_back.mask = state->zs.s_back.compare_mask;
       }
 
-      if (pipeline->dynamic_state_mask & (1 << VK_DYNAMIC_STATE_STENCIL_WRITE_MASK)) {
-         cfg.stencil_mask_misc.stencil_mask_front = state->zs.s_front.write_mask;
+      if (pipeline->dynamic_state_mask &
+          (1 << VK_DYNAMIC_STATE_STENCIL_WRITE_MASK)) {
+         cfg.stencil_mask_misc.stencil_mask_front =
+            state->zs.s_front.write_mask;
          cfg.stencil_mask_misc.stencil_mask_back = state->zs.s_back.write_mask;
       }
 
-      if (pipeline->dynamic_state_mask & (1 << VK_DYNAMIC_STATE_STENCIL_REFERENCE)) {
+      if (pipeline->dynamic_state_mask &
+          (1 << VK_DYNAMIC_STATE_STENCIL_REFERENCE)) {
          cfg.stencil_front.reference_value = state->zs.s_front.ref;
          cfg.stencil_back.reference_value = state->zs.s_back.ref;
       }
@@ -716,13 +722,12 @@
       if (pipeline->fs.required) {
          pan_shader_prepare_rsd(info, pipeline->fs.address, &cfg);
 
-         uint8_t rt_written = pipeline->fs.info.outputs_written >> FRAG_RESULT_DATA0;
+         uint8_t rt_written =
+            pipeline->fs.info.outputs_written >> FRAG_RESULT_DATA0;
          uint8_t rt_mask = pipeline->fs.rt_mask;
          cfg.properties.allow_forward_pixel_to_kill =
-                 pipeline->fs.info.fs.can_fpk &&
-                 !(rt_mask & ~rt_written) &&
-                 !pipeline->ms.alpha_to_coverage &&
-                 !pipeline->blend.reads_dest;
+            pipeline->fs.info.fs.can_fpk && !(rt_mask & ~rt_written) &&
+            !pipeline->ms.alpha_to_coverage && !pipeline->blend.reads_dest;
 
          bool writes_zs = pipeline->zs.z_write || pipeline->zs.s_test;
          bool zs_always_passes = !pipeline->zs.z_test && !pipeline->zs.s_test;
@@ -750,34 +755,45 @@
          pipeline->zs.z_test ? pipeline->zs.z_compare_func : MALI_FUNC_ALWAYS;
 
       cfg.multisample_misc.depth_write_mask = pipeline->zs.z_write;
-      cfg.multisample_misc.fixed_function_near_discard = !pipeline->rast.clamp_depth;
-      cfg.multisample_misc.fixed_function_far_discard = !pipeline->rast.clamp_depth;
+      cfg.multisample_misc.fixed_function_near_discard =
+         !pipeline->rast.clamp_depth;
+      cfg.multisample_misc.fixed_function_far_discard =
+         !pipeline->rast.clamp_depth;
       cfg.multisample_misc.shader_depth_range_fixed = true;
 
       cfg.stencil_mask_misc.stencil_enable = pipeline->zs.s_test;
       cfg.stencil_mask_misc.alpha_to_coverage = pipeline->ms.alpha_to_coverage;
       cfg.stencil_mask_misc.alpha_test_compare_function = MALI_FUNC_ALWAYS;
-      cfg.stencil_mask_misc.front_facing_depth_bias = pipeline->rast.depth_bias.enable;
-      cfg.stencil_mask_misc.back_facing_depth_bias = pipeline->rast.depth_bias.enable;
-      cfg.stencil_mask_misc.single_sampled_lines = pipeline->ms.rast_samples <= 1;
+      cfg.stencil_mask_misc.front_facing_depth_bias =
+         pipeline->rast.depth_bias.enable;
+      cfg.stencil_mask_misc.back_facing_depth_bias =
+         pipeline->rast.depth_bias.enable;
+      cfg.stencil_mask_misc.single_sampled_lines =
+         pipeline->ms.rast_samples <= 1;
 
-      if (!(pipeline->dynamic_state_mask & (1 << VK_DYNAMIC_STATE_DEPTH_BIAS))) {
+      if (!(pipeline->dynamic_state_mask &
+            (1 << VK_DYNAMIC_STATE_DEPTH_BIAS))) {
          cfg.depth_units = pipeline->rast.depth_bias.constant_factor * 2.0f;
          cfg.depth_factor = pipeline->rast.depth_bias.slope_factor;
          cfg.depth_bias_clamp = pipeline->rast.depth_bias.clamp;
       }
 
-      if (!(pipeline->dynamic_state_mask & (1 << VK_DYNAMIC_STATE_STENCIL_COMPARE_MASK))) {
+      if (!(pipeline->dynamic_state_mask &
+            (1 << VK_DYNAMIC_STATE_STENCIL_COMPARE_MASK))) {
          cfg.stencil_front.mask = pipeline->zs.s_front.compare_mask;
          cfg.stencil_back.mask = pipeline->zs.s_back.compare_mask;
       }
 
-      if (!(pipeline->dynamic_state_mask & (1 << VK_DYNAMIC_STATE_STENCIL_WRITE_MASK))) {
-         cfg.stencil_mask_misc.stencil_mask_front = pipeline->zs.s_front.write_mask;
-         cfg.stencil_mask_misc.stencil_mask_back = pipeline->zs.s_back.write_mask;
+      if (!(pipeline->dynamic_state_mask &
+            (1 << VK_DYNAMIC_STATE_STENCIL_WRITE_MASK))) {
+         cfg.stencil_mask_misc.stencil_mask_front =
+            pipeline->zs.s_front.write_mask;
+         cfg.stencil_mask_misc.stencil_mask_back =
+            pipeline->zs.s_back.write_mask;
       }
 
-      if (!(pipeline->dynamic_state_mask & (1 << VK_DYNAMIC_STATE_STENCIL_REFERENCE))) {
+      if (!(pipeline->dynamic_state_mask &
+            (1 << VK_DYNAMIC_STATE_STENCIL_REFERENCE))) {
          cfg.stencil_front.reference_value = pipeline->zs.s_front.ref;
          cfg.stencil_back.reference_value = pipeline->zs.s_back.ref;
       }
@@ -796,8 +812,7 @@
 void
 panvk_per_arch(emit_non_fs_rsd)(const struct panvk_device *dev,
                                 const struct pan_shader_info *shader_info,
-                                mali_ptr shader_ptr,
-                                void *rsd)
+                                mali_ptr shader_ptr, void *rsd)
 {
    assert(shader_info->stage != MESA_SHADER_FRAGMENT);
 
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_vX_cs.h mesa/src/panfrost/vulkan/panvk_vX_cs.h
--- mesa-23.0.0/src/panfrost/vulkan/panvk_vX_cs.h	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_vX_cs.h	2023-03-06 19:19:32.531307669 +0100
@@ -29,100 +29,80 @@
 #error "no arch"
 #endif
 
-#include <vulkan/vulkan.h>
 #include "compiler/shader_enums.h"
+#include <vulkan/vulkan.h>
 
-void
-panvk_per_arch(emit_varying)(const struct panvk_device *dev,
-                             const struct panvk_varyings_info *varyings,
-                             gl_shader_stage stage, unsigned idx,
-                             void *attrib);
+void panvk_per_arch(emit_varying)(const struct panvk_device *dev,
+                                  const struct panvk_varyings_info *varyings,
+                                  gl_shader_stage stage, unsigned idx,
+                                  void *attrib);
 
-void
-panvk_per_arch(emit_varyings)(const struct panvk_device *dev,
-                              const struct panvk_varyings_info *varyings,
-                              gl_shader_stage stage,
-                              void *descs);
+void panvk_per_arch(emit_varyings)(const struct panvk_device *dev,
+                                   const struct panvk_varyings_info *varyings,
+                                   gl_shader_stage stage, void *descs);
 
 void
-panvk_per_arch(emit_varying_bufs)(const struct panvk_varyings_info *varyings,
-                                  void *descs);
+   panvk_per_arch(emit_varying_bufs)(const struct panvk_varyings_info *varyings,
+                                     void *descs);
 
-void
-panvk_per_arch(emit_attrib_bufs)(const struct panvk_attribs_info *info,
-                                 const struct panvk_attrib_buf *bufs,
-                                 unsigned buf_count,
-                                 const struct panvk_draw_info *draw,
-                                 void *descs);
+void panvk_per_arch(emit_attrib_bufs)(const struct panvk_attribs_info *info,
+                                      const struct panvk_attrib_buf *bufs,
+                                      unsigned buf_count,
+                                      const struct panvk_draw_info *draw,
+                                      void *descs);
 
-void
-panvk_per_arch(emit_attribs)(const struct panvk_device *dev,
-                             const struct panvk_draw_info *draw,
-                             const struct panvk_attribs_info *attribs,
-                             const struct panvk_attrib_buf *bufs,
-                             unsigned buf_count,
-                             void *descs);
+void panvk_per_arch(emit_attribs)(const struct panvk_device *dev,
+                                  const struct panvk_draw_info *draw,
+                                  const struct panvk_attribs_info *attribs,
+                                  const struct panvk_attrib_buf *bufs,
+                                  unsigned buf_count, void *descs);
 
-void
-panvk_per_arch(emit_ubo)(mali_ptr address, size_t size,  void *desc);
+void panvk_per_arch(emit_ubo)(mali_ptr address, size_t size, void *desc);
 
-void
-panvk_per_arch(emit_ubos)(const struct panvk_pipeline *pipeline,
-                          const struct panvk_descriptor_state *state,
-                          void *descs);
+void panvk_per_arch(emit_ubos)(const struct panvk_pipeline *pipeline,
+                               const struct panvk_descriptor_state *state,
+                               void *descs);
 
-void
-panvk_per_arch(emit_sampler)(const VkSamplerCreateInfo *pCreateInfo,
-                             void *desc);
+void panvk_per_arch(emit_sampler)(const VkSamplerCreateInfo *pCreateInfo,
+                                  void *desc);
 
-void
-panvk_per_arch(emit_vertex_job)(const struct panvk_pipeline *pipeline,
-                                const struct panvk_draw_info *draw,
-                                void *job);
+void panvk_per_arch(emit_vertex_job)(const struct panvk_pipeline *pipeline,
+                                     const struct panvk_draw_info *draw,
+                                     void *job);
 
 void
-panvk_per_arch(emit_compute_job)(const struct panvk_pipeline *pipeline,
-                                 const struct panvk_dispatch_info *dispatch,
-                                 void *job);
+   panvk_per_arch(emit_compute_job)(const struct panvk_pipeline *pipeline,
+                                    const struct panvk_dispatch_info *dispatch,
+                                    void *job);
 
-void
-panvk_per_arch(emit_tiler_job)(const struct panvk_pipeline *pipeline,
-                               const struct panvk_draw_info *draw,
-                               void *job);
+void panvk_per_arch(emit_tiler_job)(const struct panvk_pipeline *pipeline,
+                                    const struct panvk_draw_info *draw,
+                                    void *job);
 
-void
-panvk_per_arch(emit_viewport)(const VkViewport *viewport,
-                              const VkRect2D *scissor,
-                              void *vpd);
+void panvk_per_arch(emit_viewport)(const VkViewport *viewport,
+                                   const VkRect2D *scissor, void *vpd);
 
-void
-panvk_per_arch(emit_blend)(const struct panvk_device *dev,
-                           const struct panvk_pipeline *pipeline,
-                           unsigned rt, void *bd);
+void panvk_per_arch(emit_blend)(const struct panvk_device *dev,
+                                const struct panvk_pipeline *pipeline,
+                                unsigned rt, void *bd);
 
-void
-panvk_per_arch(emit_blend_constant)(const struct panvk_device *dev,
-                                    const struct panvk_pipeline *pipeline,
-                                    unsigned rt, const float *constants,
-                                    void *bd);
+void panvk_per_arch(emit_blend_constant)(const struct panvk_device *dev,
+                                         const struct panvk_pipeline *pipeline,
+                                         unsigned rt, const float *constants,
+                                         void *bd);
 
-void
-panvk_per_arch(emit_dyn_fs_rsd)(const struct panvk_pipeline *pipeline,
-                                const struct panvk_cmd_state *state,
-                                void *rsd);
+void panvk_per_arch(emit_dyn_fs_rsd)(const struct panvk_pipeline *pipeline,
+                                     const struct panvk_cmd_state *state,
+                                     void *rsd);
 
-void
-panvk_per_arch(emit_base_fs_rsd)(const struct panvk_device *dev,
-                                 const struct panvk_pipeline *pipeline,
-                                 void *rsd);
+void panvk_per_arch(emit_base_fs_rsd)(const struct panvk_device *dev,
+                                      const struct panvk_pipeline *pipeline,
+                                      void *rsd);
 
-void
-panvk_per_arch(emit_non_fs_rsd)(const struct panvk_device *dev,
-                                const struct pan_shader_info *shader_info,
-                                mali_ptr shader_ptr,
-                                void *rsd);
+void panvk_per_arch(emit_non_fs_rsd)(const struct panvk_device *dev,
+                                     const struct pan_shader_info *shader_info,
+                                     mali_ptr shader_ptr, void *rsd);
 
-void
-panvk_per_arch(emit_tiler_context)(const struct panvk_device *dev,
-                                   unsigned width, unsigned height,
-                                   const struct panfrost_ptr *descs);
+void panvk_per_arch(emit_tiler_context)(const struct panvk_device *dev,
+                                        unsigned width, unsigned height,
+                                        const struct panfrost_ptr *descs);
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_vX_descriptor_set.c mesa/src/panfrost/vulkan/panvk_vX_descriptor_set.c
--- mesa-23.0.0/src/panfrost/vulkan/panvk_vX_descriptor_set.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_vX_descriptor_set.c	2023-03-06 19:19:32.542307742 +0100
@@ -36,8 +36,8 @@
 #include <unistd.h>
 
 #include "util/mesa-sha1.h"
-#include "vk_descriptors.h"
 #include "vk_descriptor_update_template.h"
+#include "vk_descriptors.h"
 #include "vk_util.h"
 
 #include "pan_bo.h"
@@ -82,10 +82,9 @@
 }
 
 VkResult
-panvk_per_arch(CreateDescriptorSetLayout)(VkDevice _device,
-                                          const VkDescriptorSetLayoutCreateInfo *pCreateInfo,
-                                          const VkAllocationCallbacks *pAllocator,
-                                          VkDescriptorSetLayout *pSetLayout)
+panvk_per_arch(CreateDescriptorSetLayout)(
+   VkDevice _device, const VkDescriptorSetLayoutCreateInfo *pCreateInfo,
+   const VkAllocationCallbacks *pAllocator, VkDescriptorSetLayout *pSetLayout)
 {
    VK_FROM_HANDLE(panvk_device, device, _device);
    struct panvk_descriptor_set_layout *set_layout;
@@ -94,10 +93,8 @@
    VkResult result;
 
    if (pCreateInfo->bindingCount) {
-      result =
-         vk_create_sorted_bindings(pCreateInfo->pBindings,
-                                   pCreateInfo->bindingCount,
-                                   &bindings);
+      result = vk_create_sorted_bindings(pCreateInfo->pBindings,
+                                         pCreateInfo->bindingCount, &bindings);
       if (result != VK_SUCCESS)
          return vk_error(device, result);
 
@@ -110,10 +107,10 @@
          num_immutable_samplers += bindings[i].descriptorCount;
    }
 
-   size_t size = sizeof(*set_layout) +
-                 (sizeof(struct panvk_descriptor_set_binding_layout) *
-                  num_bindings) +
-                 (sizeof(struct panvk_sampler *) * num_immutable_samplers);
+   size_t size =
+      sizeof(*set_layout) +
+      (sizeof(struct panvk_descriptor_set_binding_layout) * num_bindings) +
+      (sizeof(struct panvk_sampler *) * num_immutable_samplers);
    set_layout = vk_descriptor_set_layout_zalloc(&device->vk, size);
    if (!set_layout) {
       result = VK_ERROR_OUT_OF_HOST_MEMORY;
@@ -122,7 +119,8 @@
 
    struct panvk_sampler **immutable_samplers =
       (struct panvk_sampler **)((uint8_t *)set_layout + sizeof(*set_layout) +
-                                (sizeof(struct panvk_descriptor_set_binding_layout) *
+                                (sizeof(
+                                    struct panvk_descriptor_set_binding_layout) *
                                  num_bindings));
 
    set_layout->binding_count = num_bindings;
@@ -144,7 +142,8 @@
          binding_layout->immutable_samplers = immutable_samplers;
          immutable_samplers += binding_layout->array_size;
          for (unsigned j = 0; j < binding_layout->array_size; j++) {
-            VK_FROM_HANDLE(panvk_sampler, sampler, binding->pImmutableSamplers[j]);
+            VK_FROM_HANDLE(panvk_sampler, sampler,
+                           binding->pImmutableSamplers[j]);
             binding_layout->immutable_samplers[j] = sampler;
          }
       }
@@ -203,8 +202,8 @@
 
       desc_ubo_size = ALIGN_POT(desc_ubo_size, PANVK_DESCRIPTOR_ALIGN);
       binding_layout->desc_ubo_offset = desc_ubo_size;
-      desc_ubo_size += binding_layout->desc_ubo_stride *
-                       binding_layout->array_size;
+      desc_ubo_size +=
+         binding_layout->desc_ubo_stride * binding_layout->array_size;
    }
 
    set_layout->desc_ubo_size = desc_ubo_size;
@@ -227,23 +226,22 @@
    return vk_error(device, result);
 }
 
-static void
-panvk_write_sampler_desc_raw(struct panvk_descriptor_set *set,
-                             uint32_t binding, uint32_t elem,
-                             struct panvk_sampler *sampler);
+static void panvk_write_sampler_desc_raw(struct panvk_descriptor_set *set,
+                                         uint32_t binding, uint32_t elem,
+                                         struct panvk_sampler *sampler);
 
 static VkResult
-panvk_per_arch(descriptor_set_create)(struct panvk_device *device,
-                                      struct panvk_descriptor_pool *pool,
-                                      const struct panvk_descriptor_set_layout *layout,
-                                      struct panvk_descriptor_set **out_set)
+panvk_per_arch(descriptor_set_create)(
+   struct panvk_device *device, struct panvk_descriptor_pool *pool,
+   const struct panvk_descriptor_set_layout *layout,
+   struct panvk_descriptor_set **out_set)
 {
    struct panvk_descriptor_set *set;
 
    /* TODO: Allocate from the pool! */
-   set = vk_object_zalloc(&device->vk, NULL,
-                          sizeof(struct panvk_descriptor_set),
-                          VK_OBJECT_TYPE_DESCRIPTOR_SET);
+   set =
+      vk_object_zalloc(&device->vk, NULL, sizeof(struct panvk_descriptor_set),
+                       VK_OBJECT_TYPE_DESCRIPTOR_SET);
    if (!set)
       return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
 
@@ -259,24 +257,24 @@
 
    if (layout->num_dyn_ubos) {
       set->dyn_ubos = vk_zalloc(&device->vk.alloc,
-                            sizeof(*set->dyn_ubos) * layout->num_dyn_ubos, 8,
-                            VK_OBJECT_TYPE_DESCRIPTOR_SET);
+                                sizeof(*set->dyn_ubos) * layout->num_dyn_ubos,
+                                8, VK_OBJECT_TYPE_DESCRIPTOR_SET);
       if (!set->dyn_ubos)
          goto err_free_set;
    }
 
    if (layout->num_dyn_ssbos) {
-      set->dyn_ssbos = vk_zalloc(&device->vk.alloc,
-                            sizeof(*set->dyn_ssbos) * layout->num_dyn_ssbos, 8,
-                            VK_OBJECT_TYPE_DESCRIPTOR_SET);
+      set->dyn_ssbos = vk_zalloc(
+         &device->vk.alloc, sizeof(*set->dyn_ssbos) * layout->num_dyn_ssbos, 8,
+         VK_OBJECT_TYPE_DESCRIPTOR_SET);
       if (!set->dyn_ssbos)
          goto err_free_set;
    }
 
    if (layout->num_samplers) {
-      set->samplers = vk_zalloc(&device->vk.alloc,
-                                pan_size(SAMPLER) * layout->num_samplers, 8,
-                                VK_OBJECT_TYPE_DESCRIPTOR_SET);
+      set->samplers =
+         vk_zalloc(&device->vk.alloc, pan_size(SAMPLER) * layout->num_samplers,
+                   8, VK_OBJECT_TYPE_DESCRIPTOR_SET);
       if (!set->samplers)
          goto err_free_set;
    }
@@ -291,31 +289,28 @@
 
    if (layout->num_imgs) {
       set->img_fmts =
-         vk_zalloc(&device->vk.alloc,
-                   sizeof(*set->img_fmts) * layout->num_imgs,
+         vk_zalloc(&device->vk.alloc, sizeof(*set->img_fmts) * layout->num_imgs,
                    8, VK_OBJECT_TYPE_DESCRIPTOR_SET);
       if (!set->img_fmts)
          goto err_free_set;
 
-      set->img_attrib_bufs =
-         vk_zalloc(&device->vk.alloc,
-                   pan_size(ATTRIBUTE_BUFFER) * 2 * layout->num_imgs,
-                   8, VK_OBJECT_TYPE_DESCRIPTOR_SET);
+      set->img_attrib_bufs = vk_zalloc(
+         &device->vk.alloc, pan_size(ATTRIBUTE_BUFFER) * 2 * layout->num_imgs,
+         8, VK_OBJECT_TYPE_DESCRIPTOR_SET);
       if (!set->img_attrib_bufs)
          goto err_free_set;
    }
 
    if (layout->desc_ubo_size) {
-      set->desc_bo = panfrost_bo_create(&device->physical_device->pdev,
-                                        layout->desc_ubo_size,
-                                        0, "Descriptor set");
+      set->desc_bo =
+         panfrost_bo_create(&device->physical_device->pdev,
+                            layout->desc_ubo_size, 0, "Descriptor set");
       if (!set->desc_bo)
          goto err_free_set;
 
       struct mali_uniform_buffer_packed *ubos = set->ubos;
 
-      panvk_per_arch(emit_ubo)(set->desc_bo->ptr.gpu,
-                               layout->desc_ubo_size,
+      panvk_per_arch(emit_ubo)(set->desc_bo->ptr.gpu, layout->desc_ubo_size,
                                &ubos[layout->desc_ubo_index]);
    }
 
@@ -348,9 +343,9 @@
 }
 
 VkResult
-panvk_per_arch(AllocateDescriptorSets)(VkDevice _device,
-                                       const VkDescriptorSetAllocateInfo *pAllocateInfo,
-                                       VkDescriptorSet *pDescriptorSets)
+panvk_per_arch(AllocateDescriptorSets)(
+   VkDevice _device, const VkDescriptorSetAllocateInfo *pAllocateInfo,
+   VkDescriptorSet *pDescriptorSets)
 {
    VK_FROM_HANDLE(panvk_device, device, _device);
    VK_FROM_HANDLE(panvk_descriptor_pool, pool, pAllocateInfo->descriptorPool);
@@ -362,7 +357,8 @@
                      pAllocateInfo->pSetLayouts[i]);
       struct panvk_descriptor_set *set = NULL;
 
-      result = panvk_per_arch(descriptor_set_create)(device, pool, layout, &set);
+      result =
+         panvk_per_arch(descriptor_set_create)(device, pool, layout, &set);
       if (result != VK_SUCCESS)
          goto err_free_sets;
 
@@ -372,28 +368,28 @@
    return VK_SUCCESS;
 
 err_free_sets:
-   panvk_FreeDescriptorSets(_device, pAllocateInfo->descriptorPool, i, pDescriptorSets);
+   panvk_FreeDescriptorSets(_device, pAllocateInfo->descriptorPool, i,
+                            pDescriptorSets);
    for (i = 0; i < pAllocateInfo->descriptorSetCount; i++)
       pDescriptorSets[i] = VK_NULL_HANDLE;
 
-   return result; 
+   return result;
 }
 
 static void *
-panvk_desc_ubo_data(struct panvk_descriptor_set *set,
-                    uint32_t binding, uint32_t elem)
+panvk_desc_ubo_data(struct panvk_descriptor_set *set, uint32_t binding,
+                    uint32_t elem)
 {
    const struct panvk_descriptor_set_binding_layout *binding_layout =
       &set->layout->bindings[binding];
 
-   return (char *)set->desc_bo->ptr.cpu +
-          binding_layout->desc_ubo_offset +
+   return (char *)set->desc_bo->ptr.cpu + binding_layout->desc_ubo_offset +
           elem * binding_layout->desc_ubo_stride;
 }
 
 static struct mali_sampler_packed *
-panvk_sampler_desc(struct panvk_descriptor_set *set,
-                   uint32_t binding, uint32_t elem)
+panvk_sampler_desc(struct panvk_descriptor_set *set, uint32_t binding,
+                   uint32_t elem)
 {
    const struct panvk_descriptor_set_binding_layout *binding_layout =
       &set->layout->bindings[binding];
@@ -404,19 +400,18 @@
 }
 
 static void
-panvk_write_sampler_desc_raw(struct panvk_descriptor_set *set,
-                             uint32_t binding, uint32_t elem,
-                             struct panvk_sampler *sampler)
+panvk_write_sampler_desc_raw(struct panvk_descriptor_set *set, uint32_t binding,
+                             uint32_t elem, struct panvk_sampler *sampler)
 {
-   memcpy(panvk_sampler_desc(set, binding, elem),
-          &sampler->desc, sizeof(sampler->desc));
+   memcpy(panvk_sampler_desc(set, binding, elem), &sampler->desc,
+          sizeof(sampler->desc));
 }
 
 static void
 panvk_write_sampler_desc(UNUSED struct panvk_device *dev,
-                         struct panvk_descriptor_set *set,
-                         uint32_t binding, uint32_t elem,
-                         const VkDescriptorImageInfo * const pImageInfo)
+                         struct panvk_descriptor_set *set, uint32_t binding,
+                         uint32_t elem,
+                         const VkDescriptorImageInfo *const pImageInfo)
 {
    const struct panvk_descriptor_set_binding_layout *binding_layout =
       &set->layout->bindings[binding];
@@ -446,8 +441,8 @@
 }
 
 static struct mali_texture_packed *
-panvk_tex_desc(struct panvk_descriptor_set *set,
-               uint32_t binding, uint32_t elem)
+panvk_tex_desc(struct panvk_descriptor_set *set, uint32_t binding,
+               uint32_t elem)
 {
    const struct panvk_descriptor_set_binding_layout *binding_layout =
       &set->layout->bindings[binding];
@@ -459,22 +454,21 @@
 
 static void
 panvk_write_tex_desc(UNUSED struct panvk_device *dev,
-                     struct panvk_descriptor_set *set,
-                     uint32_t binding, uint32_t elem,
-                     const VkDescriptorImageInfo * const pImageInfo)
+                     struct panvk_descriptor_set *set, uint32_t binding,
+                     uint32_t elem,
+                     const VkDescriptorImageInfo *const pImageInfo)
 {
    VK_FROM_HANDLE(panvk_image_view, view, pImageInfo->imageView);
 
-   memcpy(panvk_tex_desc(set, binding, elem),
-          view->descs.tex, pan_size(TEXTURE));
+   memcpy(panvk_tex_desc(set, binding, elem), view->descs.tex,
+          pan_size(TEXTURE));
 
    panvk_fill_image_desc(panvk_desc_ubo_data(set, binding, elem), view);
 }
 
 static void
-panvk_copy_tex_desc(struct panvk_descriptor_set *dst_set,
-                    uint32_t dst_binding, uint32_t dst_elem,
-                    struct panvk_descriptor_set *src_set,
+panvk_copy_tex_desc(struct panvk_descriptor_set *dst_set, uint32_t dst_binding,
+                    uint32_t dst_elem, struct panvk_descriptor_set *src_set,
                     uint32_t src_binding, uint32_t src_elem)
 {
    *panvk_tex_desc(dst_set, dst_binding, dst_elem) =
@@ -485,21 +479,19 @@
 
 static void
 panvk_write_tex_buf_desc(UNUSED struct panvk_device *dev,
-                         struct panvk_descriptor_set *set,
-                         uint32_t binding, uint32_t elem,
-                         const VkBufferView bufferView)
+                         struct panvk_descriptor_set *set, uint32_t binding,
+                         uint32_t elem, const VkBufferView bufferView)
 {
    VK_FROM_HANDLE(panvk_buffer_view, view, bufferView);
 
-   memcpy(panvk_tex_desc(set, binding, elem),
-          view->descs.tex, pan_size(TEXTURE));
+   memcpy(panvk_tex_desc(set, binding, elem), view->descs.tex,
+          pan_size(TEXTURE));
 
    panvk_fill_bview_desc(panvk_desc_ubo_data(set, binding, elem), view);
 }
 
 static uint32_t
-panvk_img_idx(struct panvk_descriptor_set *set,
-              uint32_t binding, uint32_t elem)
+panvk_img_idx(struct panvk_descriptor_set *set, uint32_t binding, uint32_t elem)
 {
    const struct panvk_descriptor_set_binding_layout *binding_layout =
       &set->layout->bindings[binding];
@@ -508,8 +500,7 @@
 }
 
 static void
-panvk_write_img_desc(struct panvk_device *dev,
-                     struct panvk_descriptor_set *set,
+panvk_write_img_desc(struct panvk_device *dev, struct panvk_descriptor_set *set,
                      uint32_t binding, uint32_t elem,
                      const VkDescriptorImageInfo *pImageInfo)
 {
@@ -521,15 +512,15 @@
                       (pan_size(ATTRIBUTE_BUFFER) * 2 * img_idx);
 
    set->img_fmts[img_idx] = pdev->formats[view->pview.format].hw;
-   memcpy(attrib_buf, view->descs.img_attrib_buf, pan_size(ATTRIBUTE_BUFFER) * 2);
+   memcpy(attrib_buf, view->descs.img_attrib_buf,
+          pan_size(ATTRIBUTE_BUFFER) * 2);
 
    panvk_fill_image_desc(panvk_desc_ubo_data(set, binding, elem), view);
 }
 
 static void
-panvk_copy_img_desc(struct panvk_descriptor_set *dst_set,
-                    uint32_t dst_binding, uint32_t dst_elem,
-                    struct panvk_descriptor_set *src_set,
+panvk_copy_img_desc(struct panvk_descriptor_set *dst_set, uint32_t dst_binding,
+                    uint32_t dst_elem, struct panvk_descriptor_set *src_set,
                     uint32_t src_binding, uint32_t src_elem)
 {
    unsigned dst_img_idx = panvk_img_idx(dst_set, dst_binding, dst_elem);
@@ -548,9 +539,8 @@
 
 static void
 panvk_write_img_buf_desc(struct panvk_device *dev,
-                         struct panvk_descriptor_set *set,
-                         uint32_t binding, uint32_t elem,
-                         const VkBufferView bufferView)
+                         struct panvk_descriptor_set *set, uint32_t binding,
+                         uint32_t elem, const VkBufferView bufferView)
 {
    const struct panfrost_device *pdev = &dev->physical_device->pdev;
    VK_FROM_HANDLE(panvk_buffer_view, view, bufferView);
@@ -560,14 +550,15 @@
                       (pan_size(ATTRIBUTE_BUFFER) * 2 * img_idx);
 
    set->img_fmts[img_idx] = pdev->formats[view->fmt].hw;
-   memcpy(attrib_buf, view->descs.img_attrib_buf, pan_size(ATTRIBUTE_BUFFER) * 2);
+   memcpy(attrib_buf, view->descs.img_attrib_buf,
+          pan_size(ATTRIBUTE_BUFFER) * 2);
 
    panvk_fill_bview_desc(panvk_desc_ubo_data(set, binding, elem), view);
 }
 
 static struct mali_uniform_buffer_packed *
-panvk_ubo_desc(struct panvk_descriptor_set *set,
-               uint32_t binding, uint32_t elem)
+panvk_ubo_desc(struct panvk_descriptor_set *set, uint32_t binding,
+               uint32_t elem)
 {
    const struct panvk_descriptor_set_binding_layout *binding_layout =
       &set->layout->bindings[binding];
@@ -579,23 +570,21 @@
 
 static void
 panvk_write_ubo_desc(UNUSED struct panvk_device *dev,
-                     struct panvk_descriptor_set *set,
-                     uint32_t binding, uint32_t elem,
-                     const VkDescriptorBufferInfo *pBufferInfo)
+                     struct panvk_descriptor_set *set, uint32_t binding,
+                     uint32_t elem, const VkDescriptorBufferInfo *pBufferInfo)
 {
    VK_FROM_HANDLE(panvk_buffer, buffer, pBufferInfo->buffer);
 
    mali_ptr ptr = panvk_buffer_gpu_ptr(buffer, pBufferInfo->offset);
-   size_t size = panvk_buffer_range(buffer, pBufferInfo->offset,
-                                    pBufferInfo->range);
+   size_t size =
+      panvk_buffer_range(buffer, pBufferInfo->offset, pBufferInfo->range);
 
    panvk_per_arch(emit_ubo)(ptr, size, panvk_ubo_desc(set, binding, elem));
 }
 
 static void
-panvk_copy_ubo_desc(struct panvk_descriptor_set *dst_set,
-                    uint32_t dst_binding, uint32_t dst_elem,
-                    struct panvk_descriptor_set *src_set,
+panvk_copy_ubo_desc(struct panvk_descriptor_set *dst_set, uint32_t dst_binding,
+                    uint32_t dst_elem, struct panvk_descriptor_set *src_set,
                     uint32_t src_binding, uint32_t src_elem)
 {
    *panvk_ubo_desc(dst_set, dst_binding, dst_elem) =
@@ -603,8 +592,8 @@
 }
 
 static struct panvk_buffer_desc *
-panvk_dyn_ubo_desc(struct panvk_descriptor_set *set,
-                   uint32_t binding, uint32_t elem)
+panvk_dyn_ubo_desc(struct panvk_descriptor_set *set, uint32_t binding,
+                   uint32_t elem)
 {
    const struct panvk_descriptor_set_binding_layout *binding_layout =
       &set->layout->bindings[binding];
@@ -614,13 +603,13 @@
 
 static void
 panvk_write_dyn_ubo_desc(UNUSED struct panvk_device *dev,
-                         struct panvk_descriptor_set *set,
-                         uint32_t binding, uint32_t elem,
+                         struct panvk_descriptor_set *set, uint32_t binding,
+                         uint32_t elem,
                          const VkDescriptorBufferInfo *pBufferInfo)
 {
    VK_FROM_HANDLE(panvk_buffer, buffer, pBufferInfo->buffer);
 
-   *panvk_dyn_ubo_desc(set, binding, elem) = (struct panvk_buffer_desc) {
+   *panvk_dyn_ubo_desc(set, binding, elem) = (struct panvk_buffer_desc){
       .buffer = buffer,
       .offset = pBufferInfo->offset,
       .size = pBufferInfo->range,
@@ -639,32 +628,30 @@
 
 static void
 panvk_write_ssbo_desc(UNUSED struct panvk_device *dev,
-                      struct panvk_descriptor_set *set,
-                      uint32_t binding, uint32_t elem,
-                      const VkDescriptorBufferInfo *pBufferInfo)
+                      struct panvk_descriptor_set *set, uint32_t binding,
+                      uint32_t elem, const VkDescriptorBufferInfo *pBufferInfo)
 {
    VK_FROM_HANDLE(panvk_buffer, buffer, pBufferInfo->buffer);
 
    struct panvk_ssbo_addr *desc = panvk_desc_ubo_data(set, binding, elem);
-   *desc = (struct panvk_ssbo_addr) {
+   *desc = (struct panvk_ssbo_addr){
       .base_addr = panvk_buffer_gpu_ptr(buffer, pBufferInfo->offset),
-      .size = panvk_buffer_range(buffer, pBufferInfo->offset,
-                                         pBufferInfo->range),
+      .size =
+         panvk_buffer_range(buffer, pBufferInfo->offset, pBufferInfo->range),
    };
 }
 
 static void
-panvk_copy_ssbo_desc(struct panvk_descriptor_set *dst_set,
-                     uint32_t dst_binding, uint32_t dst_elem,
-                     struct panvk_descriptor_set *src_set,
+panvk_copy_ssbo_desc(struct panvk_descriptor_set *dst_set, uint32_t dst_binding,
+                     uint32_t dst_elem, struct panvk_descriptor_set *src_set,
                      uint32_t src_binding, uint32_t src_elem)
 {
    /* Descriptor UBO data gets copied automatically */
 }
 
 static struct panvk_buffer_desc *
-panvk_dyn_ssbo_desc(struct panvk_descriptor_set *set,
-                   uint32_t binding, uint32_t elem)
+panvk_dyn_ssbo_desc(struct panvk_descriptor_set *set, uint32_t binding,
+                    uint32_t elem)
 {
    const struct panvk_descriptor_set_binding_layout *binding_layout =
       &set->layout->bindings[binding];
@@ -674,13 +661,13 @@
 
 static void
 panvk_write_dyn_ssbo_desc(UNUSED struct panvk_device *dev,
-                          struct panvk_descriptor_set *set,
-                          uint32_t binding, uint32_t elem,
+                          struct panvk_descriptor_set *set, uint32_t binding,
+                          uint32_t elem,
                           const VkDescriptorBufferInfo *pBufferInfo)
 {
    VK_FROM_HANDLE(panvk_buffer, buffer, pBufferInfo->buffer);
 
-   *panvk_dyn_ssbo_desc(set, binding, elem) = (struct panvk_buffer_desc) {
+   *panvk_dyn_ssbo_desc(set, binding, elem) = (struct panvk_buffer_desc){
       .buffer = buffer,
       .offset = pBufferInfo->offset,
       .size = pBufferInfo->range,
@@ -698,11 +685,10 @@
 }
 
 void
-panvk_per_arch(UpdateDescriptorSets)(VkDevice _device,
-                                     uint32_t descriptorWriteCount,
-                                     const VkWriteDescriptorSet *pDescriptorWrites,
-                                     uint32_t descriptorCopyCount,
-                                     const VkCopyDescriptorSet *pDescriptorCopies)
+panvk_per_arch(UpdateDescriptorSets)(
+   VkDevice _device, uint32_t descriptorWriteCount,
+   const VkWriteDescriptorSet *pDescriptorWrites, uint32_t descriptorCopyCount,
+   const VkCopyDescriptorSet *pDescriptorCopies)
 {
    VK_FROM_HANDLE(panvk_device, dev, _device);
 
@@ -713,8 +699,7 @@
       switch (write->descriptorType) {
       case VK_DESCRIPTOR_TYPE_SAMPLER:
          for (uint32_t j = 0; j < write->descriptorCount; j++) {
-            panvk_write_sampler_desc(dev, set,
-                                     write->dstBinding,
+            panvk_write_sampler_desc(dev, set, write->dstBinding,
                                      write->dstArrayElement + j,
                                      &write->pImageInfo[j]);
          }
@@ -722,12 +707,10 @@
 
       case VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER:
          for (uint32_t j = 0; j < write->descriptorCount; j++) {
-            panvk_write_sampler_desc(dev, set,
-                                     write->dstBinding,
+            panvk_write_sampler_desc(dev, set, write->dstBinding,
                                      write->dstArrayElement + j,
                                      &write->pImageInfo[j]);
-            panvk_write_tex_desc(dev, set,
-                                 write->dstBinding,
+            panvk_write_tex_desc(dev, set, write->dstBinding,
                                  write->dstArrayElement + j,
                                  &write->pImageInfo[j]);
          }
@@ -735,8 +718,7 @@
 
       case VK_DESCRIPTOR_TYPE_SAMPLED_IMAGE:
          for (uint32_t j = 0; j < write->descriptorCount; j++) {
-            panvk_write_tex_desc(dev, set,
-                                 write->dstBinding,
+            panvk_write_tex_desc(dev, set, write->dstBinding,
                                  write->dstArrayElement + j,
                                  &write->pImageInfo[j]);
          }
@@ -745,8 +727,7 @@
       case VK_DESCRIPTOR_TYPE_STORAGE_IMAGE:
       case VK_DESCRIPTOR_TYPE_INPUT_ATTACHMENT:
          for (uint32_t j = 0; j < write->descriptorCount; j++) {
-            panvk_write_img_desc(dev, set,
-                                 write->dstBinding,
+            panvk_write_img_desc(dev, set, write->dstBinding,
                                  write->dstArrayElement + j,
                                  &write->pImageInfo[j]);
          }
@@ -754,8 +735,7 @@
 
       case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
          for (uint32_t j = 0; j < write->descriptorCount; j++) {
-            panvk_write_tex_buf_desc(dev, set,
-                                     write->dstBinding,
+            panvk_write_tex_buf_desc(dev, set, write->dstBinding,
                                      write->dstArrayElement + j,
                                      write->pTexelBufferView[j]);
          }
@@ -763,8 +743,7 @@
 
       case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
          for (uint32_t j = 0; j < write->descriptorCount; j++) {
-            panvk_write_img_buf_desc(dev, set,
-                                     write->dstBinding,
+            panvk_write_img_buf_desc(dev, set, write->dstBinding,
                                      write->dstArrayElement + j,
                                      write->pTexelBufferView[j]);
          }
@@ -772,8 +751,7 @@
 
       case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER:
          for (uint32_t j = 0; j < write->descriptorCount; j++) {
-            panvk_write_ubo_desc(dev, set,
-                                 write->dstBinding,
+            panvk_write_ubo_desc(dev, set, write->dstBinding,
                                  write->dstArrayElement + j,
                                  &write->pBufferInfo[j]);
          }
@@ -781,8 +759,7 @@
 
       case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC:
          for (uint32_t j = 0; j < write->descriptorCount; j++) {
-            panvk_write_dyn_ubo_desc(dev, set,
-                                     write->dstBinding,
+            panvk_write_dyn_ubo_desc(dev, set, write->dstBinding,
                                      write->dstArrayElement + j,
                                      &write->pBufferInfo[j]);
          }
@@ -790,8 +767,7 @@
 
       case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER:
          for (uint32_t j = 0; j < write->descriptorCount; j++) {
-            panvk_write_ssbo_desc(dev, set,
-                                  write->dstBinding,
+            panvk_write_ssbo_desc(dev, set, write->dstBinding,
                                   write->dstArrayElement + j,
                                   &write->pBufferInfo[j]);
          }
@@ -799,8 +775,7 @@
 
       case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER_DYNAMIC:
          for (uint32_t j = 0; j < write->descriptorCount; j++) {
-            panvk_write_dyn_ssbo_desc(dev, set,
-                                      write->dstBinding,
+            panvk_write_dyn_ssbo_desc(dev, set, write->dstBinding,
                                       write->dstArrayElement + j,
                                       &write->pBufferInfo[j]);
          }
@@ -838,23 +813,20 @@
       switch (src_binding_layout->type) {
       case VK_DESCRIPTOR_TYPE_SAMPLER:
          for (uint32_t j = 0; j < copy->descriptorCount; j++) {
-            panvk_copy_sampler_desc(dst_set, copy->dstBinding,
-                                    copy->dstArrayElement + j,
-                                    src_set, copy->srcBinding,
-                                    copy->srcArrayElement + j);
+            panvk_copy_sampler_desc(
+               dst_set, copy->dstBinding, copy->dstArrayElement + j, src_set,
+               copy->srcBinding, copy->srcArrayElement + j);
          }
          break;
 
       case VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER:
          for (uint32_t j = 0; j < copy->descriptorCount; j++) {
-            panvk_copy_sampler_desc(dst_set, copy->dstBinding,
-                                    copy->dstArrayElement + j,
-                                    src_set, copy->srcBinding,
-                                    copy->srcArrayElement + j);
+            panvk_copy_sampler_desc(
+               dst_set, copy->dstBinding, copy->dstArrayElement + j, src_set,
+               copy->srcBinding, copy->srcArrayElement + j);
             panvk_copy_tex_desc(dst_set, copy->dstBinding,
-                                copy->dstArrayElement + j,
-                                src_set, copy->srcBinding,
-                                copy->srcArrayElement + j);
+                                copy->dstArrayElement + j, src_set,
+                                copy->srcBinding, copy->srcArrayElement + j);
          }
          break;
 
@@ -862,9 +834,8 @@
       case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
          for (uint32_t j = 0; j < copy->descriptorCount; j++) {
             panvk_copy_tex_desc(dst_set, copy->dstBinding,
-                                copy->dstArrayElement + j,
-                                src_set, copy->srcBinding,
-                                copy->srcArrayElement + j);
+                                copy->dstArrayElement + j, src_set,
+                                copy->srcBinding, copy->srcArrayElement + j);
          }
          break;
 
@@ -873,45 +844,40 @@
       case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
          for (uint32_t j = 0; j < copy->descriptorCount; j++) {
             panvk_copy_img_desc(dst_set, copy->dstBinding,
-                                copy->dstArrayElement + j,
-                                src_set, copy->srcBinding,
-                                copy->srcArrayElement + j);
+                                copy->dstArrayElement + j, src_set,
+                                copy->srcBinding, copy->srcArrayElement + j);
          }
          break;
 
       case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER:
          for (uint32_t j = 0; j < copy->descriptorCount; j++) {
             panvk_copy_ubo_desc(dst_set, copy->dstBinding,
-                                copy->dstArrayElement + j,
-                                src_set, copy->srcBinding,
-                                copy->srcArrayElement + j);
+                                copy->dstArrayElement + j, src_set,
+                                copy->srcBinding, copy->srcArrayElement + j);
          }
          break;
 
       case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC:
          for (uint32_t j = 0; j < copy->descriptorCount; j++) {
-            panvk_copy_dyn_ubo_desc(dst_set, copy->dstBinding,
-                                    copy->dstArrayElement + j,
-                                    src_set, copy->srcBinding,
-                                    copy->srcArrayElement + j);
+            panvk_copy_dyn_ubo_desc(
+               dst_set, copy->dstBinding, copy->dstArrayElement + j, src_set,
+               copy->srcBinding, copy->srcArrayElement + j);
          }
          break;
 
       case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER:
          for (uint32_t j = 0; j < copy->descriptorCount; j++) {
             panvk_copy_ssbo_desc(dst_set, copy->dstBinding,
-                                 copy->dstArrayElement + j,
-                                 src_set, copy->srcBinding,
-                                 copy->srcArrayElement + j);
+                                 copy->dstArrayElement + j, src_set,
+                                 copy->srcBinding, copy->srcArrayElement + j);
          }
          break;
 
       case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER_DYNAMIC:
          for (uint32_t j = 0; j < copy->descriptorCount; j++) {
-            panvk_copy_dyn_ssbo_desc(dst_set, copy->dstBinding,
-                                     copy->dstArrayElement + j,
-                                     src_set, copy->srcBinding,
-                                     copy->srcArrayElement + j);
+            panvk_copy_dyn_ssbo_desc(
+               dst_set, copy->dstBinding, copy->dstArrayElement + j, src_set,
+               copy->srcBinding, copy->srcArrayElement + j);
          }
          break;
 
@@ -922,10 +888,9 @@
 }
 
 void
-panvk_per_arch(UpdateDescriptorSetWithTemplate)(VkDevice _device,
-                                                VkDescriptorSet descriptorSet,
-                                                VkDescriptorUpdateTemplate descriptorUpdateTemplate,
-                                                const void *data)
+panvk_per_arch(UpdateDescriptorSetWithTemplate)(
+   VkDevice _device, VkDescriptorSet descriptorSet,
+   VkDescriptorUpdateTemplate descriptorUpdateTemplate, const void *data)
 {
    VK_FROM_HANDLE(panvk_device, dev, _device);
    VK_FROM_HANDLE(panvk_descriptor_set, set, descriptorSet);
@@ -935,8 +900,7 @@
    const struct panvk_descriptor_set_layout *layout = set->layout;
 
    for (uint32_t i = 0; i < template->entry_count; i++) {
-      const struct vk_descriptor_template_entry *entry =
-         &template->entries[i];
+      const struct vk_descriptor_template_entry *entry = &template->entries[i];
       const struct panvk_descriptor_set_binding_layout *binding_layout =
          &layout->bindings[entry->binding];
 
@@ -952,19 +916,15 @@
                  entry->type == VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER) &&
                 !binding_layout->immutable_samplers) {
 
-               panvk_write_sampler_desc(dev, set,
-                                        entry->binding,
-                                        entry->array_element + j,
-                                        info);
+               panvk_write_sampler_desc(dev, set, entry->binding,
+                                        entry->array_element + j, info);
             }
 
             if (entry->type == VK_DESCRIPTOR_TYPE_SAMPLED_IMAGE ||
                 entry->type == VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER) {
 
-               panvk_write_tex_desc(dev, set,
-                                    entry->binding,
-                                    entry->array_element + j,
-                                    info);
+               panvk_write_tex_desc(dev, set, entry->binding,
+                                    entry->array_element + j, info);
             }
          }
          break;
@@ -975,34 +935,26 @@
             const VkDescriptorImageInfo *info =
                data + entry->offset + j * entry->stride;
 
-               panvk_write_img_desc(dev, set,
-                                    entry->binding,
-                                    entry->array_element + j,
-                                    info);
+            panvk_write_img_desc(dev, set, entry->binding,
+                                 entry->array_element + j, info);
          }
          break;
 
       case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
          for (unsigned j = 0; j < entry->array_count; j++) {
-            const VkBufferView *view =
-               data + entry->offset + j * entry->stride;
+            const VkBufferView *view = data + entry->offset + j * entry->stride;
 
-            panvk_write_tex_buf_desc(dev, set,
-                                     entry->binding,
-                                     entry->array_element + j,
-                                     *view);
+            panvk_write_tex_buf_desc(dev, set, entry->binding,
+                                     entry->array_element + j, *view);
          }
          break;
 
       case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
          for (unsigned j = 0; j < entry->array_count; j++) {
-            const VkBufferView *view =
-               data + entry->offset + j * entry->stride;
+            const VkBufferView *view = data + entry->offset + j * entry->stride;
 
-            panvk_write_img_buf_desc(dev, set,
-                                     entry->binding,
-                                     entry->array_element + j,
-                                     *view);
+            panvk_write_img_buf_desc(dev, set, entry->binding,
+                                     entry->array_element + j, *view);
          }
          break;
 
@@ -1011,10 +963,8 @@
             const VkDescriptorBufferInfo *info =
                data + entry->offset + j * entry->stride;
 
-            panvk_write_ubo_desc(dev, set,
-                                 entry->binding,
-                                 entry->array_element + j,
-                                 info);
+            panvk_write_ubo_desc(dev, set, entry->binding,
+                                 entry->array_element + j, info);
          }
          break;
 
@@ -1023,10 +973,8 @@
             const VkDescriptorBufferInfo *info =
                data + entry->offset + j * entry->stride;
 
-            panvk_write_dyn_ubo_desc(dev, set,
-                                 entry->binding,
-                                 entry->array_element + j,
-                                 info);
+            panvk_write_dyn_ubo_desc(dev, set, entry->binding,
+                                     entry->array_element + j, info);
          }
          break;
 
@@ -1035,10 +983,8 @@
             const VkDescriptorBufferInfo *info =
                data + entry->offset + j * entry->stride;
 
-            panvk_write_ssbo_desc(dev, set,
-                                  entry->binding,
-                                  entry->array_element + j,
-                                  info);
+            panvk_write_ssbo_desc(dev, set, entry->binding,
+                                  entry->array_element + j, info);
          }
          break;
 
@@ -1047,10 +993,8 @@
             const VkDescriptorBufferInfo *info =
                data + entry->offset + j * entry->stride;
 
-            panvk_write_dyn_ssbo_desc(dev, set,
-                                      entry->binding,
-                                      entry->array_element + j,
-                                      info);
+            panvk_write_dyn_ssbo_desc(dev, set, entry->binding,
+                                      entry->array_element + j, info);
          }
          break;
       default:
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_vX_device.c mesa/src/panfrost/vulkan/panvk_vX_device.c
--- mesa-23.0.0/src/panfrost/vulkan/panvk_vX_device.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_vX_device.c	2023-03-06 19:19:32.546307768 +0100
@@ -30,16 +30,14 @@
 
 #include "decode.h"
 
-#include "panvk_private.h"
 #include "panvk_cs.h"
+#include "panvk_private.h"
 
 #include "vk_drm_syncobj.h"
 
 static void
-panvk_queue_submit_batch(struct panvk_queue *queue,
-                         struct panvk_batch *batch,
-                         uint32_t *bos, unsigned nr_bos,
-                         uint32_t *in_fences,
+panvk_queue_submit_batch(struct panvk_queue *queue, struct panvk_batch *batch,
+                         uint32_t *bos, unsigned nr_bos, uint32_t *in_fences,
                          unsigned nr_in_fences)
 {
    const struct panvk_device *dev = queue->device;
@@ -73,13 +71,14 @@
       assert(!ret);
 
       if (debug & (PANVK_DEBUG_TRACE | PANVK_DEBUG_SYNC)) {
-         ret = drmSyncobjWait(pdev->fd, &submit.out_sync, 1, INT64_MAX, 0, NULL);
+         ret =
+            drmSyncobjWait(pdev->fd, &submit.out_sync, 1, INT64_MAX, 0, NULL);
          assert(!ret);
       }
 
       if (debug & PANVK_DEBUG_TRACE)
          GENX(pandecode_jc)(batch->scoreboard.first_job, pdev->gpu_id);
-      
+
       if (debug & PANVK_DEBUG_DUMP)
          pandecode_dump_mappings();
    }
@@ -104,7 +103,8 @@
       ret = drmIoctl(pdev->fd, DRM_IOCTL_PANFROST_SUBMIT, &submit);
       assert(!ret);
       if (debug & (PANVK_DEBUG_TRACE | PANVK_DEBUG_SYNC)) {
-         ret = drmSyncobjWait(pdev->fd, &submit.out_sync, 1, INT64_MAX, 0, NULL);
+         ret =
+            drmSyncobjWait(pdev->fd, &submit.out_sync, 1, INT64_MAX, 0, NULL);
          assert(!ret);
       }
 
@@ -145,7 +145,8 @@
 }
 
 static void
-panvk_add_wait_event_syncobjs(struct panvk_batch *batch, uint32_t *in_fences, unsigned *nr_in_fences)
+panvk_add_wait_event_syncobjs(struct panvk_batch *batch, uint32_t *in_fences,
+                              unsigned *nr_in_fences)
 {
    util_dynarray_foreach(&batch->event_ops, struct panvk_event_op, op) {
       switch (op->type) {
@@ -165,7 +166,8 @@
 }
 
 static void
-panvk_signal_event_syncobjs(struct panvk_queue *queue, struct panvk_batch *batch)
+panvk_signal_event_syncobjs(struct panvk_queue *queue,
+                            struct panvk_batch *batch)
 {
    const struct panfrost_device *pdev = &queue->device->physical_device->pdev;
 
@@ -179,9 +181,8 @@
          struct panvk_event *event = op->event;
 
          struct drm_syncobj_array objs = {
-            .handles = (uint64_t) (uintptr_t) &event->syncobj,
-            .count_handles = 1
-         };
+            .handles = (uint64_t)(uintptr_t)&event->syncobj,
+            .count_handles = 1};
 
          int ret = drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_RESET, &objs);
          assert(!ret);
@@ -200,8 +201,7 @@
 panvk_per_arch(queue_submit)(struct vk_queue *vk_queue,
                              struct vk_queue_submit *submit)
 {
-   struct panvk_queue *queue =
-      container_of(vk_queue, struct panvk_queue, vk);
+   struct panvk_queue *queue = container_of(vk_queue, struct panvk_queue, vk);
    const struct panfrost_device *pdev = &queue->device->physical_device->pdev;
 
    unsigned nr_semaphores = submit->wait_count + 1;
@@ -227,8 +227,7 @@
             panvk_pool_num_bos(&cmdbuf->varying_pool) +
             panvk_pool_num_bos(&cmdbuf->tls_pool) +
             (batch->fb.info ? batch->fb.info->attachment_count : 0) +
-            (batch->blit.src ? 1 : 0) +
-            (batch->blit.dst ? 1 : 0) +
+            (batch->blit.src ? 1 : 0) + (batch->blit.dst ? 1 : 0) +
             (batch->scoreboard.first_tiler ? 1 : 0) + 1;
          unsigned bo_idx = 0;
          uint32_t bos[nr_bos];
@@ -244,7 +243,8 @@
 
          if (batch->fb.info) {
             for (unsigned i = 0; i < batch->fb.info->attachment_count; i++) {
-               bos[bo_idx++] = batch->fb.info->attachments[i].iview->pview.image->data.bo->gem_handle;
+               bos[bo_idx++] = batch->fb.info->attachments[i]
+                                  .iview->pview.image->data.bo->gem_handle;
             }
          }
 
@@ -262,7 +262,7 @@
 
          /* Merge identical BO entries. */
          for (unsigned x = 0; x < nr_bos; x++) {
-            for (unsigned y = x + 1; y < nr_bos; ) {
+            for (unsigned y = x + 1; y < nr_bos;) {
                if (bos[x] == bos[y])
                   bos[y] = bos[--nr_bos];
                else
@@ -271,16 +271,16 @@
          }
 
          unsigned nr_in_fences = 0;
-         unsigned max_wait_event_syncobjs =
-            util_dynarray_num_elements(&batch->event_ops,
-                                       struct panvk_event_op);
+         unsigned max_wait_event_syncobjs = util_dynarray_num_elements(
+            &batch->event_ops, struct panvk_event_op);
          uint32_t in_fences[nr_semaphores + max_wait_event_syncobjs];
          memcpy(in_fences, semaphores, nr_semaphores * sizeof(*in_fences));
          nr_in_fences += nr_semaphores;
 
          panvk_add_wait_event_syncobjs(batch, in_fences, &nr_in_fences);
 
-         panvk_queue_submit_batch(queue, batch, bos, nr_bos, in_fences, nr_in_fences);
+         panvk_queue_submit_batch(queue, batch, bos, nr_bos, in_fences,
+                                  nr_in_fences);
 
          panvk_signal_event_syncobjs(queue, batch);
       }
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_vX_device.h mesa/src/panfrost/vulkan/panvk_vX_device.h
--- mesa-23.0.0/src/panfrost/vulkan/panvk_vX_device.h	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_vX_device.h	2023-03-06 19:19:32.547307775 +0100
@@ -29,6 +29,5 @@
 #error "no arch"
 #endif
 
-VkResult
-panvk_per_arch(queue_submit)(struct vk_queue *queue,
-                             struct vk_queue_submit *submit);
+VkResult panvk_per_arch(queue_submit)(struct vk_queue *queue,
+                                      struct vk_queue_submit *submit);
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_vX_image.c mesa/src/panfrost/vulkan/panvk_vX_image.c
--- mesa-23.0.0/src/panfrost/vulkan/panvk_vX_image.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_vX_image.c	2023-03-06 19:19:32.550307795 +0100
@@ -29,12 +29,12 @@
 #include "genxml/gen_macros.h"
 #include "panvk_private.h"
 
-#include "util/u_debug.h"
+#include "drm-uapi/drm_fourcc.h"
 #include "util/u_atomic.h"
+#include "util/u_debug.h"
 #include "vk_format.h"
 #include "vk_object.h"
 #include "vk_util.h"
-#include "drm-uapi/drm_fourcc.h"
 
 static enum mali_texture_dimension
 panvk_view_type_to_mali_tex_dim(VkImageViewType type)
@@ -57,8 +57,7 @@
 }
 
 static void
-panvk_convert_swizzle(const VkComponentMapping *in,
-                      unsigned char *out)
+panvk_convert_swizzle(const VkComponentMapping *in, unsigned char *out)
 {
    const VkComponentSwizzle *comp = &in->r;
    for (unsigned i = 0; i < 4; i++) {
@@ -97,22 +96,20 @@
    VK_FROM_HANDLE(panvk_image, image, pCreateInfo->image);
    struct panvk_image_view *view;
 
-   view = vk_image_view_create(&device->vk, false, pCreateInfo,
-                               pAllocator, sizeof(*view));
+   view = vk_image_view_create(&device->vk, false, pCreateInfo, pAllocator,
+                               sizeof(*view));
    if (view == NULL)
       return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
 
-   view->pview = (struct pan_image_view) {
+   view->pview = (struct pan_image_view){
       .image = &image->pimage,
       .format = vk_format_to_pipe_format(view->vk.view_format),
       .dim = panvk_view_type_to_mali_tex_dim(view->vk.view_type),
       .nr_samples = image->pimage.layout.nr_samples,
       .first_level = view->vk.base_mip_level,
-      .last_level = view->vk.base_mip_level +
-                    view->vk.level_count - 1,
+      .last_level = view->vk.base_mip_level + view->vk.level_count - 1,
       .first_layer = view->vk.base_array_layer,
-      .last_layer = view->vk.base_array_layer +
-                    view->vk.layer_count - 1,
+      .last_layer = view->vk.base_array_layer + view->vk.layer_count - 1,
    };
    panvk_convert_swizzle(&view->vk.swizzle, view->pview.swizzle);
 
@@ -127,21 +124,23 @@
       view->bo = panfrost_bo_create(pdev, bo_size, 0, "Texture descriptor");
 
       STATIC_ASSERT(sizeof(view->descs.tex) >= pan_size(TEXTURE));
-      GENX(panfrost_new_texture)(pdev, &view->pview, &view->descs.tex, &view->bo->ptr);
+      GENX(panfrost_new_texture)
+      (pdev, &view->pview, &view->descs.tex, &view->bo->ptr);
    }
 
    if (view->vk.usage & VK_IMAGE_USAGE_STORAGE_BIT) {
       uint8_t *attrib_buf = (uint8_t *)view->descs.img_attrib_buf;
       bool is_3d = image->pimage.layout.dim == MALI_TEXTURE_DIMENSION_3D;
       unsigned offset = image->pimage.data.offset;
-      offset += panfrost_texture_offset(&image->pimage.layout,
-                                        view->pview.first_level,
-                                        is_3d ? 0 : view->pview.first_layer,
-                                        is_3d ? view->pview.first_layer : 0);
+      offset +=
+         panfrost_texture_offset(&image->pimage.layout, view->pview.first_level,
+                                 is_3d ? 0 : view->pview.first_layer,
+                                 is_3d ? view->pview.first_layer : 0);
 
       pan_pack(attrib_buf, ATTRIBUTE_BUFFER, cfg) {
-         cfg.type = image->pimage.layout.modifier == DRM_FORMAT_MOD_LINEAR ?
-                    MALI_ATTRIBUTE_TYPE_3D_LINEAR : MALI_ATTRIBUTE_TYPE_3D_INTERLEAVED;
+         cfg.type = image->pimage.layout.modifier == DRM_FORMAT_MOD_LINEAR
+                       ? MALI_ATTRIBUTE_TYPE_3D_LINEAR
+                       : MALI_ATTRIBUTE_TYPE_3D_INTERLEAVED;
          cfg.pointer = image->pimage.data.bo->ptr.gpu + offset;
          cfg.stride = util_format_get_blocksize(view->pview.format);
          cfg.size = image->pimage.data.bo->size - offset;
@@ -154,9 +153,9 @@
          cfg.s_dimension = u_minify(image->pimage.layout.width, level);
          cfg.t_dimension = u_minify(image->pimage.layout.height, level);
          cfg.r_dimension =
-            view->pview.dim == MALI_TEXTURE_DIMENSION_3D ?
-            u_minify(image->pimage.layout.depth, level) :
-            (view->pview.last_layer - view->pview.first_layer + 1);
+            view->pview.dim == MALI_TEXTURE_DIMENSION_3D
+               ? u_minify(image->pimage.layout.depth, level)
+               : (view->pview.last_layer - view->pview.first_layer + 1);
          cfg.row_stride = image->pimage.layout.slices[level].row_stride;
          if (cfg.r_dimension > 1) {
             cfg.slice_stride =
@@ -178,9 +177,8 @@
    VK_FROM_HANDLE(panvk_device, device, _device);
    VK_FROM_HANDLE(panvk_buffer, buffer, pCreateInfo->buffer);
 
-   struct panvk_buffer_view *view =
-      vk_object_zalloc(&device->vk, pAllocator, sizeof(*view),
-                       VK_OBJECT_TYPE_BUFFER_VIEW);
+   struct panvk_buffer_view *view = vk_object_zalloc(
+      &device->vk, pAllocator, sizeof(*view), VK_OBJECT_TYPE_BUFFER_VIEW);
 
    if (!view)
       return vk_error(device->instance, VK_ERROR_OUT_OF_HOST_MEMORY);
@@ -189,8 +187,8 @@
 
    struct panfrost_device *pdev = &device->physical_device->pdev;
    mali_ptr address = panvk_buffer_gpu_ptr(buffer, pCreateInfo->offset);
-   unsigned size = panvk_buffer_range(buffer, pCreateInfo->offset,
-                                      pCreateInfo->range);
+   unsigned size =
+      panvk_buffer_range(buffer, pCreateInfo->offset, pCreateInfo->range);
    unsigned blksz = util_format_get_blocksize(view->fmt);
    view->elems = size / blksz;
 
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_vX_meta_blit.c mesa/src/panfrost/vulkan/panvk_vX_meta_blit.c
--- mesa-23.0.0/src/panfrost/vulkan/panvk_vX_meta_blit.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_vX_meta_blit.c	2023-03-06 19:19:32.571307933 +0100
@@ -42,19 +42,23 @@
          .nr_samples = blitinfo->dst.planes[0].image->layout.nr_samples,
          .first_level = blitinfo->dst.level,
          .last_level = blitinfo->dst.level,
-         .swizzle = { PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_Z, PIPE_SWIZZLE_W },
+         .swizzle = {PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_Z,
+                     PIPE_SWIZZLE_W},
       },
    };
 
    *fbinfo = (struct pan_fb_info){
-      .width = u_minify(blitinfo->dst.planes[0].image->layout.width, blitinfo->dst.level),
-      .height = u_minify(blitinfo->dst.planes[0].image->layout.height, blitinfo->dst.level),
-      .extent = {
-         .minx = MAX2(MIN2(blitinfo->dst.start.x, blitinfo->dst.end.x), 0),
-         .miny = MAX2(MIN2(blitinfo->dst.start.y, blitinfo->dst.end.y), 0),
-         .maxx = MAX2(blitinfo->dst.start.x, blitinfo->dst.end.x),
-         .maxy = MAX2(blitinfo->dst.start.y, blitinfo->dst.end.y),
-      },
+      .width = u_minify(blitinfo->dst.planes[0].image->layout.width,
+                        blitinfo->dst.level),
+      .height = u_minify(blitinfo->dst.planes[0].image->layout.height,
+                         blitinfo->dst.level),
+      .extent =
+         {
+            .minx = MAX2(MIN2(blitinfo->dst.start.x, blitinfo->dst.end.x), 0),
+            .miny = MAX2(MIN2(blitinfo->dst.start.y, blitinfo->dst.end.y), 0),
+            .maxx = MAX2(blitinfo->dst.start.x, blitinfo->dst.end.x),
+            .maxy = MAX2(blitinfo->dst.start.y, blitinfo->dst.end.y),
+         },
       .nr_samples = blitinfo->dst.planes[0].image->layout.nr_samples,
    };
 
@@ -121,8 +125,8 @@
       tsd = batch->tls.gpu;
       tiler = batch->tiler.descs.gpu;
 
-      struct panfrost_ptr job =
-         GENX(pan_blit)(&ctx, &cmdbuf->desc_pool.base, &batch->scoreboard, tsd, tiler);
+      struct panfrost_ptr job = GENX(pan_blit)(&ctx, &cmdbuf->desc_pool.base,
+                                               &batch->scoreboard, tsd, tiler);
       util_dynarray_append(&batch->jobs, void *, job.cpu);
       panvk_per_arch(cmd_close_batch)(cmdbuf);
    } while (pan_blit_next_surface(&ctx));
@@ -139,52 +143,64 @@
    for (unsigned i = 0; i < pBlitImageInfo->regionCount; i++) {
       const VkImageBlit2 *region = &pBlitImageInfo->pRegions[i];
       struct pan_blit_info info = {
-         .src = {
-            .planes[0].image = &src->pimage,
-            .planes[0].format = src->pimage.layout.format,
-            .level = region->srcSubresource.mipLevel,
-            .start = {
-               region->srcOffsets[0].x,
-               region->srcOffsets[0].y,
-               region->srcOffsets[0].z,
-               region->srcSubresource.baseArrayLayer,
-            },
-            .end = {
-               region->srcOffsets[1].x,
-               region->srcOffsets[1].y,
-               region->srcOffsets[1].z,
-               region->srcSubresource.baseArrayLayer + region->srcSubresource.layerCount - 1,
+         .src =
+            {
+               .planes[0].image = &src->pimage,
+               .planes[0].format = src->pimage.layout.format,
+               .level = region->srcSubresource.mipLevel,
+               .start =
+                  {
+                     region->srcOffsets[0].x,
+                     region->srcOffsets[0].y,
+                     region->srcOffsets[0].z,
+                     region->srcSubresource.baseArrayLayer,
+                  },
+               .end =
+                  {
+                     region->srcOffsets[1].x,
+                     region->srcOffsets[1].y,
+                     region->srcOffsets[1].z,
+                     region->srcSubresource.baseArrayLayer +
+                        region->srcSubresource.layerCount - 1,
+                  },
             },
-         },
-         .dst = {
-            .planes[0].image = &dst->pimage,
-            .planes[0].format = dst->pimage.layout.format,
-            .level = region->dstSubresource.mipLevel,
-            .start = {
-               region->dstOffsets[0].x,
-               region->dstOffsets[0].y,
-               region->dstOffsets[0].z,
-               region->dstSubresource.baseArrayLayer,
-            },
-            .end = {
-               region->dstOffsets[1].x,
-               region->dstOffsets[1].y,
-               region->dstOffsets[1].z,
-               region->dstSubresource.baseArrayLayer + region->dstSubresource.layerCount - 1,
+         .dst =
+            {
+               .planes[0].image = &dst->pimage,
+               .planes[0].format = dst->pimage.layout.format,
+               .level = region->dstSubresource.mipLevel,
+               .start =
+                  {
+                     region->dstOffsets[0].x,
+                     region->dstOffsets[0].y,
+                     region->dstOffsets[0].z,
+                     region->dstSubresource.baseArrayLayer,
+                  },
+               .end =
+                  {
+                     region->dstOffsets[1].x,
+                     region->dstOffsets[1].y,
+                     region->dstOffsets[1].z,
+                     region->dstSubresource.baseArrayLayer +
+                        region->dstSubresource.layerCount - 1,
+                  },
             },
-         },
          .nearest = pBlitImageInfo->filter == VK_FILTER_NEAREST,
       };
 
       if (region->srcSubresource.aspectMask == VK_IMAGE_ASPECT_STENCIL_BIT)
-         info.src.planes[0].format = util_format_stencil_only(info.src.planes[0].format);
+         info.src.planes[0].format =
+            util_format_stencil_only(info.src.planes[0].format);
       else if (region->srcSubresource.aspectMask == VK_IMAGE_ASPECT_DEPTH_BIT)
-         info.src.planes[0].format = util_format_get_depth_only(info.src.planes[0].format);
+         info.src.planes[0].format =
+            util_format_get_depth_only(info.src.planes[0].format);
 
       if (region->dstSubresource.aspectMask == VK_IMAGE_ASPECT_STENCIL_BIT)
-         info.dst.planes[0].format = util_format_stencil_only(info.dst.planes[0].format);
+         info.dst.planes[0].format =
+            util_format_stencil_only(info.dst.planes[0].format);
       else if (region->dstSubresource.aspectMask == VK_IMAGE_ASPECT_DEPTH_BIT)
-         info.dst.planes[0].format = util_format_get_depth_only(info.dst.planes[0].format);
+         info.dst.planes[0].format =
+            util_format_get_depth_only(info.dst.planes[0].format);
 
       panvk_meta_blit(cmdbuf, &info);
    }
@@ -192,7 +208,7 @@
 
 void
 panvk_per_arch(CmdResolveImage2)(VkCommandBuffer commandBuffer,
-                                 const VkResolveImageInfo2* pResolveImageInfo)
+                                 const VkResolveImageInfo2 *pResolveImageInfo)
 {
    panvk_stub();
 }
@@ -201,14 +217,14 @@
 panvk_per_arch(meta_blit_init)(struct panvk_physical_device *dev)
 {
    panvk_pool_init(&dev->meta.blitter.bin_pool, &dev->pdev, NULL,
-                   PAN_BO_EXECUTE, 16 * 1024,
-                   "panvk_meta blitter binary pool", false);
-   panvk_pool_init(&dev->meta.blitter.desc_pool, &dev->pdev, NULL,
-                   0, 16 * 1024, "panvk_meta blitter descriptor pool",
+                   PAN_BO_EXECUTE, 16 * 1024, "panvk_meta blitter binary pool",
                    false);
+   panvk_pool_init(&dev->meta.blitter.desc_pool, &dev->pdev, NULL, 0, 16 * 1024,
+                   "panvk_meta blitter descriptor pool", false);
    pan_blend_shaders_init(&dev->pdev);
-   GENX(pan_blitter_init)(&dev->pdev, &dev->meta.blitter.bin_pool.base,
-                          &dev->meta.blitter.desc_pool.base);
+   GENX(pan_blitter_init)
+   (&dev->pdev, &dev->meta.blitter.bin_pool.base,
+    &dev->meta.blitter.desc_pool.base);
 }
 
 void
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_vX_meta.c mesa/src/panfrost/vulkan/panvk_vX_meta.c
--- mesa-23.0.0/src/panfrost/vulkan/panvk_vX_meta.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_vX_meta.c	2023-03-06 19:19:32.551307801 +0100
@@ -32,9 +32,8 @@
 #include "vk_format.h"
 
 mali_ptr
-panvk_per_arch(meta_emit_viewport)(struct pan_pool *pool,
-                                   uint16_t minx, uint16_t miny,
-                                   uint16_t maxx, uint16_t maxy)
+panvk_per_arch(meta_emit_viewport)(struct pan_pool *pool, uint16_t minx,
+                                   uint16_t miny, uint16_t maxx, uint16_t maxy)
 {
    struct panfrost_ptr vp = pan_pool_alloc_desc(pool, VIEWPORT);
 
@@ -53,8 +52,8 @@
 {
    panvk_pool_init(&dev->meta.bin_pool, &dev->pdev, NULL, PAN_BO_EXECUTE,
                    16 * 1024, "panvk_meta binary pool", false);
-   panvk_pool_init(&dev->meta.desc_pool, &dev->pdev, NULL, 0,
-                   16 * 1024, "panvk_meta descriptor pool", false);
+   panvk_pool_init(&dev->meta.desc_pool, &dev->pdev, NULL, 0, 16 * 1024,
+                   "panvk_meta descriptor pool", false);
    panvk_per_arch(meta_blit_init)(dev);
    panvk_per_arch(meta_copy_init)(dev);
    panvk_per_arch(meta_clear_init)(dev);
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_vX_meta_clear.c mesa/src/panfrost/vulkan/panvk_vX_meta_clear.c
--- mesa-23.0.0/src/panfrost/vulkan/panvk_vX_meta_clear.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_vX_meta_clear.c	2023-03-06 19:19:32.578307980 +0100
@@ -37,19 +37,17 @@
                                          enum glsl_base_type base_type,
                                          struct pan_shader_info *shader_info)
 {
-   nir_builder b = nir_builder_init_simple_shader(MESA_SHADER_FRAGMENT,
-                                     GENX(pan_shader_get_compiler_options)(),
-                                     "panvk_meta_clear_attachment(base_type=%d)",
-                                     base_type);
+   nir_builder b = nir_builder_init_simple_shader(
+      MESA_SHADER_FRAGMENT, GENX(pan_shader_get_compiler_options)(),
+      "panvk_meta_clear_attachment(base_type=%d)", base_type);
 
    const struct glsl_type *out_type = glsl_vector_type(base_type, 4);
    nir_variable *out =
       nir_variable_create(b.shader, nir_var_shader_out, out_type, "out");
    out->data.location = FRAG_RESULT_DATA0;
 
-   nir_ssa_def *clear_values = nir_load_push_constant(&b, 4, 32,
-                                                      nir_imm_int(&b, 0),
-                                                     .range = ~0);
+   nir_ssa_def *clear_values =
+      nir_load_push_constant(&b, 4, 32, nir_imm_int(&b, 0), .range = ~0);
    nir_store_var(&b, out, clear_values, 0xff);
 
    struct panfrost_compile_inputs inputs = {
@@ -77,15 +75,12 @@
 static mali_ptr
 panvk_meta_clear_color_attachment_emit_rsd(struct panfrost_device *pdev,
                                            struct pan_pool *desc_pool,
-                                           enum pipe_format format,
-                                           unsigned rt,
+                                           enum pipe_format format, unsigned rt,
                                            struct pan_shader_info *shader_info,
                                            mali_ptr shader)
 {
-   struct panfrost_ptr rsd_ptr =
-      pan_pool_alloc_desc_aggregate(desc_pool,
-                                    PAN_DESC(RENDERER_STATE),
-                                    PAN_DESC_ARRAY(rt + 1, BLEND));
+   struct panfrost_ptr rsd_ptr = pan_pool_alloc_desc_aggregate(
+      desc_pool, PAN_DESC(RENDERER_STATE), PAN_DESC_ARRAY(rt + 1, BLEND));
 
    pan_pack(rsd_ptr.cpu, RENDERER_STATE, cfg) {
       pan_shader_prepare_rsd(shader_info, shader, &cfg);
@@ -152,9 +147,9 @@
          cfg.stencil_mask_misc.stencil_mask_front = 0xFF;
          cfg.stencil_mask_misc.stencil_mask_back = 0xFF;
 
-         cfg.stencil_front.compare_function =
-            (mask & VK_IMAGE_ASPECT_DEPTH_BIT) ?
-            MALI_FUNC_ALWAYS : MALI_FUNC_NOT_EQUAL;
+         cfg.stencil_front.compare_function = (mask & VK_IMAGE_ASPECT_DEPTH_BIT)
+                                                 ? MALI_FUNC_ALWAYS
+                                                 : MALI_FUNC_NOT_EQUAL;
 
          cfg.stencil_front.stencil_fail = MALI_STENCIL_OP_KEEP;
          cfg.stencil_front.depth_fail = MALI_STENCIL_OP_REPLACE;
@@ -173,10 +168,9 @@
 }
 
 static void
-panvk_meta_clear_attachment_emit_dcd(struct pan_pool *pool,
-                                     mali_ptr coords, mali_ptr push_constants,
-                                     mali_ptr vpd, mali_ptr tsd, mali_ptr rsd,
-                                     void *out)
+panvk_meta_clear_attachment_emit_dcd(struct pan_pool *pool, mali_ptr coords,
+                                     mali_ptr push_constants, mali_ptr vpd,
+                                     mali_ptr tsd, mali_ptr rsd, void *out)
 {
    pan_pack(out, DRAW, cfg) {
       cfg.thread_storage = tsd;
@@ -195,14 +189,11 @@
                                            mali_ptr vpd, mali_ptr rsd,
                                            mali_ptr tsd, mali_ptr tiler)
 {
-   struct panfrost_ptr job =
-      pan_pool_alloc_desc(desc_pool, TILER_JOB);
+   struct panfrost_ptr job = pan_pool_alloc_desc(desc_pool, TILER_JOB);
 
-   panvk_meta_clear_attachment_emit_dcd(desc_pool,
-                                        coords,
-                                        push_constants,
-                                        vpd, tsd, rsd,
-                                        pan_section_ptr(job.cpu, TILER_JOB, DRAW));
+   panvk_meta_clear_attachment_emit_dcd(
+      desc_pool, coords, push_constants, vpd, tsd, rsd,
+      pan_section_ptr(job.cpu, TILER_JOB, DRAW));
 
    pan_section_pack(job.cpu, TILER_JOB, PRIMITIVE, cfg) {
       cfg.draw_mode = MALI_DRAW_MODE_TRIANGLE_STRIP;
@@ -214,19 +205,17 @@
       cfg.constant = 1.0f;
    }
 
-   void *invoc = pan_section_ptr(job.cpu,
-                                 TILER_JOB,
-                                 INVOCATION);
-   panfrost_pack_work_groups_compute(invoc, 1, 4,
-                                     1, 1, 1, 1, true, false);
+   void *invoc = pan_section_ptr(job.cpu, TILER_JOB, INVOCATION);
+   panfrost_pack_work_groups_compute(invoc, 1, 4, 1, 1, 1, 1, true, false);
 
-   pan_section_pack(job.cpu, TILER_JOB, PADDING, cfg);
+   pan_section_pack(job.cpu, TILER_JOB, PADDING, cfg)
+      ;
    pan_section_pack(job.cpu, TILER_JOB, TILER, cfg) {
       cfg.address = tiler;
    }
 
-   panfrost_add_job(desc_pool, scoreboard, MALI_JOB_TYPE_TILER,
-                    false, false, 0, 0, &job, false);
+   panfrost_add_job(desc_pool, scoreboard, MALI_JOB_TYPE_TILER, false, false, 0,
+                    0, &job, false);
    return job;
 }
 
@@ -242,7 +231,7 @@
    if (desc->channel[i].normalized)
       return GLSL_TYPE_FLOAT;
 
-   switch(desc->channel[i].type) {
+   switch (desc->channel[i].type) {
 
    case UTIL_FORMAT_TYPE_UNSIGNED:
       return GLSL_TYPE_UINT;
@@ -271,28 +260,28 @@
    struct panvk_meta *meta = &cmdbuf->device->physical_device->meta;
    struct panvk_batch *batch = cmdbuf->state.batch;
    const struct panvk_render_pass *pass = cmdbuf->state.pass;
-   const struct panvk_render_pass_attachment *att = &pass->attachments[attachment];
+   const struct panvk_render_pass_attachment *att =
+      &pass->attachments[attachment];
    unsigned minx = MAX2(clear_rect->rect.offset.x, 0);
    unsigned miny = MAX2(clear_rect->rect.offset.y, 0);
-   unsigned maxx = MAX2(clear_rect->rect.offset.x + clear_rect->rect.extent.width - 1, 0);
-   unsigned maxy = MAX2(clear_rect->rect.offset.y + clear_rect->rect.extent.height - 1, 0);
+   unsigned maxx =
+      MAX2(clear_rect->rect.offset.x + clear_rect->rect.extent.width - 1, 0);
+   unsigned maxy =
+      MAX2(clear_rect->rect.offset.y + clear_rect->rect.extent.height - 1, 0);
 
    panvk_per_arch(cmd_alloc_fb_desc)(cmdbuf);
    panvk_per_arch(cmd_alloc_tls_desc)(cmdbuf, true);
    panvk_per_arch(cmd_prepare_tiler_context)(cmdbuf);
 
-   mali_ptr vpd =
-      panvk_per_arch(meta_emit_viewport)(&cmdbuf->desc_pool.base,
-                                         minx, miny, maxx, maxy);
+   mali_ptr vpd = panvk_per_arch(meta_emit_viewport)(&cmdbuf->desc_pool.base,
+                                                     minx, miny, maxx, maxy);
 
    float rect[] = {
-      minx, miny, 0.0, 1.0,
-      maxx + 1, miny, 0.0, 1.0,
-      minx, maxy + 1, 0.0, 1.0,
-      maxx + 1, maxy + 1, 0.0, 1.0,
+      minx, miny,     0.0, 1.0, maxx + 1, miny,     0.0, 1.0,
+      minx, maxy + 1, 0.0, 1.0, maxx + 1, maxy + 1, 0.0, 1.0,
    };
-   mali_ptr coordinates = pan_pool_upload_aligned(&cmdbuf->desc_pool.base,
-                                                  rect, sizeof(rect), 64);
+   mali_ptr coordinates =
+      pan_pool_upload_aligned(&cmdbuf->desc_pool.base, rect, sizeof(rect), 64);
 
    enum glsl_base_type base_type = panvk_meta_get_format_type(att->format);
 
@@ -303,29 +292,24 @@
 
    if (mask & VK_IMAGE_ASPECT_COLOR_BIT) {
       mali_ptr shader = meta->clear_attachment.color[base_type].shader;
-      struct pan_shader_info *shader_info = &meta->clear_attachment.color[base_type].shader_info;
+      struct pan_shader_info *shader_info =
+         &meta->clear_attachment.color[base_type].shader_info;
 
-      pushconsts = pan_pool_upload_aligned(&cmdbuf->desc_pool.base,
-                              clear_value, sizeof(*clear_value), 16);
+      pushconsts = pan_pool_upload_aligned(&cmdbuf->desc_pool.base, clear_value,
+                                           sizeof(*clear_value), 16);
 
-      rsd = panvk_meta_clear_color_attachment_emit_rsd(pdev,
-                                                       &cmdbuf->desc_pool.base,
-                                                       att->format, rt,
-                                                       shader_info,
-                                                       shader);
+      rsd = panvk_meta_clear_color_attachment_emit_rsd(
+         pdev, &cmdbuf->desc_pool.base, att->format, rt, shader_info, shader);
    } else {
-      rsd = panvk_meta_clear_zs_attachment_emit_rsd(pdev,
-                                                    &cmdbuf->desc_pool.base,
-                                                    mask,
-                                                    clear_value->depthStencil);
+      rsd = panvk_meta_clear_zs_attachment_emit_rsd(
+         pdev, &cmdbuf->desc_pool.base, mask, clear_value->depthStencil);
    }
 
    struct panfrost_ptr job;
 
-   job = panvk_meta_clear_attachment_emit_tiler_job(&cmdbuf->desc_pool.base,
-                                                    &batch->scoreboard,
-                                                    coordinates, pushconsts,
-                                                    vpd, rsd, tsd, tiler);
+   job = panvk_meta_clear_attachment_emit_tiler_job(
+      &cmdbuf->desc_pool.base, &batch->scoreboard, coordinates, pushconsts, vpd,
+      rsd, tsd, tiler);
 
    util_dynarray_append(&batch->jobs, void *, job.cpu);
 }
@@ -342,7 +326,8 @@
       .dim = MALI_TEXTURE_DIMENSION_2D,
       .image = &img->pimage,
       .nr_samples = img->pimage.layout.nr_samples,
-      .swizzle = { PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_Z, PIPE_SWIZZLE_W },
+      .swizzle = {PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_Z,
+                  PIPE_SWIZZLE_W},
    };
 
    cmdbuf->state.fb.crc_valid[0] = false;
@@ -357,7 +342,8 @@
    uint32_t clearval[4];
    pan_pack_color(clearval, (union pipe_color_union *)color,
                   img->pimage.layout.format, false);
-   memcpy(fbinfo->rts[0].clear_value, clearval, sizeof(fbinfo->rts[0].clear_value));
+   memcpy(fbinfo->rts[0].clear_value, clearval,
+          sizeof(fbinfo->rts[0].clear_value));
 
    unsigned level_count = vk_image_subresource_level_count(&img->vk, range);
    unsigned layer_count = vk_image_subresource_layer_count(&img->vk, range);
@@ -381,8 +367,7 @@
 }
 
 void
-panvk_per_arch(CmdClearColorImage)(VkCommandBuffer commandBuffer,
-                                   VkImage image,
+panvk_per_arch(CmdClearColorImage)(VkCommandBuffer commandBuffer, VkImage image,
                                    VkImageLayout imageLayout,
                                    const VkClearColorValue *pColor,
                                    uint32_t rangeCount,
@@ -409,7 +394,8 @@
       .dim = MALI_TEXTURE_DIMENSION_2D,
       .image = &img->pimage,
       .nr_samples = img->pimage.layout.nr_samples,
-      .swizzle = { PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_Z, PIPE_SWIZZLE_W },
+      .swizzle = {PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_Z,
+                  PIPE_SWIZZLE_W},
    };
 
    cmdbuf->state.fb.crc_valid[0] = false;
@@ -419,8 +405,7 @@
       .zs.clear_value.depth = value->depth,
       .zs.clear_value.stencil = value->stencil,
       .zs.clear.z = range->aspectMask & VK_IMAGE_ASPECT_DEPTH_BIT,
-      .zs.clear.s = range->aspectMask & VK_IMAGE_ASPECT_STENCIL_BIT
-   };
+      .zs.clear.s = range->aspectMask & VK_IMAGE_ASPECT_STENCIL_BIT};
 
    const struct util_format_description *fdesc =
       util_format_description(view.format);
@@ -457,12 +442,10 @@
 }
 
 void
-panvk_per_arch(CmdClearDepthStencilImage)(VkCommandBuffer commandBuffer,
-                                          VkImage image,
-                                          VkImageLayout imageLayout,
-                                          const VkClearDepthStencilValue *pDepthStencil,
-                                          uint32_t rangeCount,
-                                          const VkImageSubresourceRange *pRanges)
+panvk_per_arch(CmdClearDepthStencilImage)(
+   VkCommandBuffer commandBuffer, VkImage image, VkImageLayout imageLayout,
+   const VkClearDepthStencilValue *pDepthStencil, uint32_t rangeCount,
+   const VkImageSubresourceRange *pRanges)
 {
    VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
    VK_FROM_HANDLE(panvk_image, img, image);
@@ -495,12 +478,11 @@
          }
 
          if (attachment == VK_ATTACHMENT_UNUSED)
-               continue;
+            continue;
 
          panvk_meta_clear_attachment(cmdbuf, attachment, rt,
                                      pAttachments[i].aspectMask,
-                                     &pAttachments[i].clearValue,
-                                     &pRects[j]);
+                                     &pAttachments[i].clearValue, &pRects[j]);
       }
    }
 }
@@ -510,24 +492,18 @@
 {
    dev->meta.clear_attachment.color[GLSL_TYPE_UINT].shader =
       panvk_meta_clear_color_attachment_shader(
-            &dev->pdev,
-            &dev->meta.bin_pool.base,
-            GLSL_TYPE_UINT,
-            &dev->meta.clear_attachment.color[GLSL_TYPE_UINT].shader_info);
+         &dev->pdev, &dev->meta.bin_pool.base, GLSL_TYPE_UINT,
+         &dev->meta.clear_attachment.color[GLSL_TYPE_UINT].shader_info);
 
    dev->meta.clear_attachment.color[GLSL_TYPE_INT].shader =
       panvk_meta_clear_color_attachment_shader(
-            &dev->pdev,
-            &dev->meta.bin_pool.base,
-            GLSL_TYPE_INT,
-            &dev->meta.clear_attachment.color[GLSL_TYPE_INT].shader_info);
+         &dev->pdev, &dev->meta.bin_pool.base, GLSL_TYPE_INT,
+         &dev->meta.clear_attachment.color[GLSL_TYPE_INT].shader_info);
 
    dev->meta.clear_attachment.color[GLSL_TYPE_FLOAT].shader =
       panvk_meta_clear_color_attachment_shader(
-            &dev->pdev,
-            &dev->meta.bin_pool.base,
-            GLSL_TYPE_FLOAT,
-            &dev->meta.clear_attachment.color[GLSL_TYPE_FLOAT].shader_info);
+         &dev->pdev, &dev->meta.bin_pool.base, GLSL_TYPE_FLOAT,
+         &dev->meta.clear_attachment.color[GLSL_TYPE_FLOAT].shader_info);
 }
 
 void
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_vX_meta_copy.c mesa/src/panfrost/vulkan/panvk_vX_meta_copy.c
--- mesa-23.0.0/src/panfrost/vulkan/panvk_vX_meta_copy.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_vX_meta_copy.c	2023-03-06 19:19:32.613308211 +0100
@@ -34,13 +34,10 @@
                                  struct pan_pool *desc_pool,
                                  const struct pan_image_view *view)
 {
-   struct panfrost_ptr texture =
-      pan_pool_alloc_desc(desc_pool, TEXTURE);
-   size_t payload_size =
-      GENX(panfrost_estimate_texture_payload_size)(view);
-   struct panfrost_ptr surfaces =
-      pan_pool_alloc_aligned(desc_pool, payload_size,
-                             pan_alignment(SURFACE_WITH_STRIDE));
+   struct panfrost_ptr texture = pan_pool_alloc_desc(desc_pool, TEXTURE);
+   size_t payload_size = GENX(panfrost_estimate_texture_payload_size)(view);
+   struct panfrost_ptr surfaces = pan_pool_alloc_aligned(
+      desc_pool, payload_size, pan_alignment(SURFACE_WITH_STRIDE));
 
    GENX(panfrost_new_texture)(pdev, view, texture.cpu, &surfaces);
 
@@ -51,8 +48,7 @@
 panvk_meta_copy_img_emit_sampler(struct panfrost_device *pdev,
                                  struct pan_pool *desc_pool)
 {
-   struct panfrost_ptr sampler =
-      pan_pool_alloc_desc(desc_pool, SAMPLER);
+   struct panfrost_ptr sampler = pan_pool_alloc_desc(desc_pool, SAMPLER);
 
    pan_pack(sampler.cpu, SAMPLER, cfg) {
       cfg.seamless_cube_map = false;
@@ -65,13 +61,10 @@
 }
 
 static void
-panvk_meta_copy_emit_varying(struct pan_pool *pool,
-                             mali_ptr coordinates,
-                             mali_ptr *varying_bufs,
-                             mali_ptr *varyings)
+panvk_meta_copy_emit_varying(struct pan_pool *pool, mali_ptr coordinates,
+                             mali_ptr *varying_bufs, mali_ptr *varyings)
 {
-   struct panfrost_ptr varying =
-      pan_pool_alloc_desc(pool, ATTRIBUTE);
+   struct panfrost_ptr varying = pan_pool_alloc_desc(pool, ATTRIBUTE);
    struct panfrost_ptr varying_buffer =
       pan_pool_alloc_desc_array(pool, 2, ATTRIBUTE_BUFFER);
 
@@ -82,8 +75,9 @@
    }
 
    /* Bifrost needs an empty desc to mark end of prefetching */
-   pan_pack(varying_buffer.cpu + pan_size(ATTRIBUTE_BUFFER),
-            ATTRIBUTE_BUFFER, cfg);
+   pan_pack(varying_buffer.cpu + pan_size(ATTRIBUTE_BUFFER), ATTRIBUTE_BUFFER,
+            cfg)
+      ;
 
    pan_pack(varying.cpu, ATTRIBUTE, cfg) {
       cfg.buffer_index = 0;
@@ -95,11 +89,10 @@
 }
 
 static void
-panvk_meta_copy_emit_dcd(struct pan_pool *pool,
-                         mali_ptr src_coords, mali_ptr dst_coords,
-                         mali_ptr texture, mali_ptr sampler,
-                         mali_ptr vpd, mali_ptr tsd, mali_ptr rsd,
-                         mali_ptr push_constants, void *out)
+panvk_meta_copy_emit_dcd(struct pan_pool *pool, mali_ptr src_coords,
+                         mali_ptr dst_coords, mali_ptr texture,
+                         mali_ptr sampler, mali_ptr vpd, mali_ptr tsd,
+                         mali_ptr rsd, mali_ptr push_constants, void *out)
 {
    pan_pack(out, DRAW, cfg) {
       cfg.thread_storage = tsd;
@@ -107,9 +100,8 @@
       cfg.push_uniforms = push_constants;
       cfg.position = dst_coords;
       if (src_coords) {
-              panvk_meta_copy_emit_varying(pool, src_coords,
-                                           &cfg.varying_buffers,
-                                           &cfg.varyings);
+         panvk_meta_copy_emit_varying(pool, src_coords, &cfg.varying_buffers,
+                                      &cfg.varyings);
       }
       cfg.viewport = vpd;
       cfg.textures = texture;
@@ -122,15 +114,13 @@
                                struct pan_scoreboard *scoreboard,
                                mali_ptr src_coords, mali_ptr dst_coords,
                                mali_ptr texture, mali_ptr sampler,
-                               mali_ptr push_constants,
-                               mali_ptr vpd, mali_ptr rsd,
-                               mali_ptr tsd, mali_ptr tiler)
+                               mali_ptr push_constants, mali_ptr vpd,
+                               mali_ptr rsd, mali_ptr tsd, mali_ptr tiler)
 {
-   struct panfrost_ptr job =
-      pan_pool_alloc_desc(desc_pool, TILER_JOB);
+   struct panfrost_ptr job = pan_pool_alloc_desc(desc_pool, TILER_JOB);
 
-   panvk_meta_copy_emit_dcd(desc_pool, src_coords, dst_coords,
-                            texture, sampler, vpd, tsd, rsd, push_constants,
+   panvk_meta_copy_emit_dcd(desc_pool, src_coords, dst_coords, texture, sampler,
+                            vpd, tsd, rsd, push_constants,
                             pan_section_ptr(job.cpu, TILER_JOB, DRAW));
 
    pan_section_pack(job.cpu, TILER_JOB, PRIMITIVE, cfg) {
@@ -143,19 +133,17 @@
       cfg.constant = 1.0f;
    }
 
-   void *invoc = pan_section_ptr(job.cpu,
-                                 TILER_JOB,
-                                 INVOCATION);
-   panfrost_pack_work_groups_compute(invoc, 1, 4,
-                                     1, 1, 1, 1, true, false);
+   void *invoc = pan_section_ptr(job.cpu, TILER_JOB, INVOCATION);
+   panfrost_pack_work_groups_compute(invoc, 1, 4, 1, 1, 1, 1, true, false);
 
-   pan_section_pack(job.cpu, TILER_JOB, PADDING, cfg);
+   pan_section_pack(job.cpu, TILER_JOB, PADDING, cfg)
+      ;
    pan_section_pack(job.cpu, TILER_JOB, TILER, cfg) {
       cfg.address = tiler;
    }
 
-   panfrost_add_job(desc_pool, scoreboard, MALI_JOB_TYPE_TILER,
-                    false, false, 0, 0, &job, false);
+   panfrost_add_job(desc_pool, scoreboard, MALI_JOB_TYPE_TILER, false, false, 0,
+                    0, &job, false);
    return job;
 }
 
@@ -165,57 +153,55 @@
                                  const struct pan_compute_dim *num_wg,
                                  const struct pan_compute_dim *wg_sz,
                                  mali_ptr texture, mali_ptr sampler,
-                                 mali_ptr push_constants,
-                                 mali_ptr rsd, mali_ptr tsd)
+                                 mali_ptr push_constants, mali_ptr rsd,
+                                 mali_ptr tsd)
 {
-   struct panfrost_ptr job =
-      pan_pool_alloc_desc(desc_pool, COMPUTE_JOB);
+   struct panfrost_ptr job = pan_pool_alloc_desc(desc_pool, COMPUTE_JOB);
 
-   void *invoc = pan_section_ptr(job.cpu,
-                                 COMPUTE_JOB,
-                                 INVOCATION);
+   void *invoc = pan_section_ptr(job.cpu, COMPUTE_JOB, INVOCATION);
    panfrost_pack_work_groups_compute(invoc, num_wg->x, num_wg->y, num_wg->z,
-                                     wg_sz->x, wg_sz->y, wg_sz->z,
-                                     false, false);
+                                     wg_sz->x, wg_sz->y, wg_sz->z, false,
+                                     false);
 
    pan_section_pack(job.cpu, COMPUTE_JOB, PARAMETERS, cfg) {
       cfg.job_task_split = 8;
    }
 
-   panvk_meta_copy_emit_dcd(desc_pool, 0, 0, texture, sampler,
-                            0, tsd, rsd, push_constants,
+   panvk_meta_copy_emit_dcd(desc_pool, 0, 0, texture, sampler, 0, tsd, rsd,
+                            push_constants,
                             pan_section_ptr(job.cpu, COMPUTE_JOB, DRAW));
 
-   panfrost_add_job(desc_pool, scoreboard, MALI_JOB_TYPE_COMPUTE,
-                    false, false, 0, 0, &job, false);
+   panfrost_add_job(desc_pool, scoreboard, MALI_JOB_TYPE_COMPUTE, false, false,
+                    0, 0, &job, false);
    return job;
 }
 
-
 static uint32_t
 panvk_meta_copy_img_bifrost_raw_format(unsigned texelsize)
 {
    switch (texelsize) {
-   case 6: return MALI_RGB16UI << 12;
-   case 8: return MALI_RG32UI << 12;
-   case 12: return MALI_RGB32UI << 12;
-   case 16: return MALI_RGBA32UI << 12;
-   default: unreachable("Invalid texel size\n");
+   case 6:
+      return MALI_RGB16UI << 12;
+   case 8:
+      return MALI_RG32UI << 12;
+   case 12:
+      return MALI_RGB32UI << 12;
+   case 16:
+      return MALI_RGBA32UI << 12;
+   default:
+      unreachable("Invalid texel size\n");
    }
 }
 
 static mali_ptr
 panvk_meta_copy_to_img_emit_rsd(struct panfrost_device *pdev,
-                                struct pan_pool *desc_pool,
-                                mali_ptr shader,
+                                struct pan_pool *desc_pool, mali_ptr shader,
                                 const struct pan_shader_info *shader_info,
                                 enum pipe_format fmt, unsigned wrmask,
                                 bool from_img)
 {
-   struct panfrost_ptr rsd_ptr =
-      pan_pool_alloc_desc_aggregate(desc_pool,
-                                    PAN_DESC(RENDERER_STATE),
-                                    PAN_DESC_ARRAY(1, BLEND));
+   struct panfrost_ptr rsd_ptr = pan_pool_alloc_desc_aggregate(
+      desc_pool, PAN_DESC(RENDERER_STATE), PAN_DESC_ARRAY(1, BLEND));
 
    bool raw = util_format_get_blocksize(fmt) > 4;
    unsigned fullmask = (1 << util_format_get_nr_components(fmt)) - 1;
@@ -242,12 +228,9 @@
       cfg.stencil_back = cfg.stencil_front;
 
       cfg.properties.allow_forward_pixel_to_be_killed = true;
-      cfg.properties.allow_forward_pixel_to_kill =
-         !partialwrite && !readstb;
-      cfg.properties.zs_update_operation =
-         MALI_PIXEL_KILL_STRONG_EARLY;
-      cfg.properties.pixel_kill_operation =
-         MALI_PIXEL_KILL_FORCE_EARLY;
+      cfg.properties.allow_forward_pixel_to_kill = !partialwrite && !readstb;
+      cfg.properties.zs_update_operation = MALI_PIXEL_KILL_STRONG_EARLY;
+      cfg.properties.pixel_kill_operation = MALI_PIXEL_KILL_FORCE_EARLY;
    }
 
    pan_pack(rsd_ptr.cpu + pan_size(RENDERER_STATE), BLEND, cfg) {
@@ -260,9 +243,7 @@
       cfg.equation.alpha.b = MALI_BLEND_OPERAND_B_SRC;
       cfg.equation.alpha.c = MALI_BLEND_OPERAND_C_ZERO;
       cfg.internal.mode =
-         partialwrite ?
-         MALI_BLEND_MODE_FIXED_FUNCTION :
-         MALI_BLEND_MODE_OPAQUE;
+         partialwrite ? MALI_BLEND_MODE_FIXED_FUNCTION : MALI_BLEND_MODE_OPAQUE;
       cfg.equation.color_mask = partialwrite ? wrmask : 0xf;
       cfg.internal.fixed_function.num_comps = 4;
       if (!raw) {
@@ -276,9 +257,8 @@
          cfg.internal.fixed_function.conversion.memory_format =
             panvk_meta_copy_img_bifrost_raw_format(imgtexelsz);
          cfg.internal.fixed_function.conversion.register_format =
-            (imgtexelsz & 2) ?
-            MALI_REGISTER_FILE_FORMAT_U16 :
-            MALI_REGISTER_FILE_FORMAT_U32;
+            (imgtexelsz & 2) ? MALI_REGISTER_FILE_FORMAT_U16
+                             : MALI_REGISTER_FILE_FORMAT_U32;
       }
    }
 
@@ -287,14 +267,12 @@
 
 static mali_ptr
 panvk_meta_copy_to_buf_emit_rsd(struct panfrost_device *pdev,
-                                struct pan_pool *desc_pool,
-                                mali_ptr shader,
+                                struct pan_pool *desc_pool, mali_ptr shader,
                                 const struct pan_shader_info *shader_info,
                                 bool from_img)
 {
    struct panfrost_ptr rsd_ptr =
-      pan_pool_alloc_desc_aggregate(desc_pool,
-                                    PAN_DESC(RENDERER_STATE));
+      pan_pool_alloc_desc_aggregate(desc_pool, PAN_DESC(RENDERER_STATE));
 
    pan_pack(rsd_ptr.cpu, RENDERER_STATE, cfg) {
       pan_shader_prepare_rsd(shader_info, shader, &cfg);
@@ -310,22 +288,20 @@
 static mali_ptr
 panvk_meta_copy_img2img_shader(struct panfrost_device *pdev,
                                struct pan_pool *bin_pool,
-                               enum pipe_format srcfmt,
-                               enum pipe_format dstfmt, unsigned dstmask,
-                               unsigned texdim, bool texisarray, bool is_ms,
+                               enum pipe_format srcfmt, enum pipe_format dstfmt,
+                               unsigned dstmask, unsigned texdim,
+                               bool texisarray, bool is_ms,
                                struct pan_shader_info *shader_info)
 {
-   nir_builder b =
-      nir_builder_init_simple_shader(MESA_SHADER_FRAGMENT,
-                                     GENX(pan_shader_get_compiler_options)(),
-                                     "panvk_meta_copy_img2img(srcfmt=%s,dstfmt=%s,%dD%s%s)",
-                                     util_format_name(srcfmt), util_format_name(dstfmt),
-                                     texdim, texisarray ? "[]" : "", is_ms ? ",ms" : "");
-
-   nir_variable *coord_var =
-      nir_variable_create(b.shader, nir_var_shader_in,
-                          glsl_vector_type(GLSL_TYPE_FLOAT, texdim + texisarray),
-                          "coord");
+   nir_builder b = nir_builder_init_simple_shader(
+      MESA_SHADER_FRAGMENT, GENX(pan_shader_get_compiler_options)(),
+      "panvk_meta_copy_img2img(srcfmt=%s,dstfmt=%s,%dD%s%s)",
+      util_format_name(srcfmt), util_format_name(dstfmt), texdim,
+      texisarray ? "[]" : "", is_ms ? ",ms" : "");
+
+   nir_variable *coord_var = nir_variable_create(
+      b.shader, nir_var_shader_in,
+      glsl_vector_type(GLSL_TYPE_FLOAT, texdim + texisarray), "coord");
    coord_var->data.location = VARYING_SLOT_VAR0;
    nir_ssa_def *coord = nir_f2u32(&b, nir_load_var(&b, coord_var));
 
@@ -333,14 +309,21 @@
    tex->op = is_ms ? nir_texop_txf_ms : nir_texop_txf;
    tex->texture_index = 0;
    tex->is_array = texisarray;
-   tex->dest_type = util_format_is_unorm(srcfmt) ?
-                    nir_type_float32 : nir_type_uint32;
+   tex->dest_type =
+      util_format_is_unorm(srcfmt) ? nir_type_float32 : nir_type_uint32;
 
    switch (texdim) {
-   case 1: tex->sampler_dim = GLSL_SAMPLER_DIM_1D; break;
-   case 2: tex->sampler_dim = GLSL_SAMPLER_DIM_2D; break;
-   case 3: tex->sampler_dim = GLSL_SAMPLER_DIM_3D; break;
-   default: unreachable("Invalid texture dimension");
+   case 1:
+      tex->sampler_dim = GLSL_SAMPLER_DIM_1D;
+      break;
+   case 2:
+      tex->sampler_dim = GLSL_SAMPLER_DIM_2D;
+      break;
+   case 3:
+      tex->sampler_dim = GLSL_SAMPLER_DIM_3D;
+      break;
+   default:
+      unreachable("Invalid texture dimension");
    }
 
    tex->src[0].src_type = nir_tex_src_coord;
@@ -364,44 +347,34 @@
    const struct glsl_type *outtype = NULL;
 
    if (srcfmt == PIPE_FORMAT_R5G6B5_UNORM && dstfmt == PIPE_FORMAT_R8G8_UNORM) {
-      nir_ssa_def *rgb =
-         nir_f2u32(&b, nir_fmul(&b, texel,
-                                nir_vec3(&b,
-                                         nir_imm_float(&b, 31),
-                                         nir_imm_float(&b, 63),
-                                         nir_imm_float(&b, 31))));
-      nir_ssa_def *rg =
-         nir_vec2(&b,
-                  nir_ior(&b, nir_channel(&b, rgb, 0),
-                          nir_ishl(&b, nir_channel(&b, rgb, 1),
-                                   nir_imm_int(&b, 5))),
-                  nir_ior(&b,
-                          nir_ushr_imm(&b, nir_channel(&b, rgb, 1), 3),
-                          nir_ishl(&b, nir_channel(&b, rgb, 2),
-                                   nir_imm_int(&b, 3))));
+      nir_ssa_def *rgb = nir_f2u32(
+         &b, nir_fmul(&b, texel,
+                      nir_vec3(&b, nir_imm_float(&b, 31), nir_imm_float(&b, 63),
+                               nir_imm_float(&b, 31))));
+      nir_ssa_def *rg = nir_vec2(
+         &b,
+         nir_ior(&b, nir_channel(&b, rgb, 0),
+                 nir_ishl(&b, nir_channel(&b, rgb, 1), nir_imm_int(&b, 5))),
+         nir_ior(&b, nir_ushr_imm(&b, nir_channel(&b, rgb, 1), 3),
+                 nir_ishl(&b, nir_channel(&b, rgb, 2), nir_imm_int(&b, 3))));
       rg = nir_iand_imm(&b, rg, 255);
       texel = nir_fmul_imm(&b, nir_u2f32(&b, rg), 1.0 / 255);
       outtype = glsl_vector_type(GLSL_TYPE_FLOAT, 2);
-   } else if (srcfmt == PIPE_FORMAT_R8G8_UNORM && dstfmt == PIPE_FORMAT_R5G6B5_UNORM) {
+   } else if (srcfmt == PIPE_FORMAT_R8G8_UNORM &&
+              dstfmt == PIPE_FORMAT_R5G6B5_UNORM) {
       nir_ssa_def *rg = nir_f2u32(&b, nir_fmul_imm(&b, texel, 255));
-      nir_ssa_def *rgb =
-         nir_vec3(&b,
-                  nir_channel(&b, rg, 0),
-                  nir_ior(&b,
-                          nir_ushr_imm(&b, nir_channel(&b, rg, 0), 5),
-                          nir_ishl(&b, nir_channel(&b, rg, 1),
-                                   nir_imm_int(&b, 3))),
-                  nir_ushr_imm(&b, nir_channel(&b, rg, 1), 3));
+      nir_ssa_def *rgb = nir_vec3(
+         &b, nir_channel(&b, rg, 0),
+         nir_ior(&b, nir_ushr_imm(&b, nir_channel(&b, rg, 0), 5),
+                 nir_ishl(&b, nir_channel(&b, rg, 1), nir_imm_int(&b, 3))),
+         nir_ushr_imm(&b, nir_channel(&b, rg, 1), 3));
       rgb = nir_iand(&b, rgb,
-                     nir_vec3(&b,
-                              nir_imm_int(&b, 31),
-                              nir_imm_int(&b, 63),
+                     nir_vec3(&b, nir_imm_int(&b, 31), nir_imm_int(&b, 63),
                               nir_imm_int(&b, 31)));
-      texel = nir_fmul(&b, nir_u2f32(&b, rgb),
-                       nir_vec3(&b,
-                                nir_imm_float(&b, 1.0 / 31),
-                                nir_imm_float(&b, 1.0 / 63),
-                                nir_imm_float(&b, 1.0 / 31)));
+      texel = nir_fmul(
+         &b, nir_u2f32(&b, rgb),
+         nir_vec3(&b, nir_imm_float(&b, 1.0 / 31), nir_imm_float(&b, 1.0 / 63),
+                  nir_imm_float(&b, 1.0 / 31)));
       outtype = glsl_vector_type(GLSL_TYPE_FLOAT, 3);
    } else {
       assert(srcfmt == dstfmt);
@@ -451,9 +424,8 @@
 
    pan_pack(&inputs.bifrost.rt_conv[0], INTERNAL_CONVERSION, cfg) {
       cfg.memory_format = (dstcompsz == 2 ? MALI_RG16UI : MALI_RG32UI) << 12;
-      cfg.register_format = dstcompsz == 2 ?
-                            MALI_REGISTER_FILE_FORMAT_U16 :
-                            MALI_REGISTER_FILE_FORMAT_U32;
+      cfg.register_format = dstcompsz == 2 ? MALI_REGISTER_FILE_FORMAT_U16
+                                           : MALI_REGISTER_FILE_FORMAT_U32;
    }
    inputs.bifrost.static_rt_conv = true;
 
@@ -486,16 +458,25 @@
     * matching the texel size.
     */
    switch (util_format_get_blocksize(fmt)) {
-   case 16: return PIPE_FORMAT_R32G32B32A32_UINT;
-   case 12: return PIPE_FORMAT_R32G32B32_UINT;
-   case 8: return PIPE_FORMAT_R32G32_UINT;
-   case 6: return PIPE_FORMAT_R16G16B16_UINT;
-   case 4: return PIPE_FORMAT_R8G8B8A8_UNORM;
-   case 2: return (fmt == PIPE_FORMAT_R5G6B5_UNORM ||
-                   fmt == PIPE_FORMAT_B5G6R5_UNORM) ?
-                  PIPE_FORMAT_R5G6B5_UNORM : PIPE_FORMAT_R8G8_UNORM;
-   case 1: return PIPE_FORMAT_R8_UNORM;
-   default: unreachable("Unsupported format\n");
+   case 16:
+      return PIPE_FORMAT_R32G32B32A32_UINT;
+   case 12:
+      return PIPE_FORMAT_R32G32B32_UINT;
+   case 8:
+      return PIPE_FORMAT_R32G32_UINT;
+   case 6:
+      return PIPE_FORMAT_R16G16B16_UINT;
+   case 4:
+      return PIPE_FORMAT_R8G8B8A8_UNORM;
+   case 2:
+      return (fmt == PIPE_FORMAT_R5G6B5_UNORM ||
+              fmt == PIPE_FORMAT_B5G6R5_UNORM)
+                ? PIPE_FORMAT_R5G6B5_UNORM
+                : PIPE_FORMAT_R8G8_UNORM;
+   case 1:
+      return PIPE_FORMAT_R8_UNORM;
+   default:
+      unreachable("Unsupported format\n");
    }
 }
 
@@ -505,31 +486,34 @@
    unsigned dstmask;
 } PACKED;
 
-static const struct panvk_meta_copy_img2img_format_info panvk_meta_copy_img2img_fmts[] = {
-   { PIPE_FORMAT_R8_UNORM, PIPE_FORMAT_R8_UNORM, 0x1},
-   { PIPE_FORMAT_R5G6B5_UNORM, PIPE_FORMAT_R5G6B5_UNORM, 0x7},
-   { PIPE_FORMAT_R5G6B5_UNORM, PIPE_FORMAT_R8G8_UNORM, 0x3},
-   { PIPE_FORMAT_R8G8_UNORM, PIPE_FORMAT_R5G6B5_UNORM, 0x7},
-   { PIPE_FORMAT_R8G8_UNORM, PIPE_FORMAT_R8G8_UNORM, 0x3},
-   /* Z24S8(depth) */
-   { PIPE_FORMAT_R8G8B8A8_UNORM, PIPE_FORMAT_R8G8B8A8_UNORM, 0x7 },
-   /* Z24S8(stencil) */
-   { PIPE_FORMAT_R8G8B8A8_UNORM, PIPE_FORMAT_R8G8B8A8_UNORM, 0x8 },
-   { PIPE_FORMAT_R8G8B8A8_UNORM, PIPE_FORMAT_R8G8B8A8_UNORM, 0xf },
-   { PIPE_FORMAT_R16G16B16_UINT, PIPE_FORMAT_R16G16B16_UINT, 0x7 },
-   { PIPE_FORMAT_R32G32_UINT, PIPE_FORMAT_R32G32_UINT, 0x3 },
-   /* Z32S8X24(depth) */
-   { PIPE_FORMAT_R32G32_UINT, PIPE_FORMAT_R32G32_UINT, 0x1 },
-   /* Z32S8X24(stencil) */
-   { PIPE_FORMAT_R32G32_UINT, PIPE_FORMAT_R32G32_UINT, 0x2 },
-   { PIPE_FORMAT_R32G32B32_UINT, PIPE_FORMAT_R32G32B32_UINT, 0x7 },
-   { PIPE_FORMAT_R32G32B32A32_UINT, PIPE_FORMAT_R32G32B32A32_UINT, 0xf },
+static const struct panvk_meta_copy_img2img_format_info
+   panvk_meta_copy_img2img_fmts[] = {
+      {PIPE_FORMAT_R8_UNORM, PIPE_FORMAT_R8_UNORM, 0x1},
+      {PIPE_FORMAT_R5G6B5_UNORM, PIPE_FORMAT_R5G6B5_UNORM, 0x7},
+      {PIPE_FORMAT_R5G6B5_UNORM, PIPE_FORMAT_R8G8_UNORM, 0x3},
+      {PIPE_FORMAT_R8G8_UNORM, PIPE_FORMAT_R5G6B5_UNORM, 0x7},
+      {PIPE_FORMAT_R8G8_UNORM, PIPE_FORMAT_R8G8_UNORM, 0x3},
+      /* Z24S8(depth) */
+      {PIPE_FORMAT_R8G8B8A8_UNORM, PIPE_FORMAT_R8G8B8A8_UNORM, 0x7},
+      /* Z24S8(stencil) */
+      {PIPE_FORMAT_R8G8B8A8_UNORM, PIPE_FORMAT_R8G8B8A8_UNORM, 0x8},
+      {PIPE_FORMAT_R8G8B8A8_UNORM, PIPE_FORMAT_R8G8B8A8_UNORM, 0xf},
+      {PIPE_FORMAT_R16G16B16_UINT, PIPE_FORMAT_R16G16B16_UINT, 0x7},
+      {PIPE_FORMAT_R32G32_UINT, PIPE_FORMAT_R32G32_UINT, 0x3},
+      /* Z32S8X24(depth) */
+      {PIPE_FORMAT_R32G32_UINT, PIPE_FORMAT_R32G32_UINT, 0x1},
+      /* Z32S8X24(stencil) */
+      {PIPE_FORMAT_R32G32_UINT, PIPE_FORMAT_R32G32_UINT, 0x2},
+      {PIPE_FORMAT_R32G32B32_UINT, PIPE_FORMAT_R32G32B32_UINT, 0x7},
+      {PIPE_FORMAT_R32G32B32A32_UINT, PIPE_FORMAT_R32G32B32A32_UINT, 0xf},
 };
 
 static unsigned
-panvk_meta_copy_img2img_format_idx(struct panvk_meta_copy_img2img_format_info key)
+panvk_meta_copy_img2img_format_idx(
+   struct panvk_meta_copy_img2img_format_info key)
 {
-   STATIC_ASSERT(ARRAY_SIZE(panvk_meta_copy_img2img_fmts) == PANVK_META_COPY_IMG2IMG_NUM_FORMATS);
+   STATIC_ASSERT(ARRAY_SIZE(panvk_meta_copy_img2img_fmts) ==
+                 PANVK_META_COPY_IMG2IMG_NUM_FORMATS);
 
    for (unsigned i = 0; i < ARRAY_SIZE(panvk_meta_copy_img2img_fmts); i++) {
       if (!memcmp(&key, &panvk_meta_copy_img2img_fmts[i], sizeof(key)))
@@ -587,27 +571,29 @@
 
    assert(src->pimage.layout.nr_samples == dst->pimage.layout.nr_samples);
 
-   unsigned texdimidx =
-      panvk_meta_copy_tex_type(src->pimage.layout.dim,
-                               src->pimage.layout.array_size > 1);
-   unsigned fmtidx =
-      panvk_meta_copy_img2img_format_idx(key);
+   unsigned texdimidx = panvk_meta_copy_tex_type(
+      src->pimage.layout.dim, src->pimage.layout.array_size > 1);
+   unsigned fmtidx = panvk_meta_copy_img2img_format_idx(key);
    unsigned ms = dst->pimage.layout.nr_samples > 1 ? 1 : 0;
 
    mali_ptr rsd =
-      cmdbuf->device->physical_device->meta.copy.img2img[ms][texdimidx][fmtidx].rsd;
+      cmdbuf->device->physical_device->meta.copy.img2img[ms][texdimidx][fmtidx]
+         .rsd;
 
    struct pan_image_view srcview = {
       .format = key.srcfmt,
-      .dim = src->pimage.layout.dim == MALI_TEXTURE_DIMENSION_CUBE ?
-             MALI_TEXTURE_DIMENSION_2D : src->pimage.layout.dim,
+      .dim = src->pimage.layout.dim == MALI_TEXTURE_DIMENSION_CUBE
+                ? MALI_TEXTURE_DIMENSION_2D
+                : src->pimage.layout.dim,
       .image = &src->pimage,
       .nr_samples = src->pimage.layout.nr_samples,
       .first_level = region->srcSubresource.mipLevel,
       .last_level = region->srcSubresource.mipLevel,
       .first_layer = region->srcSubresource.baseArrayLayer,
-      .last_layer = region->srcSubresource.baseArrayLayer + region->srcSubresource.layerCount - 1,
-      .swizzle = { PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_Z, PIPE_SWIZZLE_W },
+      .last_layer = region->srcSubresource.baseArrayLayer +
+                    region->srcSubresource.layerCount - 1,
+      .swizzle = {PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_Z,
+                  PIPE_SWIZZLE_W},
    };
 
    struct pan_image_view dstview = {
@@ -617,7 +603,8 @@
       .nr_samples = dst->pimage.layout.nr_samples,
       .first_level = region->dstSubresource.mipLevel,
       .last_level = region->dstSubresource.mipLevel,
-      .swizzle = { PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_Z, PIPE_SWIZZLE_W },
+      .swizzle = {PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_Z,
+                  PIPE_SWIZZLE_W},
    };
 
    unsigned minx = MAX2(region->dstOffset.x, 0);
@@ -625,25 +612,23 @@
    unsigned maxx = MAX2(region->dstOffset.x + region->extent.width - 1, 0);
    unsigned maxy = MAX2(region->dstOffset.y + region->extent.height - 1, 0);
 
-   mali_ptr vpd =
-      panvk_per_arch(meta_emit_viewport)(&cmdbuf->desc_pool.base,
-                                         minx, miny, maxx, maxy);
+   mali_ptr vpd = panvk_per_arch(meta_emit_viewport)(&cmdbuf->desc_pool.base,
+                                                     minx, miny, maxx, maxy);
 
    float dst_rect[] = {
-      minx, miny, 0.0, 1.0,
-      maxx + 1, miny, 0.0, 1.0,
-      minx, maxy + 1, 0.0, 1.0,
-      maxx + 1, maxy + 1, 0.0, 1.0,
+      minx, miny,     0.0, 1.0, maxx + 1, miny,     0.0, 1.0,
+      minx, maxy + 1, 0.0, 1.0, maxx + 1, maxy + 1, 0.0, 1.0,
    };
 
-   mali_ptr dst_coords =
-      pan_pool_upload_aligned(&cmdbuf->desc_pool.base, dst_rect,
-                              sizeof(dst_rect), 64);
+   mali_ptr dst_coords = pan_pool_upload_aligned(
+      &cmdbuf->desc_pool.base, dst_rect, sizeof(dst_rect), 64);
 
    /* TODO: don't force preloads of dst resources if unneeded */
 
-   unsigned width = u_minify(dst->pimage.layout.width, region->dstSubresource.mipLevel);
-   unsigned height = u_minify(dst->pimage.layout.height, region->dstSubresource.mipLevel);
+   unsigned width =
+      u_minify(dst->pimage.layout.width, region->dstSubresource.mipLevel);
+   unsigned height =
+      u_minify(dst->pimage.layout.height, region->dstSubresource.mipLevel);
    cmdbuf->state.fb.crc_valid[0] = false;
    *fbinfo = (struct pan_fb_info){
       .width = width,
@@ -673,20 +658,19 @@
    assert(region->dstOffset.z >= 0);
 
    unsigned first_src_layer = MAX2(0, region->srcOffset.z);
-   unsigned first_dst_layer = MAX2(region->dstSubresource.baseArrayLayer, region->dstOffset.z);
-   unsigned nlayers = MAX2(region->dstSubresource.layerCount, region->extent.depth);
+   unsigned first_dst_layer =
+      MAX2(region->dstSubresource.baseArrayLayer, region->dstOffset.z);
+   unsigned nlayers =
+      MAX2(region->dstSubresource.layerCount, region->extent.depth);
    for (unsigned l = 0; l < nlayers; l++) {
       unsigned src_l = l + first_src_layer;
       float src_rect[] = {
-         minx, miny, src_l, 1.0,
-         maxx + 1, miny, src_l, 1.0,
-         minx, maxy + 1, src_l, 1.0,
-         maxx + 1, maxy + 1, src_l, 1.0,
+         minx, miny,     src_l, 1.0, maxx + 1, miny,     src_l, 1.0,
+         minx, maxy + 1, src_l, 1.0, maxx + 1, maxy + 1, src_l, 1.0,
       };
 
-      mali_ptr src_coords =
-         pan_pool_upload_aligned(&cmdbuf->desc_pool.base, src_rect,
-                                 sizeof(src_rect), 64);
+      mali_ptr src_coords = pan_pool_upload_aligned(
+         &cmdbuf->desc_pool.base, src_rect, sizeof(src_rect), 64);
 
       struct panvk_batch *batch = panvk_cmd_open_batch(cmdbuf);
 
@@ -704,11 +688,9 @@
 
       struct panfrost_ptr job;
 
-      job = panvk_meta_copy_emit_tiler_job(&cmdbuf->desc_pool.base,
-                                           &batch->scoreboard,
-                                           src_coords, dst_coords,
-                                           texture, sampler, 0,
-                                           vpd, rsd, tsd, tiler);
+      job = panvk_meta_copy_emit_tiler_job(
+         &cmdbuf->desc_pool.base, &batch->scoreboard, src_coords, dst_coords,
+         texture, sampler, 0, vpd, rsd, tsd, tiler);
 
       util_dynarray_append(&batch->jobs, void *, job.cpu);
       panvk_per_arch(cmd_close_batch)(cmdbuf);
@@ -718,7 +700,8 @@
 static void
 panvk_meta_copy_img2img_init(struct panvk_physical_device *dev, bool is_ms)
 {
-   STATIC_ASSERT(ARRAY_SIZE(panvk_meta_copy_img2img_fmts) == PANVK_META_COPY_IMG2IMG_NUM_FORMATS);
+   STATIC_ASSERT(ARRAY_SIZE(panvk_meta_copy_img2img_fmts) ==
+                 PANVK_META_COPY_IMG2IMG_NUM_FORMATS);
 
    for (unsigned i = 0; i < ARRAY_SIZE(panvk_meta_copy_img2img_fmts); i++) {
       for (unsigned texdim = 1; texdim <= 3; texdim++) {
@@ -726,39 +709,38 @@
          assert(texdimidx < ARRAY_SIZE(dev->meta.copy.img2img[0]));
 
          /* No MSAA on 3D textures */
-         if (texdim == 3 && is_ms) continue;
+         if (texdim == 3 && is_ms)
+            continue;
 
          struct pan_shader_info shader_info;
-         mali_ptr shader =
-            panvk_meta_copy_img2img_shader(&dev->pdev, &dev->meta.bin_pool.base,
-                                           panvk_meta_copy_img2img_fmts[i].srcfmt,
-                                           panvk_meta_copy_img2img_fmts[i].dstfmt,
-                                           panvk_meta_copy_img2img_fmts[i].dstmask,
-                                           texdim, false, is_ms, &shader_info);
+         mali_ptr shader = panvk_meta_copy_img2img_shader(
+            &dev->pdev, &dev->meta.bin_pool.base,
+            panvk_meta_copy_img2img_fmts[i].srcfmt,
+            panvk_meta_copy_img2img_fmts[i].dstfmt,
+            panvk_meta_copy_img2img_fmts[i].dstmask, texdim, false, is_ms,
+            &shader_info);
          dev->meta.copy.img2img[is_ms][texdimidx][i].rsd =
-            panvk_meta_copy_to_img_emit_rsd(&dev->pdev, &dev->meta.desc_pool.base,
-                                            shader, &shader_info,
-                                            panvk_meta_copy_img2img_fmts[i].dstfmt,
-                                            panvk_meta_copy_img2img_fmts[i].dstmask,
-                                            true);
+            panvk_meta_copy_to_img_emit_rsd(
+               &dev->pdev, &dev->meta.desc_pool.base, shader, &shader_info,
+               panvk_meta_copy_img2img_fmts[i].dstfmt,
+               panvk_meta_copy_img2img_fmts[i].dstmask, true);
          if (texdim == 3)
             continue;
 
          memset(&shader_info, 0, sizeof(shader_info));
          texdimidx = panvk_meta_copy_tex_type(texdim, true);
          assert(texdimidx < ARRAY_SIZE(dev->meta.copy.img2img[0]));
-         shader =
-            panvk_meta_copy_img2img_shader(&dev->pdev, &dev->meta.bin_pool.base,
-                                           panvk_meta_copy_img2img_fmts[i].srcfmt,
-                                           panvk_meta_copy_img2img_fmts[i].dstfmt,
-                                           panvk_meta_copy_img2img_fmts[i].dstmask,
-                                           texdim, true, is_ms, &shader_info);
+         shader = panvk_meta_copy_img2img_shader(
+            &dev->pdev, &dev->meta.bin_pool.base,
+            panvk_meta_copy_img2img_fmts[i].srcfmt,
+            panvk_meta_copy_img2img_fmts[i].dstfmt,
+            panvk_meta_copy_img2img_fmts[i].dstmask, texdim, true, is_ms,
+            &shader_info);
          dev->meta.copy.img2img[is_ms][texdimidx][i].rsd =
-            panvk_meta_copy_to_img_emit_rsd(&dev->pdev, &dev->meta.desc_pool.base,
-                                            shader, &shader_info,
-                                            panvk_meta_copy_img2img_fmts[i].dstfmt,
-                                            panvk_meta_copy_img2img_fmts[i].dstmask,
-                                            true);
+            panvk_meta_copy_to_img_emit_rsd(
+               &dev->pdev, &dev->meta.desc_pool.base, shader, &shader_info,
+               panvk_meta_copy_img2img_fmts[i].dstfmt,
+               panvk_meta_copy_img2img_fmts[i].dstmask, true);
       }
    }
 }
@@ -808,18 +790,27 @@
     * texelsize otherwise.
     */
    switch (util_format_get_blocksize(imgfmt)) {
-   case 1: return PIPE_FORMAT_R8_UNORM;
+   case 1:
+      return PIPE_FORMAT_R8_UNORM;
    /* AFBC stores things differently for RGB565,
     * we can't simply map to R8G8 in that case */
-   case 2: return (imgfmt == PIPE_FORMAT_R5G6B5_UNORM ||
-                   imgfmt == PIPE_FORMAT_B5G6R5_UNORM) ?
-                  PIPE_FORMAT_R5G6B5_UNORM : PIPE_FORMAT_R8G8_UNORM;
-   case 4: return PIPE_FORMAT_R8G8B8A8_UNORM;
-   case 6: return PIPE_FORMAT_R16G16B16_UINT;
-   case 8: return PIPE_FORMAT_R32G32_UINT;
-   case 12: return PIPE_FORMAT_R32G32B32_UINT;
-   case 16: return PIPE_FORMAT_R32G32B32A32_UINT;
-   default: unreachable("Invalid format\n");
+   case 2:
+      return (imgfmt == PIPE_FORMAT_R5G6B5_UNORM ||
+              imgfmt == PIPE_FORMAT_B5G6R5_UNORM)
+                ? PIPE_FORMAT_R5G6B5_UNORM
+                : PIPE_FORMAT_R8G8_UNORM;
+   case 4:
+      return PIPE_FORMAT_R8G8B8A8_UNORM;
+   case 6:
+      return PIPE_FORMAT_R16G16B16_UINT;
+   case 8:
+      return PIPE_FORMAT_R32G32_UINT;
+   case 12:
+      return PIPE_FORMAT_R32G32B32_UINT;
+   case 16:
+      return PIPE_FORMAT_R32G32B32A32_UINT;
+   default:
+      unreachable("Invalid format\n");
    }
 }
 
@@ -828,23 +819,24 @@
    unsigned mask;
 } PACKED;
 
-static const struct panvk_meta_copy_format_info panvk_meta_copy_buf2img_fmts[] = {
-   { PIPE_FORMAT_R8_UNORM, 0x1 },
-   { PIPE_FORMAT_R8G8_UNORM, 0x3 },
-   { PIPE_FORMAT_R5G6B5_UNORM, 0x7 },
-   { PIPE_FORMAT_R8G8B8A8_UNORM, 0xf },
-   { PIPE_FORMAT_R16G16B16_UINT, 0x7 },
-   { PIPE_FORMAT_R32G32_UINT, 0x3 },
-   { PIPE_FORMAT_R32G32B32_UINT, 0x7 },
-   { PIPE_FORMAT_R32G32B32A32_UINT, 0xf },
-   /* S8 -> Z24S8 */
-   { PIPE_FORMAT_R8G8B8A8_UNORM, 0x8 },
-   /* S8 -> Z32_S8X24 */
-   { PIPE_FORMAT_R32G32_UINT, 0x2 },
-   /* Z24X8 -> Z24S8 */
-   { PIPE_FORMAT_R8G8B8A8_UNORM, 0x7 },
-   /* Z32 -> Z32_S8X24 */
-   { PIPE_FORMAT_R32G32_UINT, 0x1 },
+static const struct panvk_meta_copy_format_info panvk_meta_copy_buf2img_fmts[] =
+   {
+      {PIPE_FORMAT_R8_UNORM, 0x1},
+      {PIPE_FORMAT_R8G8_UNORM, 0x3},
+      {PIPE_FORMAT_R5G6B5_UNORM, 0x7},
+      {PIPE_FORMAT_R8G8B8A8_UNORM, 0xf},
+      {PIPE_FORMAT_R16G16B16_UINT, 0x7},
+      {PIPE_FORMAT_R32G32_UINT, 0x3},
+      {PIPE_FORMAT_R32G32B32_UINT, 0x7},
+      {PIPE_FORMAT_R32G32B32A32_UINT, 0xf},
+      /* S8 -> Z24S8 */
+      {PIPE_FORMAT_R8G8B8A8_UNORM, 0x8},
+      /* S8 -> Z32_S8X24 */
+      {PIPE_FORMAT_R32G32_UINT, 0x2},
+      /* Z24X8 -> Z24S8 */
+      {PIPE_FORMAT_R8G8B8A8_UNORM, 0x7},
+      /* Z32 -> Z32_S8X24 */
+      {PIPE_FORMAT_R32G32_UINT, 0x1},
 };
 
 struct panvk_meta_copy_buf2img_info {
@@ -857,12 +849,12 @@
    } buf;
 } PACKED;
 
-#define panvk_meta_copy_buf2img_get_info_field(b, field) \
-        nir_load_push_constant((b), 1, \
-                     sizeof(((struct panvk_meta_copy_buf2img_info *)0)->field) * 8, \
-                     nir_imm_int(b, 0), \
-                     .base = offsetof(struct panvk_meta_copy_buf2img_info, field), \
-                     .range = ~0)
+#define panvk_meta_copy_buf2img_get_info_field(b, field)                       \
+   nir_load_push_constant(                                                     \
+      (b), 1, sizeof(((struct panvk_meta_copy_buf2img_info *)0)->field) * 8,   \
+      nir_imm_int(b, 0),                                                       \
+      .base = offsetof(struct panvk_meta_copy_buf2img_info, field),            \
+      .range = ~0)
 
 static mali_ptr
 panvk_meta_copy_buf2img_shader(struct panfrost_device *pdev,
@@ -870,24 +862,20 @@
                                struct panvk_meta_copy_format_info key,
                                struct pan_shader_info *shader_info)
 {
-   nir_builder b =
-      nir_builder_init_simple_shader(MESA_SHADER_FRAGMENT,
-                                     GENX(pan_shader_get_compiler_options)(),
-                                     "panvk_meta_copy_buf2img(imgfmt=%s,mask=%x)",
-                                     util_format_name(key.imgfmt),
-                                     key.mask);
+   nir_builder b = nir_builder_init_simple_shader(
+      MESA_SHADER_FRAGMENT, GENX(pan_shader_get_compiler_options)(),
+      "panvk_meta_copy_buf2img(imgfmt=%s,mask=%x)",
+      util_format_name(key.imgfmt), key.mask);
 
    nir_variable *coord_var =
       nir_variable_create(b.shader, nir_var_shader_in,
-                          glsl_vector_type(GLSL_TYPE_FLOAT, 3),
-                          "coord");
+                          glsl_vector_type(GLSL_TYPE_FLOAT, 3), "coord");
    coord_var->data.location = VARYING_SLOT_VAR0;
    nir_ssa_def *coord = nir_load_var(&b, coord_var);
 
    coord = nir_f2u32(&b, coord);
 
-   nir_ssa_def *bufptr =
-      panvk_meta_copy_buf2img_get_info_field(&b, buf.ptr);
+   nir_ssa_def *bufptr = panvk_meta_copy_buf2img_get_info_field(&b, buf.ptr);
    nir_ssa_def *buflinestride =
       panvk_meta_copy_buf2img_get_info_field(&b, buf.stride.line);
    nir_ssa_def *bufsurfstride =
@@ -906,8 +894,9 @@
    bufptr = nir_iadd(&b, bufptr, nir_u2u64(&b, offset));
 
    unsigned imgcompsz =
-      (imgtexelsz <= 4 && key.imgfmt != PIPE_FORMAT_R5G6B5_UNORM) ?
-      1 : MIN2(1 << (ffs(imgtexelsz) - 1), 4);
+      (imgtexelsz <= 4 && key.imgfmt != PIPE_FORMAT_R5G6B5_UNORM)
+         ? 1
+         : MIN2(1 << (ffs(imgtexelsz) - 1), 4);
 
    unsigned nimgcomps = imgtexelsz / imgcompsz;
    unsigned bufcompsz = MIN2(buftexelsz, imgcompsz);
@@ -921,16 +910,14 @@
 
    enum glsl_base_type basetype;
    if (key.imgfmt == PIPE_FORMAT_R5G6B5_UNORM) {
-      texel = nir_vec3(&b,
-                       nir_iand_imm(&b, texel, BITFIELD_MASK(5)),
-                       nir_iand_imm(&b, nir_ushr_imm(&b, texel, 5), BITFIELD_MASK(6)),
-                       nir_iand_imm(&b, nir_ushr_imm(&b, texel, 11), BITFIELD_MASK(5)));
-      texel = nir_fmul(&b,
-                       nir_u2f32(&b, texel),
-                       nir_vec3(&b,
-                                nir_imm_float(&b, 1.0f / 31),
-                                nir_imm_float(&b, 1.0f / 63),
-                                nir_imm_float(&b, 1.0f / 31)));
+      texel = nir_vec3(
+         &b, nir_iand_imm(&b, texel, BITFIELD_MASK(5)),
+         nir_iand_imm(&b, nir_ushr_imm(&b, texel, 5), BITFIELD_MASK(6)),
+         nir_iand_imm(&b, nir_ushr_imm(&b, texel, 11), BITFIELD_MASK(5)));
+      texel = nir_fmul(
+         &b, nir_u2f32(&b, texel),
+         nir_vec3(&b, nir_imm_float(&b, 1.0f / 31),
+                  nir_imm_float(&b, 1.0f / 63), nir_imm_float(&b, 1.0f / 31)));
       nimgcomps = 3;
       basetype = GLSL_TYPE_FLOAT;
    } else if (imgcompsz == 1) {
@@ -938,8 +925,7 @@
       /* Blendable formats are unorm and the fixed-function blend unit
        * takes float values.
        */
-      texel = nir_fmul(&b, nir_u2f32(&b, texel),
-                       nir_imm_float(&b, 1.0f / 255));
+      texel = nir_fmul(&b, nir_u2f32(&b, texel), nir_imm_float(&b, 1.0f / 255));
       basetype = GLSL_TYPE_FLOAT;
    } else {
       texel = nir_u2uN(&b, texel, imgcompsz * 8);
@@ -949,8 +935,7 @@
    /* We always pass the texel using 32-bit regs for now */
    nir_variable *out =
       nir_variable_create(b.shader, nir_var_shader_out,
-                          glsl_vector_type(basetype, nimgcomps),
-                          "out");
+                          glsl_vector_type(basetype, nimgcomps), "out");
    out->data.location = FRAG_RESULT_DATA0;
 
    uint16_t fullmask = (1 << nimgcomps) - 1;
@@ -986,9 +971,8 @@
 
    pan_pack(&inputs.bifrost.rt_conv[0], INTERNAL_CONVERSION, cfg) {
       cfg.memory_format = (imgcompsz == 2 ? MALI_RG16UI : MALI_RG32UI) << 12;
-      cfg.register_format = imgcompsz == 2 ?
-                            MALI_REGISTER_FILE_FORMAT_U16 :
-                            MALI_REGISTER_FILE_FORMAT_U32;
+      cfg.register_format = imgcompsz == 2 ? MALI_REGISTER_FILE_FORMAT_U16
+                                           : MALI_REGISTER_FILE_FORMAT_U32;
    }
    inputs.bifrost.static_rt_conv = true;
 
@@ -996,7 +980,8 @@
 
    util_dynarray_init(&binary, NULL);
    GENX(pan_shader_compile)(b.shader, &inputs, &binary, shader_info);
-   shader_info->push.count = DIV_ROUND_UP(sizeof(struct panvk_meta_copy_buf2img_info), 4);
+   shader_info->push.count =
+      DIV_ROUND_UP(sizeof(struct panvk_meta_copy_buf2img_info), 4);
 
    mali_ptr shader =
       pan_pool_upload_aligned(bin_pool, binary.data, binary.size, 128);
@@ -1027,22 +1012,20 @@
    struct pan_fb_info *fbinfo = &cmdbuf->state.fb.info;
    unsigned minx = MAX2(region->imageOffset.x, 0);
    unsigned miny = MAX2(region->imageOffset.y, 0);
-   unsigned maxx = MAX2(region->imageOffset.x + region->imageExtent.width - 1, 0);
-   unsigned maxy = MAX2(region->imageOffset.y + region->imageExtent.height - 1, 0);
+   unsigned maxx =
+      MAX2(region->imageOffset.x + region->imageExtent.width - 1, 0);
+   unsigned maxy =
+      MAX2(region->imageOffset.y + region->imageExtent.height - 1, 0);
 
-   mali_ptr vpd =
-      panvk_per_arch(meta_emit_viewport)(&cmdbuf->desc_pool.base,
-                                         minx, miny, maxx, maxy);
+   mali_ptr vpd = panvk_per_arch(meta_emit_viewport)(&cmdbuf->desc_pool.base,
+                                                     minx, miny, maxx, maxy);
 
    float dst_rect[] = {
-      minx, miny, 0.0, 1.0,
-      maxx + 1, miny, 0.0, 1.0,
-      minx, maxy + 1, 0.0, 1.0,
-      maxx + 1, maxy + 1, 0.0, 1.0,
-   };
-   mali_ptr dst_coords =
-      pan_pool_upload_aligned(&cmdbuf->desc_pool.base, dst_rect,
-                              sizeof(dst_rect), 64);
+      minx, miny,     0.0, 1.0, maxx + 1, miny,     0.0, 1.0,
+      minx, maxy + 1, 0.0, 1.0, maxx + 1, maxy + 1, 0.0, 1.0,
+   };
+   mali_ptr dst_coords = pan_pool_upload_aligned(
+      &cmdbuf->desc_pool.base, dst_rect, sizeof(dst_rect), 64);
 
    struct panvk_meta_copy_format_info key = {
       .imgfmt = panvk_meta_copy_buf2img_format(img->pimage.layout.format),
@@ -1073,14 +1056,17 @@
       .nr_samples = img->pimage.layout.nr_samples,
       .first_level = region->imageSubresource.mipLevel,
       .last_level = region->imageSubresource.mipLevel,
-      .swizzle = { PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_Z, PIPE_SWIZZLE_W },
+      .swizzle = {PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_Z,
+                  PIPE_SWIZZLE_W},
    };
 
    /* TODO: don't force preloads of dst resources if unneeded */
    cmdbuf->state.fb.crc_valid[0] = false;
    *fbinfo = (struct pan_fb_info){
-      .width = u_minify(img->pimage.layout.width, region->imageSubresource.mipLevel),
-      .height = u_minify(img->pimage.layout.height, region->imageSubresource.mipLevel),
+      .width =
+         u_minify(img->pimage.layout.width, region->imageSubresource.mipLevel),
+      .height =
+         u_minify(img->pimage.layout.height, region->imageSubresource.mipLevel),
       .extent.minx = minx,
       .extent.maxx = maxx,
       .extent.miny = miny,
@@ -1097,19 +1083,32 @@
    assert(region->imageSubresource.layerCount == 1 ||
           region->imageExtent.depth == 1);
    assert(region->imageOffset.z >= 0);
-   unsigned first_layer = MAX2(region->imageSubresource.baseArrayLayer, region->imageOffset.z);
-   unsigned nlayers = MAX2(region->imageSubresource.layerCount, region->imageExtent.depth);
+   unsigned first_layer =
+      MAX2(region->imageSubresource.baseArrayLayer, region->imageOffset.z);
+   unsigned nlayers =
+      MAX2(region->imageSubresource.layerCount, region->imageExtent.depth);
    for (unsigned l = 0; l < nlayers; l++) {
       float src_rect[] = {
-         0, 0, l, 1.0,
-         region->imageExtent.width, 0, l, 1.0,
-         0, region->imageExtent.height, l, 1.0,
-         region->imageExtent.width, region->imageExtent.height, l, 1.0,
+         0,
+         0,
+         l,
+         1.0,
+         region->imageExtent.width,
+         0,
+         l,
+         1.0,
+         0,
+         region->imageExtent.height,
+         l,
+         1.0,
+         region->imageExtent.width,
+         region->imageExtent.height,
+         l,
+         1.0,
       };
 
-      mali_ptr src_coords =
-         pan_pool_upload_aligned(&cmdbuf->desc_pool.base, src_rect,
-                                 sizeof(src_rect), 64);
+      mali_ptr src_coords = pan_pool_upload_aligned(
+         &cmdbuf->desc_pool.base, src_rect, sizeof(src_rect), 64);
 
       struct panvk_batch *batch = panvk_cmd_open_batch(cmdbuf);
 
@@ -1127,11 +1126,9 @@
 
       struct panfrost_ptr job;
 
-      job = panvk_meta_copy_emit_tiler_job(&cmdbuf->desc_pool.base,
-                                           &batch->scoreboard,
-                                           src_coords, dst_coords,
-                                           0, 0, pushconsts,
-                                           vpd, rsd, tsd, tiler);
+      job = panvk_meta_copy_emit_tiler_job(
+         &cmdbuf->desc_pool.base, &batch->scoreboard, src_coords, dst_coords, 0,
+         0, pushconsts, vpd, rsd, tsd, tiler);
 
       util_dynarray_append(&batch->jobs, void *, job.cpu);
       panvk_per_arch(cmd_close_batch)(cmdbuf);
@@ -1141,53 +1138,54 @@
 static void
 panvk_meta_copy_buf2img_init(struct panvk_physical_device *dev)
 {
-   STATIC_ASSERT(ARRAY_SIZE(panvk_meta_copy_buf2img_fmts) == PANVK_META_COPY_BUF2IMG_NUM_FORMATS);
+   STATIC_ASSERT(ARRAY_SIZE(panvk_meta_copy_buf2img_fmts) ==
+                 PANVK_META_COPY_BUF2IMG_NUM_FORMATS);
 
    for (unsigned i = 0; i < ARRAY_SIZE(panvk_meta_copy_buf2img_fmts); i++) {
       struct pan_shader_info shader_info;
-      mali_ptr shader =
-         panvk_meta_copy_buf2img_shader(&dev->pdev, &dev->meta.bin_pool.base,
-                                        panvk_meta_copy_buf2img_fmts[i],
-                                        &shader_info);
-      dev->meta.copy.buf2img[i].rsd =
-         panvk_meta_copy_to_img_emit_rsd(&dev->pdev, &dev->meta.desc_pool.base,
-                                         shader, &shader_info,
-                                         panvk_meta_copy_buf2img_fmts[i].imgfmt,
-                                         panvk_meta_copy_buf2img_fmts[i].mask,
-                                         false);
+      mali_ptr shader = panvk_meta_copy_buf2img_shader(
+         &dev->pdev, &dev->meta.bin_pool.base, panvk_meta_copy_buf2img_fmts[i],
+         &shader_info);
+      dev->meta.copy.buf2img[i].rsd = panvk_meta_copy_to_img_emit_rsd(
+         &dev->pdev, &dev->meta.desc_pool.base, shader, &shader_info,
+         panvk_meta_copy_buf2img_fmts[i].imgfmt,
+         panvk_meta_copy_buf2img_fmts[i].mask, false);
    }
 }
 
 void
-panvk_per_arch(CmdCopyBufferToImage2)(VkCommandBuffer commandBuffer,
-                                      const VkCopyBufferToImageInfo2 *pCopyBufferToImageInfo)
+panvk_per_arch(CmdCopyBufferToImage2)(
+   VkCommandBuffer commandBuffer,
+   const VkCopyBufferToImageInfo2 *pCopyBufferToImageInfo)
 {
    VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
    VK_FROM_HANDLE(panvk_buffer, buf, pCopyBufferToImageInfo->srcBuffer);
    VK_FROM_HANDLE(panvk_image, img, pCopyBufferToImageInfo->dstImage);
 
    for (unsigned i = 0; i < pCopyBufferToImageInfo->regionCount; i++) {
-      panvk_meta_copy_buf2img(cmdbuf, buf, img, &pCopyBufferToImageInfo->pRegions[i]);
+      panvk_meta_copy_buf2img(cmdbuf, buf, img,
+                              &pCopyBufferToImageInfo->pRegions[i]);
    }
 }
 
-static const struct panvk_meta_copy_format_info panvk_meta_copy_img2buf_fmts[] = {
-   { PIPE_FORMAT_R8_UINT, 0x1 },
-   { PIPE_FORMAT_R8G8_UINT, 0x3 },
-   { PIPE_FORMAT_R5G6B5_UNORM, 0x7 },
-   { PIPE_FORMAT_R8G8B8A8_UINT, 0xf },
-   { PIPE_FORMAT_R16G16B16_UINT, 0x7 },
-   { PIPE_FORMAT_R32G32_UINT, 0x3 },
-   { PIPE_FORMAT_R32G32B32_UINT, 0x7 },
-   { PIPE_FORMAT_R32G32B32A32_UINT, 0xf },
-   /* S8 -> Z24S8 */
-   { PIPE_FORMAT_R8G8B8A8_UINT, 0x8 },
-   /* S8 -> Z32_S8X24 */
-   { PIPE_FORMAT_R32G32_UINT, 0x2 },
-   /* Z24X8 -> Z24S8 */
-   { PIPE_FORMAT_R8G8B8A8_UINT, 0x7 },
-   /* Z32 -> Z32_S8X24 */
-   { PIPE_FORMAT_R32G32_UINT, 0x1 },
+static const struct panvk_meta_copy_format_info panvk_meta_copy_img2buf_fmts[] =
+   {
+      {PIPE_FORMAT_R8_UINT, 0x1},
+      {PIPE_FORMAT_R8G8_UINT, 0x3},
+      {PIPE_FORMAT_R5G6B5_UNORM, 0x7},
+      {PIPE_FORMAT_R8G8B8A8_UINT, 0xf},
+      {PIPE_FORMAT_R16G16B16_UINT, 0x7},
+      {PIPE_FORMAT_R32G32_UINT, 0x3},
+      {PIPE_FORMAT_R32G32B32_UINT, 0x7},
+      {PIPE_FORMAT_R32G32B32A32_UINT, 0xf},
+      /* S8 -> Z24S8 */
+      {PIPE_FORMAT_R8G8B8A8_UINT, 0x8},
+      /* S8 -> Z32_S8X24 */
+      {PIPE_FORMAT_R32G32_UINT, 0x2},
+      /* Z24X8 -> Z24S8 */
+      {PIPE_FORMAT_R8G8B8A8_UINT, 0x7},
+      /* Z32 -> Z32_S8X24 */
+      {PIPE_FORMAT_R32G32_UINT, 0x1},
 };
 
 static enum pipe_format
@@ -1197,18 +1195,27 @@
     * texelsize otherwise.
     */
    switch (util_format_get_blocksize(imgfmt)) {
-   case 1: return PIPE_FORMAT_R8_UINT;
+   case 1:
+      return PIPE_FORMAT_R8_UINT;
    /* AFBC stores things differently for RGB565,
     * we can't simply map to R8G8 in that case */
-   case 2: return (imgfmt == PIPE_FORMAT_R5G6B5_UNORM ||
-                   imgfmt == PIPE_FORMAT_B5G6R5_UNORM) ?
-                  PIPE_FORMAT_R5G6B5_UNORM : PIPE_FORMAT_R8G8_UINT;
-   case 4: return PIPE_FORMAT_R8G8B8A8_UINT;
-   case 6: return PIPE_FORMAT_R16G16B16_UINT;
-   case 8: return PIPE_FORMAT_R32G32_UINT;
-   case 12: return PIPE_FORMAT_R32G32B32_UINT;
-   case 16: return PIPE_FORMAT_R32G32B32A32_UINT;
-   default: unreachable("Invalid format\n");
+   case 2:
+      return (imgfmt == PIPE_FORMAT_R5G6B5_UNORM ||
+              imgfmt == PIPE_FORMAT_B5G6R5_UNORM)
+                ? PIPE_FORMAT_R5G6B5_UNORM
+                : PIPE_FORMAT_R8G8_UINT;
+   case 4:
+      return PIPE_FORMAT_R8G8B8A8_UINT;
+   case 6:
+      return PIPE_FORMAT_R16G16B16_UINT;
+   case 8:
+      return PIPE_FORMAT_R32G32_UINT;
+   case 12:
+      return PIPE_FORMAT_R32G32B32_UINT;
+   case 16:
+      return PIPE_FORMAT_R32G32B32A32_UINT;
+   default:
+      unreachable("Invalid format\n");
    }
 }
 
@@ -1230,12 +1237,12 @@
    } img;
 } PACKED;
 
-#define panvk_meta_copy_img2buf_get_info_field(b, field) \
-        nir_load_push_constant((b), 1, \
-                     sizeof(((struct panvk_meta_copy_img2buf_info *)0)->field) * 8, \
-                     nir_imm_int(b, 0), \
-                     .base = offsetof(struct panvk_meta_copy_img2buf_info, field), \
-                     .range = ~0)
+#define panvk_meta_copy_img2buf_get_info_field(b, field)                       \
+   nir_load_push_constant(                                                     \
+      (b), 1, sizeof(((struct panvk_meta_copy_img2buf_info *)0)->field) * 8,   \
+      nir_imm_int(b, 0),                                                       \
+      .base = offsetof(struct panvk_meta_copy_img2buf_info, field),            \
+      .range = ~0)
 
 static mali_ptr
 panvk_meta_copy_img2buf_shader(struct panfrost_device *pdev,
@@ -1250,17 +1257,13 @@
    /* FIXME: Won't work on compute queues, but we can't do that with
     * a compute shader if the destination is an AFBC surface.
     */
-   nir_builder b =
-      nir_builder_init_simple_shader(MESA_SHADER_COMPUTE,
-                                     GENX(pan_shader_get_compiler_options)(),
-                                     "panvk_meta_copy_img2buf(dim=%dD%s,imgfmt=%s,mask=%x)",
-                                     texdim, texisarray ? "[]" : "",
-                                     util_format_name(key.imgfmt),
-                                     key.mask);
+   nir_builder b = nir_builder_init_simple_shader(
+      MESA_SHADER_COMPUTE, GENX(pan_shader_get_compiler_options)(),
+      "panvk_meta_copy_img2buf(dim=%dD%s,imgfmt=%s,mask=%x)", texdim,
+      texisarray ? "[]" : "", util_format_name(key.imgfmt), key.mask);
 
    nir_ssa_def *coord = nir_load_global_invocation_id(&b, 32);
-   nir_ssa_def *bufptr =
-      panvk_meta_copy_img2buf_get_info_field(&b, buf.ptr);
+   nir_ssa_def *bufptr = panvk_meta_copy_img2buf_get_info_field(&b, buf.ptr);
    nir_ssa_def *buflinestride =
       panvk_meta_copy_img2buf_get_info_field(&b, buf.stride.line);
    nir_ssa_def *bufsurfstride =
@@ -1280,52 +1283,41 @@
    switch (texdim + texisarray) {
    case 1:
       imgcoords =
-         nir_iadd(&b,
-                  nir_channel(&b, coord, 0),
+         nir_iadd(&b, nir_channel(&b, coord, 0),
                   panvk_meta_copy_img2buf_get_info_field(&b, img.offset.x));
       inbounds =
-         nir_iand(&b,
-                  nir_uge(&b, imgmaxx, nir_channel(&b, imgcoords, 0)),
+         nir_iand(&b, nir_uge(&b, imgmaxx, nir_channel(&b, imgcoords, 0)),
                   nir_uge(&b, nir_channel(&b, imgcoords, 0), imgminx));
       break;
    case 2:
-      imgcoords =
-         nir_vec2(&b,
-                  nir_iadd(&b,
-                           nir_channel(&b, coord, 0),
-                           panvk_meta_copy_img2buf_get_info_field(&b, img.offset.x)),
-                  nir_iadd(&b,
-                           nir_channel(&b, coord, 1),
-                           panvk_meta_copy_img2buf_get_info_field(&b, img.offset.y)));
-      inbounds =
-         nir_iand(&b,
-                  nir_iand(&b,
-                           nir_uge(&b, imgmaxx, nir_channel(&b, imgcoords, 0)),
-                           nir_uge(&b, imgmaxy, nir_channel(&b, imgcoords, 1))),
-                  nir_iand(&b,
-                           nir_uge(&b, nir_channel(&b, imgcoords, 0), imgminx),
-                           nir_uge(&b, nir_channel(&b, imgcoords, 1), imgminy)));
+      imgcoords = nir_vec2(
+         &b,
+         nir_iadd(&b, nir_channel(&b, coord, 0),
+                  panvk_meta_copy_img2buf_get_info_field(&b, img.offset.x)),
+         nir_iadd(&b, nir_channel(&b, coord, 1),
+                  panvk_meta_copy_img2buf_get_info_field(&b, img.offset.y)));
+      inbounds = nir_iand(
+         &b,
+         nir_iand(&b, nir_uge(&b, imgmaxx, nir_channel(&b, imgcoords, 0)),
+                  nir_uge(&b, imgmaxy, nir_channel(&b, imgcoords, 1))),
+         nir_iand(&b, nir_uge(&b, nir_channel(&b, imgcoords, 0), imgminx),
+                  nir_uge(&b, nir_channel(&b, imgcoords, 1), imgminy)));
       break;
    case 3:
-      imgcoords =
-         nir_vec3(&b,
-                  nir_iadd(&b,
-                           nir_channel(&b, coord, 0),
-                           panvk_meta_copy_img2buf_get_info_field(&b, img.offset.x)),
-                  nir_iadd(&b,
-                           nir_channel(&b, coord, 1),
-                           panvk_meta_copy_img2buf_get_info_field(&b, img.offset.y)),
-                  nir_iadd(&b,
-                           nir_channel(&b, coord, 2),
-                           panvk_meta_copy_img2buf_get_info_field(&b, img.offset.y)));
-      inbounds =
-         nir_iand(&b,
-                  nir_iand(&b,
-                           nir_uge(&b, imgmaxx, nir_channel(&b, imgcoords, 0)),
-                           nir_uge(&b, imgmaxy, nir_channel(&b, imgcoords, 1))),
-                  nir_iand(&b,
-                           nir_uge(&b, nir_channel(&b, imgcoords, 0), imgminx),
-                           nir_uge(&b, nir_channel(&b, imgcoords, 1), imgminy)));
+      imgcoords = nir_vec3(
+         &b,
+         nir_iadd(&b, nir_channel(&b, coord, 0),
+                  panvk_meta_copy_img2buf_get_info_field(&b, img.offset.x)),
+         nir_iadd(&b, nir_channel(&b, coord, 1),
+                  panvk_meta_copy_img2buf_get_info_field(&b, img.offset.y)),
+         nir_iadd(&b, nir_channel(&b, coord, 2),
+                  panvk_meta_copy_img2buf_get_info_field(&b, img.offset.y)));
+      inbounds = nir_iand(
+         &b,
+         nir_iand(&b, nir_uge(&b, imgmaxx, nir_channel(&b, imgcoords, 0)),
+                  nir_uge(&b, imgmaxy, nir_channel(&b, imgcoords, 1))),
+         nir_iand(&b, nir_uge(&b, nir_channel(&b, imgcoords, 0), imgminx),
+                  nir_uge(&b, nir_channel(&b, imgcoords, 1), imgminy)));
       break;
    default:
       unreachable("Invalid texture dimension\n");
@@ -1347,8 +1339,8 @@
                      nir_imul(&b, nir_channel(&b, coord, 2), bufsurfstride));
    bufptr = nir_iadd(&b, bufptr, nir_u2u64(&b, offset));
 
-   unsigned imgcompsz = imgtexelsz <= 4 ?
-                        1 : MIN2(1 << (ffs(imgtexelsz) - 1), 4);
+   unsigned imgcompsz =
+      imgtexelsz <= 4 ? 1 : MIN2(1 << (ffs(imgtexelsz) - 1), 4);
    unsigned nimgcomps = imgtexelsz / imgcompsz;
    assert(nimgcomps <= 4);
 
@@ -1356,14 +1348,21 @@
    tex->op = nir_texop_txf;
    tex->texture_index = 0;
    tex->is_array = texisarray;
-   tex->dest_type = util_format_is_unorm(key.imgfmt) ?
-                    nir_type_float32 : nir_type_uint32;
+   tex->dest_type =
+      util_format_is_unorm(key.imgfmt) ? nir_type_float32 : nir_type_uint32;
 
    switch (texdim) {
-   case 1: tex->sampler_dim = GLSL_SAMPLER_DIM_1D; break;
-   case 2: tex->sampler_dim = GLSL_SAMPLER_DIM_2D; break;
-   case 3: tex->sampler_dim = GLSL_SAMPLER_DIM_3D; break;
-   default: unreachable("Invalid texture dimension");
+   case 1:
+      tex->sampler_dim = GLSL_SAMPLER_DIM_1D;
+      break;
+   case 2:
+      tex->sampler_dim = GLSL_SAMPLER_DIM_2D;
+      break;
+   case 3:
+      tex->sampler_dim = GLSL_SAMPLER_DIM_3D;
+      break;
+   default:
+      unreachable("Invalid texture dimension");
    }
 
    tex->src[0].src_type = nir_tex_src_coord;
@@ -1392,15 +1391,14 @@
 
    if (key.imgfmt == PIPE_FORMAT_R5G6B5_UNORM) {
       texel = nir_fmul(&b, texel,
-                       nir_vec3(&b,
-                                nir_imm_float(&b, 31),
-                                nir_imm_float(&b, 63),
-                                nir_imm_float(&b, 31)));
+                       nir_vec3(&b, nir_imm_float(&b, 31),
+                                nir_imm_float(&b, 63), nir_imm_float(&b, 31)));
       texel = nir_f2u16(&b, texel);
-      texel = nir_ior(&b, nir_channel(&b, texel, 0),
-                      nir_ior(&b,
-                              nir_ishl(&b, nir_channel(&b, texel, 1), nir_imm_int(&b, 5)),
-                              nir_ishl(&b, nir_channel(&b, texel, 2), nir_imm_int(&b, 11))));
+      texel = nir_ior(
+         &b, nir_channel(&b, texel, 0),
+         nir_ior(&b,
+                 nir_ishl(&b, nir_channel(&b, texel, 1), nir_imm_int(&b, 5)),
+                 nir_ishl(&b, nir_channel(&b, texel, 2), nir_imm_int(&b, 11))));
       imgcompsz = 2;
       bufcompsz = 2;
       nbufcomps = 1;
@@ -1408,9 +1406,10 @@
    } else if (imgcompsz == 1) {
       nir_ssa_def *packed = nir_channel(&b, texel, 0);
       for (unsigned i = 1; i < nbufcomps; i++) {
-         packed = nir_ior(&b, packed,
-                          nir_ishl(&b, nir_iand_imm(&b, nir_channel(&b, texel, i), 0xff),
-                                   nir_imm_int(&b, i * 8)));
+         packed = nir_ior(
+            &b, packed,
+            nir_ishl(&b, nir_iand_imm(&b, nir_channel(&b, texel, i), 0xff),
+                     nir_imm_int(&b, i * 8)));
       }
       texel = packed;
 
@@ -1436,7 +1435,8 @@
    util_dynarray_init(&binary, NULL);
    GENX(pan_shader_compile)(b.shader, &inputs, &binary, shader_info);
 
-   shader_info->push.count = DIV_ROUND_UP(sizeof(struct panvk_meta_copy_img2buf_info), 4);
+   shader_info->push.count =
+      DIV_ROUND_UP(sizeof(struct panvk_meta_copy_img2buf_info), 4);
 
    mali_ptr shader =
       pan_pool_upload_aligned(bin_pool, binary.data, binary.size, 128);
@@ -1471,9 +1471,8 @@
                                        region->imageSubresource.aspectMask),
    };
    unsigned buftexelsz = panvk_meta_copy_buf_texelsize(key.imgfmt, key.mask);
-   unsigned texdimidx =
-      panvk_meta_copy_tex_type(img->pimage.layout.dim,
-                               img->pimage.layout.array_size > 1);
+   unsigned texdimidx = panvk_meta_copy_tex_type(
+      img->pimage.layout.dim, img->pimage.layout.array_size > 1);
    unsigned fmtidx = panvk_meta_copy_img2buf_format_idx(key);
 
    mali_ptr rsd =
@@ -1481,10 +1480,12 @@
 
    struct panvk_meta_copy_img2buf_info info = {
       .buf.ptr = panvk_buffer_gpu_ptr(buf, region->bufferOffset),
-      .buf.stride.line = (region->bufferRowLength ? : region->imageExtent.width) * buftexelsz,
+      .buf.stride.line =
+         (region->bufferRowLength ?: region->imageExtent.width) * buftexelsz,
       .img.offset.x = MAX2(region->imageOffset.x & ~15, 0),
       .img.extent.minx = MAX2(region->imageOffset.x, 0),
-      .img.extent.maxx = MAX2(region->imageOffset.x + region->imageExtent.width - 1, 0),
+      .img.extent.maxx =
+         MAX2(region->imageOffset.x + region->imageExtent.width - 1, 0),
    };
 
    if (img->pimage.layout.dim == MALI_TEXTURE_DIMENSION_1D) {
@@ -1493,26 +1494,31 @@
       info.img.offset.y = MAX2(region->imageOffset.y & ~15, 0);
       info.img.offset.z = MAX2(region->imageOffset.z, 0);
       info.img.extent.miny = MAX2(region->imageOffset.y, 0);
-      info.img.extent.maxy = MAX2(region->imageOffset.y + region->imageExtent.height - 1, 0);
+      info.img.extent.maxy =
+         MAX2(region->imageOffset.y + region->imageExtent.height - 1, 0);
    }
 
-   info.buf.stride.surf = (region->bufferImageHeight ? : region->imageExtent.height) *
-                          info.buf.stride.line;
+   info.buf.stride.surf =
+      (region->bufferImageHeight ?: region->imageExtent.height) *
+      info.buf.stride.line;
 
    mali_ptr pushconsts =
       pan_pool_upload_aligned(&cmdbuf->desc_pool.base, &info, sizeof(info), 16);
 
    struct pan_image_view view = {
       .format = key.imgfmt,
-      .dim = img->pimage.layout.dim == MALI_TEXTURE_DIMENSION_CUBE ?
-             MALI_TEXTURE_DIMENSION_2D : img->pimage.layout.dim,
+      .dim = img->pimage.layout.dim == MALI_TEXTURE_DIMENSION_CUBE
+                ? MALI_TEXTURE_DIMENSION_2D
+                : img->pimage.layout.dim,
       .image = &img->pimage,
       .nr_samples = img->pimage.layout.nr_samples,
       .first_level = region->imageSubresource.mipLevel,
       .last_level = region->imageSubresource.mipLevel,
       .first_layer = region->imageSubresource.baseArrayLayer,
-      .last_layer = region->imageSubresource.baseArrayLayer + region->imageSubresource.layerCount - 1,
-      .swizzle = { PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_Z, PIPE_SWIZZLE_W },
+      .last_layer = region->imageSubresource.baseArrayLayer +
+                    region->imageSubresource.layerCount - 1,
+      .swizzle = {PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_Z,
+                  PIPE_SWIZZLE_W},
    };
 
    mali_ptr texture =
@@ -1524,12 +1530,11 @@
 
    struct panvk_batch *batch = panvk_cmd_open_batch(cmdbuf);
 
-   struct pan_tls_info tlsinfo = { 0 };
+   struct pan_tls_info tlsinfo = {0};
 
    batch->blit.src = img->pimage.data.bo;
    batch->blit.dst = buf->bo;
-   batch->tls =
-      pan_pool_alloc_desc(&cmdbuf->desc_pool.base, LOCAL_STORAGE);
+   batch->tls = pan_pool_alloc_desc(&cmdbuf->desc_pool.base, LOCAL_STORAGE);
    GENX(pan_emit_tls)(&tlsinfo, batch->tls.cpu);
 
    mali_ptr tsd = batch->tls.gpu;
@@ -1541,19 +1546,18 @@
    };
 
    struct pan_compute_dim num_wg = {
-     (ALIGN_POT(info.img.extent.maxx + 1, 16) - info.img.offset.x) / 16,
-     img->pimage.layout.dim == MALI_TEXTURE_DIMENSION_1D ?
-        region->imageSubresource.layerCount :
-        (ALIGN_POT(info.img.extent.maxy + 1, 16) - info.img.offset.y) / 16,
-     img->pimage.layout.dim != MALI_TEXTURE_DIMENSION_1D ?
-        MAX2(region->imageSubresource.layerCount, region->imageExtent.depth) : 1,
+      (ALIGN_POT(info.img.extent.maxx + 1, 16) - info.img.offset.x) / 16,
+      img->pimage.layout.dim == MALI_TEXTURE_DIMENSION_1D
+         ? region->imageSubresource.layerCount
+         : (ALIGN_POT(info.img.extent.maxy + 1, 16) - info.img.offset.y) / 16,
+      img->pimage.layout.dim != MALI_TEXTURE_DIMENSION_1D
+         ? MAX2(region->imageSubresource.layerCount, region->imageExtent.depth)
+         : 1,
    };
 
-   struct panfrost_ptr job =
-      panvk_meta_copy_emit_compute_job(&cmdbuf->desc_pool.base,
-                                       &batch->scoreboard, &num_wg, &wg_sz,
-                                       texture, sampler,
-                                       pushconsts, rsd, tsd);
+   struct panfrost_ptr job = panvk_meta_copy_emit_compute_job(
+      &cmdbuf->desc_pool.base, &batch->scoreboard, &num_wg, &wg_sz, texture,
+      sampler, pushconsts, rsd, tsd);
 
    util_dynarray_append(&batch->jobs, void *, job.cpu);
 
@@ -1563,7 +1567,8 @@
 static void
 panvk_meta_copy_img2buf_init(struct panvk_physical_device *dev)
 {
-   STATIC_ASSERT(ARRAY_SIZE(panvk_meta_copy_img2buf_fmts) == PANVK_META_COPY_IMG2BUF_NUM_FORMATS);
+   STATIC_ASSERT(ARRAY_SIZE(panvk_meta_copy_img2buf_fmts) ==
+                 PANVK_META_COPY_IMG2BUF_NUM_FORMATS);
 
    for (unsigned i = 0; i < ARRAY_SIZE(panvk_meta_copy_img2buf_fmts); i++) {
       for (unsigned texdim = 1; texdim <= 3; texdim++) {
@@ -1571,14 +1576,13 @@
          assert(texdimidx < ARRAY_SIZE(dev->meta.copy.img2buf));
 
          struct pan_shader_info shader_info;
-         mali_ptr shader =
-            panvk_meta_copy_img2buf_shader(&dev->pdev, &dev->meta.bin_pool.base,
-                                           panvk_meta_copy_img2buf_fmts[i],
-                                           texdim, false, &shader_info);
+         mali_ptr shader = panvk_meta_copy_img2buf_shader(
+            &dev->pdev, &dev->meta.bin_pool.base,
+            panvk_meta_copy_img2buf_fmts[i], texdim, false, &shader_info);
          dev->meta.copy.img2buf[texdimidx][i].rsd =
             panvk_meta_copy_to_buf_emit_rsd(&dev->pdev,
-                                            &dev->meta.desc_pool.base,
-                                            shader, &shader_info, true);
+                                            &dev->meta.desc_pool.base, shader,
+                                            &shader_info, true);
 
          if (texdim == 3)
             continue;
@@ -1586,28 +1590,29 @@
          memset(&shader_info, 0, sizeof(shader_info));
          texdimidx = panvk_meta_copy_tex_type(texdim, true);
          assert(texdimidx < ARRAY_SIZE(dev->meta.copy.img2buf));
-         shader =
-            panvk_meta_copy_img2buf_shader(&dev->pdev, &dev->meta.bin_pool.base,
-                                           panvk_meta_copy_img2buf_fmts[i],
-                                           texdim, true, &shader_info);
+         shader = panvk_meta_copy_img2buf_shader(
+            &dev->pdev, &dev->meta.bin_pool.base,
+            panvk_meta_copy_img2buf_fmts[i], texdim, true, &shader_info);
          dev->meta.copy.img2buf[texdimidx][i].rsd =
             panvk_meta_copy_to_buf_emit_rsd(&dev->pdev,
-                                            &dev->meta.desc_pool.base,
-                                            shader, &shader_info, true);
+                                            &dev->meta.desc_pool.base, shader,
+                                            &shader_info, true);
       }
    }
 }
 
 void
-panvk_per_arch(CmdCopyImageToBuffer2)(VkCommandBuffer commandBuffer,
-                                      const VkCopyImageToBufferInfo2 *pCopyImageToBufferInfo)
+panvk_per_arch(CmdCopyImageToBuffer2)(
+   VkCommandBuffer commandBuffer,
+   const VkCopyImageToBufferInfo2 *pCopyImageToBufferInfo)
 {
    VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
    VK_FROM_HANDLE(panvk_buffer, buf, pCopyImageToBufferInfo->dstBuffer);
    VK_FROM_HANDLE(panvk_image, img, pCopyImageToBufferInfo->srcImage);
 
    for (unsigned i = 0; i < pCopyImageToBufferInfo->regionCount; i++) {
-      panvk_meta_copy_img2buf(cmdbuf, buf, img, &pCopyImageToBufferInfo->pRegions[i]);
+      panvk_meta_copy_img2buf(cmdbuf, buf, img,
+                              &pCopyImageToBufferInfo->pRegions[i]);
    }
 }
 
@@ -1616,32 +1621,29 @@
    mali_ptr dst;
 } PACKED;
 
-#define panvk_meta_copy_buf2buf_get_info_field(b, field) \
-        nir_load_push_constant((b), 1, \
-                     sizeof(((struct panvk_meta_copy_buf2buf_info *)0)->field) * 8, \
-                     nir_imm_int(b, 0), \
-                     .base = offsetof(struct panvk_meta_copy_buf2buf_info, field), \
-                     .range = ~0)
+#define panvk_meta_copy_buf2buf_get_info_field(b, field)                       \
+   nir_load_push_constant(                                                     \
+      (b), 1, sizeof(((struct panvk_meta_copy_buf2buf_info *)0)->field) * 8,   \
+      nir_imm_int(b, 0),                                                       \
+      .base = offsetof(struct panvk_meta_copy_buf2buf_info, field),            \
+      .range = ~0)
 
 static mali_ptr
 panvk_meta_copy_buf2buf_shader(struct panfrost_device *pdev,
-                               struct pan_pool *bin_pool,
-                               unsigned blksz,
+                               struct pan_pool *bin_pool, unsigned blksz,
                                struct pan_shader_info *shader_info)
 {
    /* FIXME: Won't work on compute queues, but we can't do that with
     * a compute shader if the destination is an AFBC surface.
     */
-   nir_builder b =
-      nir_builder_init_simple_shader(MESA_SHADER_COMPUTE,
-                                     GENX(pan_shader_get_compiler_options)(),
-                                     "panvk_meta_copy_buf2buf(blksz=%d)",
-                                     blksz);
+   nir_builder b = nir_builder_init_simple_shader(
+      MESA_SHADER_COMPUTE, GENX(pan_shader_get_compiler_options)(),
+      "panvk_meta_copy_buf2buf(blksz=%d)", blksz);
 
    nir_ssa_def *coord = nir_load_global_invocation_id(&b, 32);
 
-   nir_ssa_def *offset =
-      nir_u2u64(&b, nir_imul(&b, nir_channel(&b, coord, 0), nir_imm_int(&b, blksz)));
+   nir_ssa_def *offset = nir_u2u64(
+      &b, nir_imul(&b, nir_channel(&b, coord, 0), nir_imm_int(&b, blksz)));
    nir_ssa_def *srcptr =
       nir_iadd(&b, panvk_meta_copy_buf2buf_get_info_field(&b, src), offset);
    nir_ssa_def *dstptr =
@@ -1664,7 +1666,8 @@
    util_dynarray_init(&binary, NULL);
    GENX(pan_shader_compile)(b.shader, &inputs, &binary, shader_info);
 
-   shader_info->push.count = DIV_ROUND_UP(sizeof(struct panvk_meta_copy_buf2buf_info), 4);
+   shader_info->push.count =
+      DIV_ROUND_UP(sizeof(struct panvk_meta_copy_buf2buf_info), 4);
 
    mali_ptr shader =
       pan_pool_upload_aligned(bin_pool, binary.data, binary.size, 128);
@@ -1680,12 +1683,10 @@
 {
    for (unsigned i = 0; i < ARRAY_SIZE(dev->meta.copy.buf2buf); i++) {
       struct pan_shader_info shader_info;
-      mali_ptr shader =
-         panvk_meta_copy_buf2buf_shader(&dev->pdev, &dev->meta.bin_pool.base,
-                                        1 << i, &shader_info);
-      dev->meta.copy.buf2buf[i].rsd =
-         panvk_meta_copy_to_buf_emit_rsd(&dev->pdev, &dev->meta.desc_pool.base,
-                                         shader, &shader_info, false);
+      mali_ptr shader = panvk_meta_copy_buf2buf_shader(
+         &dev->pdev, &dev->meta.bin_pool.base, 1 << i, &shader_info);
+      dev->meta.copy.buf2buf[i].rsd = panvk_meta_copy_to_buf_emit_rsd(
+         &dev->pdev, &dev->meta.desc_pool.base, shader, &shader_info, false);
    }
 }
 
@@ -1703,7 +1704,8 @@
    unsigned alignment = ffs((info.src | info.dst | region->size) & 15);
    unsigned log2blksz = alignment ? alignment - 1 : 4;
 
-   assert(log2blksz < ARRAY_SIZE(cmdbuf->device->physical_device->meta.copy.buf2buf));
+   assert(log2blksz <
+          ARRAY_SIZE(cmdbuf->device->physical_device->meta.copy.buf2buf));
    mali_ptr rsd =
       cmdbuf->device->physical_device->meta.copy.buf2buf[log2blksz].rsd;
 
@@ -1719,13 +1721,11 @@
    mali_ptr tsd = batch->tls.gpu;
 
    unsigned nblocks = region->size >> log2blksz;
-   struct pan_compute_dim num_wg = { nblocks, 1, 1 };
-   struct pan_compute_dim wg_sz = { 1, 1, 1};
-   struct panfrost_ptr job =
-     panvk_meta_copy_emit_compute_job(&cmdbuf->desc_pool.base,
-                                      &batch->scoreboard,
-                                      &num_wg, &wg_sz,
-                                      0, 0, pushconsts, rsd, tsd);
+   struct pan_compute_dim num_wg = {nblocks, 1, 1};
+   struct pan_compute_dim wg_sz = {1, 1, 1};
+   struct panfrost_ptr job = panvk_meta_copy_emit_compute_job(
+      &cmdbuf->desc_pool.base, &batch->scoreboard, &num_wg, &wg_sz, 0, 0,
+      pushconsts, rsd, tsd);
 
    util_dynarray_append(&batch->jobs, void *, job.cpu);
 
@@ -1752,12 +1752,11 @@
    uint32_t val;
 } PACKED;
 
-#define panvk_meta_fill_buf_get_info_field(b, field) \
-        nir_load_push_constant((b), 1, \
-                     sizeof(((struct panvk_meta_fill_buf_info *)0)->field) * 8, \
-                     nir_imm_int(b, 0), \
-                     .base = offsetof(struct panvk_meta_fill_buf_info, field), \
-                     .range = ~0)
+#define panvk_meta_fill_buf_get_info_field(b, field)                           \
+   nir_load_push_constant(                                                     \
+      (b), 1, sizeof(((struct panvk_meta_fill_buf_info *)0)->field) * 8,       \
+      nir_imm_int(b, 0),                                                       \
+      .base = offsetof(struct panvk_meta_fill_buf_info, field), .range = ~0)
 
 static mali_ptr
 panvk_meta_fill_buf_shader(struct panfrost_device *pdev,
@@ -1767,15 +1766,15 @@
    /* FIXME: Won't work on compute queues, but we can't do that with
     * a compute shader if the destination is an AFBC surface.
     */
-   nir_builder b =
-      nir_builder_init_simple_shader(MESA_SHADER_COMPUTE,
-                                     GENX(pan_shader_get_compiler_options)(),
-                                     "panvk_meta_fill_buf()");
+   nir_builder b = nir_builder_init_simple_shader(
+      MESA_SHADER_COMPUTE, GENX(pan_shader_get_compiler_options)(),
+      "panvk_meta_fill_buf()");
 
    nir_ssa_def *coord = nir_load_global_invocation_id(&b, 32);
 
    nir_ssa_def *offset =
-      nir_u2u64(&b, nir_imul(&b, nir_channel(&b, coord, 0), nir_imm_int(&b, sizeof(uint32_t))));
+      nir_u2u64(&b, nir_imul(&b, nir_channel(&b, coord, 0),
+                             nir_imm_int(&b, sizeof(uint32_t))));
    nir_ssa_def *ptr =
       nir_iadd(&b, panvk_meta_fill_buf_get_info_field(&b, start), offset);
    nir_ssa_def *val = panvk_meta_fill_buf_get_info_field(&b, val);
@@ -1793,7 +1792,8 @@
    util_dynarray_init(&binary, NULL);
    GENX(pan_shader_compile)(b.shader, &inputs, &binary, shader_info);
 
-   shader_info->push.count = DIV_ROUND_UP(sizeof(struct panvk_meta_fill_buf_info), 4);
+   shader_info->push.count =
+      DIV_ROUND_UP(sizeof(struct panvk_meta_fill_buf_info), 4);
 
    mali_ptr shader =
       pan_pool_upload_aligned(bin_pool, binary.data, binary.size, 128);
@@ -1811,12 +1811,10 @@
 {
    struct pan_shader_info shader_info;
 
-   mali_ptr shader =
-      panvk_meta_fill_buf_shader(pdev, bin_pool, &shader_info);
+   mali_ptr shader = panvk_meta_fill_buf_shader(pdev, bin_pool, &shader_info);
 
    struct panfrost_ptr rsd_ptr =
-      pan_pool_alloc_desc_aggregate(desc_pool,
-                                    PAN_DESC(RENDERER_STATE));
+      pan_pool_alloc_desc_aggregate(desc_pool, PAN_DESC(RENDERER_STATE));
 
    pan_pack(rsd_ptr.cpu, RENDERER_STATE, cfg) {
       pan_shader_prepare_rsd(&shader_info, shader, &cfg);
@@ -1828,16 +1826,14 @@
 static void
 panvk_meta_fill_buf_init(struct panvk_physical_device *dev)
 {
-   dev->meta.copy.fillbuf.rsd =
-      panvk_meta_fill_buf_emit_rsd(&dev->pdev, &dev->meta.bin_pool.base,
-                                   &dev->meta.desc_pool.base);
+   dev->meta.copy.fillbuf.rsd = panvk_meta_fill_buf_emit_rsd(
+      &dev->pdev, &dev->meta.bin_pool.base, &dev->meta.desc_pool.base);
 }
 
 static void
 panvk_meta_fill_buf(struct panvk_cmd_buffer *cmdbuf,
-                    const struct panvk_buffer *dst,
-                    VkDeviceSize size, VkDeviceSize offset,
-                    uint32_t val)
+                    const struct panvk_buffer *dst, VkDeviceSize size,
+                    VkDeviceSize offset, uint32_t val)
 {
    struct panvk_meta_fill_buf_info info = {
       .start = panvk_buffer_gpu_ptr(dst, offset),
@@ -1858,8 +1854,7 @@
    assert(!(offset & 3) && !(size & 3));
 
    unsigned nwords = size / sizeof(uint32_t);
-   mali_ptr rsd =
-      cmdbuf->device->physical_device->meta.copy.fillbuf.rsd;
+   mali_ptr rsd = cmdbuf->device->physical_device->meta.copy.fillbuf.rsd;
 
    mali_ptr pushconsts =
       pan_pool_upload_aligned(&cmdbuf->desc_pool.base, &info, sizeof(info), 16);
@@ -1872,13 +1867,11 @@
 
    mali_ptr tsd = batch->tls.gpu;
 
-   struct pan_compute_dim num_wg = { nwords, 1, 1 };
-   struct pan_compute_dim wg_sz = { 1, 1, 1};
-   struct panfrost_ptr job =
-     panvk_meta_copy_emit_compute_job(&cmdbuf->desc_pool.base,
-                                      &batch->scoreboard,
-                                      &num_wg, &wg_sz,
-                                      0, 0, pushconsts, rsd, tsd);
+   struct pan_compute_dim num_wg = {nwords, 1, 1};
+   struct pan_compute_dim wg_sz = {1, 1, 1};
+   struct panfrost_ptr job = panvk_meta_copy_emit_compute_job(
+      &cmdbuf->desc_pool.base, &batch->scoreboard, &num_wg, &wg_sz, 0, 0,
+      pushconsts, rsd, tsd);
 
    util_dynarray_append(&batch->jobs, void *, job.cpu);
 
@@ -1887,10 +1880,8 @@
 }
 
 void
-panvk_per_arch(CmdFillBuffer)(VkCommandBuffer commandBuffer,
-                              VkBuffer dstBuffer,
-                              VkDeviceSize dstOffset,
-                              VkDeviceSize fillSize,
+panvk_per_arch(CmdFillBuffer)(VkCommandBuffer commandBuffer, VkBuffer dstBuffer,
+                              VkDeviceSize dstOffset, VkDeviceSize fillSize,
                               uint32_t data)
 {
    VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
@@ -1926,13 +1917,11 @@
    mali_ptr tsd = batch->tls.gpu;
 
    unsigned nblocks = size >> log2blksz;
-   struct pan_compute_dim num_wg = { nblocks, 1, 1 };
-   struct pan_compute_dim wg_sz = { 1, 1, 1};
-   struct panfrost_ptr job =
-     panvk_meta_copy_emit_compute_job(&cmdbuf->desc_pool.base,
-                                      &batch->scoreboard,
-                                      &num_wg, &wg_sz,
-                                      0, 0, pushconsts, rsd, tsd);
+   struct pan_compute_dim num_wg = {nblocks, 1, 1};
+   struct pan_compute_dim wg_sz = {1, 1, 1};
+   struct panfrost_ptr job = panvk_meta_copy_emit_compute_job(
+      &cmdbuf->desc_pool.base, &batch->scoreboard, &num_wg, &wg_sz, 0, 0,
+      pushconsts, rsd, tsd);
 
    util_dynarray_append(&batch->jobs, void *, job.cpu);
 
@@ -1942,10 +1931,8 @@
 
 void
 panvk_per_arch(CmdUpdateBuffer)(VkCommandBuffer commandBuffer,
-                                VkBuffer dstBuffer,
-                                VkDeviceSize dstOffset,
-                                VkDeviceSize dataSize,
-                                const void *pData)
+                                VkBuffer dstBuffer, VkDeviceSize dstOffset,
+                                VkDeviceSize dataSize, const void *pData)
 {
    VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
    VK_FROM_HANDLE(panvk_buffer, dst, dstBuffer);
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_vX_meta.h mesa/src/panfrost/vulkan/panvk_vX_meta.h
--- mesa-23.0.0/src/panfrost/vulkan/panvk_vX_meta.h	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_vX_meta.h	2023-03-06 19:19:32.551307801 +0100
@@ -29,25 +29,18 @@
 #error "no arch"
 #endif
 
-void
-panvk_per_arch(meta_init)(struct panvk_physical_device *dev);
+void panvk_per_arch(meta_init)(struct panvk_physical_device *dev);
 
-void
-panvk_per_arch(meta_cleanup)(struct panvk_physical_device *dev);
+void panvk_per_arch(meta_cleanup)(struct panvk_physical_device *dev);
 
-mali_ptr
-panvk_per_arch(meta_emit_viewport)(struct pan_pool *pool,
-                                   uint16_t minx, uint16_t miny,
-                                   uint16_t maxx, uint16_t maxy);
+mali_ptr panvk_per_arch(meta_emit_viewport)(struct pan_pool *pool,
+                                            uint16_t minx, uint16_t miny,
+                                            uint16_t maxx, uint16_t maxy);
 
-void
-panvk_per_arch(meta_clear_init)(struct panvk_physical_device *dev);
+void panvk_per_arch(meta_clear_init)(struct panvk_physical_device *dev);
 
-void
-panvk_per_arch(meta_blit_init)(struct panvk_physical_device *dev);
+void panvk_per_arch(meta_blit_init)(struct panvk_physical_device *dev);
 
-void
-panvk_per_arch(meta_blit_cleanup)(struct panvk_physical_device *dev);
+void panvk_per_arch(meta_blit_cleanup)(struct panvk_physical_device *dev);
 
-void
-panvk_per_arch(meta_copy_init)(struct panvk_physical_device *dev);
+void panvk_per_arch(meta_copy_init)(struct panvk_physical_device *dev);
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_vX_nir_lower_descriptors.c mesa/src/panfrost/vulkan/panvk_vX_nir_lower_descriptors.c
--- mesa-23.0.0/src/panfrost/vulkan/panvk_vX_nir_lower_descriptors.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_vX_nir_lower_descriptors.c	2023-03-06 19:19:32.619308251 +0100
@@ -125,33 +125,30 @@
          panvk_pipeline_layout_ubo_start(ctx->layout, set, false) +
          set_layout->desc_ubo_index;
 
-      const uint32_t packed = (bind_layout->desc_ubo_stride << 16 ) |
-                              set_ubo_idx;
+      const uint32_t packed =
+         (bind_layout->desc_ubo_stride << 16) | set_ubo_idx;
 
       return nir_vec4(b, nir_imm_int(b, packed),
-                         nir_imm_int(b, bind_layout->desc_ubo_offset),
-                         nir_imm_int(b, array_size - 1),
-                         array_index);
+                      nir_imm_int(b, bind_layout->desc_ubo_offset),
+                      nir_imm_int(b, array_size - 1), array_index);
    }
 
    case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER_DYNAMIC: {
       assert(addr_format == nir_address_format_64bit_bounded_global ||
              addr_format == nir_address_format_64bit_global_32bit_offset);
 
-      const unsigned dyn_ssbo_idx = ctx->layout->sets[set].dyn_ssbo_offset +
-                                    bind_layout->dyn_ssbo_idx;
+      const unsigned dyn_ssbo_idx =
+         ctx->layout->sets[set].dyn_ssbo_offset + bind_layout->dyn_ssbo_idx;
 
       const unsigned ubo_idx = PANVK_SYSVAL_UBO_INDEX;
       const unsigned desc_stride = sizeof(struct panvk_ssbo_addr);
-      const uint32_t ubo_offset = offsetof(struct panvk_sysvals, dyn_ssbos) +
-                                  dyn_ssbo_idx * desc_stride;
+      const uint32_t ubo_offset =
+         offsetof(struct panvk_sysvals, dyn_ssbos) + dyn_ssbo_idx * desc_stride;
 
       const uint32_t packed = (desc_stride << 16) | ubo_idx;
 
-      return nir_vec4(b, nir_imm_int(b, packed),
-                         nir_imm_int(b, ubo_offset),
-                         nir_imm_int(b, array_size - 1),
-                         array_index);
+      return nir_vec4(b, nir_imm_int(b, packed), nir_imm_int(b, ubo_offset),
+                      nir_imm_int(b, array_size - 1), array_index);
    }
 
    default:
@@ -174,14 +171,13 @@
    switch (addr_format) {
    case nir_address_format_32bit_index_offset:
       return nir_vec2(b, nir_channel(b, orig, 0),
-                         nir_iadd(b, nir_channel(b, orig, 1), delta));
+                      nir_iadd(b, nir_channel(b, orig, 1), delta));
 
    case nir_address_format_64bit_bounded_global:
    case nir_address_format_64bit_global_32bit_offset:
-      return nir_vec4(b, nir_channel(b, orig, 0),
-                         nir_channel(b, orig, 1),
-                         nir_channel(b, orig, 2),
-                         nir_iadd(b, nir_channel(b, orig, 3), delta));
+      return nir_vec4(b, nir_channel(b, orig, 0), nir_channel(b, orig, 1),
+                      nir_channel(b, orig, 2),
+                      nir_iadd(b, nir_channel(b, orig, 3), delta));
 
    default:
       unreachable("Unhandled address format");
@@ -196,8 +192,7 @@
  * See build_res_index for details about each resource index format.
  */
 static nir_ssa_def *
-build_buffer_addr_for_res_index(nir_builder *b,
-                                nir_ssa_def *res_index,
+build_buffer_addr_for_res_index(nir_builder *b, nir_ssa_def *res_index,
                                 nir_address_format addr_format,
                                 const struct apply_descriptors_ctx *ctx)
 {
@@ -205,14 +200,15 @@
    case nir_address_format_32bit_index_offset: {
       nir_ssa_def *packed = nir_channel(b, res_index, 0);
       nir_ssa_def *array_index = nir_channel(b, res_index, 1);
-      nir_ssa_def *surface_index = nir_extract_u16(b, packed, nir_imm_int(b, 0));
+      nir_ssa_def *surface_index =
+         nir_extract_u16(b, packed, nir_imm_int(b, 0));
       nir_ssa_def *array_max = nir_extract_u16(b, packed, nir_imm_int(b, 1));
 
       if (ctx->add_bounds_checks)
          array_index = nir_umin(b, array_index, array_max);
 
       return nir_vec2(b, nir_iadd(b, surface_index, array_index),
-                         nir_imm_int(b, 0));
+                      nir_imm_int(b, 0));
    }
 
    case nir_address_format_64bit_bounded_global:
@@ -223,26 +219,24 @@
       nir_ssa_def *array_index = nir_channel(b, res_index, 3);
 
       nir_ssa_def *desc_ubo_idx = nir_extract_u16(b, packed, nir_imm_int(b, 0));
-      nir_ssa_def *desc_ubo_stride = nir_extract_u16(b, packed, nir_imm_int(b, 1));
+      nir_ssa_def *desc_ubo_stride =
+         nir_extract_u16(b, packed, nir_imm_int(b, 1));
 
       if (ctx->add_bounds_checks)
          array_index = nir_umin(b, array_index, array_max);
 
       desc_ubo_offset = nir_iadd(b, desc_ubo_offset,
-                                    nir_imul(b, array_index, desc_ubo_stride));
+                                 nir_imul(b, array_index, desc_ubo_stride));
 
-      nir_ssa_def *desc = nir_load_ubo(b, 4, 32, desc_ubo_idx,
-                                       desc_ubo_offset,
-                                       .align_mul=16, .range=~0);
+      nir_ssa_def *desc = nir_load_ubo(b, 4, 32, desc_ubo_idx, desc_ubo_offset,
+                                       .align_mul = 16, .range = ~0);
 
       /* The offset in the descriptor is guaranteed to be zero when it's
        * written into the descriptor set.  This lets us avoid some unnecessary
        * adds.
        */
-      return nir_vec4(b, nir_channel(b, desc, 0),
-                         nir_channel(b, desc, 1),
-                         nir_channel(b, desc, 2),
-                         nir_imm_int(b, 0));
+      return nir_vec4(b, nir_channel(b, desc, 0), nir_channel(b, desc, 1),
+                      nir_channel(b, desc, 2), nir_imm_int(b, 0));
    }
 
    default:
@@ -250,7 +244,6 @@
    }
 }
 
-
 static bool
 lower_res_intrinsic(nir_builder *b, nir_intrinsic_instr *intrin,
                     const struct apply_descriptors_ctx *ctx)
@@ -265,22 +258,20 @@
    case nir_intrinsic_vulkan_resource_index:
       assert(intrin->src[0].is_ssa);
       res = build_res_index(b, nir_intrinsic_desc_set(intrin),
-                               nir_intrinsic_binding(intrin),
-                               intrin->src[0].ssa,
-                               addr_format, ctx);
+                            nir_intrinsic_binding(intrin), intrin->src[0].ssa,
+                            addr_format, ctx);
       break;
 
    case nir_intrinsic_vulkan_resource_reindex:
       assert(intrin->src[0].is_ssa && intrin->src[1].is_ssa);
-      res = build_res_reindex(b, intrin->src[0].ssa,
-                                 intrin->src[1].ssa,
-                                 addr_format);
+      res = build_res_reindex(b, intrin->src[0].ssa, intrin->src[1].ssa,
+                              addr_format);
       break;
 
    case nir_intrinsic_load_vulkan_descriptor:
       assert(intrin->src[0].is_ssa);
-      res = build_buffer_addr_for_res_index(b, intrin->src[0].ssa,
-                                               addr_format, ctx);
+      res = build_buffer_addr_for_res_index(b, intrin->src[0].ssa, addr_format,
+                                            ctx);
       break;
 
    default:
@@ -306,8 +297,8 @@
       addr_format_for_desc_type(VK_DESCRIPTOR_TYPE_STORAGE_BUFFER, ctx);
 
    assert(intrin->src[0].is_ssa);
-   nir_ssa_def *desc = build_buffer_addr_for_res_index(b, intrin->src[0].ssa,
-                                                          addr_format, ctx);
+   nir_ssa_def *desc =
+      build_buffer_addr_for_res_index(b, intrin->src[0].ssa, addr_format, ctx);
 
    switch (addr_format) {
    case nir_address_format_64bit_bounded_global:
@@ -326,9 +317,9 @@
 }
 
 static void
-get_resource_deref_binding(nir_deref_instr *deref,
-                           uint32_t *set, uint32_t *binding,
-                           uint32_t *index_imm, nir_ssa_def **index_ssa)
+get_resource_deref_binding(nir_deref_instr *deref, uint32_t *set,
+                           uint32_t *binding, uint32_t *index_imm,
+                           nir_ssa_def **index_ssa)
 {
    *index_imm = 0;
    *index_ssa = NULL;
@@ -352,14 +343,13 @@
 
 static nir_ssa_def *
 load_resource_deref_desc(nir_builder *b, nir_deref_instr *deref,
-                         unsigned desc_offset,
-                         unsigned num_components, unsigned bit_size,
+                         unsigned desc_offset, unsigned num_components,
+                         unsigned bit_size,
                          const struct apply_descriptors_ctx *ctx)
 {
    uint32_t set, binding, index_imm;
    nir_ssa_def *index_ssa;
-   get_resource_deref_binding(deref, &set, &binding,
-                              &index_imm, &index_ssa);
+   get_resource_deref_binding(deref, &set, &binding, &index_imm, &index_ssa);
 
    const struct panvk_descriptor_set_layout *set_layout =
       get_set_layout(set, ctx);
@@ -375,20 +365,16 @@
       set_layout->desc_ubo_index;
 
    nir_ssa_def *desc_ubo_offset =
-      nir_iadd_imm(b, nir_imul_imm(b, index_ssa,
-                                      bind_layout->desc_ubo_stride),
-                      bind_layout->desc_ubo_offset + desc_offset);
+      nir_iadd_imm(b, nir_imul_imm(b, index_ssa, bind_layout->desc_ubo_stride),
+                   bind_layout->desc_ubo_offset + desc_offset);
 
    assert(bind_layout->desc_ubo_stride > 0);
    unsigned desc_align = (1 << (ffs(bind_layout->desc_ubo_stride) - 1));
    desc_align = MIN2(desc_align, 16);
 
-   return nir_load_ubo(b, num_components, bit_size,
-                       nir_imm_int(b, set_ubo_idx),
-                       desc_ubo_offset,
-                       .align_mul=desc_align,
-                       .align_offset=(desc_offset % desc_align),
-                       .range=~0);
+   return nir_load_ubo(b, num_components, bit_size, nir_imm_int(b, set_ubo_idx),
+                       desc_ubo_offset, .align_mul = desc_align,
+                       .align_offset = (desc_offset % desc_align), .range = ~0);
 }
 
 static nir_ssa_def *
@@ -436,8 +422,7 @@
 
    b->cursor = nir_before_instr(&tex->instr);
 
-   if (tex->op == nir_texop_txs ||
-       tex->op == nir_texop_query_levels ||
+   if (tex->op == nir_texop_txs || tex->op == nir_texop_query_levels ||
        tex->op == nir_texop_texture_samples) {
       int tex_src_idx = nir_tex_instr_src_index(tex, nir_tex_src_texture_deref);
       assert(tex_src_idx >= 0);
@@ -468,15 +453,15 @@
       return true;
    }
 
-   int sampler_src_idx = nir_tex_instr_src_index(tex, nir_tex_src_sampler_deref);
+   int sampler_src_idx =
+      nir_tex_instr_src_index(tex, nir_tex_src_sampler_deref);
    if (sampler_src_idx >= 0) {
       nir_deref_instr *deref = nir_src_as_deref(tex->src[sampler_src_idx].src);
       nir_tex_instr_remove_src(tex, sampler_src_idx);
 
       uint32_t set, binding, index_imm;
       nir_ssa_def *index_ssa;
-      get_resource_deref_binding(deref, &set, &binding,
-                                 &index_imm, &index_ssa);
+      get_resource_deref_binding(deref, &set, &binding, &index_imm, &index_ssa);
 
       const struct panvk_descriptor_set_binding_layout *bind_layout =
          get_binding_layout(set, binding, ctx);
@@ -498,14 +483,13 @@
 
       uint32_t set, binding, index_imm;
       nir_ssa_def *index_ssa;
-      get_resource_deref_binding(deref, &set, &binding,
-                                 &index_imm, &index_ssa);
+      get_resource_deref_binding(deref, &set, &binding, &index_imm, &index_ssa);
 
       const struct panvk_descriptor_set_binding_layout *bind_layout =
          get_binding_layout(set, binding, ctx);
 
-      tex->texture_index = ctx->layout->sets[set].tex_offset +
-                           bind_layout->tex_idx + index_imm;
+      tex->texture_index =
+         ctx->layout->sets[set].tex_offset + bind_layout->tex_idx + index_imm;
 
       if (index_ssa != NULL) {
          nir_tex_instr_add_src(tex, nir_tex_src_texture_offset,
@@ -531,8 +515,8 @@
           bind_layout->type == VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER ||
           bind_layout->type == VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER);
 
-   unsigned img_offset = ctx->layout->sets[set].img_offset +
-                         bind_layout->img_idx;
+   unsigned img_offset =
+      ctx->layout->sets[set].img_offset + bind_layout->img_idx;
 
    if (index_ssa == NULL) {
       return nir_imm_int(b, img_offset + index_imm);
@@ -608,13 +592,10 @@
    default:
       return false;
    }
-
 }
 
 static bool
-lower_descriptors_instr(nir_builder *b,
-                        nir_instr *instr,
-                        void *data)
+lower_descriptors_instr(nir_builder *b, nir_instr *instr, void *data)
 {
    struct apply_descriptors_ctx *ctx = data;
 
@@ -629,8 +610,7 @@
 }
 
 bool
-panvk_per_arch(nir_lower_descriptors)(nir_shader *nir,
-                                      struct panvk_device *dev,
+panvk_per_arch(nir_lower_descriptors)(nir_shader *nir, struct panvk_device *dev,
                                       const struct panvk_pipeline_layout *layout,
                                       bool *has_img_access_out)
 {
@@ -638,15 +618,14 @@
       .layout = layout,
       .desc_addr_format = nir_address_format_32bit_index_offset,
       .ubo_addr_format = nir_address_format_32bit_index_offset,
-      .ssbo_addr_format = dev->vk.enabled_features.robustBufferAccess ?
-                          nir_address_format_64bit_bounded_global :
-                          nir_address_format_64bit_global_32bit_offset,
+      .ssbo_addr_format = dev->vk.enabled_features.robustBufferAccess
+                             ? nir_address_format_64bit_bounded_global
+                             : nir_address_format_64bit_global_32bit_offset,
    };
 
-   bool progress = nir_shader_instructions_pass(nir, lower_descriptors_instr,
-                                                nir_metadata_block_index |
-                                                nir_metadata_dominance,
-                                                (void *)&ctx);
+   bool progress = nir_shader_instructions_pass(
+      nir, lower_descriptors_instr,
+      nir_metadata_block_index | nir_metadata_dominance, (void *)&ctx);
    if (has_img_access_out)
       *has_img_access_out = ctx.has_img_access;
 
diff -urN mesa-23.0.0/src/panfrost/vulkan/panvk_vX_pipeline.c mesa/src/panfrost/vulkan/panvk_vX_pipeline.c
--- mesa-23.0.0/src/panfrost/vulkan/panvk_vX_pipeline.c	2023-02-23 03:36:49.000000000 +0100
+++ mesa/src/panfrost/vulkan/panvk_vX_pipeline.c	2023-03-06 19:19:32.631308329 +0100
@@ -34,17 +34,15 @@
 #include "nir/nir.h"
 #include "nir/nir_builder.h"
 #include "spirv/nir_spirv.h"
-#include "util/u_debug.h"
 #include "util/mesa-sha1.h"
 #include "util/u_atomic.h"
+#include "util/u_debug.h"
 #include "vk_format.h"
 #include "vk_util.h"
 
 #include "panfrost/util/pan_lower_framebuffer.h"
 
-
-struct panvk_pipeline_builder
-{
+struct panvk_pipeline_builder {
    struct panvk_device *device;
    struct panvk_pipeline_cache *cache;
    const VkAllocationCallbacks *alloc;
@@ -78,9 +76,8 @@
 {
    struct panvk_device *dev = builder->device;
 
-   struct panvk_pipeline *pipeline =
-      vk_object_zalloc(&dev->vk, builder->alloc,
-                       sizeof(*pipeline), VK_OBJECT_TYPE_PIPELINE);
+   struct panvk_pipeline *pipeline = vk_object_zalloc(
+      &dev->vk, builder->alloc, sizeof(*pipeline), VK_OBJECT_TYPE_PIPELINE);
    if (!pipeline)
       return VK_ERROR_OUT_OF_HOST_MEMORY;
 
@@ -95,7 +92,8 @@
    for (uint32_t i = 0; i < MESA_SHADER_STAGES; i++) {
       if (!builder->shaders[i])
          continue;
-      panvk_shader_destroy(builder->device, builder->shaders[i], builder->alloc);
+      panvk_shader_destroy(builder->device, builder->shaders[i],
+                           builder->alloc);
    }
 }
 
@@ -110,12 +108,10 @@
                                        struct panvk_pipeline *pipeline)
 {
    const VkPipelineShaderStageCreateInfo *stage_infos[MESA_SHADER_STAGES] = {
-      NULL
-   };
+      NULL};
    const VkPipelineShaderStageCreateInfo *stages =
-      builder->create_info.gfx ?
-      builder->create_info.gfx->pStages :
-      &builder->create_info.compute->stage;
+      builder->create_info.gfx ? builder->create_info.gfx->pStages
+                               : &builder->create_info.compute->stage;
    unsigned stage_count =
       builder->create_info.gfx ? builder->create_info.gfx->stageCount : 1;
 
@@ -133,16 +129,15 @@
 
       struct panvk_shader *shader;
 
-      shader = panvk_per_arch(shader_create)(builder->device, stage, stage_info,
-                                             builder->layout,
-                                             PANVK_SYSVAL_UBO_INDEX,
-                                             &pipeline->blend.state,
-                                             panvk_pipeline_static_state(pipeline,
-                                                                         VK_DYNAMIC_STATE_BLEND_CONSTANTS),
-                                             builder->alloc);
+      shader = panvk_per_arch(shader_create)(
+         builder->device, stage, stage_info, builder->layout,
+         PANVK_SYSVAL_UBO_INDEX, &pipeline->blend.state,
+         panvk_pipeline_static_state(pipeline,
+                                     VK_DYNAMIC_STATE_BLEND_CONSTANTS),
+         builder->alloc);
       if (!shader)
          return VK_ERROR_OUT_OF_HOST_MEMORY;
- 
+
       builder->shaders[stage] = shader;
       builder->shader_total_size = ALIGN_POT(builder->shader_total_size, 128);
       builder->stages[stage].shader_offset = builder->shader_total_size;
@@ -165,8 +160,7 @@
 
    struct panfrost_bo *bin_bo =
       panfrost_bo_create(&builder->device->physical_device->pdev,
-                         builder->shader_total_size, PAN_BO_EXECUTE,
-                         "Shader");
+                         builder->shader_total_size, PAN_BO_EXECUTE, "Shader");
 
    pipeline->binary_bo = bin_bo;
    panfrost_bo_mmap(bin_bo);
@@ -185,8 +179,7 @@
 }
 
 static bool
-panvk_pipeline_static_sysval(struct panvk_pipeline *pipeline,
-                             unsigned id)
+panvk_pipeline_static_sysval(struct panvk_pipeline *pipeline, unsigned id)
 {
    switch (id) {
    case PAN_SYSVAL_VIEWPORT_SCALE:
@@ -198,11 +191,10 @@
 }
 
 static void
-panvk_pipeline_builder_alloc_static_state_bo(struct panvk_pipeline_builder *builder,
-                                             struct panvk_pipeline *pipeline)
+panvk_pipeline_builder_alloc_static_state_bo(
+   struct panvk_pipeline_builder *builder, struct panvk_pipeline *pipeline)
 {
-   struct panfrost_device *pdev =
-      &builder->device->physical_device->pdev;
+   struct panfrost_device *pdev = &builder->device->physical_device->pdev;
    unsigned bo_size = 0;
 
    for (uint32_t i = 0; i < MESA_SHADER_STAGES; i++) {
@@ -277,15 +269,18 @@
 
       /* Handle empty shaders gracefully */
       if (util_dynarray_num_elements(&builder->shaders[i]->binary, uint8_t)) {
-         shader_ptr = pipeline->binary_bo->ptr.gpu +
-                      builder->stages[i].shader_offset;
+         shader_ptr =
+            pipeline->binary_bo->ptr.gpu + builder->stages[i].shader_offset;
       }
 
       if (i != MESA_SHADER_FRAGMENT) {
-         void *rsd = pipeline->state_bo->ptr.cpu + builder->stages[i].rsd_offset;
-         mali_ptr gpu_rsd = pipeline->state_bo->ptr.gpu + builder->stages[i].rsd_offset;
+         void *rsd =
+            pipeline->state_bo->ptr.cpu + builder->stages[i].rsd_offset;
+         mali_ptr gpu_rsd =
+            pipeline->state_bo->ptr.gpu + builder->stages[i].rsd_offset;
 
-         panvk_per_arch(emit_non_fs_rsd)(builder->device, &shader->info, shader_ptr, rsd);
+         panvk_per_arch(emit_non_fs_rsd)(builder->device, &shader->info,
+                                         shader_ptr, rsd);
          pipeline->rsds[i] = gpu_rsd;
       }
 
@@ -296,8 +291,10 @@
    }
 
    if (builder->create_info.gfx && !pipeline->fs.dynamic_rsd) {
-      void *rsd = pipeline->state_bo->ptr.cpu + builder->stages[MESA_SHADER_FRAGMENT].rsd_offset;
-      mali_ptr gpu_rsd = pipeline->state_bo->ptr.gpu + builder->stages[MESA_SHADER_FRAGMENT].rsd_offset;
+      void *rsd = pipeline->state_bo->ptr.cpu +
+                  builder->stages[MESA_SHADER_FRAGMENT].rsd_offset;
+      mali_ptr gpu_rsd = pipeline->state_bo->ptr.gpu +
+                         builder->stages[MESA_SHADER_FRAGMENT].rsd_offset;
       void *bd = rsd + pan_size(RENDERER_STATE);
 
       panvk_per_arch(emit_base_fs_rsd)(builder->device, pipeline, rsd);
@@ -308,19 +305,19 @@
 
       pipeline->rsds[MESA_SHADER_FRAGMENT] = gpu_rsd;
    } else if (builder->create_info.gfx) {
-      panvk_per_arch(emit_base_fs_rsd)(builder->device, pipeline, &pipeline->fs.rsd_template);
-      for (unsigned rt = 0; rt < MAX2(pipeline->blend.state.rt_count, 1); rt++) {
+      panvk_per_arch(emit_base_fs_rsd)(builder->device, pipeline,
+                                       &pipeline->fs.rsd_template);
+      for (unsigned rt = 0; rt < MAX2(pipeline->blend.state.rt_count, 1);
+           rt++) {
          panvk_per_arch(emit_blend)(builder->device, pipeline, rt,
                                     &pipeline->blend.bd_template[rt]);
       }
    }
 
-   pipeline->num_ubos = PANVK_NUM_BUILTIN_UBOS +
-                        builder->layout->num_ubos +
+   pipeline->num_ubos = PANVK_NUM_BUILTIN_UBOS + builder->layout->num_ubos +
                         builder->layout->num_dyn_ubos;
 }
 
-
 static void
 panvk_pipeline_builder_parse_viewport(struct panvk_pipeline_builder *builder,
                                       struct panvk_pipeline *pipeline)
@@ -335,17 +332,18 @@
        panvk_pipeline_static_state(pipeline, VK_DYNAMIC_STATE_VIEWPORT) &&
        panvk_pipeline_static_state(pipeline, VK_DYNAMIC_STATE_SCISSOR)) {
       void *vpd = pipeline->state_bo->ptr.cpu + builder->vpd_offset;
-      panvk_per_arch(emit_viewport)(builder->create_info.gfx->pViewportState->pViewports,
-                                    builder->create_info.gfx->pViewportState->pScissors,
-                                    vpd);
-      pipeline->vpd = pipeline->state_bo->ptr.gpu +
-                      builder->vpd_offset;
+      panvk_per_arch(emit_viewport)(
+         builder->create_info.gfx->pViewportState->pViewports,
+         builder->create_info.gfx->pViewportState->pScissors, vpd);
+      pipeline->vpd = pipeline->state_bo->ptr.gpu + builder->vpd_offset;
    }
    if (panvk_pipeline_static_state(pipeline, VK_DYNAMIC_STATE_VIEWPORT))
-      pipeline->viewport = builder->create_info.gfx->pViewportState->pViewports[0];
+      pipeline->viewport =
+         builder->create_info.gfx->pViewportState->pViewports[0];
 
    if (panvk_pipeline_static_state(pipeline, VK_DYNAMIC_STATE_SCISSOR))
-      pipeline->scissor = builder->create_info.gfx->pViewportState->pScissors[0];
+      pipeline->scissor =
+         builder->create_info.gfx->pViewportState->pScissors[0];
 }
 
 static void
@@ -368,7 +366,6 @@
          unreachable("unsupported dynamic state");
       }
    }
-
 }
 
 static enum mali_draw_mode
@@ -398,36 +395,53 @@
 }
 
 static void
-panvk_pipeline_builder_parse_input_assembly(struct panvk_pipeline_builder *builder,
-                                            struct panvk_pipeline *pipeline)
+panvk_pipeline_builder_parse_input_assembly(
+   struct panvk_pipeline_builder *builder, struct panvk_pipeline *pipeline)
 {
    pipeline->ia.primitive_restart =
       builder->create_info.gfx->pInputAssemblyState->primitiveRestartEnable;
-   pipeline->ia.topology =
-      translate_prim_topology(builder->create_info.gfx->pInputAssemblyState->topology);
+   pipeline->ia.topology = translate_prim_topology(
+      builder->create_info.gfx->pInputAssemblyState->topology);
 }
 
 static enum pipe_logicop
 translate_logicop(VkLogicOp in)
 {
    switch (in) {
-   case VK_LOGIC_OP_CLEAR: return PIPE_LOGICOP_CLEAR;
-   case VK_LOGIC_OP_AND: return PIPE_LOGICOP_AND;
-   case VK_LOGIC_OP_AND_REVERSE: return PIPE_LOGICOP_AND_REVERSE;
-   case VK_LOGIC_OP_COPY: return PIPE_LOGICOP_COPY;
-   case VK_LOGIC_OP_AND_INVERTED: return PIPE_LOGICOP_AND_INVERTED;
-   case VK_LOGIC_OP_NO_OP: return PIPE_LOGICOP_NOOP;
-   case VK_LOGIC_OP_XOR: return PIPE_LOGICOP_XOR;
-   case VK_LOGIC_OP_OR: return PIPE_LOGICOP_OR;
-   case VK_LOGIC_OP_NOR: return PIPE_LOGICOP_NOR;
-   case VK_LOGIC_OP_EQUIVALENT: return PIPE_LOGICOP_EQUIV;
-   case VK_LOGIC_OP_INVERT: return PIPE_LOGICOP_INVERT;
-   case VK_LOGIC_OP_OR_REVERSE: return PIPE_LOGICOP_OR_REVERSE;
-   case VK_LOGIC_OP_COPY_INVERTED: return PIPE_LOGICOP_COPY_INVERTED;
-   case VK_LOGIC_OP_OR_INVERTED: return PIPE_LOGICOP_OR_INVERTED;
-   case VK_LOGIC_OP_NAND: return PIPE_LOGICOP_NAND;
-   case VK_LOGIC_OP_SET: return PIPE_LOGICOP_SET;
-   default: unreachable("Invalid logicop");
+   case VK_LOGIC_OP_CLEAR:
+      return PIPE_LOGICOP_CLEAR;
+   case VK_LOGIC_OP_AND:
+      return PIPE_LOGICOP_AND;
+   case VK_LOGIC_OP_AND_REVERSE:
+      return PIPE_LOGICOP_AND_REVERSE;
+   case VK_LOGIC_OP_COPY:
+      return PIPE_LOGICOP_COPY;
+   case VK_LOGIC_OP_AND_INVERTED:
+      return PIPE_LOGICOP_AND_INVERTED;
+   case VK_LOGIC_OP_NO_OP:
+      return PIPE_LOGICOP_NOOP;
+   case VK_LOGIC_OP_XOR:
+      return PIPE_LOGICOP_XOR;
+   case VK_LOGIC_OP_OR:
+      return PIPE_LOGICOP_OR;
+   case VK_LOGIC_OP_NOR:
+      return PIPE_LOGICOP_NOR;
+   case VK_LOGIC_OP_EQUIVALENT:
+      return PIPE_LOGICOP_EQUIV;
+   case VK_LOGIC_OP_INVERT:
+      return PIPE_LOGICOP_INVERT;
+   case VK_LOGIC_OP_OR_REVERSE:
+      return PIPE_LOGICOP_OR_REVERSE;
+   case VK_LOGIC_OP_COPY_INVERTED:
+      return PIPE_LOGICOP_COPY_INVERTED;
+   case VK_LOGIC_OP_OR_INVERTED:
+      return PIPE_LOGICOP_OR_INVERTED;
+   case VK_LOGIC_OP_NAND:
+      return PIPE_LOGICOP_NAND;
+   case VK_LOGIC_OP_SET:
+      return PIPE_LOGICOP_SET;
+   default:
+      unreachable("Invalid logicop");
    }
 }
 
@@ -435,12 +449,18 @@
 translate_blend_op(VkBlendOp in)
 {
    switch (in) {
-   case VK_BLEND_OP_ADD: return BLEND_FUNC_ADD;
-   case VK_BLEND_OP_SUBTRACT: return BLEND_FUNC_SUBTRACT;
-   case VK_BLEND_OP_REVERSE_SUBTRACT: return BLEND_FUNC_REVERSE_SUBTRACT;
-   case VK_BLEND_OP_MIN: return BLEND_FUNC_MIN;
-   case VK_BLEND_OP_MAX: return BLEND_FUNC_MAX;
-   default: unreachable("Invalid blend op");
+   case VK_BLEND_OP_ADD:
+      return BLEND_FUNC_ADD;
+   case VK_BLEND_OP_SUBTRACT:
+      return BLEND_FUNC_SUBTRACT;
+   case VK_BLEND_OP_REVERSE_SUBTRACT:
+      return BLEND_FUNC_REVERSE_SUBTRACT;
+   case VK_BLEND_OP_MIN:
+      return BLEND_FUNC_MIN;
+   case VK_BLEND_OP_MAX:
+      return BLEND_FUNC_MAX;
+   default:
+      unreachable("Invalid blend op");
    }
 }
 
@@ -477,7 +497,8 @@
       return BLEND_FACTOR_SRC1_ALPHA;
    case VK_BLEND_FACTOR_SRC_ALPHA_SATURATE:
       return BLEND_FACTOR_SRC_ALPHA_SATURATE;
-   default: unreachable("Invalid blend factor");
+   default:
+      unreachable("Invalid blend factor");
    }
 }
 
@@ -541,7 +562,8 @@
       builder->create_info.gfx->pColorBlendState->logicOpEnable;
    pipeline->blend.state.logicop_func =
       translate_logicop(builder->create_info.gfx->pColorBlendState->logicOp);
-   pipeline->blend.state.rt_count = util_last_bit(builder->active_color_attachments);
+   pipeline->blend.state.rt_count =
+      util_last_bit(builder->active_color_attachments);
    memcpy(pipeline->blend.state.constants,
           builder->create_info.gfx->pColorBlendState->blendConstants,
           sizeof(pipeline->blend.state.constants));
@@ -555,25 +577,35 @@
 
       bool dest_has_alpha = util_format_has_alpha(out->format);
 
-      out->nr_samples = builder->create_info.gfx->pMultisampleState->rasterizationSamples;
+      out->nr_samples =
+         builder->create_info.gfx->pMultisampleState->rasterizationSamples;
       out->equation.blend_enable = in->blendEnable;
       out->equation.color_mask = in->colorWriteMask;
       out->equation.rgb_func = translate_blend_op(in->colorBlendOp);
-      out->equation.rgb_src_factor = translate_blend_factor(in->srcColorBlendFactor, dest_has_alpha);
-      out->equation.rgb_invert_src_factor = inverted_blend_factor(in->srcColorBlendFactor, dest_has_alpha);
-      out->equation.rgb_dst_factor = translate_blend_factor(in->dstColorBlendFactor, dest_has_alpha);
-      out->equation.rgb_invert_dst_factor = inverted_blend_factor(in->dstColorBlendFactor, dest_has_alpha);
+      out->equation.rgb_src_factor =
+         translate_blend_factor(in->srcColorBlendFactor, dest_has_alpha);
+      out->equation.rgb_invert_src_factor =
+         inverted_blend_factor(in->srcColorBlendFactor, dest_has_alpha);
+      out->equation.rgb_dst_factor =
+         translate_blend_factor(in->dstColorBlendFactor, dest_has_alpha);
+      out->equation.rgb_invert_dst_factor =
+         inverted_blend_factor(in->dstColorBlendFactor, dest_has_alpha);
       out->equation.alpha_func = translate_blend_op(in->alphaBlendOp);
-      out->equation.alpha_src_factor = translate_blend_factor(in->srcAlphaBlendFactor, dest_has_alpha);
-      out->equation.alpha_invert_src_factor = inverted_blend_factor(in->srcAlphaBlendFactor, dest_has_alpha);
-      out->equation.alpha_dst_factor = translate_blend_factor(in->dstAlphaBlendFactor, dest_has_alpha);
-      out->equation.alpha_invert_dst_factor = inverted_blend_factor(in->dstAlphaBlendFactor, dest_has_alpha);
+      out->equation.alpha_src_factor =
+         translate_blend_factor(in->srcAlphaBlendFactor, dest_has_alpha);
+      out->equation.alpha_invert_src_factor =
+         inverted_blend_factor(in->srcAlphaBlendFactor, dest_has_alpha);
+      out->equation.alpha_dst_factor =
+         translate_blend_factor(in->dstAlphaBlendFactor, dest_has_alpha);
+      out->equation.alpha_invert_dst_factor =
+         inverted_blend_factor(in->dstAlphaBlendFactor, dest_has_alpha);
 
       pipeline->blend.reads_dest |= pan_blend_reads_dest(out->equation);
 
       unsigned constant_mask =
-         panvk_per_arch(blend_needs_lowering)(pdev, &pipeline->blend.state, i) ?
-         0 : pan_blend_constant_mask(out->equation);
+         panvk_per_arch(blend_needs_lowering)(pdev, &pipeline->blend.state, i)
+            ? 0
+            : pan_blend_constant_mask(out->equation);
       pipeline->blend.constant[i].index = ffs(constant_mask) - 1;
       if (constant_mask) {
          /* On Bifrost, the blend constant is expressed with a UNORM of the
@@ -588,8 +620,8 @@
          unsigned chan_size = 0;
          for (unsigned c = 0; c < format_desc->nr_channels; c++)
             chan_size = MAX2(format_desc->channel[c].size, chan_size);
-         pipeline->blend.constant[i].bifrost_factor =
-            ((1 << chan_size) - 1) << (16 - chan_size);
+         pipeline->blend.constant[i].bifrost_factor = ((1 << chan_size) - 1)
+                                                      << (16 - chan_size);
       }
    }
 }
@@ -598,31 +630,43 @@
 panvk_pipeline_builder_parse_multisample(struct panvk_pipeline_builder *builder,
                                          struct panvk_pipeline *pipeline)
 {
-   unsigned nr_samples =
-      MAX2(builder->create_info.gfx->pMultisampleState->rasterizationSamples, 1);
+   unsigned nr_samples = MAX2(
+      builder->create_info.gfx->pMultisampleState->rasterizationSamples, 1);
 
    pipeline->ms.rast_samples =
       builder->create_info.gfx->pMultisampleState->rasterizationSamples;
    pipeline->ms.sample_mask =
-      builder->create_info.gfx->pMultisampleState->pSampleMask ?
-      builder->create_info.gfx->pMultisampleState->pSampleMask[0] : UINT16_MAX;
+      builder->create_info.gfx->pMultisampleState->pSampleMask
+         ? builder->create_info.gfx->pMultisampleState->pSampleMask[0]
+         : UINT16_MAX;
    pipeline->ms.min_samples =
-      MAX2(builder->create_info.gfx->pMultisampleState->minSampleShading * nr_samples, 1);
+      MAX2(builder->create_info.gfx->pMultisampleState->minSampleShading *
+              nr_samples,
+           1);
 }
 
 static enum mali_stencil_op
 translate_stencil_op(VkStencilOp in)
 {
    switch (in) {
-   case VK_STENCIL_OP_KEEP: return MALI_STENCIL_OP_KEEP;
-   case VK_STENCIL_OP_ZERO: return MALI_STENCIL_OP_ZERO;
-   case VK_STENCIL_OP_REPLACE: return MALI_STENCIL_OP_REPLACE;
-   case VK_STENCIL_OP_INCREMENT_AND_CLAMP: return MALI_STENCIL_OP_INCR_SAT;
-   case VK_STENCIL_OP_DECREMENT_AND_CLAMP: return MALI_STENCIL_OP_DECR_SAT;
-   case VK_STENCIL_OP_INCREMENT_AND_WRAP: return MALI_STENCIL_OP_INCR_WRAP;
-   case VK_STENCIL_OP_DECREMENT_AND_WRAP: return MALI_STENCIL_OP_DECR_WRAP;
-   case VK_STENCIL_OP_INVERT: return MALI_STENCIL_OP_INVERT;
-   default: unreachable("Invalid stencil op");
+   case VK_STENCIL_OP_KEEP:
+      return MALI_STENCIL_OP_KEEP;
+   case VK_STENCIL_OP_ZERO:
+      return MALI_STENCIL_OP_ZERO;
+   case VK_STENCIL_OP_REPLACE:
+      return MALI_STENCIL_OP_REPLACE;
+   case VK_STENCIL_OP_INCREMENT_AND_CLAMP:
+      return MALI_STENCIL_OP_INCR_SAT;
+   case VK_STENCIL_OP_DECREMENT_AND_CLAMP:
+      return MALI_STENCIL_OP_DECR_SAT;
+   case VK_STENCIL_OP_INCREMENT_AND_WRAP:
+      return MALI_STENCIL_OP_INCR_WRAP;
+   case VK_STENCIL_OP_DECREMENT_AND_WRAP:
+      return MALI_STENCIL_OP_DECR_WRAP;
+   case VK_STENCIL_OP_INVERT:
+      return MALI_STENCIL_OP_INVERT;
+   default:
+      unreachable("Invalid stencil op");
    }
 }
 
@@ -633,7 +677,8 @@
    if (!builder->use_depth_stencil_attachment)
       return;
 
-   pipeline->zs.z_test = builder->create_info.gfx->pDepthStencilState->depthTestEnable;
+   pipeline->zs.z_test =
+      builder->create_info.gfx->pDepthStencilState->depthTestEnable;
 
    /* The Vulkan spec says:
     *
@@ -644,34 +689,36 @@
     * The hardware does not make this distinction, though, so we AND in the
     * condition ourselves.
     */
-   pipeline->zs.z_write = pipeline->zs.z_test &&
+   pipeline->zs.z_write =
+      pipeline->zs.z_test &&
       builder->create_info.gfx->pDepthStencilState->depthWriteEnable;
 
-   pipeline->zs.z_compare_func =
-      panvk_per_arch(translate_compare_func)(builder->create_info.gfx->pDepthStencilState->depthCompareOp);
-   pipeline->zs.s_test = builder->create_info.gfx->pDepthStencilState->stencilTestEnable;
-   pipeline->zs.s_front.fail_op =
-      translate_stencil_op(builder->create_info.gfx->pDepthStencilState->front.failOp);
-   pipeline->zs.s_front.pass_op =
-      translate_stencil_op(builder->create_info.gfx->pDepthStencilState->front.passOp);
-   pipeline->zs.s_front.z_fail_op =
-      translate_stencil_op(builder->create_info.gfx->pDepthStencilState->front.depthFailOp);
-   pipeline->zs.s_front.compare_func =
-      panvk_per_arch(translate_compare_func)(builder->create_info.gfx->pDepthStencilState->front.compareOp);
+   pipeline->zs.z_compare_func = panvk_per_arch(translate_compare_func)(
+      builder->create_info.gfx->pDepthStencilState->depthCompareOp);
+   pipeline->zs.s_test =
+      builder->create_info.gfx->pDepthStencilState->stencilTestEnable;
+   pipeline->zs.s_front.fail_op = translate_stencil_op(
+      builder->create_info.gfx->pDepthStencilState->front.failOp);
+   pipeline->zs.s_front.pass_op = translate_stencil_op(
+      builder->create_info.gfx->pDepthStencilState->front.passOp);
+   pipeline->zs.s_front.z_fail_op = translate_stencil_op(
+      builder->create_info.gfx->pDepthStencilState->front.depthFailOp);
+   pipeline->zs.s_front.compare_func = panvk_per_arch(translate_compare_func)(
+      builder->create_info.gfx->pDepthStencilState->front.compareOp);
    pipeline->zs.s_front.compare_mask =
       builder->create_info.gfx->pDepthStencilState->front.compareMask;
    pipeline->zs.s_front.write_mask =
       builder->create_info.gfx->pDepthStencilState->front.writeMask;
    pipeline->zs.s_front.ref =
       builder->create_info.gfx->pDepthStencilState->front.reference;
-   pipeline->zs.s_back.fail_op =
-      translate_stencil_op(builder->create_info.gfx->pDepthStencilState->back.failOp);
-   pipeline->zs.s_back.pass_op =
-      translate_stencil_op(builder->create_info.gfx->pDepthStencilState->back.passOp);
-   pipeline->zs.s_back.z_fail_op =
-      translate_stencil_op(builder->create_info.gfx->pDepthStencilState->back.depthFailOp);
-   pipeline->zs.s_back.compare_func =
-      panvk_per_arch(translate_compare_func)(builder->create_info.gfx->pDepthStencilState->back.compareOp);
+   pipeline->zs.s_back.fail_op = translate_stencil_op(
+      builder->create_info.gfx->pDepthStencilState->back.failOp);
+   pipeline->zs.s_back.pass_op = translate_stencil_op(
+      builder->create_info.gfx->pDepthStencilState->back.passOp);
+   pipeline->zs.s_back.z_fail_op = translate_stencil_op(
+      builder->create_info.gfx->pDepthStencilState->back.depthFailOp);
+   pipeline->zs.s_back.compare_func = panvk_per_arch(translate_compare_func)(
+      builder->create_info.gfx->pDepthStencilState->back.compareOp);
    pipeline->zs.s_back.compare_mask =
       builder->create_info.gfx->pDepthStencilState->back.compareMask;
    pipeline->zs.s_back.write_mask =
@@ -684,17 +731,29 @@
 panvk_pipeline_builder_parse_rast(struct panvk_pipeline_builder *builder,
                                   struct panvk_pipeline *pipeline)
 {
-   pipeline->rast.clamp_depth = builder->create_info.gfx->pRasterizationState->depthClampEnable;
-   pipeline->rast.depth_bias.enable = builder->create_info.gfx->pRasterizationState->depthBiasEnable;
+   pipeline->rast.clamp_depth =
+      builder->create_info.gfx->pRasterizationState->depthClampEnable;
+   pipeline->rast.depth_bias.enable =
+      builder->create_info.gfx->pRasterizationState->depthBiasEnable;
    pipeline->rast.depth_bias.constant_factor =
       builder->create_info.gfx->pRasterizationState->depthBiasConstantFactor;
-   pipeline->rast.depth_bias.clamp = builder->create_info.gfx->pRasterizationState->depthBiasClamp;
-   pipeline->rast.depth_bias.slope_factor = builder->create_info.gfx->pRasterizationState->depthBiasSlopeFactor;
-   pipeline->rast.front_ccw = builder->create_info.gfx->pRasterizationState->frontFace == VK_FRONT_FACE_COUNTER_CLOCKWISE;
-   pipeline->rast.cull_front_face = builder->create_info.gfx->pRasterizationState->cullMode & VK_CULL_MODE_FRONT_BIT;
-   pipeline->rast.cull_back_face = builder->create_info.gfx->pRasterizationState->cullMode & VK_CULL_MODE_BACK_BIT;
-   pipeline->rast.line_width = builder->create_info.gfx->pRasterizationState->lineWidth;
-   pipeline->rast.enable = !builder->create_info.gfx->pRasterizationState->rasterizerDiscardEnable;
+   pipeline->rast.depth_bias.clamp =
+      builder->create_info.gfx->pRasterizationState->depthBiasClamp;
+   pipeline->rast.depth_bias.slope_factor =
+      builder->create_info.gfx->pRasterizationState->depthBiasSlopeFactor;
+   pipeline->rast.front_ccw =
+      builder->create_info.gfx->pRasterizationState->frontFace ==
+      VK_FRONT_FACE_COUNTER_CLOCKWISE;
+   pipeline->rast.cull_front_face =
+      builder->create_info.gfx->pRasterizationState->cullMode &
+      VK_CULL_MODE_FRONT_BIT;
+   pipeline->rast.cull_back_face =
+      builder->create_info.gfx->pRasterizationState->cullMode &
+      VK_CULL_MODE_BACK_BIT;
+   pipeline->rast.line_width =
+      builder->create_info.gfx->pRasterizationState->lineWidth;
+   pipeline->rast.enable =
+      !builder->create_info.gfx->pRasterizationState->rasterizerDiscardEnable;
 }
 
 static bool
--- mesa-23.0.0/src/util/stable_array.h	2023-03-06 19:49:25.227687903 +0100
+++ mesa/src/util/stable_array.h	2023-03-06 17:54:35.861495119 +0100
@@ -0,0 +1,132 @@
+/*
+ * Copyright (C) 2022 Icecream95 <ixn@disroot.org>
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef STABLE_ARRAY_H
+#define STABLE_ARRAY_H
+
+#include "util/simple_mtx.h"
+#include "util/u_math.h"
+
+/* A thread-safe automatically growing array where elements have stable locations
+ *
+ * This data structure has these properties:
+ *
+ *  1. Accessing an element is constant time (if allocation is not required).
+ *
+ *  2. Elements are not moved in memory, so it is safe to store a pointer to
+ *     something in a stable_array.
+ *
+ *  3. The data structure is thread-safe. To improve performance, there is
+ *     also a fast path that does not require atomics.
+ *
+ *  4. Although the data structure is not lock-free, there is a limit on the
+ *     number of times that a lock is ever acquired--a maximum of 32 times the
+ *     number of accessing threads. In practice, contention will never be an
+ *     issue for long-lived stable_arrays.
+ *
+ *  5. Memory usage is similar to util_dynarray, with each allocation being
+ *     twice as large as the last. Freeing buckets is currently never done.
+ *
+ * The data structure is faster than util_sparse_array, but is not sparse.
+ */
+
+struct stable_array
+{
+   uint8_t *buckets[32];
+   simple_mtx_t lock;
+   size_t eltsize;
+};
+
+static inline void
+stable_array_init_bytes(struct stable_array *buf, size_t eltsize)
+{
+   memset(buf, 0, sizeof(*buf));
+   buf->eltsize = eltsize;
+   simple_mtx_init(&buf->lock, mtx_plain);
+}
+
+static inline void
+stable_array_fini(struct stable_array *buf)
+{
+   simple_mtx_destroy(&buf->lock);
+   for (unsigned i = 0; i < ARRAY_SIZE(buf->buckets); ++i) {
+      if (buf->buckets[i])
+         free(buf->buckets[i]);
+   }
+}
+
+struct stable_array_index
+{
+   unsigned bucket;
+   unsigned idx;
+};
+
+static inline struct stable_array_index
+stable_array_get_index(unsigned idx)
+{
+   struct stable_array_index i = {0};
+   i.bucket = util_logbase2(idx);
+   i.idx = i.bucket ? (idx -= (1 << i.bucket)) : idx;
+   return i;
+}
+
+static inline void *
+stable_array_get_bytes(struct stable_array *buf, unsigned idx, size_t eltsize)
+{
+   assert(eltsize == buf->eltsize);
+
+   struct stable_array_index i = stable_array_get_index(idx);
+
+   uint8_t *bucket = p_atomic_read(&buf->buckets[i.bucket]);
+
+   if (!bucket) {
+      simple_mtx_lock(&buf->lock);
+      bucket = buf->buckets[i.bucket];
+
+      if (!bucket) {
+         /* The first two buckets both have two elements */
+         bucket = (uint8_t *)calloc(1U << MAX2(i.bucket, 1), eltsize);
+
+         p_atomic_set(&buf->buckets[i.bucket], bucket);
+      }
+      simple_mtx_unlock(&buf->lock);
+   }
+
+   return bucket + eltsize * i.idx;
+}
+
+static inline void *
+stable_array_get_existing_bytes(struct stable_array *buf, unsigned idx, size_t eltsize)
+{
+   assert(eltsize == buf->eltsize);
+
+   struct stable_array_index i = stable_array_get_index(idx);
+
+   return buf->buckets[i.bucket] + eltsize * i.idx;
+}
+
+#define stable_array_init(buf, type) stable_array_init_bytes((buf), sizeof(type))
+#define stable_array_get(buf, type, idx) ((type*)stable_array_get_bytes((buf), (idx), sizeof(type)))
+#define stable_array_get_existing(buf, type, idx) ((type*)stable_array_get_existing_bytes((buf), (idx), sizeof(type)))
+
+#endif
diff -up mesa-23.0.0/src/panfrost/lib/meson.build.omv~ mesa-23.0.0/src/panfrost/lib/meson.build
diff -up mesa-23.0.0/src/panfrost/vulkan/meson.build.omv~ mesa-23.0.0/src/panfrost/vulkan/meson.build
--- mesa-23.0.0/src/panfrost/vulkan/meson.build.omv~	2023-03-06 18:59:09.336198247 +0000
+++ mesa-23.0.0/src/panfrost/vulkan/meson.build	2023-03-06 19:00:26.716227323 +0000
@@ -80,6 +80,7 @@ foreach arch : ['6', '7']
       inc_gallium, # XXX: util/format/u_formats.h
       inc_gallium_aux, # XXX: renderonly
       inc_panfrost,
+      inc_panfrost_hw
     ],
     dependencies : [
       idep_nir_headers,
@@ -109,6 +110,7 @@ libvulkan_panfrost = shared_library(
     inc_gallium, # XXX: util/format/u_formats.h
     inc_gallium_aux, # XXX: renderonly
     inc_panfrost,
+    inc_panfrost_hw
   ],
   link_whole : [panvk_per_arch_libs],
   link_with : [
diff -up mesa-23.0.0/include/drm-uapi/drm_fourcc.h.omv~ mesa-23.0.0/include/drm-uapi/drm_fourcc.h
--- mesa-23.0.0/include/drm-uapi/drm_fourcc.h.omv~	2023-03-06 21:30:14.816232451 +0000
+++ mesa-23.0.0/include/drm-uapi/drm_fourcc.h	2023-03-06 21:30:40.766194522 +0000
@@ -1164,6 +1164,13 @@ drm_fourcc_canonicalize_nvidia_format_mo
  */
 #define AFBC_FORMAT_MOD_USM	(1ULL << 12)
 
+/* AFBC native swizzle
+ *
+ * Indicates that the buffer is using RGBA component order regardless of the
+ * actual format.
+ */
+#define AFBC_FORMAT_MOD_NATIVE_SWIZZLE  (1ULL << 32)
+
 /*
  * Arm Fixed-Rate Compression (AFRC) modifiers
  *
diff -up mesa-23.0.0/src/gallium/drivers/panfrost/pan_blend_cso.h.omv~ mesa-23.0.0/src/gallium/drivers/panfrost/pan_blend_cso.h
--- mesa-23.0.0/src/gallium/drivers/panfrost/pan_blend_cso.h.omv~	2023-03-06 22:05:00.045355556 +0000
+++ mesa-23.0.0/src/gallium/drivers/panfrost/pan_blend_cso.h	2023-03-06 22:07:37.585339931 +0000
@@ -31,6 +31,8 @@
 #include "util/hash_table.h"
 #include "nir.h"
 #include "pan_blend.h"
+#include "pipe/p_state.h"
+#include "pan_job.h"
 
 struct panfrost_bo;
 
diff -up mesa-23.0.0/src/panfrost/lib/pan_indirect_draw.h.omv~ mesa-23.0.0/src/panfrost/lib/pan_indirect_draw.h
--- mesa-23.0.0/src/panfrost/lib/pan_indirect_draw.h.omv~	2023-03-06 22:08:20.355335849 +0000
+++ mesa-23.0.0/src/panfrost/lib/pan_indirect_draw.h	2023-03-06 22:08:32.495334702 +0000
@@ -25,6 +25,7 @@
 #define __PAN_INDIRECT_DRAW_SHADERS_H__
 
 #include "genxml/gen_macros.h"
+#include "pan_bo.h"
 
 struct pan_device;
 struct pan_scoreboard;
