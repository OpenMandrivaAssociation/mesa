From 066457ac2638d4917326f3b97ec6fc60a1274fd6 Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Mon, 3 Jul 2023 12:01:11 +0200
Subject: [PATCH 01/32] panfrost: Abstract kernel driver operations

We have generic BO management and device management layers that
directly call kernel driver-specific ioctls. The introduction of
Panthor (the new kernel driver supporting CSF hardware) forces us to
abstract some low-level operations. This could be done directly in
pan_{bo,device,props}.{c,h}, but having the abstraction clearly defined
and separated from the rest of the code makes for a cleaner
implementation.

This is also a good way to get a low-level KMD abstraction that
we can use without pulling all sort of gallium-related details in,
which will be important for some refactoring we plan to do in panvk.

Signed-off-by: Boris Brezillon <boris.brezillon@collabora.com>
---
 src/panfrost/ci/gitlab-ci.yml            |   1 +
 src/panfrost/lib/kmod/meson.build        |  38 ++
 src/panfrost/lib/kmod/pan_kmod.c         |  88 +++
 src/panfrost/lib/kmod/pan_kmod.h         | 671 +++++++++++++++++++++++
 src/panfrost/lib/kmod/pan_kmod_backend.h | 114 ++++
 src/panfrost/lib/meson.build             |   5 +-
 6 files changed, 915 insertions(+), 2 deletions(-)
 create mode 100644 src/panfrost/lib/kmod/meson.build
 create mode 100644 src/panfrost/lib/kmod/pan_kmod.c
 create mode 100644 src/panfrost/lib/kmod/pan_kmod.h
 create mode 100644 src/panfrost/lib/kmod/pan_kmod_backend.h

diff --git a/src/panfrost/ci/gitlab-ci.yml b/src/panfrost/ci/gitlab-ci.yml
index cdde45c9cb227..a081f58f35f53 100644
--- a/src/panfrost/ci/gitlab-ci.yml
+++ b/src/panfrost/ci/gitlab-ci.yml
@@ -17,6 +17,7 @@
         - src/panfrost/ci/$PIGLIT_TRACES_FILE
         - src/panfrost/include/*
         - src/panfrost/lib/*
+        - src/panfrost/lib/kmod/*
         - src/panfrost/shared/*
         - src/panfrost/util/*
       when: on_success
diff --git a/src/panfrost/lib/kmod/meson.build b/src/panfrost/lib/kmod/meson.build
new file mode 100644
index 0000000000000..8966d0a5c09d1
--- /dev/null
+++ b/src/panfrost/lib/kmod/meson.build
@@ -0,0 +1,38 @@
+# Copyright © 2023 Collabora
+
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+# SOFTWARE.
+
+libpankmod_lib_files = files(
+  'pan_kmod.c',
+)
+
+libpankmod_lib = static_library(
+  'pankmod_lib',
+  [libpankmod_lib_files],
+  include_directories : [inc_include, inc_src, inc_panfrost],
+  c_args : [no_override_init_args],
+  gnu_symbol_visibility : 'hidden',
+  dependencies: [dep_libdrm, idep_mesautil],
+  build_by_default : false,
+)
+
+libpankmod_dep = declare_dependency(
+  include_directories: [inc_include, inc_src],
+  dependencies: [dep_libdrm],
+)
diff --git a/src/panfrost/lib/kmod/pan_kmod.c b/src/panfrost/lib/kmod/pan_kmod.c
new file mode 100644
index 0000000000000..950a100d46ba3
--- /dev/null
+++ b/src/panfrost/lib/kmod/pan_kmod.c
@@ -0,0 +1,88 @@
+/*
+ * Copyright © 2023 Collabora, Ltd.
+ *
+ * SPDX-License-Identifier: MIT
+ */
+
+#include <string.h>
+#include <xf86drm.h>
+
+#include "util/macros.h"
+#include "pan_kmod.h"
+
+static const struct {
+   const char *name;
+   const struct pan_kmod_ops *ops;
+} drivers[] = {
+};
+
+static void *
+default_zalloc(const struct pan_kmod_allocator *allocator, size_t size,
+               UNUSED bool transient)
+{
+   return rzalloc_size(allocator, size);
+}
+
+static void
+default_free(const struct pan_kmod_allocator *allocator, void *data)
+{
+   return ralloc_free(data);
+}
+
+static const struct pan_kmod_allocator *
+create_default_allocator(void)
+{
+   struct pan_kmod_allocator *allocator =
+      rzalloc(NULL, struct pan_kmod_allocator);
+
+   if (allocator) {
+      allocator->zalloc = default_zalloc;
+      allocator->free = default_free;
+   }
+
+   return allocator;
+}
+
+struct pan_kmod_dev *
+pan_kmod_dev_create(int fd, const struct pan_kmod_allocator *allocator)
+{
+   drmVersionPtr version = drmGetVersion(fd);
+   struct pan_kmod_dev *dev = NULL;
+
+   if (!version)
+      return NULL;
+
+   if (!allocator) {
+      allocator = create_default_allocator();
+      if (!allocator)
+         goto out_free_version;
+   }
+
+   for (unsigned i = 0; i < ARRAY_SIZE(drivers); i++) {
+      if (!strcmp(drivers[i].name, version->name)) {
+         const struct pan_kmod_ops *ops = drivers[i].ops;
+
+         dev = ops->dev_create(fd, version, allocator);
+         if (dev)
+            goto out_free_version;
+      }
+   }
+
+   if (allocator->zalloc == default_zalloc)
+      ralloc_free((void *)allocator);
+
+out_free_version:
+   drmFreeVersion(version);
+   return dev;
+}
+
+void
+pan_kmod_dev_destroy(struct pan_kmod_dev *dev)
+{
+   const struct pan_kmod_allocator *allocator = dev->allocator;
+
+   dev->ops->dev_destroy(dev);
+
+   if (allocator->zalloc == default_zalloc)
+      ralloc_free((void *)allocator);
+}
diff --git a/src/panfrost/lib/kmod/pan_kmod.h b/src/panfrost/lib/kmod/pan_kmod.h
new file mode 100644
index 0000000000000..81071beb59880
--- /dev/null
+++ b/src/panfrost/lib/kmod/pan_kmod.h
@@ -0,0 +1,671 @@
+/*
+ * Copyright © 2023 Collabora, Ltd.
+ *
+ * SPDX-License-Identifier: MIT
+ *
+ * This file exposes some core KMD functionalities in a driver-agnostic way.
+ * The drivers are still assumed to be regular DRM drivers, such that some
+ * operations can be handled generically.
+ *
+ * Any operation that's too specific to be abstracted can either have a backend
+ * specific helper exposed through pan_kmod_<backend>.h, or no helper at all
+ * (in the latter case, users are expected to call the ioctl directly).
+ *
+ * If some operations are not natively supported by a KMD, the kmod backend
+ * should fail or emulate the functionality (if deemed necessary).
+ */
+
+#pragma once
+
+#include <fcntl.h>
+#include <unistd.h>
+#include <xf86drm.h>
+
+#include "drm-uapi/drm.h"
+
+#include "util/log.h"
+#include "util/macros.h"
+#include "util/os_file.h"
+#include "util/os_mman.h"
+#include "util/ralloc.h"
+#include "util/simple_mtx.h"
+#include "util/sparse_array.h"
+#include "util/u_atomic.h"
+
+struct pan_kmod_dev;
+
+/* GPU VM creation flags. */
+enum pan_kmod_vm_flags {
+   /* Set if you want the VM to automatically assign virtual addresses when
+    * pan_kmod_vm_map(). If this flag is set, all pan_kmod_vm_map() calls
+    * must have va=PAN_KMOD_VM_MAP_AUTO_VA.
+    */
+   PAN_KMOD_VM_FLAG_AUTO_VA = BITFIELD_BIT(0),
+};
+
+/* Object representing a GPU VM. */
+struct pan_kmod_vm {
+   /* Combination of pan_kmod_vm_flags flags. */
+   uint32_t flags;
+
+   /* The VM handle returned by the KMD. If the KMD supports only one VM per
+    * context, this should be zero.
+    */
+   uint32_t handle;
+
+   /* Device this VM was created from. */
+   struct pan_kmod_dev *dev;
+};
+
+/* Buffer object flags. */
+enum pan_kmod_bo_flags {
+   /* Allow GPU execution on this buffer. */
+   PAN_KMOD_BO_FLAG_EXECUTABLE = BITFIELD_BIT(0),
+
+   /* Allocate memory when a GPU fault occurs instead of allocating
+    * up-front.
+    */
+   PAN_KMOD_BO_FLAG_ALLOC_ON_FAULT = BITFIELD_BIT(1),
+
+   /* If set, the buffer object will never be CPU-mapped in userspace. */
+   PAN_KMOD_BO_FLAG_NO_MMAP = BITFIELD_BIT(2),
+
+   /* Set when the buffer object has been exported. Users don't directly
+    * control this flag, it's set when pan_kmod_bo_export() is called.
+    */
+   PAN_KMOD_BO_FLAG_EXPORTED = BITFIELD_BIT(3),
+
+   /* Set when the buffer object has been impported. Users don't directly
+    * control this flag, it's set when pan_kmod_bo_import() is called.
+    */
+   PAN_KMOD_BO_FLAG_IMPORTED = BITFIELD_BIT(4),
+
+   /* If set, the buffer in mapped GPU-uncached when pan_kmod_vm_map()
+    * is called.
+    */
+   PAN_KMOD_BO_FLAG_GPU_UNCACHED = BITFIELD_BIT(5),
+};
+
+/* Buffer object. */
+struct pan_kmod_bo {
+   /* Atomic reference count. The only reason we need to refcnt BOs at this
+    * level is because of how DRM prime import works: the import logic
+    * returns the handle of an existing object if the object was previously
+    * imported or was created by the driver.
+    * In order to prevent call GEM_CLOSE on an object that's still supposed
+    * to be active, we need count the number of users left.
+    */
+   int32_t refcnt;
+
+   /* Size of the buffer object. */
+   size_t size;
+
+   /* Handle attached to the buffer object. */
+   uint32_t handle;
+
+   /* Combination of pan_kmod_bo_flags flags. */
+   uint32_t flags;
+
+   /* If non-NULL, the buffer object can only by mapped on this VM. Typical
+    * the case for all internal/non-shareable buffers. The backend can
+    * optimize things based on this information. Calling pan_kmod_bo_export()
+    * on such buffer objects is forbidden.
+    */
+   struct pan_kmod_vm *exclusive_vm;
+
+   /* The device this buffer object was created from. */
+   struct pan_kmod_dev *dev;
+
+   /* User private data. Use pan_kmod_bo_{set,get}_user_priv() to access it. */
+   void *user_priv;
+};
+
+/* List of GPU properties needed by the UMD. */
+struct pan_kmod_dev_props {
+   /* GPU product ID. */
+   uint32_t gpu_prod_id;
+
+   /* GPU revision. */
+   uint32_t gpu_revision;
+
+   /* Bitmask encoding the number of shader cores exposed by the GPU. */
+   uint64_t shader_present;
+
+   /* Tiler features bits. */
+   uint32_t tiler_features;
+
+   /* Memory related feature bits. */
+   uint32_t mem_features;
+
+   /* MMU feature bits. */
+   uint32_t mmu_features;
+
+   /* Texture feature bits. */
+   uint32_t texture_features[4];
+
+   /* Maximum number of threads per core. */
+   uint32_t thread_tls_alloc;
+
+   /* AFBC feature bits. */
+   uint32_t afbc_features;
+};
+
+/* Memory allocator for kmod internal allocations. */
+struct pan_kmod_allocator {
+   /* Allocate and set to zero. */
+   void *(*zalloc)(const struct pan_kmod_allocator *allocator, size_t size,
+                   bool transient);
+
+   /* Free. */
+   void (*free)(const struct pan_kmod_allocator *allocator, void *data);
+
+   /* Private data allocator data. Can be NULL if unused. */
+   void *priv;
+};
+
+/* Synchronization type. */
+enum pan_kmod_sync_type {
+   PAN_KMOD_SYNC_TYPE_WAIT = 0,
+   PAN_KMOD_SYNC_TYPE_SIGNAL = 0,
+};
+
+/* Synchronization operation. */
+struct pan_kmod_sync_op {
+   /* Type of operation. */
+   enum pan_kmod_sync_type type;
+
+   /* Syncobj handle. */
+   uint32_t handle;
+
+   /* Syncobj point. Zero for binary syncobjs. */
+   uint64_t point;
+};
+
+/* Special value passed to pan_kmod_vm_map() to signify the VM it should
+ * automatically allocate a VA. Only valid if the VM was created with
+ * PAN_KMOD_VM_FLAG_AUTO_VA.
+ */
+#define PAN_KMOD_VM_MAP_AUTO_VA ~0ull
+
+/* Special value return when the vm_map() operation failed. */
+#define PAN_KMOD_VM_MAP_FAILED ~0ull
+
+/* VM operations can be executed in different modes. */
+enum pan_kmod_vm_op_mode {
+   /* The map/unmap operation is executed immediately, which might cause
+    * GPU faults if the GPU was still accessing buffers when we unmap or
+    * remap.
+    */
+   PAN_KMOD_VM_OP_MODE_IMMEDIATE,
+
+   /* The map/unmap operation is executed asynchronously, and the user
+    * provides explicit wait/signal sync operations.
+    */
+   PAN_KMOD_VM_OP_MODE_ASYNC,
+
+   /* The map/unmap operation is executed when the next GPU/VM idle-point
+    * is reached. This guarantees fault-free unmap/remap operations when the
+    * kmod user doesn't want to deal with synchronizations explicitly.
+    */
+   PAN_KMOD_VM_OP_MODE_DEFER_TO_NEXT_IDLE_POINT,
+};
+
+/* VM operation type. */
+enum pan_kmod_vm_op_type {
+   /* Map a buffer object. */
+   PAN_KMOD_VM_OP_TYPE_MAP,
+
+   /* Unmap a VA range. */
+   PAN_KMOD_VM_OP_TYPE_UNMAP,
+
+   /* Do nothing. Used as a way to execute sync operations on a VM queue,
+    * without touching the VM.
+    */
+   PAN_KMOD_VM_OP_TYPE_SYNC_ONLY,
+};
+
+/* VM operation data. */
+struct pan_kmod_vm_op {
+   /* The type of operation being requested. */
+   enum pan_kmod_vm_op_type type;
+
+   /* VA range. */
+   struct {
+      /* Start of the VA range.
+       * Must be PAN_KMOD_VM_MAP_AUTO_VA if PAN_KMOD_VM_FLAG_AUTO_VA was set
+       * at VM creation time. In that case, the allocated VA is returned
+       * in this field.
+       */
+      uint64_t start;
+
+      /* Size of the VA range */
+      size_t size;
+   } va;
+
+   union {
+      /* Arguments specific to map operations. */
+      struct {
+         /* Buffer object to map. */
+         struct pan_kmod_bo *bo;
+
+         /* Offset in the buffer object. */
+         off_t bo_offset;
+      } map;
+   };
+
+   /* Synchronization operations attached to the VM operation. */
+   struct {
+      /* Number of synchronization operations. Must be zero if mode is
+       * PAN_KMOD_VM_OP_MODE_IMMEDIATE or PAN_KMOD_VM_OP_MODE_WAIT_IDLE.
+       */
+      uint32_t count;
+
+      /* Array of synchronization operation descriptors. NULL if count is zero. */
+      const struct pan_kmod_sync_op *array;
+   } syncs;
+};
+
+/* VM state. */
+enum pan_kmod_vm_state {
+   PAN_KMOD_VM_USABLE,
+   PAN_KMOD_VM_FAULTY,
+};
+
+/* KMD backend vtable.
+ *
+ * All methods described there are mandatory, unless explicitly flagged as
+ * optional.
+ */
+struct pan_kmod_ops {
+   /* Create a pan_kmod_dev object.
+    * Return NULL if the creation fails for any reason.
+    */
+   struct pan_kmod_dev *(*dev_create)(
+      int fd, const drmVersionPtr version,
+      const struct pan_kmod_allocator *allocator);
+
+   /* Destroy a pan_kmod_dev object. */
+   void (*dev_destroy)(struct pan_kmod_dev *dev);
+
+   /* Query device properties. */
+   void (*dev_query_props)(struct pan_kmod_dev *dev,
+                           struct pan_kmod_dev_props *props);
+
+   /* Allocate a buffer object.
+    * Return NULL if the creation fails for any reason.
+    */
+   struct pan_kmod_bo *(*bo_alloc)(struct pan_kmod_dev *dev,
+                                   struct pan_kmod_vm *exclusive_vm,
+                                   size_t size, uint32_t flags);
+
+   /* Free buffer object. */
+   void (*bo_free)(struct pan_kmod_bo *bo);
+
+   /* Import a buffer object.
+    * Return NULL if the import fails for any reason.
+    */
+   struct pan_kmod_bo *(*bo_import)(struct pan_kmod_dev *dev, uint32_t handle,
+                                    size_t size, uint32_t flags);
+
+   /* Post export operations.
+    * Return 0 on success, -1 otherwise.
+    * This method is optional.
+    */
+   int (*bo_export)(struct pan_kmod_bo *bo, int dmabuf_fd);
+
+   /* Get the file offset to use to mmap() a buffer object. */
+   off_t (*bo_get_mmap_offset)(struct pan_kmod_bo *bo);
+
+   /* Wait for a buffer object to be ready for read or read/write accesses. */
+   bool (*bo_wait)(struct pan_kmod_bo *bo, int64_t timeout_ns,
+                   bool for_read_only_access);
+
+   /* Make a buffer object evictable. This method is optional. */
+   void (*bo_make_evictable)(struct pan_kmod_bo *bo);
+
+   /* Make the buffer object unevictable. This method is optional. */
+   bool (*bo_make_unevictable)(struct pan_kmod_bo *bo);
+
+   /* Create a VM object. */
+   struct pan_kmod_vm *(*vm_create)(struct pan_kmod_dev *dev, uint32_t flags,
+                                    uint64_t va_start, uint64_t va_range);
+
+   /* Destroy a VM object. */
+   void (*vm_destroy)(struct pan_kmod_vm *vm);
+
+   /* Execute VM operations.
+    * Return 0 if the submission suceeds, -1 otherwise.
+    * For PAN_KMOD_VM_OP_MODE_IMMEDIATE submissions, the return value also
+    * reflects the successfulness of the VM operation, for other modes,
+    * if any of the VM operation fails, the VM might be flagged as unusable
+    * and users should create a new VM to recover.
+    */
+   int (*vm_bind)(struct pan_kmod_vm *vm, enum pan_kmod_vm_op_mode mode,
+                  struct pan_kmod_vm_op *ops, uint32_t op_count);
+
+   /* Query the VM state.
+    * This method is optional. When missing the VM is assumed to always be
+    * usable.
+    */
+   enum pan_kmod_vm_state (*vm_query_state)(struct pan_kmod_vm *vm);
+};
+
+/* KMD information. */
+struct pan_kmod_driver {
+   /* KMD version. */
+   struct {
+      uint32_t major;
+      uint32_t minor;
+   } version;
+};
+
+/* Device object. */
+struct pan_kmod_dev {
+   /* FD attached to the device. */
+   int fd;
+
+   /* KMD backing this device. */
+   struct pan_kmod_driver driver;
+
+   /* kmod backend ops assigned at device creation. */
+   const struct pan_kmod_ops *ops;
+
+   /* DRM prime import returns the handle of a pre-existing GEM if we are
+    * importing an object that was created by us or previously imported.
+    * We need to make sure we return the same pan_kmod_bo in that case,
+    * otherwise freeing one pan_kmod_bo will make all other BOs sharing
+    * the same handle invalid.
+    */
+   struct {
+      struct util_sparse_array array;
+      simple_mtx_t lock;
+   } handle_to_bo;
+
+   /* Allocator attached to the device. */
+   const struct pan_kmod_allocator *allocator;
+
+   /* User private data. Use pan_kmod_dev_{set,get}_user_priv() to access it. */
+   void *user_priv;
+};
+
+struct pan_kmod_dev *
+pan_kmod_dev_create(int fd, const struct pan_kmod_allocator *allocator);
+
+void pan_kmod_dev_destroy(struct pan_kmod_dev *dev);
+
+static inline void
+pan_kmod_dev_query_props(struct pan_kmod_dev *dev,
+                         struct pan_kmod_dev_props *props)
+{
+   dev->ops->dev_query_props(dev, props);
+}
+
+static inline void
+pan_kmod_dev_set_user_priv(struct pan_kmod_dev *dev, void *data)
+{
+   dev->user_priv = data;
+}
+
+static inline void *
+pan_kmod_dev_get_user_priv(struct pan_kmod_dev *dev)
+{
+   return dev->user_priv;
+}
+
+static inline struct pan_kmod_bo *
+pan_kmod_bo_alloc(struct pan_kmod_dev *dev, struct pan_kmod_vm *exclusive_vm,
+                  size_t size, uint32_t flags)
+{
+   struct pan_kmod_bo *bo;
+
+   bo = dev->ops->bo_alloc(dev, exclusive_vm, size, flags);
+   if (!bo)
+      return NULL;
+
+   /* We intentionally don't take the lock when filling the sparse array,
+    * because we just created the BO, and haven't exported it yet, so
+    * there's no risk of imports racing with our BO insertion.
+    */
+   struct pan_kmod_bo **slot =
+      util_sparse_array_get(&dev->handle_to_bo.array, bo->handle);
+
+   if (!slot) {
+      mesa_loge("failed to allocator slot in the handle_to_bo array");
+      bo->dev->ops->bo_free(bo);
+      return NULL;
+   }
+
+   *slot = bo;
+   return bo;
+}
+
+static inline struct pan_kmod_bo *
+pan_kmod_bo_get(struct pan_kmod_bo *bo)
+{
+   if (!bo)
+      return NULL;
+
+   ASSERTED int32_t refcnt = p_atomic_inc_return(&bo->refcnt);
+
+   /* If refcnt was zero before our increment, we're in trouble. */
+   assert(refcnt > 1);
+
+   return bo;
+}
+
+static inline void
+pan_kmod_bo_put(struct pan_kmod_bo *bo)
+{
+   if (!bo)
+      return;
+
+   int32_t refcnt = p_atomic_dec_return(&bo->refcnt);
+
+   assert(refcnt >= 0);
+
+   if (refcnt)
+      return;
+
+   struct pan_kmod_dev *dev = bo->dev;
+
+   simple_mtx_lock(&dev->handle_to_bo.lock);
+
+   /* If some import took a ref on this BO while we were trying to acquire the
+    * lock, skip the destruction.
+    */
+   if (!p_atomic_read(&bo->refcnt)) {
+      struct pan_kmod_bo **slot =
+         util_sparse_array_get(&dev->handle_to_bo.array, bo->handle);
+
+      assert(slot);
+      *slot = NULL;
+      bo->dev->ops->bo_free(bo);
+   }
+
+   simple_mtx_unlock(&dev->handle_to_bo.lock);
+}
+
+static inline void *
+pan_kmod_bo_cmdxchg_user_priv(struct pan_kmod_bo *bo, void *old_data,
+                              void *new_data)
+{
+   return (void *)p_atomic_cmpxchg((uintptr_t *)&bo->user_priv,
+                                   (uintptr_t)old_data, (uintptr_t)new_data);
+}
+
+static inline void
+pan_kmod_bo_set_user_priv(struct pan_kmod_bo *bo, void *data)
+{
+   bo->user_priv = data;
+}
+
+static inline void *
+pan_kmod_bo_get_user_priv(const struct pan_kmod_bo *bo)
+{
+   return bo->user_priv;
+}
+
+static bool
+pan_kmod_bo_check_import_flags(struct pan_kmod_bo *bo, uint32_t flags)
+{
+   uint32_t mask = PAN_KMOD_BO_FLAG_EXECUTABLE |
+                   PAN_KMOD_BO_FLAG_ALLOC_ON_FAULT | PAN_KMOD_BO_FLAG_NO_MMAP |
+                   PAN_KMOD_BO_FLAG_GPU_UNCACHED;
+
+   return (bo->flags & mask) == (flags & mask);
+}
+
+static inline struct pan_kmod_bo *
+pan_kmod_bo_import(struct pan_kmod_dev *dev, int fd, uint32_t flags)
+{
+   struct pan_kmod_bo *bo = NULL;
+
+   simple_mtx_lock(&dev->handle_to_bo.lock);
+
+   uint32_t handle;
+   int ret = drmPrimeFDToHandle(dev->fd, fd, &handle);
+   if (ret)
+      goto err_unlock;
+
+   struct pan_kmod_bo **slot =
+      util_sparse_array_get(&dev->handle_to_bo.array, handle);
+
+   if (!slot)
+      goto err_close_handle;
+
+   if (*slot) {
+      if (!pan_kmod_bo_check_import_flags(*slot, flags)) {
+         mesa_loge("invalid import flags");
+         goto err_unlock;
+      }
+
+      bo = *slot;
+
+      p_atomic_inc(&bo->refcnt);
+   } else {
+      size_t size = lseek(fd, 0, SEEK_END);
+      if (size == 0 || size == (size_t)-1) {
+         mesa_loge("invalid dmabuf size");
+         goto err_close_handle;
+      }
+
+      bo = dev->ops->bo_import(dev, handle, size, flags);
+      if (!bo)
+         goto err_close_handle;
+
+      *slot = bo;
+   }
+
+   assert(p_atomic_read(&bo->refcnt) > 0);
+
+   simple_mtx_unlock(&dev->handle_to_bo.lock);
+
+   return bo;
+
+err_close_handle:
+   drmCloseBufferHandle(dev->fd, handle);
+
+err_unlock:
+   simple_mtx_unlock(&dev->handle_to_bo.lock);
+
+   return NULL;
+}
+
+static inline int
+pan_kmod_bo_export(struct pan_kmod_bo *bo)
+{
+   int fd;
+
+   if (drmPrimeHandleToFD(bo->dev->fd, bo->handle, DRM_CLOEXEC, &fd)) {
+      mesa_loge("drmPrimeHandleToFD() failed (err=%d)", errno);
+      return -1;
+   }
+
+   if (bo->dev->ops->bo_export && bo->dev->ops->bo_export(bo, fd)) {
+      close(fd);
+      return -1;
+   }
+
+   bo->flags |= PAN_KMOD_BO_FLAG_EXPORTED;
+   return fd;
+}
+
+static inline bool
+pan_kmod_bo_wait(struct pan_kmod_bo *bo, int64_t timeout_ns,
+                 bool for_read_only_access)
+{
+   return bo->dev->ops->bo_wait(bo, timeout_ns, for_read_only_access);
+}
+
+static inline void
+pan_kmod_bo_make_evictable(struct pan_kmod_bo *bo)
+{
+   if (bo->dev->ops->bo_make_evictable)
+      bo->dev->ops->bo_make_evictable(bo);
+}
+
+static inline bool
+pan_kmod_bo_make_unevictable(struct pan_kmod_bo *bo)
+{
+   if (bo->dev->ops->bo_make_unevictable)
+      return bo->dev->ops->bo_make_unevictable(bo);
+
+   return true;
+}
+
+static inline void *
+pan_kmod_bo_mmap(struct pan_kmod_bo *bo, off_t bo_offset, size_t size, int prot,
+                 int flags)
+{
+   off_t mmap_offset;
+   void *cpu_addr;
+
+   if (bo_offset + size > bo->size)
+      return MAP_FAILED;
+
+   mmap_offset = bo->dev->ops->bo_get_mmap_offset(bo);
+   if (mmap_offset < 0)
+      return MAP_FAILED;
+
+   cpu_addr =
+      os_mmap(NULL, size, prot, flags, bo->dev->fd, mmap_offset + bo_offset);
+   if (cpu_addr == MAP_FAILED)
+      mesa_loge("mmap() failed (err=%d)", errno);
+
+   return cpu_addr;
+}
+
+static inline struct pan_kmod_vm *
+pan_kmod_vm_create(struct pan_kmod_dev *dev, uint32_t flags, uint64_t va_start,
+                   uint64_t va_range)
+{
+   return dev->ops->vm_create(dev, flags, va_start, va_range);
+}
+
+static inline void
+pan_kmod_vm_destroy(struct pan_kmod_vm *vm)
+{
+   vm->dev->ops->vm_destroy(vm);
+}
+
+static inline int
+pan_kmod_vm_bind(struct pan_kmod_vm *vm, enum pan_kmod_vm_op_mode mode,
+                 struct pan_kmod_vm_op *ops, uint32_t op_count)
+{
+   return vm->dev->ops->vm_bind(vm, mode, ops, op_count);
+}
+
+static inline enum pan_kmod_vm_state
+pan_kmod_vm_query_state(struct pan_kmod_vm *vm)
+{
+   if (vm->dev->ops->vm_query_state)
+      return vm->dev->ops->vm_query_state(vm);
+
+   return PAN_KMOD_VM_USABLE;
+}
+
+static inline uint32_t
+pan_kmod_vm_handle(struct pan_kmod_vm *vm)
+{
+   return vm->handle;
+}
diff --git a/src/panfrost/lib/kmod/pan_kmod_backend.h b/src/panfrost/lib/kmod/pan_kmod_backend.h
new file mode 100644
index 0000000000000..df3b0f6512786
--- /dev/null
+++ b/src/panfrost/lib/kmod/pan_kmod_backend.h
@@ -0,0 +1,114 @@
+/*
+ * Copyright © 2023 Collabora, Ltd.
+ *
+ * SPDX-License-Identifier: MIT
+ */
+
+#pragma once
+
+#include "util/log.h"
+
+#include "pan_kmod.h"
+
+static inline void
+pan_kmod_dev_init(struct pan_kmod_dev *dev, int fd, drmVersionPtr version,
+                  const struct pan_kmod_ops *ops,
+                  const struct pan_kmod_allocator *allocator)
+{
+   simple_mtx_init(&dev->handle_to_bo.lock, mtx_plain);
+   util_sparse_array_init(&dev->handle_to_bo.array,
+                          sizeof(struct pan_kmod_bo *), 512);
+   dev->driver.version.major = version->version_major;
+   dev->driver.version.minor = version->version_minor;
+   dev->fd = fd;
+   dev->ops = ops;
+   dev->allocator = allocator;
+}
+
+static inline void
+pan_kmod_dev_cleanup(struct pan_kmod_dev *dev)
+{
+   close(dev->fd);
+   util_sparse_array_finish(&dev->handle_to_bo.array);
+   simple_mtx_destroy(&dev->handle_to_bo.lock);
+}
+
+static inline void *
+pan_kmod_alloc(const struct pan_kmod_allocator *allocator, size_t size)
+{
+   return allocator->zalloc(allocator, size, false);
+}
+
+static inline void *
+pan_kmod_alloc_transient(const struct pan_kmod_allocator *allocator,
+                         size_t size)
+{
+   return allocator->zalloc(allocator, size, true);
+}
+
+static inline void
+pan_kmod_free(const struct pan_kmod_allocator *allocator, void *data)
+{
+   return allocator->free(allocator, data);
+}
+
+static inline void *
+pan_kmod_dev_alloc(struct pan_kmod_dev *dev, size_t size)
+{
+   return pan_kmod_alloc(dev->allocator, size);
+}
+
+static inline void *
+pan_kmod_dev_alloc_transient(struct pan_kmod_dev *dev, size_t size)
+{
+   return pan_kmod_alloc_transient(dev->allocator, size);
+}
+
+static inline void
+pan_kmod_dev_free(const struct pan_kmod_dev *dev, void *data)
+{
+   return pan_kmod_free(dev->allocator, data);
+}
+
+static inline void
+pan_kmod_bo_init(struct pan_kmod_bo *bo, struct pan_kmod_dev *dev,
+                 struct pan_kmod_vm *exclusive_vm, size_t size, uint32_t flags,
+                 uint32_t handle)
+{
+   bo->dev = dev;
+   bo->exclusive_vm = exclusive_vm;
+   bo->size = size;
+   bo->flags = flags;
+   bo->handle = handle;
+   p_atomic_set(&bo->refcnt, 1);
+}
+
+static inline void
+pan_kmod_vm_init(struct pan_kmod_vm *vm, struct pan_kmod_dev *dev,
+                 uint32_t handle, uint32_t flags)
+{
+   vm->dev = dev;
+   vm->handle = handle;
+   vm->flags = flags;
+}
+
+static inline int
+pank_kmod_vm_op_check(struct pan_kmod_vm *vm, enum pan_kmod_vm_op_mode mode,
+                      struct pan_kmod_vm_op *op)
+{
+   /* We should only have sync operations on an async VM bind request. */
+   if (mode != PAN_KMOD_VM_OP_MODE_ASYNC && op->syncs.count) {
+      mesa_loge("only PAN_KMOD_VM_OP_MODE_ASYNC can be passed sync operations");
+      return -1;
+   }
+
+   /* Make sure the PAN_KMOD_VM_FLAG_AUTO_VA and VA passed to the op match. */
+   if (op->type == PAN_KMOD_VM_OP_TYPE_MAP &&
+       !!(vm->flags & PAN_KMOD_VM_FLAG_AUTO_VA) !=
+          (op->va.start == PAN_KMOD_VM_MAP_AUTO_VA)) {
+      mesa_loge("op->va.start and vm->flags don't match");
+      return -1;
+   }
+
+   return 0;
+}
diff --git a/src/panfrost/lib/meson.build b/src/panfrost/lib/meson.build
index e27483e128074..12b927a2973d7 100644
--- a/src/panfrost/lib/meson.build
+++ b/src/panfrost/lib/meson.build
@@ -20,6 +20,7 @@
 # SOFTWARE.
 
 subdir('genxml')
+subdir('kmod')
 
 pixel_format_versions = ['6', '7', '9']
 libpanfrost_pixel_format = []
@@ -92,13 +93,13 @@ libpanfrost_lib = static_library(
   gnu_symbol_visibility : 'hidden',
   dependencies: [dep_libdrm, idep_nir, idep_mesautil],
   build_by_default : false,
-  link_with: [libpanfrost_pixel_format, libpanfrost_per_arch],
+  link_with: [libpanfrost_pixel_format, libpanfrost_per_arch, libpankmod_lib],
 )
 
 libpanfrost_dep = declare_dependency(
   link_with: [libpanfrost_lib, libpanfrost_decode, libpanfrost_midgard, libpanfrost_bifrost, libpanfrost_pixel_format, libpanfrost_per_arch],
   include_directories: [inc_include, inc_src, inc_mapi, inc_mesa, inc_gallium, inc_gallium_aux, inc_panfrost_hw, inc_panfrost],
-  dependencies: [dep_libdrm, idep_nir, idep_pan_packers],
+  dependencies: [dep_libdrm, libpankmod_dep, idep_nir, idep_pan_packers],
 )
 
 if with_tests
-- 
GitLab


From e000ac3a565777dec92b80c454624a3691f9cd5b Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Mon, 20 Nov 2023 09:21:20 +0100
Subject: [PATCH 02/32] pan/kmod: Add a backend for the panfrost kernel driver

Just a few implementation details that are worth mentioning:

- Panfrost does support support explicit VM management. This means
  panfrost_kmod_vm_create() must always be created with
  PAN_KMOD_VM_FLAG_AUTO_VA and panfrost_kmod_vm_map() must always
  be passed PAN_KMOD_VM_MAP_AUTO_VA. The actual VA is assigned at
  BO creation time, and returned through drm_panfrost_create_bo.offset,
  or can be queried through DRM_IOCTL_PANFROST_GET_BO_OFFSET for
  imported BOs. We add a va_to_bo hash table to make sure we never end
  up re-using already active VAs, but that's really the kernel driver
  responsibility to ensure this never happens, so we keep these sanity
  checks under #ifndef NDEBUG sections to avoid the overhead on a
  release builds.

- Evictability is hooked up to DRM_IOCTL_PANFROST_MADVISE.

- BO wait is natively supported through DRM_IOCTL_PANFROST_WAIT_BO.

The rest is just a straightforward translation between the kmod API and
the existing panfrost ioctls.

Signed-off-by: Boris Brezillon <boris.brezillon@collabora.com>
---
 src/panfrost/lib/kmod/meson.build     |   1 +
 src/panfrost/lib/kmod/pan_kmod.c      |   6 +
 src/panfrost/lib/kmod/panfrost_kmod.c | 425 ++++++++++++++++++++++++++
 3 files changed, 432 insertions(+)
 create mode 100644 src/panfrost/lib/kmod/panfrost_kmod.c

diff --git a/src/panfrost/lib/kmod/meson.build b/src/panfrost/lib/kmod/meson.build
index 8966d0a5c09d1..1278dc6f394b1 100644
--- a/src/panfrost/lib/kmod/meson.build
+++ b/src/panfrost/lib/kmod/meson.build
@@ -20,6 +20,7 @@
 
 libpankmod_lib_files = files(
   'pan_kmod.c',
+  'panfrost_kmod.c',
 )
 
 libpankmod_lib = static_library(
diff --git a/src/panfrost/lib/kmod/pan_kmod.c b/src/panfrost/lib/kmod/pan_kmod.c
index 950a100d46ba3..be1fddca252cf 100644
--- a/src/panfrost/lib/kmod/pan_kmod.c
+++ b/src/panfrost/lib/kmod/pan_kmod.c
@@ -10,10 +10,16 @@
 #include "util/macros.h"
 #include "pan_kmod.h"
 
+extern const struct pan_kmod_ops panfrost_kmod_ops;
+
 static const struct {
    const char *name;
    const struct pan_kmod_ops *ops;
 } drivers[] = {
+   {
+      "panfrost",
+      &panfrost_kmod_ops,
+   },
 };
 
 static void *
diff --git a/src/panfrost/lib/kmod/panfrost_kmod.c b/src/panfrost/lib/kmod/panfrost_kmod.c
new file mode 100644
index 0000000000000..4308bb43a4bc1
--- /dev/null
+++ b/src/panfrost/lib/kmod/panfrost_kmod.c
@@ -0,0 +1,425 @@
+/*
+ * Copyright © 2023 Collabora, Ltd.
+ *
+ * SPDX-License-Identifier: MIT
+ */
+
+#include <errno.h>
+#include <fcntl.h>
+#include <string.h>
+#include <xf86drm.h>
+
+#include "util/hash_table.h"
+#include "util/macros.h"
+#include "util/simple_mtx.h"
+
+#include "drm-uapi/panfrost_drm.h"
+
+#include "pan_kmod_backend.h"
+
+const struct pan_kmod_ops panfrost_kmod_ops;
+
+struct panfrost_kmod_vm {
+   struct pan_kmod_vm base;
+#ifndef NDEBUG
+   /* We use va_to_bo to sanity check the VAs returned by the kernel,
+    * hence the #ifndef NDEBUG.
+    */
+   struct {
+      struct hash_table_u64 *ht;
+      simple_mtx_t lock;
+   } va_to_bo;
+#endif
+};
+
+struct panfrost_kmod_dev {
+   struct pan_kmod_dev base;
+   struct panfrost_kmod_vm *vm;
+};
+
+struct panfrost_kmod_bo {
+   struct pan_kmod_bo base;
+
+   /* This is actually the VA assigned to the BO at creation/import time.
+    * We don't control it, it's automatically assigned by the kernel driver.
+    */
+   uint64_t offset;
+};
+
+static struct pan_kmod_dev *
+panfrost_kmod_dev_create(int fd, drmVersionPtr version,
+                         const struct pan_kmod_allocator *allocator)
+{
+   struct panfrost_kmod_dev *panfrost_dev =
+      pan_kmod_alloc(allocator, sizeof(*panfrost_dev));
+   if (!panfrost_dev) {
+      mesa_loge("failed to allocate a panfrost_kmod_dev object");
+      return NULL;
+   }
+
+   pan_kmod_dev_init(&panfrost_dev->base, fd, version, &panfrost_kmod_ops,
+                     allocator);
+   return &panfrost_dev->base;
+}
+
+static void
+panfrost_kmod_dev_destroy(struct pan_kmod_dev *dev)
+{
+   struct panfrost_kmod_dev *panfrost_dev =
+      container_of(dev, struct panfrost_kmod_dev, base);
+
+   pan_kmod_dev_cleanup(dev);
+   pan_kmod_free(dev->allocator, panfrost_dev);
+}
+
+/* Abstraction over the raw drm_panfrost_get_param ioctl for fetching
+ * information about devices.
+ */
+static __u64
+panfrost_query_raw(int fd, enum drm_panfrost_param param, bool required,
+                   unsigned default_value)
+{
+   struct drm_panfrost_get_param get_param = {};
+   ASSERTED int ret;
+
+   get_param.param = param;
+   ret = drmIoctl(fd, DRM_IOCTL_PANFROST_GET_PARAM, &get_param);
+
+   if (ret) {
+      assert(!required);
+      return default_value;
+   }
+
+   return get_param.value;
+}
+
+static void
+panfrost_dev_query_props(struct pan_kmod_dev *dev,
+                         struct pan_kmod_dev_props *props)
+{
+   int fd = dev->fd;
+
+   memset(props, 0, sizeof(*props));
+   props->gpu_prod_id =
+      panfrost_query_raw(fd, DRM_PANFROST_PARAM_GPU_PROD_ID, true, 0);
+   props->gpu_revision =
+      panfrost_query_raw(fd, DRM_PANFROST_PARAM_GPU_REVISION, true, 0);
+   props->shader_present =
+      panfrost_query_raw(fd, DRM_PANFROST_PARAM_SHADER_PRESENT, false, 0xffff);
+   props->tiler_features =
+      panfrost_query_raw(fd, DRM_PANFROST_PARAM_TILER_FEATURES, false, 0x809);
+   props->mem_features =
+      panfrost_query_raw(fd, DRM_PANFROST_PARAM_MEM_FEATURES, true, 0);
+   props->mmu_features =
+      panfrost_query_raw(fd, DRM_PANFROST_PARAM_MEM_FEATURES, false, 0);
+
+   for (unsigned i = 0; i < ARRAY_SIZE(props->texture_features); i++) {
+      /* If unspecified, assume ASTC/ETC only. Factory default for Juno, and
+       * should exist on any Mali configuration. All hardware should report
+       * these texture formats but the kernel might not be new enough. */
+      static const uint32_t default_tex_features[4] = {0xfe001e, 0, 0, 0};
+
+      props->texture_features[i] =
+         panfrost_query_raw(fd, DRM_PANFROST_PARAM_TEXTURE_FEATURES0 + i, false,
+                            default_tex_features[i]);
+   }
+
+   props->thread_tls_alloc =
+      panfrost_query_raw(fd, DRM_PANFROST_PARAM_THREAD_TLS_ALLOC, false, 0);
+   props->afbc_features =
+      panfrost_query_raw(fd, DRM_PANFROST_PARAM_AFBC_FEATURES, false, 0);
+}
+
+static uint32_t
+to_panfrost_bo_flags(struct pan_kmod_dev *dev, uint32_t flags)
+{
+   uint32_t panfrost_flags = 0;
+
+   if (dev->driver.version.major > 1 || dev->driver.version.minor >= 1) {
+      /* The alloc-on-fault feature is only used for the tiler HEAP object,
+       * hence the name of the flag on panfrost.
+       */
+      if (flags & PAN_KMOD_BO_FLAG_ALLOC_ON_FAULT)
+         panfrost_flags |= PANFROST_BO_HEAP;
+
+      if (!(flags & PAN_KMOD_BO_FLAG_EXECUTABLE))
+         panfrost_flags |= PANFROST_BO_NOEXEC;
+   }
+
+   return panfrost_flags;
+}
+
+static struct pan_kmod_bo *
+panfrost_kmod_bo_alloc(struct pan_kmod_dev *dev,
+                       struct pan_kmod_vm *exclusive_vm, size_t size,
+                       uint32_t flags)
+{
+   /* We can't map GPU uncached. */
+   if (flags & PAN_KMOD_BO_FLAG_GPU_UNCACHED)
+      return NULL;
+
+   struct panfrost_kmod_bo *bo = pan_kmod_dev_alloc(dev, sizeof(*bo));
+   if (!bo)
+      return NULL;
+
+   struct drm_panfrost_create_bo req = {
+      .size = size,
+      .flags = to_panfrost_bo_flags(dev, flags),
+   };
+
+   int ret = drmIoctl(dev->fd, DRM_IOCTL_PANFROST_CREATE_BO, &req);
+   if (ret) {
+      mesa_loge("DRM_IOCTL_PANFROST_CREATE_BO failed (err=%d)", errno);
+      goto err_free_bo;
+   }
+
+   pan_kmod_bo_init(&bo->base, dev, exclusive_vm, req.size, flags, req.handle);
+   bo->offset = req.offset;
+   return &bo->base;
+
+err_free_bo:
+   pan_kmod_dev_free(dev, bo);
+   return NULL;
+}
+
+static void
+panfrost_kmod_bo_free(struct pan_kmod_bo *bo)
+{
+   drmCloseBufferHandle(bo->dev->fd, bo->handle);
+   pan_kmod_dev_free(bo->dev, bo);
+}
+
+static struct pan_kmod_bo *
+panfrost_kmod_bo_import(struct pan_kmod_dev *dev, uint32_t handle, size_t size,
+                        uint32_t flags)
+{
+   struct panfrost_kmod_bo *panfrost_bo =
+      pan_kmod_dev_alloc(dev, sizeof(*panfrost_bo));
+   if (!panfrost_bo) {
+      mesa_loge("failed to allocate a panfrost_kmod_bo object");
+      return NULL;
+   }
+
+   struct drm_panfrost_get_bo_offset get_bo_offset = {.handle = handle, 0};
+   int ret =
+      drmIoctl(dev->fd, DRM_IOCTL_PANFROST_GET_BO_OFFSET, &get_bo_offset);
+   if (ret) {
+      mesa_loge("DRM_IOCTL_PANFROST_GET_BO_OFFSET failed (err=%d)", errno);
+      goto err_free_bo;
+   }
+
+   panfrost_bo->offset = get_bo_offset.offset;
+
+   pan_kmod_bo_init(&panfrost_bo->base, dev, NULL, size,
+                    flags | PAN_KMOD_BO_FLAG_IMPORTED, handle);
+   return &panfrost_bo->base;
+
+err_free_bo:
+   pan_kmod_dev_free(dev, panfrost_bo);
+   return NULL;
+}
+
+static off_t
+panfrost_kmod_bo_get_mmap_offset(struct pan_kmod_bo *bo)
+{
+   struct drm_panfrost_mmap_bo mmap_bo = {.handle = bo->handle};
+   int ret = drmIoctl(bo->dev->fd, DRM_IOCTL_PANFROST_MMAP_BO, &mmap_bo);
+   if (ret) {
+      fprintf(stderr, "DRM_IOCTL_PANFROST_MMAP_BO failed: %m\n");
+      assert(0);
+   }
+
+   return mmap_bo.offset;
+}
+
+static bool
+panfrost_kmod_bo_wait(struct pan_kmod_bo *bo, int64_t timeout_ns,
+                      bool for_read_only_access)
+{
+   struct drm_panfrost_wait_bo req = {
+      .handle = bo->handle,
+      .timeout_ns = timeout_ns,
+   };
+
+   /* The ioctl returns >= 0 value when the BO we are waiting for is ready
+    * -1 otherwise.
+    */
+   if (drmIoctl(bo->dev->fd, DRM_IOCTL_PANFROST_WAIT_BO, &req) != -1)
+      return true;
+
+   assert(errno == ETIMEDOUT || errno == EBUSY);
+   return false;
+}
+
+static void
+panfrost_kmod_bo_make_evictable(struct pan_kmod_bo *bo)
+{
+   struct drm_panfrost_madvise req = {
+      .handle = bo->handle,
+      .madv = PANFROST_MADV_DONTNEED,
+   };
+
+   drmIoctl(bo->dev->fd, DRM_IOCTL_PANFROST_MADVISE, &req);
+}
+
+static bool
+panfrost_kmod_bo_make_unevictable(struct pan_kmod_bo *bo)
+{
+   struct drm_panfrost_madvise req = {
+      .handle = bo->handle,
+      .madv = PANFROST_MADV_WILLNEED,
+   };
+
+   if (drmIoctl(bo->dev->fd, DRM_IOCTL_PANFROST_MADVISE, &req) == 0 &&
+       req.retained == 0)
+      return false;
+
+   return true;
+}
+
+static struct pan_kmod_vm *
+panfrost_kmod_vm_create(struct pan_kmod_dev *dev, uint32_t flags,
+                        uint64_t va_start, uint64_t va_range)
+{
+   struct panfrost_kmod_dev *panfrost_dev =
+      container_of(dev, struct panfrost_kmod_dev, base);
+
+   /* Only one VM per device. */
+   if (panfrost_dev->vm) {
+      mesa_loge("failed to allocate a panfrost_kmod_dev object");
+      return NULL;
+   }
+
+   /* Panfrost kernel driver doesn't support userspace VA management. */
+   if (!(flags & PAN_KMOD_VM_FLAG_AUTO_VA)) {
+      mesa_loge("panfrost_kmod only supports PAN_KMOD_VM_FLAG_AUTO_VA");
+      return NULL;
+   }
+
+   /* Make sure he are being asked to manage a 32-bit address space with the
+    * lower 32MB reserved, for backward compatibility reasons.
+    */
+   if (va_start != 0x2000000 || va_start + va_range != 1ull << 32) {
+      mesa_loge("panfrost_kmod only supports a fixed VA range");
+      return NULL;
+   }
+
+   struct panfrost_kmod_vm *vm = pan_kmod_dev_alloc(dev, sizeof(*vm));
+   if (!vm) {
+      mesa_loge("failed to allocate a panfrost_kmod_vm object");
+      return NULL;
+   }
+
+   pan_kmod_vm_init(&vm->base, dev, 0, flags);
+
+#ifndef NDEBUG
+   vm->va_to_bo.ht = _mesa_hash_table_u64_create(NULL);
+   if (!vm->va_to_bo.ht) {
+      mesa_loge("failed to create the va_to_bo hash table");
+      pan_kmod_dev_free(dev, vm);
+      return NULL;
+   }
+   simple_mtx_init(&vm->va_to_bo.lock, mtx_plain);
+#endif
+
+   panfrost_dev->vm = vm;
+   return &vm->base;
+}
+
+static void
+panfrost_kmod_vm_destroy(struct pan_kmod_vm *vm)
+{
+   struct panfrost_kmod_vm *panfrost_vm =
+      container_of(vm, struct panfrost_kmod_vm, base);
+   struct panfrost_kmod_dev *panfrost_dev =
+      container_of(vm->dev, struct panfrost_kmod_dev, base);
+
+   panfrost_dev->vm = NULL;
+
+#ifndef NDEBUG
+   _mesa_hash_table_u64_destroy(panfrost_vm->va_to_bo.ht);
+   simple_mtx_destroy(&panfrost_vm->va_to_bo.lock);
+#endif
+
+   pan_kmod_dev_free(vm->dev, vm);
+}
+
+static int
+panfrost_kmod_vm_bind(struct pan_kmod_vm *vm, enum pan_kmod_vm_op_mode mode,
+                      struct pan_kmod_vm_op *ops, uint32_t op_count)
+{
+   UNUSED struct panfrost_kmod_vm *panfrost_vm =
+      container_of(vm, struct panfrost_kmod_vm, base);
+
+   /* We only support IMMEDIATE and WAIT_IDLE mode. Actually we always do
+    * WAIT_IDLE in practice, but it shouldn't matter.
+    */
+   if (mode != PAN_KMOD_VM_OP_MODE_IMMEDIATE &&
+       mode != PAN_KMOD_VM_OP_MODE_DEFER_TO_NEXT_IDLE_POINT)
+      return -1;
+
+   for (uint32_t i = 0; i < op_count; i++) {
+
+      if (ops[i].type == PAN_KMOD_VM_OP_TYPE_MAP) {
+         struct panfrost_kmod_bo *panfrost_bo =
+            container_of(ops[i].map.bo, struct panfrost_kmod_bo, base);
+
+         /* Panfrost kernel driver doesn't support userspace VA management. */
+         if (ops[i].va.start != PAN_KMOD_VM_MAP_AUTO_VA) {
+            mesa_loge("panfrost_kmod can only do auto-VA allocation");
+            return -1;
+         }
+
+         /* Panfrost kernel driver only support full BO mapping. */
+         if (ops[i].map.bo_offset != 0 || ops[i].va.size != ops[i].map.bo->size) {
+            mesa_loge("panfrost_kmod doesn't support partial BO mapping");
+            return -1;
+         }
+
+         ops[i].va.start = panfrost_bo->offset;
+
+#ifndef NDEBUG
+         simple_mtx_lock(&panfrost_vm->va_to_bo.lock);
+         assert(_mesa_hash_table_u64_search(panfrost_vm->va_to_bo.ht,
+                                            ops[i].va.start) == NULL);
+
+         _mesa_hash_table_u64_insert(panfrost_vm->va_to_bo.ht, ops[i].va.start,
+                                     ops[i].map.bo);
+         simple_mtx_unlock(&panfrost_vm->va_to_bo.lock);
+#endif
+      } else if (ops[i].type == PAN_KMOD_VM_OP_TYPE_UNMAP) {
+#ifndef NDEBUG
+         simple_mtx_lock(&panfrost_vm->va_to_bo.lock);
+         ASSERTED struct panfrost_kmod_bo *panfrost_bo =
+            _mesa_hash_table_u64_search(panfrost_vm->va_to_bo.ht,
+                                        ops[i].va.start);
+
+         assert(panfrost_bo && panfrost_bo->base.size == ops[i].va.size &&
+                panfrost_bo->offset == ops[i].va.start);
+
+         _mesa_hash_table_u64_remove(panfrost_vm->va_to_bo.ht, ops[i].va.start);
+         simple_mtx_unlock(&panfrost_vm->va_to_bo.lock);
+#endif
+      } else {
+         assert(!"VM operation not supported");
+      }
+   }
+
+   return 0;
+}
+
+const struct pan_kmod_ops panfrost_kmod_ops = {
+   .dev_create = panfrost_kmod_dev_create,
+   .dev_destroy = panfrost_kmod_dev_destroy,
+   .dev_query_props = panfrost_dev_query_props,
+   .bo_alloc = panfrost_kmod_bo_alloc,
+   .bo_free = panfrost_kmod_bo_free,
+   .bo_import = panfrost_kmod_bo_import,
+   .bo_get_mmap_offset = panfrost_kmod_bo_get_mmap_offset,
+   .bo_wait = panfrost_kmod_bo_wait,
+   .bo_make_evictable = panfrost_kmod_bo_make_evictable,
+   .bo_make_unevictable = panfrost_kmod_bo_make_unevictable,
+   .vm_create = panfrost_kmod_vm_create,
+   .vm_destroy = panfrost_kmod_vm_destroy,
+   .vm_bind = panfrost_kmod_vm_bind,
+};
-- 
GitLab


From 7001c8847c15f57ee3ce6164ee514d3c9c363a16 Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Mon, 20 Nov 2023 14:28:48 +0100
Subject: [PATCH 03/32] panfrost: Avoid direct accesses to some panfrost_device
 fields

We are about to delegate some device-related operations to the pan_kmod
layer, but before we can do that, we need to hide panfrost_device
internals so we can redirect such accesses to pan_kmod.

Provide a few panfrost_device_xxx() accessors and start using them.

Signed-off-by: Boris Brezillon <boris.brezillon@collabora.com>
---
 src/gallium/drivers/panfrost/pan_context.c | 11 ++++----
 src/gallium/drivers/panfrost/pan_fence.c   | 18 ++++++------
 src/gallium/drivers/panfrost/pan_jm.c      | 12 ++++----
 src/gallium/drivers/panfrost/pan_screen.c  | 11 ++++----
 src/gallium/drivers/panfrost/pan_shader.c  |  8 +++---
 src/panfrost/lib/pan_blend.c               |  4 +--
 src/panfrost/lib/pan_blitter.c             |  2 +-
 src/panfrost/lib/pan_bo.c                  | 32 +++++++++++++---------
 src/panfrost/lib/pan_device.h              | 30 ++++++++++++++++++++
 src/panfrost/lib/pan_indirect_dispatch.c   |  2 +-
 src/panfrost/lib/pan_props.c               |  8 +++---
 src/panfrost/perf/pan_perf.c               |  8 +++---
 src/panfrost/vulkan/panvk_device.c         | 24 +++++++++-------
 src/panfrost/vulkan/panvk_vX_device.c      | 32 ++++++++++++++--------
 src/panfrost/vulkan/panvk_vX_meta_clear.c  |  2 +-
 src/panfrost/vulkan/panvk_vX_meta_copy.c   | 10 +++----
 src/panfrost/vulkan/panvk_vX_shader.c      |  2 +-
 17 files changed, 133 insertions(+), 83 deletions(-)

diff --git a/src/gallium/drivers/panfrost/pan_context.c b/src/gallium/drivers/panfrost/pan_context.c
index 3c564ad54cec2..e1fed545c4bcf 100644
--- a/src/gallium/drivers/panfrost/pan_context.c
+++ b/src/gallium/drivers/panfrost/pan_context.c
@@ -562,11 +562,11 @@ panfrost_destroy(struct pipe_context *pipe)
    panfrost_pool_cleanup(&panfrost->shaders);
    panfrost_afbc_context_destroy(panfrost);
 
-   drmSyncobjDestroy(dev->fd, panfrost->in_sync_obj);
+   drmSyncobjDestroy(panfrost_device_fd(dev), panfrost->in_sync_obj);
    if (panfrost->in_sync_fd != -1)
       close(panfrost->in_sync_fd);
 
-   drmSyncobjDestroy(dev->fd, panfrost->syncobj);
+   drmSyncobjDestroy(panfrost_device_fd(dev), panfrost->syncobj);
    ralloc_free(pipe);
 }
 
@@ -860,7 +860,7 @@ panfrost_fence_server_sync(struct pipe_context *pctx,
    struct panfrost_context *ctx = pan_context(pctx);
    int fd = -1, ret;
 
-   ret = drmSyncobjExportSyncFile(dev->fd, f->syncobj, &fd);
+   ret = drmSyncobjExportSyncFile(panfrost_device_fd(dev), f->syncobj, &fd);
    assert(!ret);
 
    sync_accumulate("panfrost", &ctx->in_sync_fd, fd);
@@ -973,12 +973,13 @@ panfrost_create_context(struct pipe_screen *screen, void *priv, unsigned flags)
    /* Create a syncobj in a signaled state. Will be updated to point to the
     * last queued job out_sync every time we submit a new job.
     */
-   ret = drmSyncobjCreate(dev->fd, DRM_SYNCOBJ_CREATE_SIGNALED, &ctx->syncobj);
+   ret = drmSyncobjCreate(panfrost_device_fd(dev), DRM_SYNCOBJ_CREATE_SIGNALED,
+                          &ctx->syncobj);
    assert(!ret && ctx->syncobj);
 
    /* Sync object/FD used for NATIVE_FENCE_FD. */
    ctx->in_sync_fd = -1;
-   ret = drmSyncobjCreate(dev->fd, 0, &ctx->in_sync_obj);
+   ret = drmSyncobjCreate(panfrost_device_fd(dev), 0, &ctx->in_sync_obj);
    assert(!ret);
 
    return gallium;
diff --git a/src/gallium/drivers/panfrost/pan_fence.c b/src/gallium/drivers/panfrost/pan_fence.c
index 792550371f87f..1390ddfe4001a 100644
--- a/src/gallium/drivers/panfrost/pan_fence.c
+++ b/src/gallium/drivers/panfrost/pan_fence.c
@@ -42,7 +42,7 @@ panfrost_fence_reference(struct pipe_screen *pscreen,
    struct pipe_fence_handle *old = *ptr;
 
    if (pipe_reference(&old->reference, &fence->reference)) {
-      drmSyncobjDestroy(dev->fd, old->syncobj);
+      drmSyncobjDestroy(panfrost_device_fd(dev), old->syncobj);
       free(old);
    }
 
@@ -63,8 +63,8 @@ panfrost_fence_finish(struct pipe_screen *pscreen, struct pipe_context *ctx,
    if (abs_timeout == OS_TIMEOUT_INFINITE)
       abs_timeout = INT64_MAX;
 
-   ret = drmSyncobjWait(dev->fd, &fence->syncobj, 1, abs_timeout,
-                        DRM_SYNCOBJ_WAIT_FLAGS_WAIT_ALL, NULL);
+   ret = drmSyncobjWait(panfrost_device_fd(dev), &fence->syncobj, 1,
+                        abs_timeout, DRM_SYNCOBJ_WAIT_FLAGS_WAIT_ALL, NULL);
 
    fence->signaled = (ret >= 0);
    return fence->signaled;
@@ -76,7 +76,7 @@ panfrost_fence_get_fd(struct pipe_screen *screen, struct pipe_fence_handle *f)
    struct panfrost_device *dev = pan_device(screen);
    int fd = -1;
 
-   drmSyncobjExportSyncFile(dev->fd, f->syncobj, &fd);
+   drmSyncobjExportSyncFile(panfrost_device_fd(dev), f->syncobj, &fd);
    return fd;
 }
 
@@ -92,20 +92,20 @@ panfrost_fence_from_fd(struct panfrost_context *ctx, int fd,
       return NULL;
 
    if (type == PIPE_FD_TYPE_NATIVE_SYNC) {
-      ret = drmSyncobjCreate(dev->fd, 0, &f->syncobj);
+      ret = drmSyncobjCreate(panfrost_device_fd(dev), 0, &f->syncobj);
       if (ret) {
          fprintf(stderr, "create syncobj failed\n");
          goto err_free_fence;
       }
 
-      ret = drmSyncobjImportSyncFile(dev->fd, f->syncobj, fd);
+      ret = drmSyncobjImportSyncFile(panfrost_device_fd(dev), f->syncobj, fd);
       if (ret) {
          fprintf(stderr, "import syncfile failed\n");
          goto err_destroy_syncobj;
       }
    } else {
       assert(type == PIPE_FD_TYPE_SYNCOBJ);
-      ret = drmSyncobjFDToHandle(dev->fd, fd, &f->syncobj);
+      ret = drmSyncobjFDToHandle(panfrost_device_fd(dev), fd, &f->syncobj);
       if (ret) {
          fprintf(stderr, "import syncobj FD failed\n");
          goto err_free_fence;
@@ -117,7 +117,7 @@ panfrost_fence_from_fd(struct panfrost_context *ctx, int fd,
    return f;
 
 err_destroy_syncobj:
-   drmSyncobjDestroy(dev->fd, f->syncobj);
+   drmSyncobjDestroy(panfrost_device_fd(dev), f->syncobj);
 err_free_fence:
    free(f);
    return NULL;
@@ -134,7 +134,7 @@ panfrost_fence_create(struct panfrost_context *ctx)
     * (HandleToFD/FDToHandle just gives you another syncobj ID for the
     * same syncobj).
     */
-   ret = drmSyncobjExportSyncFile(dev->fd, ctx->syncobj, &fd);
+   ret = drmSyncobjExportSyncFile(panfrost_device_fd(dev), ctx->syncobj, &fd);
    if (ret || fd == -1) {
       fprintf(stderr, "export failed\n");
       return NULL;
diff --git a/src/gallium/drivers/panfrost/pan_jm.c b/src/gallium/drivers/panfrost/pan_jm.c
index a2270f55bebca..518203dcbcacc 100644
--- a/src/gallium/drivers/panfrost/pan_jm.c
+++ b/src/gallium/drivers/panfrost/pan_jm.c
@@ -97,8 +97,8 @@ jm_submit_jc(struct panfrost_batch *batch, mali_ptr first_job_desc,
    submit.requirements = reqs;
 
    if (ctx->in_sync_fd >= 0) {
-      ret =
-         drmSyncobjImportSyncFile(dev->fd, ctx->in_sync_obj, ctx->in_sync_fd);
+      ret = drmSyncobjImportSyncFile(panfrost_device_fd(dev), ctx->in_sync_obj,
+                                     ctx->in_sync_fd);
       assert(!ret);
 
       in_syncs[submit.in_sync_count++] = ctx->in_sync_obj;
@@ -158,7 +158,7 @@ jm_submit_jc(struct panfrost_batch *batch, mali_ptr first_job_desc,
    if (ctx->is_noop)
       ret = 0;
    else
-      ret = drmIoctl(dev->fd, DRM_IOCTL_PANFROST_SUBMIT, &submit);
+      ret = drmIoctl(panfrost_device_fd(dev), DRM_IOCTL_PANFROST_SUBMIT, &submit);
    free(bo_handles);
 
    if (ret)
@@ -167,17 +167,17 @@ jm_submit_jc(struct panfrost_batch *batch, mali_ptr first_job_desc,
    /* Trace the job if we're doing that */
    if (dev->debug & (PAN_DBG_TRACE | PAN_DBG_SYNC)) {
       /* Wait so we can get errors reported back */
-      drmSyncobjWait(dev->fd, &out_sync, 1, INT64_MAX, 0, NULL);
+      drmSyncobjWait(panfrost_device_fd(dev), &out_sync, 1, INT64_MAX, 0, NULL);
 
       if (dev->debug & PAN_DBG_TRACE)
-         pandecode_jc(dev->decode_ctx, submit.jc, dev->gpu_id);
+         pandecode_jc(dev->decode_ctx, submit.jc, panfrost_device_gpu_id(dev));
 
       if (dev->debug & PAN_DBG_DUMP)
          pandecode_dump_mappings(dev->decode_ctx);
 
       /* Jobs won't be complete if blackhole rendering, that's ok */
       if (!ctx->is_noop && dev->debug & PAN_DBG_SYNC)
-         pandecode_abort_on_fault(dev->decode_ctx, submit.jc, dev->gpu_id);
+         pandecode_abort_on_fault(dev->decode_ctx, submit.jc, panfrost_device_gpu_id(dev));
    }
 
    return 0;
diff --git a/src/gallium/drivers/panfrost/pan_screen.c b/src/gallium/drivers/panfrost/pan_screen.c
index 2b12dfa8a6968..876833e799192 100644
--- a/src/gallium/drivers/panfrost/pan_screen.c
+++ b/src/gallium/drivers/panfrost/pan_screen.c
@@ -109,8 +109,8 @@ panfrost_get_param(struct pipe_screen *screen, enum pipe_cap param)
    bool has_mrt = (dev->arch >= 5);
 
    /* Only kernel drivers >= 1.1 can allocate HEAP BOs */
-   bool has_heap = dev->kernel_version->version_major > 1 ||
-                   dev->kernel_version->version_minor >= 1;
+   bool has_heap = panfrost_device_kmod_version_major(dev) > 1 ||
+                   panfrost_device_kmod_version_minor(dev) >= 1;
 
    switch (param) {
    case PIPE_CAP_NPOT_TEXTURES:
@@ -143,7 +143,7 @@ panfrost_get_param(struct pipe_screen *screen, enum pipe_cap param)
       return true;
 
    case PIPE_CAP_ANISOTROPIC_FILTER:
-      return dev->revision >= dev->model->min_rev_anisotropic;
+      return panfrost_device_gpu_rev(dev) >= dev->model->min_rev_anisotropic;
 
    /* Compile side is done for Bifrost, Midgard TODO. Needs some kernel
     * work to turn on, since CYCLE_COUNT_START needs to be issued. In
@@ -807,7 +807,7 @@ panfrost_get_disk_shader_cache(struct pipe_screen *pscreen)
 static int
 panfrost_get_screen_fd(struct pipe_screen *pscreen)
 {
-   return pan_device(pscreen)->fd;
+   return panfrost_device_fd(pan_device(pscreen));
 }
 
 int
@@ -851,7 +851,8 @@ panfrost_create_screen(int fd, const struct pipe_screen_config *config,
 
    /* Bail early on unsupported hardware */
    if (dev->model == NULL) {
-      debug_printf("panfrost: Unsupported model %X", dev->gpu_id);
+      debug_printf("panfrost: Unsupported model %X",
+                   panfrost_device_gpu_id(dev));
       panfrost_destroy_screen(&(screen->base));
       return NULL;
    }
diff --git a/src/gallium/drivers/panfrost/pan_shader.c b/src/gallium/drivers/panfrost/pan_shader.c
index 546910f8e6687..f0d46d32629e3 100644
--- a/src/gallium/drivers/panfrost/pan_shader.c
+++ b/src/gallium/drivers/panfrost/pan_shader.c
@@ -84,11 +84,11 @@ panfrost_shader_compile(struct panfrost_screen *screen, const nir_shader *ir,
     * happens at CSO create time regardless.
     */
    if (gl_shader_stage_is_compute(s->info.stage))
-      pan_shader_preprocess(s, dev->gpu_id);
+      pan_shader_preprocess(s, panfrost_device_gpu_id(dev));
 
    struct panfrost_compile_inputs inputs = {
       .debug = dbg,
-      .gpu_id = dev->gpu_id,
+      .gpu_id = panfrost_device_gpu_id(dev),
    };
 
    /* Lower this early so the backends don't have to worry about it */
@@ -130,7 +130,7 @@ panfrost_shader_compile(struct panfrost_screen *screen, const nir_shader *ir,
    if (dev->arch <= 5 && s->info.stage == MESA_SHADER_FRAGMENT) {
       NIR_PASS_V(s, pan_lower_framebuffer, key->fs.rt_formats,
                  pan_raw_format_mask_midgard(key->fs.rt_formats), 0,
-                 dev->gpu_id < 0x700);
+                 panfrost_device_gpu_id(dev) < 0x700);
    }
 
    NIR_PASS_V(s, panfrost_nir_lower_sysvals, &out->sysvals);
@@ -375,7 +375,7 @@ panfrost_create_shader_state(struct pipe_context *pctx,
 
    /* Then run the suite of lowering and optimization, including I/O lowering */
    struct panfrost_device *dev = pan_device(pctx->screen);
-   pan_shader_preprocess(nir, dev->gpu_id);
+   pan_shader_preprocess(nir, panfrost_device_gpu_id(dev));
 
    /* If this shader uses transform feedback, compile the transform
     * feedback program. This is a special shader variant.
diff --git a/src/panfrost/lib/pan_blend.c b/src/panfrost/lib/pan_blend.c
index 558ecb5e64c0f..3fb3c00663db2 100644
--- a/src/panfrost/lib/pan_blend.c
+++ b/src/panfrost/lib/pan_blend.c
@@ -878,7 +878,7 @@ GENX(pan_blend_get_shader_locked)(const struct panfrost_device *dev,
 
    /* Compile the NIR shader */
    struct panfrost_compile_inputs inputs = {
-      .gpu_id = dev->gpu_id,
+      .gpu_id = panfrost_device_gpu_id(dev),
       .is_blend = true,
       .blend.nr_samples = key.nr_samples,
    };
@@ -899,7 +899,7 @@ GENX(pan_blend_get_shader_locked)(const struct panfrost_device *dev,
 #else
    NIR_PASS_V(nir, pan_lower_framebuffer, rt_formats,
               pan_raw_format_mask_midgard(rt_formats), MAX2(key.nr_samples, 1),
-              dev->gpu_id < 0x700);
+              panfrost_device_gpu_id(dev) < 0x700);
 #endif
 
    GENX(pan_shader_compile)(nir, &inputs, &variant->binary, &info);
diff --git a/src/panfrost/lib/pan_blitter.c b/src/panfrost/lib/pan_blitter.c
index c195763d337a5..84bcba6361fa8 100644
--- a/src/panfrost/lib/pan_blitter.c
+++ b/src/panfrost/lib/pan_blitter.c
@@ -606,7 +606,7 @@ pan_blitter_get_blit_shader(struct panfrost_device *dev,
    }
 
    struct panfrost_compile_inputs inputs = {
-      .gpu_id = dev->gpu_id,
+      .gpu_id = panfrost_device_gpu_id(dev),
       .is_blit = true,
       .no_idvs = true,
    };
diff --git a/src/panfrost/lib/pan_bo.c b/src/panfrost/lib/pan_bo.c
index b84eeeef1b571..9018b6ac9f58d 100644
--- a/src/panfrost/lib/pan_bo.c
+++ b/src/panfrost/lib/pan_bo.c
@@ -63,15 +63,16 @@ panfrost_bo_alloc(struct panfrost_device *dev, size_t size, uint32_t flags,
    struct panfrost_bo *bo;
    int ret;
 
-   if (dev->kernel_version->version_major > 1 ||
-       dev->kernel_version->version_minor >= 1) {
+   if (panfrost_device_kmod_version_major(dev) > 1 ||
+       panfrost_device_kmod_version_minor(dev) >= 1) {
       if (flags & PAN_BO_GROWABLE)
          create_bo.flags |= PANFROST_BO_HEAP;
       if (!(flags & PAN_BO_EXECUTE))
          create_bo.flags |= PANFROST_BO_NOEXEC;
    }
 
-   ret = drmIoctl(dev->fd, DRM_IOCTL_PANFROST_CREATE_BO, &create_bo);
+   ret = drmIoctl(panfrost_device_fd(dev), DRM_IOCTL_PANFROST_CREATE_BO,
+                  &create_bo);
    if (ret) {
       fprintf(stderr, "DRM_IOCTL_PANFROST_CREATE_BO failed: %m\n");
       return NULL;
@@ -93,7 +94,7 @@ static void
 panfrost_bo_free(struct panfrost_bo *bo)
 {
    struct drm_gem_close gem_close = {.handle = bo->gem_handle};
-   int fd = bo->dev->fd;
+   int fd = panfrost_device_fd(bo->dev);
    int ret;
 
    /* BO will be freed with the sparse array, but zero to indicate free */
@@ -138,7 +139,8 @@ panfrost_bo_wait(struct panfrost_bo *bo, int64_t timeout_ns, bool wait_readers)
    /* The ioctl returns >= 0 value when the BO we are waiting for is ready
     * -1 otherwise.
     */
-   ret = drmIoctl(bo->dev->fd, DRM_IOCTL_PANFROST_WAIT_BO, &req);
+   ret =
+      drmIoctl(panfrost_device_fd(bo->dev), DRM_IOCTL_PANFROST_WAIT_BO, &req);
    if (ret != -1) {
       /* Set gpu_access to 0 so that the next call to bo_wait()
        * doesn't have to call the WAIT_BO ioctl.
@@ -211,7 +213,8 @@ panfrost_bo_cache_fetch(struct panfrost_device *dev, size_t size,
       list_del(&entry->bucket_link);
       list_del(&entry->lru_link);
 
-      ret = drmIoctl(dev->fd, DRM_IOCTL_PANFROST_MADVISE, &madv);
+      ret =
+         drmIoctl(panfrost_device_fd(dev), DRM_IOCTL_PANFROST_MADVISE, &madv);
       if (!ret && !madv.retained) {
          panfrost_bo_free(entry);
          continue;
@@ -273,7 +276,7 @@ panfrost_bo_cache_put(struct panfrost_bo *bo)
    madv.madv = PANFROST_MADV_DONTNEED;
    madv.retained = 0;
 
-   drmIoctl(dev->fd, DRM_IOCTL_PANFROST_MADVISE, &madv);
+   drmIoctl(panfrost_device_fd(dev), DRM_IOCTL_PANFROST_MADVISE, &madv);
 
    /* Add us to the bucket */
    list_addtail(&bo->bucket_link, bucket);
@@ -327,19 +330,20 @@ panfrost_bo_mmap(struct panfrost_bo *bo)
    if (bo->ptr.cpu)
       return;
 
-   ret = drmIoctl(bo->dev->fd, DRM_IOCTL_PANFROST_MMAP_BO, &mmap_bo);
+   ret = drmIoctl(panfrost_device_fd(bo->dev), DRM_IOCTL_PANFROST_MMAP_BO,
+                  &mmap_bo);
    if (ret) {
       fprintf(stderr, "DRM_IOCTL_PANFROST_MMAP_BO failed: %m\n");
       assert(0);
    }
 
    bo->ptr.cpu = os_mmap(NULL, bo->size, PROT_READ | PROT_WRITE, MAP_SHARED,
-                         bo->dev->fd, mmap_bo.offset);
+                         panfrost_device_fd(bo->dev), mmap_bo.offset);
    if (bo->ptr.cpu == MAP_FAILED) {
       bo->ptr.cpu = NULL;
       fprintf(stderr,
               "mmap failed: result=%p size=0x%llx fd=%i offset=0x%llx %m\n",
-              bo->ptr.cpu, (long long)bo->size, bo->dev->fd,
+              bo->ptr.cpu, (long long)bo->size, panfrost_device_fd(bo->dev),
               (long long)mmap_bo.offset);
    }
 }
@@ -470,14 +474,15 @@ panfrost_bo_import(struct panfrost_device *dev, int fd)
 
    pthread_mutex_lock(&dev->bo_map_lock);
 
-   ret = drmPrimeFDToHandle(dev->fd, fd, &gem_handle);
+   ret = drmPrimeFDToHandle(panfrost_device_fd(dev), fd, &gem_handle);
    assert(!ret);
 
    bo = pan_lookup_bo(dev, gem_handle);
 
    if (!bo->dev) {
       get_bo_offset.handle = gem_handle;
-      ret = drmIoctl(dev->fd, DRM_IOCTL_PANFROST_GET_BO_OFFSET, &get_bo_offset);
+      ret = drmIoctl(panfrost_device_fd(dev), DRM_IOCTL_PANFROST_GET_BO_OFFSET,
+                     &get_bo_offset);
       assert(!ret);
 
       bo->dev = dev;
@@ -523,7 +528,8 @@ panfrost_bo_export(struct panfrost_bo *bo)
       .flags = DRM_CLOEXEC,
    };
 
-   int ret = drmIoctl(bo->dev->fd, DRM_IOCTL_PRIME_HANDLE_TO_FD, &args);
+   int ret = drmIoctl(panfrost_device_fd(bo->dev), DRM_IOCTL_PRIME_HANDLE_TO_FD,
+                      &args);
    if (ret == -1)
       return -1;
 
diff --git a/src/panfrost/lib/pan_device.h b/src/panfrost/lib/pan_device.h
index f21361aefdc92..d185439fb5fab 100644
--- a/src/panfrost/lib/pan_device.h
+++ b/src/panfrost/lib/pan_device.h
@@ -215,6 +215,36 @@ struct panfrost_device {
    struct panfrost_bo *sample_positions;
 };
 
+static inline int
+panfrost_device_fd(const struct panfrost_device *dev)
+{
+   return dev->fd;
+}
+
+static inline uint32_t
+panfrost_device_gpu_id(const struct panfrost_device *dev)
+{
+   return dev->gpu_id;
+}
+
+static inline uint32_t
+panfrost_device_gpu_rev(const struct panfrost_device *dev)
+{
+   return dev->revision;
+}
+
+static inline int
+panfrost_device_kmod_version_major(const struct panfrost_device *dev)
+{
+   return dev->kernel_version->version_major;
+}
+
+static inline int
+panfrost_device_kmod_version_minor(const struct panfrost_device *dev)
+{
+   return dev->kernel_version->version_minor;
+}
+
 void panfrost_open_device(void *memctx, int fd, struct panfrost_device *dev);
 
 void panfrost_close_device(struct panfrost_device *dev);
diff --git a/src/panfrost/lib/pan_indirect_dispatch.c b/src/panfrost/lib/pan_indirect_dispatch.c
index 3fa8e95e07f09..82ddaa4cee315 100644
--- a/src/panfrost/lib/pan_indirect_dispatch.c
+++ b/src/panfrost/lib/pan_indirect_dispatch.c
@@ -121,7 +121,7 @@ pan_indirect_dispatch_init(struct panfrost_device *dev)
    nir_pop_if(&b, NULL);
 
    struct panfrost_compile_inputs inputs = {
-      .gpu_id = dev->gpu_id,
+      .gpu_id = panfrost_device_gpu_id(dev),
       .no_ubo_to_push = true,
    };
    struct pan_shader_info shader_info;
diff --git a/src/panfrost/lib/pan_props.c b/src/panfrost/lib/pan_props.c
index f63f320eadffd..b0b60db9fa751 100644
--- a/src/panfrost/lib/pan_props.c
+++ b/src/panfrost/lib/pan_props.c
@@ -132,8 +132,8 @@ unsigned
 panfrost_query_l2_slices(const struct panfrost_device *dev)
 {
    /* Query MEM_FEATURES register */
-   uint32_t mem_features =
-      panfrost_query_raw(dev->fd, DRM_PANFROST_PARAM_MEM_FEATURES, true, 0);
+   uint32_t mem_features = panfrost_query_raw(
+      panfrost_device_fd(dev), DRM_PANFROST_PARAM_MEM_FEATURES, true, 0);
 
    /* L2_SLICES is MEM_FEATURES[11:8] minus(1) */
    return ((mem_features >> 8) & 0xF) + 1;
@@ -251,10 +251,10 @@ panfrost_open_device(void *memctx, int fd, struct panfrost_device *dev)
    dev->fd = fd;
    dev->memctx = memctx;
    dev->gpu_id = panfrost_query_gpu_version(fd);
-   dev->arch = pan_arch(dev->gpu_id);
+   dev->arch = pan_arch(panfrost_device_gpu_id(dev));
    dev->kernel_version = drmGetVersion(fd);
    dev->revision = panfrost_query_gpu_revision(fd);
-   dev->model = panfrost_get_model(dev->gpu_id);
+   dev->model = panfrost_get_model(panfrost_device_gpu_id(dev));
 
    if (!dev->kernel_version)
       return;
diff --git a/src/panfrost/perf/pan_perf.c b/src/panfrost/perf/pan_perf.c
index c35d0f87c0239..86b121ac3fde2 100644
--- a/src/panfrost/perf/pan_perf.c
+++ b/src/panfrost/perf/pan_perf.c
@@ -92,8 +92,8 @@ static int
 panfrost_perf_query(struct panfrost_perf *perf, uint32_t enable)
 {
    struct drm_panfrost_perfcnt_enable perfcnt_enable = {enable, 0};
-   return drmIoctl(perf->dev->fd, DRM_IOCTL_PANFROST_PERFCNT_ENABLE,
-                   &perfcnt_enable);
+   return drmIoctl(panfrost_device_fd(perf->dev),
+                   DRM_IOCTL_PANFROST_PERFCNT_ENABLE, &perfcnt_enable);
 }
 
 int
@@ -115,6 +115,6 @@ panfrost_perf_dump(struct panfrost_perf *perf)
    // counter_values
    struct drm_panfrost_perfcnt_dump perfcnt_dump = {
       (uint64_t)(uintptr_t)perf->counter_values};
-   return drmIoctl(perf->dev->fd, DRM_IOCTL_PANFROST_PERFCNT_DUMP,
-                   &perfcnt_dump);
+   return drmIoctl(panfrost_device_fd(perf->dev),
+                   DRM_IOCTL_PANFROST_PERFCNT_DUMP, &perfcnt_dump);
 }
diff --git a/src/panfrost/vulkan/panvk_device.c b/src/panfrost/vulkan/panvk_device.c
index 1db43d95b98b5..88f2c58d170a5 100644
--- a/src/panfrost/vulkan/panvk_device.c
+++ b/src/panfrost/vulkan/panvk_device.c
@@ -460,7 +460,8 @@ panvk_physical_device_init(struct panvk_physical_device *device,
    memset(device->name, 0, sizeof(device->name));
    sprintf(device->name, "%s", device->pdev.model->name);
 
-   if (panvk_device_get_cache_uuid(device->pdev.gpu_id, device->cache_uuid)) {
+   if (panvk_device_get_cache_uuid(panfrost_device_gpu_id(&device->pdev),
+                                   device->cache_uuid)) {
       result = vk_errorf(instance, VK_ERROR_INITIALIZATION_FAILED,
                          "cannot generate UUID");
       goto fail_close_device;
@@ -471,7 +472,8 @@ panvk_physical_device_init(struct panvk_physical_device *device,
    panvk_get_driver_uuid(&device->device_uuid);
    panvk_get_device_uuid(&device->device_uuid);
 
-   device->drm_syncobj_type = vk_drm_syncobj_get_type(device->pdev.fd);
+   device->drm_syncobj_type =
+      vk_drm_syncobj_get_type(panfrost_device_fd(&device->pdev));
    /* We don't support timelines in the uAPI yet and we don't want it getting
     * suddenly turned on by vk_drm_syncobj_get_type() without us adding panvk
     * code for it first.
@@ -800,7 +802,8 @@ panvk_queue_init(struct panvk_device *device, struct panvk_queue *queue,
       .flags = DRM_SYNCOBJ_CREATE_SIGNALED,
    };
 
-   int ret = drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_CREATE, &create);
+   int ret =
+      drmIoctl(panfrost_device_fd(pdev), DRM_IOCTL_SYNCOBJ_CREATE, &create);
    if (ret) {
       vk_queue_finish(&queue->vk);
       return VK_ERROR_OUT_OF_HOST_MEMORY;
@@ -897,7 +900,7 @@ panvk_CreateDevice(VkPhysicalDevice physicalDevice,
    device->physical_device = physical_device;
 
    const struct panfrost_device *pdev = &physical_device->pdev;
-   vk_device_set_drm_fd(&device->vk, pdev->fd);
+   vk_device_set_drm_fd(&device->vk, panfrost_device_fd(pdev));
 
    for (unsigned i = 0; i < pCreateInfo->queueCreateInfoCount; i++) {
       const VkDeviceQueueCreateInfo *queue_create =
@@ -983,7 +986,7 @@ panvk_QueueWaitIdle(VkQueue _queue)
    };
    int ret;
 
-   ret = drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_WAIT, &wait);
+   ret = drmIoctl(panfrost_device_fd(pdev), DRM_IOCTL_SYNCOBJ_WAIT, &wait);
    assert(!ret);
 
    return VK_SUCCESS;
@@ -1247,7 +1250,8 @@ panvk_CreateEvent(VkDevice _device, const VkEventCreateInfo *pCreateInfo,
       .flags = 0,
    };
 
-   int ret = drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_CREATE, &create);
+   int ret =
+      drmIoctl(panfrost_device_fd(pdev), DRM_IOCTL_SYNCOBJ_CREATE, &create);
    if (ret)
       return VK_ERROR_OUT_OF_HOST_MEMORY;
 
@@ -1269,7 +1273,7 @@ panvk_DestroyEvent(VkDevice _device, VkEvent _event,
       return;
 
    struct drm_syncobj_destroy destroy = {.handle = event->syncobj};
-   drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_DESTROY, &destroy);
+   drmIoctl(panfrost_device_fd(pdev), DRM_IOCTL_SYNCOBJ_DESTROY, &destroy);
 
    vk_object_free(&device->vk, pAllocator, event);
 }
@@ -1289,7 +1293,7 @@ panvk_GetEventStatus(VkDevice _device, VkEvent _event)
       .flags = DRM_SYNCOBJ_WAIT_FLAGS_WAIT_FOR_SUBMIT,
    };
 
-   int ret = drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_WAIT, &wait);
+   int ret = drmIoctl(panfrost_device_fd(pdev), DRM_IOCTL_SYNCOBJ_WAIT, &wait);
    if (ret) {
       if (errno == ETIME)
          signaled = false;
@@ -1320,7 +1324,7 @@ panvk_SetEvent(VkDevice _device, VkEvent _event)
     * command executes.
     * https://www.khronos.org/registry/vulkan/specs/1.2/html/chap6.html#commandbuffers-submission-progress
     */
-   if (drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_SIGNAL, &objs))
+   if (drmIoctl(panfrost_device_fd(pdev), DRM_IOCTL_SYNCOBJ_SIGNAL, &objs))
       return VK_ERROR_DEVICE_LOST;
 
    return VK_SUCCESS;
@@ -1337,7 +1341,7 @@ panvk_ResetEvent(VkDevice _device, VkEvent _event)
       .handles = (uint64_t)(uintptr_t)&event->syncobj,
       .count_handles = 1};
 
-   if (drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_RESET, &objs))
+   if (drmIoctl(panfrost_device_fd(pdev), DRM_IOCTL_SYNCOBJ_RESET, &objs))
       return VK_ERROR_DEVICE_LOST;
 
    return VK_SUCCESS;
diff --git a/src/panfrost/vulkan/panvk_vX_device.c b/src/panfrost/vulkan/panvk_vX_device.c
index e10e21d9ecc72..8467b86b0216c 100644
--- a/src/panfrost/vulkan/panvk_vX_device.c
+++ b/src/panfrost/vulkan/panvk_vX_device.c
@@ -67,17 +67,20 @@ panvk_queue_submit_batch(struct panvk_queue *queue, struct panvk_batch *batch,
          .jc = batch->jc.first_job,
       };
 
-      ret = drmIoctl(pdev->fd, DRM_IOCTL_PANFROST_SUBMIT, &submit);
+      ret =
+         drmIoctl(panfrost_device_fd(pdev), DRM_IOCTL_PANFROST_SUBMIT, &submit);
       assert(!ret);
 
       if (debug & (PANVK_DEBUG_TRACE | PANVK_DEBUG_SYNC)) {
-         ret =
-            drmSyncobjWait(pdev->fd, &submit.out_sync, 1, INT64_MAX, 0, NULL);
+         ret = drmSyncobjWait(panfrost_device_fd(pdev), &submit.out_sync, 1,
+                              INT64_MAX, 0, NULL);
          assert(!ret);
       }
 
-      if (debug & PANVK_DEBUG_TRACE)
-         pandecode_jc(pdev->decode_ctx, batch->jc.first_job, pdev->gpu_id);
+      if (debug & PANVK_DEBUG_TRACE) {
+         pandecode_jc(pdev->decode_ctx, batch->jc.first_job,
+                      panfrost_device_gpu_id(pdev));
+      }
 
       if (debug & PANVK_DEBUG_DUMP)
          pandecode_dump_mappings(pdev->decode_ctx);
@@ -100,16 +103,18 @@ panvk_queue_submit_batch(struct panvk_queue *queue, struct panvk_batch *batch,
          submit.in_sync_count = nr_in_fences;
       }
 
-      ret = drmIoctl(pdev->fd, DRM_IOCTL_PANFROST_SUBMIT, &submit);
+      ret =
+         drmIoctl(panfrost_device_fd(pdev), DRM_IOCTL_PANFROST_SUBMIT, &submit);
       assert(!ret);
       if (debug & (PANVK_DEBUG_TRACE | PANVK_DEBUG_SYNC)) {
-         ret =
-            drmSyncobjWait(pdev->fd, &submit.out_sync, 1, INT64_MAX, 0, NULL);
+         ret = drmSyncobjWait(panfrost_device_fd(pdev), &submit.out_sync, 1,
+                              INT64_MAX, 0, NULL);
          assert(!ret);
       }
 
       if (debug & PANVK_DEBUG_TRACE)
-         pandecode_jc(pdev->decode_ctx, batch->fragment_job, pdev->gpu_id);
+         pandecode_jc(pdev->decode_ctx, batch->fragment_job,
+                      panfrost_device_gpu_id(pdev));
 
       if (debug & PANVK_DEBUG_DUMP)
          pandecode_dump_mappings(pdev->decode_ctx);
@@ -133,12 +138,14 @@ panvk_queue_transfer_sync(struct panvk_queue *queue, uint32_t syncobj)
       .fd = -1,
    };
 
-   ret = drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_HANDLE_TO_FD, &handle);
+   ret = drmIoctl(panfrost_device_fd(pdev), DRM_IOCTL_SYNCOBJ_HANDLE_TO_FD,
+                  &handle);
    assert(!ret);
    assert(handle.fd >= 0);
 
    handle.handle = syncobj;
-   ret = drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_FD_TO_HANDLE, &handle);
+   ret = drmIoctl(panfrost_device_fd(pdev), DRM_IOCTL_SYNCOBJ_FD_TO_HANDLE,
+                  &handle);
    assert(!ret);
 
    close(handle.fd);
@@ -184,7 +191,8 @@ panvk_signal_event_syncobjs(struct panvk_queue *queue,
             .handles = (uint64_t)(uintptr_t)&event->syncobj,
             .count_handles = 1};
 
-         int ret = drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_RESET, &objs);
+         int ret =
+            drmIoctl(panfrost_device_fd(pdev), DRM_IOCTL_SYNCOBJ_RESET, &objs);
          assert(!ret);
          break;
       }
diff --git a/src/panfrost/vulkan/panvk_vX_meta_clear.c b/src/panfrost/vulkan/panvk_vX_meta_clear.c
index 4238c8352af13..3e92ccb4823b1 100644
--- a/src/panfrost/vulkan/panvk_vX_meta_clear.c
+++ b/src/panfrost/vulkan/panvk_vX_meta_clear.c
@@ -51,7 +51,7 @@ panvk_meta_clear_color_attachment_shader(struct panfrost_device *pdev,
    nir_store_var(&b, out, clear_values, 0xff);
 
    struct panfrost_compile_inputs inputs = {
-      .gpu_id = pdev->gpu_id,
+      .gpu_id = panfrost_device_gpu_id(pdev),
       .is_blit = true,
       .no_ubo_to_push = true,
    };
diff --git a/src/panfrost/vulkan/panvk_vX_meta_copy.c b/src/panfrost/vulkan/panvk_vX_meta_copy.c
index fe21fcb41279a..6abb644e76cf8 100644
--- a/src/panfrost/vulkan/panvk_vX_meta_copy.c
+++ b/src/panfrost/vulkan/panvk_vX_meta_copy.c
@@ -416,7 +416,7 @@ panvk_meta_copy_img2img_shader(struct panfrost_device *pdev,
    nir_store_var(&b, out, texel, 0xff);
 
    struct panfrost_compile_inputs inputs = {
-      .gpu_id = pdev->gpu_id,
+      .gpu_id = panfrost_device_gpu_id(pdev),
       .is_blit = true,
       .no_ubo_to_push = true,
    };
@@ -958,7 +958,7 @@ panvk_meta_copy_buf2img_shader(struct panfrost_device *pdev,
    nir_store_var(&b, out, texel, 0xff);
 
    struct panfrost_compile_inputs inputs = {
-      .gpu_id = pdev->gpu_id,
+      .gpu_id = panfrost_device_gpu_id(pdev),
       .is_blit = true,
       .no_ubo_to_push = true,
    };
@@ -1416,7 +1416,7 @@ panvk_meta_copy_img2buf_shader(struct panfrost_device *pdev,
    nir_pop_if(&b, NULL);
 
    struct panfrost_compile_inputs inputs = {
-      .gpu_id = pdev->gpu_id,
+      .gpu_id = panfrost_device_gpu_id(pdev),
       .is_blit = true,
       .no_ubo_to_push = true,
    };
@@ -1648,7 +1648,7 @@ panvk_meta_copy_buf2buf_shader(struct panfrost_device *pdev,
                     (1 << ncomps) - 1);
 
    struct panfrost_compile_inputs inputs = {
-      .gpu_id = pdev->gpu_id,
+      .gpu_id = panfrost_device_gpu_id(pdev),
       .is_blit = true,
       .no_ubo_to_push = true,
    };
@@ -1774,7 +1774,7 @@ panvk_meta_fill_buf_shader(struct panfrost_device *pdev,
    nir_store_global(&b, ptr, sizeof(uint32_t), val, 1);
 
    struct panfrost_compile_inputs inputs = {
-      .gpu_id = pdev->gpu_id,
+      .gpu_id = panfrost_device_gpu_id(pdev),
       .is_blit = true,
       .no_ubo_to_push = true,
    };
diff --git a/src/panfrost/vulkan/panvk_vX_shader.c b/src/panfrost/vulkan/panvk_vX_shader.c
index 38040d9e2ef9e..3e3f9e8f9bc44 100644
--- a/src/panfrost/vulkan/panvk_vX_shader.c
+++ b/src/panfrost/vulkan/panvk_vX_shader.c
@@ -254,7 +254,7 @@ panvk_per_arch(shader_create)(struct panvk_device *dev, gl_shader_stage stage,
               true, true);
 
    struct panfrost_compile_inputs inputs = {
-      .gpu_id = pdev->gpu_id,
+      .gpu_id = panfrost_device_gpu_id(pdev),
       .no_ubo_to_push = true,
       .no_idvs = true, /* TODO */
    };
-- 
GitLab


From fd51e238814469dfa3f9f01a0c01cae713264ee0 Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Mon, 20 Nov 2023 15:06:34 +0100
Subject: [PATCH 04/32] panfrost: Avoid direct accesses to some panfrost_bo
 fields

We are about to delegate some BO-related operations to the pan_kmod
layer, but before we can do that, we need to hide panfrost_bo
internals so we can redirect such accesses to pan_kmod.

Provide panfrost_bo_{size,handle}() accessors and start using them.

Signed-off-by: Boris Brezillon <boris.brezillon@collabora.com>
---
 src/gallium/drivers/panfrost/pan_cmdstream.c |  2 +-
 src/gallium/drivers/panfrost/pan_jm.c        | 13 ++++---
 src/gallium/drivers/panfrost/pan_job.c       |  9 ++---
 src/gallium/drivers/panfrost/pan_mempool.c   |  4 +--
 src/gallium/drivers/panfrost/pan_resource.c  | 25 +++++++------
 src/panfrost/lib/pan_bo.c                    | 38 ++++++++++----------
 src/panfrost/lib/pan_bo.h                    | 12 +++++++
 src/panfrost/lib/pan_desc.c                  |  3 +-
 src/panfrost/vulkan/panvk_mempool.c          |  6 ++--
 src/panfrost/vulkan/panvk_vX_cs.c            |  4 +--
 src/panfrost/vulkan/panvk_vX_device.c        | 10 +++---
 src/panfrost/vulkan/panvk_vX_image.c         |  2 +-
 12 files changed, 76 insertions(+), 52 deletions(-)

diff --git a/src/gallium/drivers/panfrost/pan_cmdstream.c b/src/gallium/drivers/panfrost/pan_cmdstream.c
index f0159f0892a8d..a047044f73a09 100644
--- a/src/gallium/drivers/panfrost/pan_cmdstream.c
+++ b/src/gallium/drivers/panfrost/pan_cmdstream.c
@@ -1769,7 +1769,7 @@ emit_image_bufs(struct panfrost_batch *batch, enum pipe_shader_type shader,
          cfg.type = pan_modifier_to_attr_type(rsrc->image.layout.modifier);
          cfg.pointer = rsrc->image.data.bo->ptr.gpu + offset;
          cfg.stride = util_format_get_blocksize(image->format);
-         cfg.size = rsrc->image.data.bo->size - offset;
+         cfg.size = panfrost_bo_size(rsrc->image.data.bo) - offset;
       }
 
       if (is_buffer) {
diff --git a/src/gallium/drivers/panfrost/pan_jm.c b/src/gallium/drivers/panfrost/pan_jm.c
index 518203dcbcacc..10f550ba922c9 100644
--- a/src/gallium/drivers/panfrost/pan_jm.c
+++ b/src/gallium/drivers/panfrost/pan_jm.c
@@ -148,11 +148,14 @@ jm_submit_jc(struct panfrost_batch *batch, mali_ptr first_job_desc,
     * least one tiler job. Tiler heap is written by tiler jobs and read
     * by fragment jobs (the polygon list is coming from this heap).
     */
-   if (batch->jm.jobs.vtc_jc.first_tiler)
-      bo_handles[submit.bo_handle_count++] = dev->tiler_heap->gem_handle;
+   if (batch->jm.jobs.vtc_jc.first_tiler) {
+      bo_handles[submit.bo_handle_count++] =
+         panfrost_bo_handle(dev->tiler_heap);
+   }
 
    /* Always used on Bifrost, occassionally used on Midgard */
-   bo_handles[submit.bo_handle_count++] = dev->sample_positions->gem_handle;
+   bo_handles[submit.bo_handle_count++] =
+      panfrost_bo_handle(dev->sample_positions);
 
    submit.bo_handles = (u64)(uintptr_t)bo_handles;
    if (ctx->is_noop)
@@ -365,10 +368,10 @@ jm_emit_tiler_desc(struct panfrost_batch *batch)
    struct panfrost_ptr t = pan_pool_alloc_desc(&batch->pool.base, TILER_HEAP);
 
    pan_pack(t.cpu, TILER_HEAP, heap) {
-      heap.size = dev->tiler_heap->size;
+      heap.size = panfrost_bo_size(dev->tiler_heap);
       heap.base = dev->tiler_heap->ptr.gpu;
       heap.bottom = dev->tiler_heap->ptr.gpu;
-      heap.top = dev->tiler_heap->ptr.gpu + dev->tiler_heap->size;
+      heap.top = dev->tiler_heap->ptr.gpu + panfrost_bo_size(dev->tiler_heap);
    }
 
    mali_ptr heap = t.gpu;
diff --git a/src/gallium/drivers/panfrost/pan_job.c b/src/gallium/drivers/panfrost/pan_job.c
index 4fe40919443a8..b48329f52e202 100644
--- a/src/gallium/drivers/panfrost/pan_job.c
+++ b/src/gallium/drivers/panfrost/pan_job.c
@@ -300,7 +300,7 @@ panfrost_batch_uses_resource(struct panfrost_batch *batch,
                              struct panfrost_resource *rsrc)
 {
    /* A resource is used iff its current BO is used */
-   uint32_t handle = rsrc->image.data.bo->gem_handle;
+   uint32_t handle = panfrost_bo_handle(rsrc->image.data.bo);
    unsigned size = util_dynarray_num_elements(&batch->bos, pan_bo_access);
 
    /* If out of bounds, certainly not used */
@@ -318,7 +318,8 @@ panfrost_batch_add_bo_old(struct panfrost_batch *batch, struct panfrost_bo *bo,
    if (!bo)
       return;
 
-   pan_bo_access *entry = panfrost_batch_get_bo_access(batch, bo->gem_handle);
+   pan_bo_access *entry =
+      panfrost_batch_get_bo_access(batch, panfrost_bo_handle(bo));
    pan_bo_access old_flags = *entry;
 
    if (!old_flags) {
@@ -417,7 +418,7 @@ panfrost_batch_get_scratchpad(struct panfrost_batch *batch,
       size_per_thread, thread_tls_alloc, core_id_range);
 
    if (batch->scratchpad) {
-      assert(batch->scratchpad->size >= size);
+      assert(panfrost_bo_size(batch->scratchpad) >= size);
    } else {
       batch->scratchpad =
          panfrost_batch_create_bo(batch, size, PAN_BO_INVISIBLE,
@@ -434,7 +435,7 @@ panfrost_batch_get_shared_memory(struct panfrost_batch *batch, unsigned size,
                                  unsigned workgroup_count)
 {
    if (batch->shared_memory) {
-      assert(batch->shared_memory->size >= size);
+      assert(panfrost_bo_size(batch->shared_memory) >= size);
    } else {
       batch->shared_memory = panfrost_batch_create_bo(
          batch, size, PAN_BO_INVISIBLE, PIPE_SHADER_VERTEX,
diff --git a/src/gallium/drivers/panfrost/pan_mempool.c b/src/gallium/drivers/panfrost/pan_mempool.c
index 89797cc39355f..23791cb06f563 100644
--- a/src/gallium/drivers/panfrost/pan_mempool.c
+++ b/src/gallium/drivers/panfrost/pan_mempool.c
@@ -104,8 +104,8 @@ panfrost_pool_get_bo_handles(struct panfrost_pool *pool, uint32_t *handles)
 
    unsigned idx = 0;
    util_dynarray_foreach(&pool->bos, struct panfrost_bo *, bo) {
-      assert((*bo)->gem_handle > 0);
-      handles[idx++] = (*bo)->gem_handle;
+      assert(panfrost_bo_handle(*bo) > 0);
+      handles[idx++] = panfrost_bo_handle(*bo);
 
       /* Update the BO access flags so that panfrost_bo_wait() knows
        * about all pending accesses.
diff --git a/src/gallium/drivers/panfrost/pan_resource.c b/src/gallium/drivers/panfrost/pan_resource.c
index 4710ef9983b76..a942c35cc3bcc 100644
--- a/src/gallium/drivers/panfrost/pan_resource.c
+++ b/src/gallium/drivers/panfrost/pan_resource.c
@@ -194,7 +194,7 @@ panfrost_resource_get_handle(struct pipe_screen *pscreen,
    if (handle->type == WINSYS_HANDLE_TYPE_KMS && dev->ro) {
       return renderonly_get_handle(scanout, handle);
    } else if (handle->type == WINSYS_HANDLE_TYPE_KMS) {
-      handle->handle = rsrc->image.data.bo->gem_handle;
+      handle->handle = panfrost_bo_handle(rsrc->image.data.bo);
    } else if (handle->type == WINSYS_HANDLE_TYPE_FD) {
       int fd = panfrost_bo_export(rsrc->image.data.bo);
 
@@ -1140,9 +1140,10 @@ panfrost_ptr_map(struct pipe_context *pctx, struct pipe_resource *resource,
    /* If we haven't already mmaped, now's the time */
    panfrost_bo_mmap(bo);
 
-   if (dev->debug & (PAN_DBG_TRACE | PAN_DBG_SYNC))
-      pandecode_inject_mmap(dev->decode_ctx, bo->ptr.gpu, bo->ptr.cpu, bo->size,
-                            NULL);
+   if (dev->debug & (PAN_DBG_TRACE | PAN_DBG_SYNC)) {
+      pandecode_inject_mmap(dev->decode_ctx, bo->ptr.gpu, bo->ptr.cpu,
+                            panfrost_bo_size(bo), NULL);
+   }
 
    /* Upgrade writes to uninitialized ranges to UNSYNCHRONIZED */
    if ((usage & PIPE_MAP_WRITE) && resource->target == PIPE_BUFFER &&
@@ -1208,12 +1209,16 @@ panfrost_ptr_map(struct pipe_context *pctx, struct pipe_resource *resource,
           * importer/exporter wouldn't see the change we're
           * doing to it.
           */
-         if (!(bo->flags & PAN_BO_SHARED))
-            newbo = panfrost_bo_create(dev, bo->size, flags, bo->label);
+         if (!(bo->flags & PAN_BO_SHARED)) {
+            newbo =
+               panfrost_bo_create(dev, panfrost_bo_size(bo), flags, bo->label);
+         }
 
          if (newbo) {
-            if (copy_resource)
-               memcpy(newbo->ptr.cpu, rsrc->image.data.bo->ptr.cpu, bo->size);
+            if (copy_resource) {
+               memcpy(newbo->ptr.cpu, rsrc->image.data.bo->ptr.cpu,
+                      panfrost_bo_size(bo));
+            }
 
             /* Swap the pointers, dropping a reference to
              * the old BO which is no long referenced from
@@ -1518,7 +1523,7 @@ panfrost_pack_afbc(struct panfrost_context *ctx,
    }
 
    unsigned new_size = ALIGN_POT(total_size, 4096); // FIXME
-   unsigned old_size = prsrc->image.data.bo->size;
+   unsigned old_size = panfrost_bo_size(prsrc->image.data.bo);
    unsigned ratio = 100 * new_size / old_size;
 
    if (ratio > screen->max_afbc_packing_ratio)
@@ -1606,7 +1611,7 @@ panfrost_ptr_unmap(struct pipe_context *pctx, struct pipe_transfer *transfer)
             if (panfrost_should_linear_convert(dev, prsrc, transfer)) {
                panfrost_resource_setup(dev, prsrc, DRM_FORMAT_MOD_LINEAR,
                                        prsrc->image.layout.format);
-               if (prsrc->image.layout.data_size > bo->size) {
+               if (prsrc->image.layout.data_size > panfrost_bo_size(bo)) {
                   const char *label = bo->label;
                   panfrost_bo_unreference(bo);
                   bo = prsrc->image.data.bo = panfrost_bo_create(
diff --git a/src/panfrost/lib/pan_bo.c b/src/panfrost/lib/pan_bo.c
index 9018b6ac9f58d..a9587aa3422f0 100644
--- a/src/panfrost/lib/pan_bo.c
+++ b/src/panfrost/lib/pan_bo.c
@@ -93,7 +93,7 @@ panfrost_bo_alloc(struct panfrost_device *dev, size_t size, uint32_t flags,
 static void
 panfrost_bo_free(struct panfrost_bo *bo)
 {
-   struct drm_gem_close gem_close = {.handle = bo->gem_handle};
+   struct drm_gem_close gem_close = {.handle = panfrost_bo_handle(bo)};
    int fd = panfrost_device_fd(bo->dev);
    int ret;
 
@@ -116,7 +116,7 @@ bool
 panfrost_bo_wait(struct panfrost_bo *bo, int64_t timeout_ns, bool wait_readers)
 {
    struct drm_panfrost_wait_bo req = {
-      .handle = bo->gem_handle,
+      .handle = panfrost_bo_handle(bo),
       .timeout_ns = timeout_ns,
    };
    int ret;
@@ -195,7 +195,7 @@ panfrost_bo_cache_fetch(struct panfrost_device *dev, size_t size,
 
    /* Iterate the bucket looking for something suitable */
    list_for_each_entry_safe(struct panfrost_bo, entry, bucket, bucket_link) {
-      if (entry->size < size || entry->flags != flags)
+      if (panfrost_bo_size(entry) < size || entry->flags != flags)
          continue;
 
       /* If the oldest BO in the cache is busy, likely so is
@@ -204,7 +204,7 @@ panfrost_bo_cache_fetch(struct panfrost_device *dev, size_t size,
          break;
 
       struct drm_panfrost_madvise madv = {
-         .handle = entry->gem_handle,
+         .handle = panfrost_bo_handle(entry),
          .madv = PANFROST_MADV_WILLNEED,
       };
       int ret;
@@ -268,11 +268,11 @@ panfrost_bo_cache_put(struct panfrost_bo *bo)
    /* Must be first */
    pthread_mutex_lock(&dev->bo_cache.lock);
 
-   struct list_head *bucket = pan_bucket(dev, MAX2(bo->size, 4096));
+   struct list_head *bucket = pan_bucket(dev, MAX2(panfrost_bo_size(bo), 4096));
    struct drm_panfrost_madvise madv;
    struct timespec time;
 
-   madv.handle = bo->gem_handle;
+   madv.handle = panfrost_bo_handle(bo);
    madv.madv = PANFROST_MADV_DONTNEED;
    madv.retained = 0;
 
@@ -324,7 +324,7 @@ panfrost_bo_cache_evict_all(struct panfrost_device *dev)
 void
 panfrost_bo_mmap(struct panfrost_bo *bo)
 {
-   struct drm_panfrost_mmap_bo mmap_bo = {.handle = bo->gem_handle};
+   struct drm_panfrost_mmap_bo mmap_bo = {.handle = panfrost_bo_handle(bo)};
    int ret;
 
    if (bo->ptr.cpu)
@@ -337,14 +337,15 @@ panfrost_bo_mmap(struct panfrost_bo *bo)
       assert(0);
    }
 
-   bo->ptr.cpu = os_mmap(NULL, bo->size, PROT_READ | PROT_WRITE, MAP_SHARED,
-                         panfrost_device_fd(bo->dev), mmap_bo.offset);
+   bo->ptr.cpu =
+      os_mmap(NULL, panfrost_bo_size(bo), PROT_READ | PROT_WRITE, MAP_SHARED,
+              panfrost_device_fd(bo->dev), mmap_bo.offset);
    if (bo->ptr.cpu == MAP_FAILED) {
       bo->ptr.cpu = NULL;
       fprintf(stderr,
               "mmap failed: result=%p size=0x%llx fd=%i offset=0x%llx %m\n",
-              bo->ptr.cpu, (long long)bo->size, panfrost_device_fd(bo->dev),
-              (long long)mmap_bo.offset);
+              bo->ptr.cpu, (long long)panfrost_bo_size(bo),
+              panfrost_device_fd(bo->dev), (long long)mmap_bo.offset);
    }
 }
 
@@ -354,7 +355,7 @@ panfrost_bo_munmap(struct panfrost_bo *bo)
    if (!bo->ptr.cpu)
       return;
 
-   if (os_munmap((void *)(uintptr_t)bo->ptr.cpu, bo->size)) {
+   if (os_munmap((void *)(uintptr_t)bo->ptr.cpu, panfrost_bo_size(bo))) {
       perror("munmap");
       abort();
    }
@@ -409,11 +410,11 @@ panfrost_bo_create(struct panfrost_device *dev, size_t size, uint32_t flags,
 
    if (dev->debug & (PAN_DBG_TRACE | PAN_DBG_SYNC)) {
       if (flags & PAN_BO_INVISIBLE)
-         pandecode_inject_mmap(dev->decode_ctx, bo->ptr.gpu, NULL, bo->size,
-                               NULL);
+         pandecode_inject_mmap(dev->decode_ctx, bo->ptr.gpu, NULL,
+                               panfrost_bo_size(bo), NULL);
       else if (!(flags & PAN_BO_DELAY_MMAP))
          pandecode_inject_mmap(dev->decode_ctx, bo->ptr.gpu, bo->ptr.cpu,
-                               bo->size, NULL);
+                               panfrost_bo_size(bo), NULL);
    }
 
    return bo;
@@ -451,7 +452,8 @@ panfrost_bo_unreference(struct panfrost_bo *bo)
       panfrost_bo_munmap(bo);
 
       if (dev->debug & (PAN_DBG_TRACE | PAN_DBG_SYNC))
-         pandecode_inject_free(dev->decode_ctx, bo->ptr.gpu, bo->size);
+         pandecode_inject_free(dev->decode_ctx, bo->ptr.gpu,
+                               panfrost_bo_size(bo));
 
       /* Rather than freeing the BO now, we'll cache the BO for later
        * allocations if we're allowed to.
@@ -492,7 +494,7 @@ panfrost_bo_import(struct panfrost_device *dev, int fd)
        * a nice thing for mmap to try mmap. Be more robust also
        * for zero sized maps and fail nicely too
        */
-      if ((bo->size == 0) || (bo->size == (size_t)-1)) {
+      if ((panfrost_bo_size(bo) == 0) || (panfrost_bo_size(bo) == (size_t)-1)) {
          pthread_mutex_unlock(&dev->bo_map_lock);
          return NULL;
       }
@@ -524,7 +526,7 @@ int
 panfrost_bo_export(struct panfrost_bo *bo)
 {
    struct drm_prime_handle args = {
-      .handle = bo->gem_handle,
+      .handle = panfrost_bo_handle(bo),
       .flags = DRM_CLOEXEC,
    };
 
diff --git a/src/panfrost/lib/pan_bo.h b/src/panfrost/lib/pan_bo.h
index 4742fec5bd16c..2f0940e3755ef 100644
--- a/src/panfrost/lib/pan_bo.h
+++ b/src/panfrost/lib/pan_bo.h
@@ -117,6 +117,18 @@ struct panfrost_bo {
    const char *label;
 };
 
+static inline size_t
+panfrost_bo_size(struct panfrost_bo *bo)
+{
+   return bo->size;
+}
+
+static inline size_t
+panfrost_bo_handle(struct panfrost_bo *bo)
+{
+   return bo->gem_handle;
+}
+
 bool panfrost_bo_wait(struct panfrost_bo *bo, int64_t timeout_ns,
                       bool wait_readers);
 void panfrost_bo_reference(struct panfrost_bo *bo);
diff --git a/src/panfrost/lib/pan_desc.c b/src/panfrost/lib/pan_desc.c
index 91171a4a246f8..575d344cbc946 100644
--- a/src/panfrost/lib/pan_desc.c
+++ b/src/panfrost/lib/pan_desc.c
@@ -589,7 +589,8 @@ pan_emit_midgard_tiler(const struct panfrost_device *dev,
          cfg.polygon_list_size = panfrost_tiler_full_size(
             fb->width, fb->height, cfg.hierarchy_mask, hierarchy);
          cfg.heap_start = dev->tiler_heap->ptr.gpu;
-         cfg.heap_end = dev->tiler_heap->ptr.gpu + dev->tiler_heap->size;
+         cfg.heap_end =
+            dev->tiler_heap->ptr.gpu + panfrost_bo_size(dev->tiler_heap);
       }
 
       cfg.polygon_list = tiler_ctx->midgard.polygon_list->ptr.gpu;
diff --git a/src/panfrost/vulkan/panvk_mempool.c b/src/panfrost/vulkan/panvk_mempool.c
index 30e6670feba74..7e76b8dea2d3f 100644
--- a/src/panfrost/vulkan/panvk_mempool.c
+++ b/src/panfrost/vulkan/panvk_mempool.c
@@ -61,7 +61,7 @@ panvk_pool_alloc_backing(struct panvk_pool *pool, size_t bo_sz)
                               pool->base.label);
    }
 
-   if (bo->size == pool->base.slab_size)
+   if (panfrost_bo_size(bo) == pool->base.slab_size)
       util_dynarray_append(&pool->bos, struct panfrost_bo *, bo);
    else
       util_dynarray_append(&pool->big_bos, struct panfrost_bo *, bo);
@@ -149,7 +149,7 @@ panvk_pool_get_bo_handles(struct panvk_pool *pool, uint32_t *handles)
 {
    unsigned idx = 0;
    util_dynarray_foreach(&pool->bos, struct panfrost_bo *, bo) {
-      assert((*bo)->gem_handle > 0);
-      handles[idx++] = (*bo)->gem_handle;
+      assert(panfrost_bo_handle(*bo) > 0);
+      handles[idx++] = panfrost_bo_handle(*bo);
    }
 }
diff --git a/src/panfrost/vulkan/panvk_vX_cs.c b/src/panfrost/vulkan/panvk_vX_cs.c
index 69605b304c486..c78e4ed701c92 100644
--- a/src/panfrost/vulkan/panvk_vX_cs.c
+++ b/src/panfrost/vulkan/panvk_vX_cs.c
@@ -829,10 +829,10 @@ panvk_per_arch(emit_tiler_context)(const struct panvk_device *dev,
    const struct panfrost_device *pdev = &dev->physical_device->pdev;
 
    pan_pack(descs->cpu + pan_size(TILER_CONTEXT), TILER_HEAP, cfg) {
-      cfg.size = pdev->tiler_heap->size;
+      cfg.size = panfrost_bo_size(pdev->tiler_heap);
       cfg.base = pdev->tiler_heap->ptr.gpu;
       cfg.bottom = pdev->tiler_heap->ptr.gpu;
-      cfg.top = pdev->tiler_heap->ptr.gpu + pdev->tiler_heap->size;
+      cfg.top = pdev->tiler_heap->ptr.gpu + panfrost_bo_size(pdev->tiler_heap);
    }
 
    pan_pack(descs->cpu, TILER_CONTEXT, cfg) {
diff --git a/src/panfrost/vulkan/panvk_vX_device.c b/src/panfrost/vulkan/panvk_vX_device.c
index 8467b86b0216c..c2ae92a4511d0 100644
--- a/src/panfrost/vulkan/panvk_vX_device.c
+++ b/src/panfrost/vulkan/panvk_vX_device.c
@@ -253,20 +253,20 @@ panvk_per_arch(queue_submit)(struct vk_queue *vk_queue,
             for (unsigned i = 0; i < batch->fb.info->attachment_count; i++) {
                const struct pan_image *image = pan_image_view_get_plane(
                   &batch->fb.info->attachments[i].iview->pview, 0);
-               bos[bo_idx++] = image->data.bo->gem_handle;
+               bos[bo_idx++] = panfrost_bo_handle(image->data.bo);
             }
          }
 
          if (batch->blit.src)
-            bos[bo_idx++] = batch->blit.src->gem_handle;
+            bos[bo_idx++] = panfrost_bo_handle(batch->blit.src);
 
          if (batch->blit.dst)
-            bos[bo_idx++] = batch->blit.dst->gem_handle;
+            bos[bo_idx++] = panfrost_bo_handle(batch->blit.dst);
 
          if (batch->jc.first_tiler)
-            bos[bo_idx++] = pdev->tiler_heap->gem_handle;
+            bos[bo_idx++] = panfrost_bo_handle(pdev->tiler_heap);
 
-         bos[bo_idx++] = pdev->sample_positions->gem_handle;
+         bos[bo_idx++] = panfrost_bo_handle(pdev->sample_positions);
          assert(bo_idx == nr_bos);
 
          /* Merge identical BO entries. */
diff --git a/src/panfrost/vulkan/panvk_vX_image.c b/src/panfrost/vulkan/panvk_vX_image.c
index e4ad70cf0c601..0e75eb1e6b3fc 100644
--- a/src/panfrost/vulkan/panvk_vX_image.c
+++ b/src/panfrost/vulkan/panvk_vX_image.c
@@ -143,7 +143,7 @@ panvk_per_arch(CreateImageView)(VkDevice _device,
                        : MALI_ATTRIBUTE_TYPE_3D_INTERLEAVED;
          cfg.pointer = image->pimage.data.bo->ptr.gpu + offset;
          cfg.stride = util_format_get_blocksize(view->pview.format);
-         cfg.size = image->pimage.data.bo->size - offset;
+         cfg.size = panfrost_bo_size(image->pimage.data.bo) - offset;
       }
 
       attrib_buf += pan_size(ATTRIBUTE_BUFFER);
-- 
GitLab


From 719996f61b679e6e39fb59c28b7a6a03c42d0d8d Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Mon, 20 Nov 2023 16:54:13 +0100
Subject: [PATCH 05/32] panfrost: Back panfrost_device with pan_kmod_dev object

Back panfrost_device with pan_kmod_dev object and query all props using
the pan_kmod_dev_query_props().

Signed-off-by: Boris Brezillon <boris.brezillon@collabora.com>
---
 src/panfrost/lib/pan_device.h |  25 ++++---
 src/panfrost/lib/pan_props.c  | 118 +++++++++++-----------------------
 2 files changed, 51 insertions(+), 92 deletions(-)

diff --git a/src/panfrost/lib/pan_device.h b/src/panfrost/lib/pan_device.h
index d185439fb5fab..5df6d6db18916 100644
--- a/src/panfrost/lib/pan_device.h
+++ b/src/panfrost/lib/pan_device.h
@@ -41,6 +41,8 @@
 #include "pan_pool.h"
 #include "pan_util.h"
 
+#include "kmod/pan_kmod.h"
+
 #include <genxml/gen_macros.h>
 
 #if defined(__cplusplus)
@@ -128,15 +130,20 @@ struct panfrost_device {
    /* For ralloc */
    void *memctx;
 
-   int fd;
+   /* Kmod objects. */
+   struct {
+      /* The pan_kmod_dev object backing this device. */
+      struct pan_kmod_dev *dev;
+
+      /* Cached pan_kmod_dev_props properties queried at device create time. */
+      struct pan_kmod_dev_props props;
+   } kmod;
 
    /* For pandecode */
    struct pandecode_context *decode_ctx;
 
    /* Properties of the GPU in use */
    unsigned arch;
-   unsigned gpu_id;
-   unsigned revision;
 
    /* Number of shader cores */
    unsigned core_count;
@@ -164,8 +171,6 @@ struct panfrost_device {
    /* debug flags, see pan_util.h how to interpret */
    unsigned debug;
 
-   drmVersionPtr kernel_version;
-
    struct renderonly *ro;
 
    pthread_mutex_t bo_map_lock;
@@ -218,31 +223,31 @@ struct panfrost_device {
 static inline int
 panfrost_device_fd(const struct panfrost_device *dev)
 {
-   return dev->fd;
+   return dev->kmod.dev->fd;
 }
 
 static inline uint32_t
 panfrost_device_gpu_id(const struct panfrost_device *dev)
 {
-   return dev->gpu_id;
+   return dev->kmod.props.gpu_prod_id;
 }
 
 static inline uint32_t
 panfrost_device_gpu_rev(const struct panfrost_device *dev)
 {
-   return dev->revision;
+   return dev->kmod.props.gpu_revision;
 }
 
 static inline int
 panfrost_device_kmod_version_major(const struct panfrost_device *dev)
 {
-   return dev->kernel_version->version_major;
+   return dev->kmod.dev->driver.version.major;
 }
 
 static inline int
 panfrost_device_kmod_version_minor(const struct panfrost_device *dev)
 {
-   return dev->kernel_version->version_minor;
+   return dev->kmod.dev->driver.version.minor;
 }
 
 void panfrost_open_device(void *memctx, int fd, struct panfrost_device *dev);
diff --git a/src/panfrost/lib/pan_props.c b/src/panfrost/lib/pan_props.c
index b0b60db9fa751..092e2b1c9ff61 100644
--- a/src/panfrost/lib/pan_props.c
+++ b/src/panfrost/lib/pan_props.c
@@ -93,58 +93,18 @@ panfrost_get_model(uint32_t gpu_id)
    return NULL;
 }
 
-/* Abstraction over the raw drm_panfrost_get_param ioctl for fetching
- * information about devices */
-
-static __u64
-panfrost_query_raw(int fd, enum drm_panfrost_param param, bool required,
-                   unsigned default_value)
-{
-   struct drm_panfrost_get_param get_param = {
-      0,
-   };
-   ASSERTED int ret;
-
-   get_param.param = param;
-   ret = drmIoctl(fd, DRM_IOCTL_PANFROST_GET_PARAM, &get_param);
-
-   if (ret) {
-      assert(!required);
-      return default_value;
-   }
-
-   return get_param.value;
-}
-
-static unsigned
-panfrost_query_gpu_version(int fd)
-{
-   return panfrost_query_raw(fd, DRM_PANFROST_PARAM_GPU_PROD_ID, true, 0);
-}
-
-static unsigned
-panfrost_query_gpu_revision(int fd)
-{
-   return panfrost_query_raw(fd, DRM_PANFROST_PARAM_GPU_REVISION, true, 0);
-}
-
 unsigned
 panfrost_query_l2_slices(const struct panfrost_device *dev)
 {
-   /* Query MEM_FEATURES register */
-   uint32_t mem_features = panfrost_query_raw(
-      panfrost_device_fd(dev), DRM_PANFROST_PARAM_MEM_FEATURES, true, 0);
-
    /* L2_SLICES is MEM_FEATURES[11:8] minus(1) */
-   return ((mem_features >> 8) & 0xF) + 1;
+   return ((dev->kmod.props.mem_features >> 8) & 0xF) + 1;
 }
 
 static struct panfrost_tiler_features
-panfrost_query_tiler_features(int fd)
+panfrost_query_tiler_features(const struct panfrost_device *dev)
 {
    /* Default value (2^9 bytes and 8 levels) to match old behaviour */
-   uint32_t raw =
-      panfrost_query_raw(fd, DRM_PANFROST_PARAM_TILER_FEATURES, false, 0x809);
+   uint32_t raw = dev->kmod.props.tiler_features;
 
    /* Bin size is log2 in the first byte, max levels in the second byte */
    return (struct panfrost_tiler_features){
@@ -154,12 +114,12 @@ panfrost_query_tiler_features(int fd)
 }
 
 static unsigned
-panfrost_query_core_count(int fd, unsigned *core_id_range)
+panfrost_query_core_count(const struct panfrost_device *dev,
+                          unsigned *core_id_range)
 {
    /* On older kernels, worst-case to 16 cores */
 
-   unsigned mask =
-      panfrost_query_raw(fd, DRM_PANFROST_PARAM_SHADER_PRESENT, false, 0xffff);
+   unsigned mask = dev->kmod.props.shader_present;
 
    /* Some cores might be absent. In some cases, we care
     * about the range of core IDs (that is, the greatest core ID + 1). If
@@ -172,31 +132,18 @@ panfrost_query_core_count(int fd, unsigned *core_id_range)
 }
 
 static unsigned
-panfrost_query_thread_tls_alloc(int fd, unsigned major)
+panfrost_query_thread_tls_alloc(const struct panfrost_device *dev,
+                                unsigned major)
 {
-   unsigned tls =
-      panfrost_query_raw(fd, DRM_PANFROST_PARAM_THREAD_TLS_ALLOC, false, 0);
+   unsigned tls = dev->kmod.props.thread_tls_alloc;
 
    return (tls > 0) ? tls : panfrost_max_thread_count(major, 0);
 }
 
 static uint32_t
-panfrost_query_compressed_formats(int fd)
+panfrost_query_compressed_formats(const struct panfrost_device *dev)
 {
-   /* If unspecified, assume ASTC/ETC only. Factory default for Juno, and
-    * should exist on any Mali configuration. All hardware should report
-    * these texture formats but the kernel might not be new enough. */
-
-   uint32_t default_set = (1 << MALI_ETC2_RGB8) | (1 << MALI_ETC2_R11_UNORM) |
-                          (1 << MALI_ETC2_RGBA8) | (1 << MALI_ETC2_RG11_UNORM) |
-                          (1 << MALI_ETC2_R11_SNORM) |
-                          (1 << MALI_ETC2_RG11_SNORM) |
-                          (1 << MALI_ETC2_RGB8A1) | (1 << MALI_ASTC_3D_LDR) |
-                          (1 << MALI_ASTC_3D_HDR) | (1 << MALI_ASTC_2D_LDR) |
-                          (1 << MALI_ASTC_2D_HDR);
-
-   return panfrost_query_raw(fd, DRM_PANFROST_PARAM_TEXTURE_FEATURES0, false,
-                             default_set);
+   return dev->kmod.props.texture_features[0];
 }
 
 /* DRM_PANFROST_PARAM_TEXTURE_FEATURES0 will return a bitmask of supported
@@ -218,10 +165,9 @@ panfrost_supports_compressed_format(struct panfrost_device *dev, unsigned fmt)
  * may omit it, signaled as a nonzero value in the AFBC_FEATURES property. */
 
 static bool
-panfrost_query_afbc(int fd, unsigned arch)
+panfrost_query_afbc(struct panfrost_device *dev, unsigned arch)
 {
-   unsigned reg =
-      panfrost_query_raw(fd, DRM_PANFROST_PARAM_AFBC_FEATURES, false, 0);
+   unsigned reg = dev->kmod.props.afbc_features;
 
    return (arch >= 5) && (reg == 0);
 }
@@ -248,27 +194,30 @@ panfrost_query_optimal_tib_size(const struct panfrost_device *dev)
 void
 panfrost_open_device(void *memctx, int fd, struct panfrost_device *dev)
 {
-   dev->fd = fd;
    dev->memctx = memctx;
-   dev->gpu_id = panfrost_query_gpu_version(fd);
-   dev->arch = pan_arch(panfrost_device_gpu_id(dev));
-   dev->kernel_version = drmGetVersion(fd);
-   dev->revision = panfrost_query_gpu_revision(fd);
-   dev->model = panfrost_get_model(panfrost_device_gpu_id(dev));
 
-   if (!dev->kernel_version)
+   dev->kmod.dev = pan_kmod_dev_create(fd, NULL);
+   if (!dev->kmod.dev) {
+      close(fd);
       return;
+   }
+
+   pan_kmod_dev_query_props(dev->kmod.dev, &dev->kmod.props);
+
+   dev->arch = pan_arch(dev->kmod.props.gpu_prod_id);
+   dev->model = panfrost_get_model(dev->kmod.props.gpu_prod_id);
 
    /* If we don't recognize the model, bail early */
    if (!dev->model)
-      return;
+      goto err_free_kmod_dev;
+
 
-   dev->core_count = panfrost_query_core_count(fd, &dev->core_id_range);
-   dev->thread_tls_alloc = panfrost_query_thread_tls_alloc(fd, dev->arch);
+   dev->core_count = panfrost_query_core_count(dev, &dev->core_id_range);
+   dev->thread_tls_alloc = panfrost_query_thread_tls_alloc(dev, dev->arch);
    dev->optimal_tib_size = panfrost_query_optimal_tib_size(dev);
-   dev->compressed_formats = panfrost_query_compressed_formats(fd);
-   dev->tiler_features = panfrost_query_tiler_features(fd);
-   dev->has_afbc = panfrost_query_afbc(fd, dev->arch);
+   dev->compressed_formats = panfrost_query_compressed_formats(dev);
+   dev->tiler_features = panfrost_query_tiler_features(dev);
+   dev->has_afbc = panfrost_query_afbc(dev, dev->arch);
 
    if (dev->arch <= 6) {
       dev->formats = panfrost_pipe_format_v6;
@@ -304,6 +253,11 @@ panfrost_open_device(void *memctx, int fd, struct panfrost_device *dev)
 
    /* Done once on init */
    panfrost_upload_sample_positions(dev);
+   return;
+
+err_free_kmod_dev:
+   pan_kmod_dev_destroy(dev->kmod.dev);
+   dev->kmod.dev = NULL;
 }
 
 void
@@ -321,6 +275,6 @@ panfrost_close_device(struct panfrost_device *dev)
       util_sparse_array_finish(&dev->bo_map);
    }
 
-   drmFreeVersion(dev->kernel_version);
-   close(dev->fd);
+   if (dev->kmod.dev)
+      pan_kmod_dev_destroy(dev->kmod.dev);
 }
-- 
GitLab


From 2c805eda9eaf7590a821c07777708e0a388f93eb Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Mon, 20 Nov 2023 17:46:15 +0100
Subject: [PATCH 06/32] panfrost: Add a VM to panfrost_device

The current code assumes the logical device comes with a VM, so let's
explicitly create this default VM so we can map BOs with the kmod API.

Signed-off-by: Boris Brezillon <boris.brezillon@collabora.com>
---
 src/panfrost/lib/pan_device.h |  3 +++
 src/panfrost/lib/pan_props.c  | 11 +++++++++++
 2 files changed, 14 insertions(+)

diff --git a/src/panfrost/lib/pan_device.h b/src/panfrost/lib/pan_device.h
index 5df6d6db18916..bbf94dc305dd0 100644
--- a/src/panfrost/lib/pan_device.h
+++ b/src/panfrost/lib/pan_device.h
@@ -137,6 +137,9 @@ struct panfrost_device {
 
       /* Cached pan_kmod_dev_props properties queried at device create time. */
       struct pan_kmod_dev_props props;
+
+      /* VM attached to this device. */
+      struct pan_kmod_vm *vm;
    } kmod;
 
    /* For pandecode */
diff --git a/src/panfrost/lib/pan_props.c b/src/panfrost/lib/pan_props.c
index 092e2b1c9ff61..db08acf9c7c62 100644
--- a/src/panfrost/lib/pan_props.c
+++ b/src/panfrost/lib/pan_props.c
@@ -211,6 +211,14 @@ panfrost_open_device(void *memctx, int fd, struct panfrost_device *dev)
    if (!dev->model)
       goto err_free_kmod_dev;
 
+   /* 4G address space, with the lower 32MB reserved. */
+   uint64_t user_va_start = 0x2000000;
+   uint64_t user_va_range = (1ull << 32) - user_va_start;
+
+   dev->kmod.vm = pan_kmod_vm_create(dev->kmod.dev, PAN_KMOD_VM_FLAG_AUTO_VA,
+                                     user_va_start, user_va_range);
+   if (!dev->kmod.vm)
+      goto err_free_kmod_dev;
 
    dev->core_count = panfrost_query_core_count(dev, &dev->core_id_range);
    dev->thread_tls_alloc = panfrost_query_thread_tls_alloc(dev, dev->arch);
@@ -275,6 +283,9 @@ panfrost_close_device(struct panfrost_device *dev)
       util_sparse_array_finish(&dev->bo_map);
    }
 
+   if (dev->kmod.vm)
+      pan_kmod_vm_destroy(dev->kmod.vm);
+
    if (dev->kmod.dev)
       pan_kmod_dev_destroy(dev->kmod.dev);
 }
-- 
GitLab


From 33080f492191926c006d560110229d139817f580 Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Mon, 20 Nov 2023 17:50:27 +0100
Subject: [PATCH 07/32] panfrost: Back panfrost_bo with pan_kmod_bo object

We keep the existing implementation unchanged but use pan_kmod for
all interactions with the kernel driver.

Signed-off-by: Boris Brezillon <boris.brezillon@collabora.com>
---
 src/panfrost/lib/pan_bo.c | 199 +++++++++++++++++---------------------
 src/panfrost/lib/pan_bo.h |  14 +--
 2 files changed, 97 insertions(+), 116 deletions(-)

diff --git a/src/panfrost/lib/pan_bo.c b/src/panfrost/lib/pan_bo.c
index a9587aa3422f0..5e51039d4bcca 100644
--- a/src/panfrost/lib/pan_bo.c
+++ b/src/panfrost/lib/pan_bo.c
@@ -28,7 +28,6 @@
 #include <pthread.h>
 #include <stdio.h>
 #include <xf86drm.h>
-#include "drm-uapi/panfrost_drm.h"
 
 #include "pan_bo.h"
 #include "pan_device.h"
@@ -55,35 +54,55 @@
  * around the linked list.
  */
 
+static uint32_t
+to_kmod_bo_flags(uint32_t flags)
+{
+   uint32_t kmod_bo_flags = 0;
+
+   if (flags & PAN_BO_EXECUTE)
+      kmod_bo_flags |= PAN_KMOD_BO_FLAG_EXECUTABLE;
+   if (flags & PAN_BO_GROWABLE)
+      kmod_bo_flags |= PAN_KMOD_BO_FLAG_ALLOC_ON_FAULT;
+   if (flags & PAN_BO_INVISIBLE)
+      kmod_bo_flags |= PAN_KMOD_BO_FLAG_NO_MMAP;
+
+   return kmod_bo_flags;
+}
+
 static struct panfrost_bo *
 panfrost_bo_alloc(struct panfrost_device *dev, size_t size, uint32_t flags,
                   const char *label)
 {
-   struct drm_panfrost_create_bo create_bo = {.size = size};
+   struct pan_kmod_bo *kmod_bo;
    struct panfrost_bo *bo;
-   int ret;
-
-   if (panfrost_device_kmod_version_major(dev) > 1 ||
-       panfrost_device_kmod_version_minor(dev) >= 1) {
-      if (flags & PAN_BO_GROWABLE)
-         create_bo.flags |= PANFROST_BO_HEAP;
-      if (!(flags & PAN_BO_EXECUTE))
-         create_bo.flags |= PANFROST_BO_NOEXEC;
-   }
 
-   ret = drmIoctl(panfrost_device_fd(dev), DRM_IOCTL_PANFROST_CREATE_BO,
-                  &create_bo);
-   if (ret) {
-      fprintf(stderr, "DRM_IOCTL_PANFROST_CREATE_BO failed: %m\n");
-      return NULL;
-   }
+   kmod_bo =
+      pan_kmod_bo_alloc(dev->kmod.dev, NULL, size, to_kmod_bo_flags(flags));
+   assert(kmod_bo);
 
-   bo = pan_lookup_bo(dev, create_bo.handle);
+   bo = pan_lookup_bo(dev, kmod_bo->handle);
    assert(!memcmp(bo, &((struct panfrost_bo){}), sizeof(*bo)));
+   bo->kmod_bo = kmod_bo;
+
+   struct pan_kmod_vm_op vm_op = {
+      .type = PAN_KMOD_VM_OP_TYPE_MAP,
+      .va =
+         {
+            .start = PAN_KMOD_VM_MAP_AUTO_VA,
+            .size = bo->kmod_bo->size,
+         },
+      .map =
+         {
+            .bo = bo->kmod_bo,
+            .bo_offset = 0,
+         },
+   };
 
-   bo->size = create_bo.size;
-   bo->ptr.gpu = create_bo.offset;
-   bo->gem_handle = create_bo.handle;
+   ASSERTED int ret =
+      pan_kmod_vm_bind(dev->kmod.vm, PAN_KMOD_VM_OP_MODE_IMMEDIATE, &vm_op, 1);
+   assert(!ret);
+
+   bo->ptr.gpu = vm_op.va.start;
    bo->flags = flags;
    bo->dev = dev;
    bo->label = label;
@@ -93,18 +112,27 @@ panfrost_bo_alloc(struct panfrost_device *dev, size_t size, uint32_t flags,
 static void
 panfrost_bo_free(struct panfrost_bo *bo)
 {
-   struct drm_gem_close gem_close = {.handle = panfrost_bo_handle(bo)};
-   int fd = panfrost_device_fd(bo->dev);
-   int ret;
+   struct pan_kmod_bo *kmod_bo = bo->kmod_bo;
+   struct pan_kmod_vm *vm = bo->dev->kmod.vm;
+   uint64_t gpu_va = bo->ptr.gpu;
 
    /* BO will be freed with the sparse array, but zero to indicate free */
    memset(bo, 0, sizeof(*bo));
 
-   ret = drmIoctl(fd, DRM_IOCTL_GEM_CLOSE, &gem_close);
-   if (ret) {
-      fprintf(stderr, "DRM_IOCTL_GEM_CLOSE failed: %m\n");
-      assert(0);
-   }
+   struct pan_kmod_vm_op vm_op = {
+      .type = PAN_KMOD_VM_OP_TYPE_UNMAP,
+      .va =
+         {
+            .start = gpu_va,
+            .size = kmod_bo->size,
+         },
+   };
+
+   ASSERTED int ret = pan_kmod_vm_bind(
+      vm, PAN_KMOD_VM_OP_MODE_DEFER_TO_NEXT_IDLE_POINT, &vm_op, 1);
+   assert(!ret);
+
+   pan_kmod_bo_put(kmod_bo);
 }
 
 /* Returns true if the BO is ready, false otherwise.
@@ -115,12 +143,6 @@ panfrost_bo_free(struct panfrost_bo *bo)
 bool
 panfrost_bo_wait(struct panfrost_bo *bo, int64_t timeout_ns, bool wait_readers)
 {
-   struct drm_panfrost_wait_bo req = {
-      .handle = panfrost_bo_handle(bo),
-      .timeout_ns = timeout_ns,
-   };
-   int ret;
-
    /* If the BO has been exported or imported we can't rely on the cached
     * state, we need to call the WAIT_BO ioctl.
     */
@@ -136,12 +158,7 @@ panfrost_bo_wait(struct panfrost_bo *bo, int64_t timeout_ns, bool wait_readers)
          return true;
    }
 
-   /* The ioctl returns >= 0 value when the BO we are waiting for is ready
-    * -1 otherwise.
-    */
-   ret =
-      drmIoctl(panfrost_device_fd(bo->dev), DRM_IOCTL_PANFROST_WAIT_BO, &req);
-   if (ret != -1) {
+   if (pan_kmod_bo_wait(bo->kmod_bo, timeout_ns, !wait_readers)) {
       /* Set gpu_access to 0 so that the next call to bo_wait()
        * doesn't have to call the WAIT_BO ioctl.
        */
@@ -149,10 +166,6 @@ panfrost_bo_wait(struct panfrost_bo *bo, int64_t timeout_ns, bool wait_readers)
       return true;
    }
 
-   /* If errno is not ETIMEDOUT or EBUSY that means the handle we passed
-    * is invalid, which shouldn't happen here.
-    */
-   assert(errno == ETIMEDOUT || errno == EBUSY);
    return false;
 }
 
@@ -200,22 +213,14 @@ panfrost_bo_cache_fetch(struct panfrost_device *dev, size_t size,
 
       /* If the oldest BO in the cache is busy, likely so is
        * everything newer, so bail. */
-      if (!panfrost_bo_wait(entry, dontwait ? 0 : INT64_MAX, PAN_BO_ACCESS_RW))
+      if (!panfrost_bo_wait(entry, dontwait ? 0 : INT64_MAX, true))
          break;
 
-      struct drm_panfrost_madvise madv = {
-         .handle = panfrost_bo_handle(entry),
-         .madv = PANFROST_MADV_WILLNEED,
-      };
-      int ret;
-
       /* This one works, splice it out of the cache */
       list_del(&entry->bucket_link);
       list_del(&entry->lru_link);
 
-      ret =
-         drmIoctl(panfrost_device_fd(dev), DRM_IOCTL_PANFROST_MADVISE, &madv);
-      if (!ret && !madv.retained) {
+      if (!pan_kmod_bo_make_unevictable(entry->kmod_bo)) {
          panfrost_bo_free(entry);
          continue;
       }
@@ -269,14 +274,9 @@ panfrost_bo_cache_put(struct panfrost_bo *bo)
    pthread_mutex_lock(&dev->bo_cache.lock);
 
    struct list_head *bucket = pan_bucket(dev, MAX2(panfrost_bo_size(bo), 4096));
-   struct drm_panfrost_madvise madv;
    struct timespec time;
 
-   madv.handle = panfrost_bo_handle(bo);
-   madv.madv = PANFROST_MADV_DONTNEED;
-   madv.retained = 0;
-
-   drmIoctl(panfrost_device_fd(dev), DRM_IOCTL_PANFROST_MADVISE, &madv);
+   pan_kmod_bo_make_evictable(bo->kmod_bo);
 
    /* Add us to the bucket */
    list_addtail(&bo->bucket_link, bucket);
@@ -324,28 +324,15 @@ panfrost_bo_cache_evict_all(struct panfrost_device *dev)
 void
 panfrost_bo_mmap(struct panfrost_bo *bo)
 {
-   struct drm_panfrost_mmap_bo mmap_bo = {.handle = panfrost_bo_handle(bo)};
-   int ret;
-
    if (bo->ptr.cpu)
       return;
 
-   ret = drmIoctl(panfrost_device_fd(bo->dev), DRM_IOCTL_PANFROST_MMAP_BO,
-                  &mmap_bo);
-   if (ret) {
-      fprintf(stderr, "DRM_IOCTL_PANFROST_MMAP_BO failed: %m\n");
-      assert(0);
-   }
-
-   bo->ptr.cpu =
-      os_mmap(NULL, panfrost_bo_size(bo), PROT_READ | PROT_WRITE, MAP_SHARED,
-              panfrost_device_fd(bo->dev), mmap_bo.offset);
+   bo->ptr.cpu = pan_kmod_bo_mmap(bo->kmod_bo, 0, panfrost_bo_size(bo),
+                                  PROT_READ | PROT_WRITE, MAP_SHARED);
    if (bo->ptr.cpu == MAP_FAILED) {
       bo->ptr.cpu = NULL;
-      fprintf(stderr,
-              "mmap failed: result=%p size=0x%llx fd=%i offset=0x%llx %m\n",
-              bo->ptr.cpu, (long long)panfrost_bo_size(bo),
-              panfrost_device_fd(bo->dev), (long long)mmap_bo.offset);
+      fprintf(stderr, "mmap failed: result=%p size=0x%llx\n", bo->ptr.cpu,
+              (long long)panfrost_bo_size(bo));
    }
 }
 
@@ -468,38 +455,39 @@ struct panfrost_bo *
 panfrost_bo_import(struct panfrost_device *dev, int fd)
 {
    struct panfrost_bo *bo;
-   struct drm_panfrost_get_bo_offset get_bo_offset = {
-      0,
-   };
    ASSERTED int ret;
    unsigned gem_handle;
 
    pthread_mutex_lock(&dev->bo_map_lock);
-
-   ret = drmPrimeFDToHandle(panfrost_device_fd(dev), fd, &gem_handle);
+   ret = drmPrimeFDToHandle(dev->kmod.dev->fd, fd, &gem_handle);
    assert(!ret);
 
    bo = pan_lookup_bo(dev, gem_handle);
 
    if (!bo->dev) {
-      get_bo_offset.handle = gem_handle;
-      ret = drmIoctl(panfrost_device_fd(dev), DRM_IOCTL_PANFROST_GET_BO_OFFSET,
-                     &get_bo_offset);
+      bo->dev = dev;
+      bo->kmod_bo = pan_kmod_bo_import(dev->kmod.dev, fd, 0);
+
+      struct pan_kmod_vm_op vm_op = {
+         .type = PAN_KMOD_VM_OP_TYPE_MAP,
+         .va =
+            {
+               .start = PAN_KMOD_VM_MAP_AUTO_VA,
+               .size = bo->kmod_bo->size,
+            },
+         .map =
+            {
+               .bo = bo->kmod_bo,
+               .bo_offset = 0,
+            },
+      };
+
+      ASSERTED int ret = pan_kmod_vm_bind(
+         dev->kmod.vm, PAN_KMOD_VM_OP_MODE_IMMEDIATE, &vm_op, 1);
       assert(!ret);
 
-      bo->dev = dev;
-      bo->ptr.gpu = (mali_ptr)get_bo_offset.offset;
-      bo->size = lseek(fd, 0, SEEK_END);
-      /* Sometimes this can fail and return -1. size of -1 is not
-       * a nice thing for mmap to try mmap. Be more robust also
-       * for zero sized maps and fail nicely too
-       */
-      if ((panfrost_bo_size(bo) == 0) || (panfrost_bo_size(bo) == (size_t)-1)) {
-         pthread_mutex_unlock(&dev->bo_map_lock);
-         return NULL;
-      }
+      bo->ptr.gpu = vm_op.va.start;
       bo->flags = PAN_BO_SHARED;
-      bo->gem_handle = gem_handle;
       p_atomic_set(&bo->refcnt, 1);
    } else {
       /* bo->refcnt == 0 can happen if the BO
@@ -525,16 +513,9 @@ panfrost_bo_import(struct panfrost_device *dev, int fd)
 int
 panfrost_bo_export(struct panfrost_bo *bo)
 {
-   struct drm_prime_handle args = {
-      .handle = panfrost_bo_handle(bo),
-      .flags = DRM_CLOEXEC,
-   };
-
-   int ret = drmIoctl(panfrost_device_fd(bo->dev), DRM_IOCTL_PRIME_HANDLE_TO_FD,
-                      &args);
-   if (ret == -1)
-      return -1;
+   int ret = pan_kmod_bo_export(bo->kmod_bo);
+   if (ret >= 0)
+      bo->flags |= PAN_BO_SHARED;
 
-   bo->flags |= PAN_BO_SHARED;
-   return args.fd;
+   return ret;
 }
diff --git a/src/panfrost/lib/pan_bo.h b/src/panfrost/lib/pan_bo.h
index 2f0940e3755ef..244149983682a 100644
--- a/src/panfrost/lib/pan_bo.h
+++ b/src/panfrost/lib/pan_bo.h
@@ -30,6 +30,8 @@
 #include "util/list.h"
 #include "panfrost-job.h"
 
+#include "kmod/pan_kmod.h"
+
 /* Flags for allocated memory */
 
 /* This memory region is executable */
@@ -95,16 +97,14 @@ struct panfrost_bo {
    /* Atomic reference count */
    int32_t refcnt;
 
+   /* Kernel representation of a buffer object. */
+   struct pan_kmod_bo *kmod_bo;
+
    struct panfrost_device *dev;
 
    /* Mapping for the entire object (all levels) */
    struct panfrost_ptr ptr;
 
-   /* Size of all entire trees */
-   size_t size;
-
-   int gem_handle;
-
    uint32_t flags;
 
    /* Combination of PAN_BO_ACCESS_{READ,WRITE} flags encoding pending
@@ -120,13 +120,13 @@ struct panfrost_bo {
 static inline size_t
 panfrost_bo_size(struct panfrost_bo *bo)
 {
-   return bo->size;
+   return bo->kmod_bo->size;
 }
 
 static inline size_t
 panfrost_bo_handle(struct panfrost_bo *bo)
 {
-   return bo->gem_handle;
+   return bo->kmod_bo->handle;
 }
 
 bool panfrost_bo_wait(struct panfrost_bo *bo, int64_t timeout_ns,
-- 
GitLab


From 0aacc702c169e9c8d2f677a560ebe2a1fce6c02e Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Wed, 5 Jul 2023 10:25:58 +0200
Subject: [PATCH 08/32] panfrost: Introduce a PAN_BO_SHAREABLE flag

This flag reflects the ability to share a BO. This lets the kmod
backend optimize the case where the BO stays private to a specific
VM.

Signed-off-by: Boris Brezillon <boris.brezillon@collabora.com>
---
 src/panfrost/lib/pan_bo.c | 6 ++++--
 src/panfrost/lib/pan_bo.h | 4 ++++
 2 files changed, 8 insertions(+), 2 deletions(-)

diff --git a/src/panfrost/lib/pan_bo.c b/src/panfrost/lib/pan_bo.c
index 5e51039d4bcca..618a872c510b8 100644
--- a/src/panfrost/lib/pan_bo.c
+++ b/src/panfrost/lib/pan_bo.c
@@ -73,11 +73,13 @@ static struct panfrost_bo *
 panfrost_bo_alloc(struct panfrost_device *dev, size_t size, uint32_t flags,
                   const char *label)
 {
+   struct pan_kmod_vm *exclusive_vm =
+      !(flags & PAN_BO_SHAREABLE) ? dev->kmod.vm : NULL;
    struct pan_kmod_bo *kmod_bo;
    struct panfrost_bo *bo;
 
-   kmod_bo =
-      pan_kmod_bo_alloc(dev->kmod.dev, NULL, size, to_kmod_bo_flags(flags));
+   kmod_bo = pan_kmod_bo_alloc(dev->kmod.dev, exclusive_vm, size,
+                               to_kmod_bo_flags(flags));
    assert(kmod_bo);
 
    bo = pan_lookup_bo(dev, kmod_bo->handle);
diff --git a/src/panfrost/lib/pan_bo.h b/src/panfrost/lib/pan_bo.h
index 244149983682a..51fed3149a4da 100644
--- a/src/panfrost/lib/pan_bo.h
+++ b/src/panfrost/lib/pan_bo.h
@@ -52,6 +52,10 @@
  * cached locally */
 #define PAN_BO_SHARED (1 << 4)
 
+/* BO might be exported at some point. PAN_BO_SHAREABLE does not imply
+ * PAN_BO_SHARED if the BO has not been exported yet */
+#define PAN_BO_SHAREABLE (1 << 5)
+
 /* GPU access flags */
 
 /* BO is either shared (can be accessed by more than one GPU batch) or private
-- 
GitLab


From c9ca0644732151a9bd367c4006716b6c79bcb4fc Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Wed, 5 Jul 2023 10:35:20 +0200
Subject: [PATCH 09/32] panvk: Pass PAN_BO_SHAREABLE when relevant

Check VkExportMemoryAllocateInfo to know if we might export the BO
object at some point.

Signed-off-by: Boris Brezillon <boris.brezillon@collabora.com>
---
 src/panfrost/vulkan/panvk_device.c | 19 ++++++++++++++++---
 1 file changed, 16 insertions(+), 3 deletions(-)

diff --git a/src/panfrost/vulkan/panvk_device.c b/src/panfrost/vulkan/panvk_device.c
index 88f2c58d170a5..c9ba00b4c4642 100644
--- a/src/panfrost/vulkan/panvk_device.c
+++ b/src/panfrost/vulkan/panvk_device.c
@@ -1030,6 +1030,7 @@ panvk_AllocateMemory(VkDevice _device,
 {
    VK_FROM_HANDLE(panvk_device, device, _device);
    struct panvk_device_memory *mem;
+   bool can_be_exported = false;
 
    assert(pAllocateInfo->sType == VK_STRUCTURE_TYPE_MEMORY_ALLOCATE_INFO);
 
@@ -1039,6 +1040,18 @@ panvk_AllocateMemory(VkDevice _device,
       return VK_SUCCESS;
    }
 
+   const VkExportMemoryAllocateInfo *export_info =
+      vk_find_struct_const(pAllocateInfo->pNext, EXPORT_MEMORY_ALLOCATE_INFO);
+
+   if (export_info) {
+      if (export_info->handleTypes &
+          ~(VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT |
+            VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD_BIT))
+         return vk_error(device, VK_ERROR_INVALID_EXTERNAL_HANDLE);
+      else if (export_info->handleTypes)
+         can_be_exported = true;
+   }
+
    mem = vk_object_alloc(&device->vk, pAllocator, sizeof(*mem),
                          VK_OBJECT_TYPE_DEVICE_MEMORY);
    if (mem == NULL)
@@ -1064,9 +1077,9 @@ panvk_AllocateMemory(VkDevice _device,
       /* take ownership and close the fd */
       close(fd_info->fd);
    } else {
-      mem->bo = panfrost_bo_create(&device->physical_device->pdev,
-                                   pAllocateInfo->allocationSize, 0,
-                                   "User-requested memory");
+      mem->bo = panfrost_bo_create(
+         &device->physical_device->pdev, pAllocateInfo->allocationSize,
+         can_be_exported ? PAN_BO_SHAREABLE : 0, "User-requested memory");
    }
 
    assert(mem->bo);
-- 
GitLab


From 50aa1a8c94d4caf4430cc71afd6a6905b1662a1e Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Wed, 5 Jul 2023 10:44:35 +0200
Subject: [PATCH 10/32] panfrost: Flag BO shareable when appropriate

Let the kmod backend know when we might end up exporting a BO. This
doesn't change anything for the Panfrost kmod backend, but will be
needed for Panthor.

Signed-off-by: Boris Brezillon <boris.brezillon@collabora.com>
---
 src/gallium/drivers/panfrost/pan_resource.c | 9 +++++++--
 1 file changed, 7 insertions(+), 2 deletions(-)

diff --git a/src/gallium/drivers/panfrost/pan_resource.c b/src/gallium/drivers/panfrost/pan_resource.c
index a942c35cc3bcc..011a5386785dd 100644
--- a/src/gallium/drivers/panfrost/pan_resource.c
+++ b/src/gallium/drivers/panfrost/pan_resource.c
@@ -737,9 +737,14 @@ panfrost_resource_create_with_modifier(struct pipe_screen *screen,
    } else {
       /* We create a BO immediately but don't bother mapping, since we don't
        * care to map e.g. FBOs which the CPU probably won't touch */
+      uint32_t flags = PAN_BO_DELAY_MMAP;
 
-      so->image.data.bo = panfrost_bo_create(dev, so->image.layout.data_size,
-                                             PAN_BO_DELAY_MMAP, label);
+      /* If the resource is never exported, we can make the BO private. */
+      if (template->bind & PIPE_BIND_SHARED)
+         flags |= PAN_BO_SHAREABLE;
+
+      so->image.data.bo =
+         panfrost_bo_create(dev, so->image.layout.data_size, flags, label);
 
       so->constant_stencil = true;
    }
-- 
GitLab


From 60e6d7555cbda0ddd13efc1a571a47441d0215bf Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Tue, 21 Nov 2023 09:56:15 +0100
Subject: [PATCH 11/32] drm-uapi: Add panthor uAPI

Latest version of panthor uAPI. Make sure it gets updated before merging
mesa v10/panthor MR.

Signed-off-by: Boris Brezillon <boris.brezillon@collabora.com>
---
 include/drm-uapi/panthor_drm.h | 892 +++++++++++++++++++++++++++++++++
 1 file changed, 892 insertions(+)
 create mode 100644 include/drm-uapi/panthor_drm.h

diff --git a/include/drm-uapi/panthor_drm.h b/include/drm-uapi/panthor_drm.h
new file mode 100644
index 0000000000000..6d815df5e8291
--- /dev/null
+++ b/include/drm-uapi/panthor_drm.h
@@ -0,0 +1,892 @@
+/* SPDX-License-Identifier: MIT */
+/* Copyright (C) 2023 Collabora ltd. */
+#ifndef _PANTHOR_DRM_H_
+#define _PANTHOR_DRM_H_
+
+#include "drm.h"
+
+#if defined(__cplusplus)
+extern "C" {
+#endif
+
+/**
+ * DOC: Introduction
+ *
+ * This documentation describes the Panthor IOCTLs.
+ *
+ * Just a few generic rules about the data passed to the Panthor IOCTLs:
+ *
+ * - Structures must be aligned on 64-bit/8-byte. If the object is not
+ *   naturally aligned, a padding field must be added.
+ * - Fields must be explicitly aligned to their natural type alignment with
+ *   pad[0..N] fields.
+ * - All padding fields will be checked by the driver to make sure they are
+ *   zeroed.
+ * - Flags can be added, but not removed/replaced.
+ * - New fields can be added to the main structures (the structures
+ *   directly passed to the ioctl). Those fields can be added at the end of
+ *   the structure, or replace existing padding fields. Any new field being
+ *   added must preserve the behavior that existed before those fields were
+ *   added when a value of zero is passed.
+ * - New fields can be added to indirect objects (objects pointed by the
+ *   main structure), iff those objects are passed a size to reflect the
+ *   size known by the userspace driver (see drm_panthor_obj_array::stride
+ *   or drm_panthor_dev_query::size).
+ * - If the kernel driver is too old to know some fields, those will be
+ *   ignored if zero, and otherwise rejected (and so will be zero on output).
+ * - If userspace is too old to know some fields, those will be zeroed
+ *   (input) before the structure is parsed by the kernel driver.
+ * - Each new flag/field addition must come with a driver version update so
+ *   the userspace driver doesn't have to trial and error to know which
+ *   flags are supported.
+ * - Structures should not contain unions, as this would defeat the
+ *   extensibility of such structures.
+ * - IOCTLs can't be removed or replaced. New IOCTL IDs should be placed
+ *   at the end of the drm_panthor_ioctl_id enum.
+ */
+
+/**
+ * DOC: MMIO regions exposed to userspace.
+ *
+ * .. c:macro:: DRM_PANTHOR_USER_MMIO_OFFSET
+ *
+ * File offset for all MMIO regions being exposed to userspace. Don't use
+ * this value directly, use DRM_PANTHOR_USER_<name>_OFFSET values instead.
+ * pgoffset passed to mmap2() is an unsigned long, which forces us to use a
+ * different offset on 32-bit and 64-bit systems.
+ *
+ * .. c:macro:: DRM_PANTHOR_USER_FLUSH_ID_MMIO_OFFSET
+ *
+ * File offset for the LATEST_FLUSH_ID register. The Userspace driver controls
+ * GPU cache flushing through CS instructions, but the flush reduction
+ * mechanism requires a flush_id. This flush_id could be queried with an
+ * ioctl, but Arm provides a well-isolated register page containing only this
+ * read-only register, so let's expose this page through a static mmap offset
+ * and allow direct mapping of this MMIO region so we can avoid the
+ * user <-> kernel round-trip.
+ */
+#define DRM_PANTHOR_USER_MMIO_OFFSET_32BIT	(1ull << 43)
+#define DRM_PANTHOR_USER_MMIO_OFFSET_64BIT	(1ull << 56)
+#define DRM_PANTHOR_USER_MMIO_OFFSET		(sizeof(unsigned long) < 8 ? \
+						 DRM_PANTHOR_USER_MMIO_OFFSET_32BIT : \
+						 DRM_PANTHOR_USER_MMIO_OFFSET_64BIT)
+#define DRM_PANTHOR_USER_FLUSH_ID_MMIO_OFFSET	(DRM_PANTHOR_USER_MMIO_OFFSET | 0)
+
+/**
+ * DOC: IOCTL IDs
+ *
+ * enum drm_panthor_ioctl_id - IOCTL IDs
+ *
+ * Place new ioctls at the end, don't re-order, don't replace or remove entries.
+ *
+ * These IDs are not meant to be used directly. Use the DRM_IOCTL_PANTHOR_xxx
+ * definitions instead.
+ */
+enum drm_panthor_ioctl_id {
+	/** @DRM_PANTHOR_DEV_QUERY: Query device information. */
+	DRM_PANTHOR_DEV_QUERY = 0,
+
+	/** @DRM_PANTHOR_VM_CREATE: Create a VM. */
+	DRM_PANTHOR_VM_CREATE,
+
+	/** @DRM_PANTHOR_VM_DESTROY: Destroy a VM. */
+	DRM_PANTHOR_VM_DESTROY,
+
+	/** @DRM_PANTHOR_VM_BIND: Bind/unbind memory to a VM. */
+	DRM_PANTHOR_VM_BIND,
+
+	/** @DRM_PANTHOR_BO_CREATE: Create a buffer object. */
+	DRM_PANTHOR_BO_CREATE,
+
+	/**
+	 * @DRM_PANTHOR_BO_MMAP_OFFSET: Get the file offset to pass to
+	 * mmap to map a GEM object.
+	 */
+	DRM_PANTHOR_BO_MMAP_OFFSET,
+
+	/** @DRM_PANTHOR_GROUP_CREATE: Create a scheduling group. */
+	DRM_PANTHOR_GROUP_CREATE,
+
+	/** @DRM_PANTHOR_GROUP_DESTROY: Destroy a scheduling group. */
+	DRM_PANTHOR_GROUP_DESTROY,
+
+	/**
+	 * @DRM_PANTHOR_GROUP_SUBMIT: Submit jobs to queues belonging
+	 * to a specific scheduling group.
+	 */
+	DRM_PANTHOR_GROUP_SUBMIT,
+
+	/** @DRM_PANTHOR_GROUP_GET_STATE: Get the state of a scheduling group. */
+	DRM_PANTHOR_GROUP_GET_STATE,
+
+	/** @DRM_PANTHOR_TILER_HEAP_CREATE: Create a tiler heap. */
+	DRM_PANTHOR_TILER_HEAP_CREATE,
+
+	/** @DRM_PANTHOR_TILER_HEAP_DESTROY: Destroy a tiler heap. */
+	DRM_PANTHOR_TILER_HEAP_DESTROY,
+};
+
+/**
+ * DRM_IOCTL_PANTHOR() - Build a Panthor IOCTL number
+ * @__access: Access type. Must be R, W or RW.
+ * @__id: One of the DRM_PANTHOR_xxx id.
+ * @__type: Suffix of the type being passed to the IOCTL.
+ *
+ * Don't use this macro directly, use the DRM_IOCTL_PANTHOR_xxx
+ * values instead.
+ *
+ * Return: An IOCTL number to be passed to ioctl() from userspace.
+ */
+#define DRM_IOCTL_PANTHOR(__access, __id, __type) \
+	DRM_IO ## __access(DRM_COMMAND_BASE + DRM_PANTHOR_ ## __id, \
+			   struct drm_panthor_ ## __type)
+
+#define DRM_IOCTL_PANTHOR_DEV_QUERY \
+	DRM_IOCTL_PANTHOR(WR, DEV_QUERY, dev_query)
+#define DRM_IOCTL_PANTHOR_VM_CREATE \
+	DRM_IOCTL_PANTHOR(WR, VM_CREATE, vm_create)
+#define DRM_IOCTL_PANTHOR_VM_DESTROY \
+	DRM_IOCTL_PANTHOR(WR, VM_DESTROY, vm_destroy)
+#define DRM_IOCTL_PANTHOR_VM_BIND \
+	DRM_IOCTL_PANTHOR(WR, VM_BIND, vm_bind)
+#define DRM_IOCTL_PANTHOR_BO_CREATE \
+	DRM_IOCTL_PANTHOR(WR, BO_CREATE, bo_create)
+#define DRM_IOCTL_PANTHOR_BO_MMAP_OFFSET \
+	DRM_IOCTL_PANTHOR(WR, BO_MMAP_OFFSET, bo_mmap_offset)
+#define DRM_IOCTL_PANTHOR_GROUP_CREATE \
+	DRM_IOCTL_PANTHOR(WR, GROUP_CREATE, group_create)
+#define DRM_IOCTL_PANTHOR_GROUP_DESTROY \
+	DRM_IOCTL_PANTHOR(WR, GROUP_DESTROY, group_destroy)
+#define DRM_IOCTL_PANTHOR_GROUP_SUBMIT \
+	DRM_IOCTL_PANTHOR(WR, GROUP_SUBMIT, group_submit)
+#define DRM_IOCTL_PANTHOR_GROUP_GET_STATE \
+	DRM_IOCTL_PANTHOR(WR, GROUP_GET_STATE, group_get_state)
+#define DRM_IOCTL_PANTHOR_TILER_HEAP_CREATE \
+	DRM_IOCTL_PANTHOR(WR, TILER_HEAP_CREATE, tiler_heap_create)
+#define DRM_IOCTL_PANTHOR_TILER_HEAP_DESTROY \
+	DRM_IOCTL_PANTHOR(WR, TILER_HEAP_DESTROY, tiler_heap_destroy)
+
+/**
+ * DOC: IOCTL arguments
+ */
+
+/**
+ * struct drm_panthor_obj_array - Object array.
+ *
+ * This object is used to pass an array of objects whose size is subject to changes in
+ * future versions of the driver. In order to support this mutability, we pass a stride
+ * describing the size of the object as known by userspace.
+ *
+ * You shouldn't fill drm_panthor_obj_array fields directly. You should instead use
+ * the DRM_PANTHOR_OBJ_ARRAY() macro that takes care of initializing the stride to
+ * the object size.
+ */
+struct drm_panthor_obj_array {
+	/** @stride: Stride of object struct. Used for versioning. */
+	__u32 stride;
+
+	/** @count: Number of objects in the array. */
+	__u32 count;
+
+	/** @array: User pointer to an array of objects. */
+	__u64 array;
+};
+
+/**
+ * DRM_PANTHOR_OBJ_ARRAY() - Initialize a drm_panthor_obj_array field.
+ * @cnt: Number of elements in the array.
+ * @ptr: Pointer to the array to pass to the kernel.
+ *
+ * Macro initializing a drm_panthor_obj_array based on the object size as known
+ * by userspace.
+ */
+#define DRM_PANTHOR_OBJ_ARRAY(cnt, ptr) \
+	{ .stride = sizeof((ptr)[0]), .count = (cnt), .array = (__u64)(uintptr_t)(ptr) }
+
+/**
+ * enum drm_panthor_sync_op_flags - Synchronization operation flags.
+ */
+enum drm_panthor_sync_op_flags {
+	/** @DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_MASK: Synchronization handle type mask. */
+	DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_MASK = 0xff,
+
+	/** @DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_SYNCOBJ: Synchronization object type. */
+	DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_SYNCOBJ = 0,
+
+	/**
+	 * @DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_TIMELINE_SYNCOBJ: Timeline synchronization
+	 * object type.
+	 */
+	DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_TIMELINE_SYNCOBJ = 1,
+
+	/** @DRM_PANTHOR_SYNC_OP_WAIT: Wait operation. */
+	DRM_PANTHOR_SYNC_OP_WAIT = 0 << 31,
+
+	/** @DRM_PANTHOR_SYNC_OP_SIGNAL: Signal operation. */
+	DRM_PANTHOR_SYNC_OP_SIGNAL = (int)(1u << 31),
+};
+
+/**
+ * struct drm_panthor_sync_op - Synchronization operation.
+ */
+struct drm_panthor_sync_op {
+	/** @flags: Synchronization operation flags. Combination of DRM_PANTHOR_SYNC_OP values. */
+	__u32 flags;
+
+	/** @handle: Sync handle. */
+	__u32 handle;
+
+	/**
+	 * @timeline_value: MBZ if
+	 * (flags & DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_MASK) !=
+	 * DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_TIMELINE_SYNCOBJ.
+	 */
+	__u64 timeline_value;
+};
+
+/**
+ * enum drm_panthor_dev_query_type - Query type
+ *
+ * Place new types at the end, don't re-order, don't remove or replace.
+ */
+enum drm_panthor_dev_query_type {
+	/** @DRM_PANTHOR_DEV_QUERY_GPU_INFO: Query GPU information. */
+	DRM_PANTHOR_DEV_QUERY_GPU_INFO = 0,
+
+	/** @DRM_PANTHOR_DEV_QUERY_CSIF_INFO: Query command-stream interface information. */
+	DRM_PANTHOR_DEV_QUERY_CSIF_INFO,
+};
+
+/**
+ * struct drm_panthor_gpu_info - GPU information
+ *
+ * Structure grouping all queryable information relating to the GPU.
+ */
+struct drm_panthor_gpu_info {
+	/** @gpu_id : GPU ID. */
+	__u32 gpu_id;
+#define DRM_PANTHOR_ARCH_MAJOR(x)		((x) >> 28)
+#define DRM_PANTHOR_ARCH_MINOR(x)		(((x) >> 24) & 0xf)
+#define DRM_PANTHOR_ARCH_REV(x)			(((x) >> 20) & 0xf)
+#define DRM_PANTHOR_PRODUCT_MAJOR(x)		(((x) >> 16) & 0xf)
+#define DRM_PANTHOR_VERSION_MAJOR(x)		(((x) >> 12) & 0xf)
+#define DRM_PANTHOR_VERSION_MINOR(x)		(((x) >> 4) & 0xff)
+#define DRM_PANTHOR_VERSION_STATUS(x)		((x) & 0xf)
+
+	/** @gpu_rev: GPU revision. */
+	__u32 gpu_rev;
+
+	/** @csf_id: Command stream frontend ID. */
+	__u32 csf_id;
+#define DRM_PANTHOR_CSHW_MAJOR(x)		(((x) >> 26) & 0x3f)
+#define DRM_PANTHOR_CSHW_MINOR(x)		(((x) >> 20) & 0x3f)
+#define DRM_PANTHOR_CSHW_REV(x)			(((x) >> 16) & 0xf)
+#define DRM_PANTHOR_MCU_MAJOR(x)		(((x) >> 10) & 0x3f)
+#define DRM_PANTHOR_MCU_MINOR(x)		(((x) >> 4) & 0x3f)
+#define DRM_PANTHOR_MCU_REV(x)			((x) & 0xf)
+
+	/** @l2_features: L2-cache features. */
+	__u32 l2_features;
+
+	/** @tiler_features: Tiler features. */
+	__u32 tiler_features;
+
+	/** @mem_features: Memory features. */
+	__u32 mem_features;
+
+	/** @mmu_features: MMU features. */
+	__u32 mmu_features;
+#define DRM_PANTHOR_MMU_VA_BITS(x)		((x) & 0xff)
+
+	/** @thread_features: Thread features. */
+	__u32 thread_features;
+
+	/** @max_threads: Maximum number of threads. */
+	__u32 max_threads;
+
+	/** @thread_max_workgroup_size: Maximum workgroup size. */
+	__u32 thread_max_workgroup_size;
+
+	/**
+	 * @thread_max_barrier_size: Maximum number of threads that can wait
+	 * simultaneously on a barrier.
+	 */
+	__u32 thread_max_barrier_size;
+
+	/** @coherency_features: Coherency features. */
+	__u32 coherency_features;
+
+	/** @texture_features: Texture features. */
+	__u32 texture_features[4];
+
+	/** @as_present: Bitmask encoding the number of address-space exposed by the MMU. */
+	__u32 as_present;
+
+	/** @shader_present: Bitmask encoding the shader cores exposed by the GPU. */
+	__u64 shader_present;
+
+	/** @l2_present: Bitmask encoding the L2 caches exposed by the GPU. */
+	__u64 l2_present;
+
+	/** @tiler_present: Bitmask encoding the tiler units exposed by the GPU. */
+	__u64 tiler_present;
+};
+
+/**
+ * struct drm_panthor_csif_info - Command stream interface information
+ *
+ * Structure grouping all queryable information relating to the command stream interface.
+ */
+struct drm_panthor_csif_info {
+	/** @csg_slot_count: Number of command stream group slots exposed by the firmware. */
+	__u32 csg_slot_count;
+
+	/** @cs_slot_count: Number of command stream slots per group. */
+	__u32 cs_slot_count;
+
+	/** @cs_reg_count: Number of command stream registers. */
+	__u32 cs_reg_count;
+
+	/** @scoreboard_slot_count: Number of scoreboard slots. */
+	__u32 scoreboard_slot_count;
+
+	/**
+	 * @unpreserved_cs_reg_count: Number of command stream registers reserved by
+	 * the kernel driver to call a userspace command stream.
+	 *
+	 * All registers can be used by a userspace command stream, but the
+	 * [cs_slot_count - unpreserved_cs_reg_count .. cs_slot_count] registers are
+	 * used by the kernel when DRM_PANTHOR_IOCTL_GROUP_SUBMIT is called.
+	 */
+	__u32 unpreserved_cs_reg_count;
+
+	/**
+	 * @pad: Padding field, set to zero.
+	 */
+	__u32 pad;
+};
+
+/**
+ * struct drm_panthor_dev_query - Arguments passed to DRM_PANTHOR_IOCTL_DEV_QUERY
+ */
+struct drm_panthor_dev_query {
+	/** @type: the query type (see drm_panthor_dev_query_type). */
+	__u32 type;
+
+	/**
+	 * @size: size of the type being queried.
+	 *
+	 * If pointer is NULL, size is updated by the driver to provide the
+	 * output structure size. If pointer is not NULL, the driver will
+	 * only copy min(size, actual_structure_size) bytes to the pointer,
+	 * and update the size accordingly. This allows us to extend query
+	 * types without breaking userspace.
+	 */
+	__u32 size;
+
+	/**
+	 * @pointer: user pointer to a query type struct.
+	 *
+	 * Pointer can be NULL, in which case, nothing is copied, but the
+	 * actual structure size is returned. If not NULL, it must point to
+	 * a location that's large enough to hold size bytes.
+	 */
+	__u64 pointer;
+};
+
+/**
+ * struct drm_panthor_vm_create - Arguments passed to DRM_PANTHOR_IOCTL_VM_CREATE
+ */
+struct drm_panthor_vm_create {
+	/** @flags: VM flags, MBZ. */
+	__u32 flags;
+
+	/** @id: Returned VM ID. */
+	__u32 id;
+
+	/**
+	 * @user_va_range: Size of the VA space reserved for user objects.
+	 *
+	 * The kernel will pick the remaining space to map kernel-only objects to the
+	 * VM (heap chunks, heap context, ring buffers, kernel synchronization objects,
+	 * ...). If the space left for kernel objects is too small, kernel object
+	 * allocation will fail further down the road. One can use
+	 * drm_panthor_gpu_info::mmu_features to extract the total virtual address
+	 * range, and chose a user_va_range that leaves some space to the kernel.
+	 *
+	 * If user_va_range is zero, the kernel will pick a sensible value based on
+	 * TASK_SIZE and the virtual range supported by the GPU MMU (the kernel/user
+	 * split should leave enough VA space for userspace processes to support SVM,
+	 * while still allowing the kernel to map some amount of kernel objects in
+	 * the kernel VA range). The value chosen by the driver will be returned in
+	 * @user_va_range.
+	 *
+	 * User VA space always starts at 0x0, kernel VA space is always placed after
+	 * the user VA range.
+	 */
+	__u64 user_va_range;
+};
+
+/**
+ * struct drm_panthor_vm_destroy - Arguments passed to DRM_PANTHOR_IOCTL_VM_DESTROY
+ */
+struct drm_panthor_vm_destroy {
+	/** @id: ID of the VM to destroy. */
+	__u32 id;
+
+	/** @pad: MBZ. */
+	__u32 pad;
+};
+
+/**
+ * enum drm_panthor_vm_bind_op_flags - VM bind operation flags
+ */
+enum drm_panthor_vm_bind_op_flags {
+	/**
+	 * @DRM_PANTHOR_VM_BIND_OP_MAP_READONLY: Map the memory read-only.
+	 *
+	 * Only valid with DRM_PANTHOR_VM_BIND_OP_TYPE_MAP.
+	 */
+	DRM_PANTHOR_VM_BIND_OP_MAP_READONLY = 1 << 0,
+
+	/**
+	 * @DRM_PANTHOR_VM_BIND_OP_MAP_NOEXEC: Map the memory not-executable.
+	 *
+	 * Only valid with DRM_PANTHOR_VM_BIND_OP_TYPE_MAP.
+	 */
+	DRM_PANTHOR_VM_BIND_OP_MAP_NOEXEC = 1 << 1,
+
+	/**
+	 * @DRM_PANTHOR_VM_BIND_OP_MAP_UNCACHED: Map the memory uncached.
+	 *
+	 * Only valid with DRM_PANTHOR_VM_BIND_OP_TYPE_MAP.
+	 */
+	DRM_PANTHOR_VM_BIND_OP_MAP_UNCACHED = 1 << 2,
+
+	/**
+	 * @DRM_PANTHOR_VM_BIND_OP_TYPE_MASK: Mask used to determine the type of operation.
+	 */
+	DRM_PANTHOR_VM_BIND_OP_TYPE_MASK = (int)(0xfu << 28),
+
+	/** @DRM_PANTHOR_VM_BIND_OP_TYPE_MAP: Map operation. */
+	DRM_PANTHOR_VM_BIND_OP_TYPE_MAP = 0 << 28,
+
+	/** @DRM_PANTHOR_VM_BIND_OP_TYPE_UNMAP: Unmap operation. */
+	DRM_PANTHOR_VM_BIND_OP_TYPE_UNMAP = 1 << 28,
+
+	/**
+	 * @DRM_PANTHOR_VM_BIND_OP_TYPE_SYNC_ONLY: No VM operation.
+	 *
+	 * Just serves as a synchronization point on a VM queue.
+	 *
+	 * Only valid if %DRM_PANTHOR_VM_BIND_ASYNC is set in drm_panthor_vm_bind::flags,
+	 * and drm_panthor_vm_bind_op::syncs contains at least one element.
+	 */
+	DRM_PANTHOR_VM_BIND_OP_TYPE_SYNC_ONLY = 2 << 28,
+};
+
+/**
+ * struct drm_panthor_vm_bind_op - VM bind operation
+ */
+struct drm_panthor_vm_bind_op {
+	/** @flags: Combination of drm_panthor_vm_bind_op_flags flags. */
+	__u32 flags;
+
+	/**
+	 * @bo_handle: Handle of the buffer object to map.
+	 * MBZ for unmap or sync-only operations.
+	 */
+	__u32 bo_handle;
+
+	/**
+	 * @bo_offset: Buffer object offset.
+	 * MBZ for unmap or sync-only operations.
+	 */
+	__u64 bo_offset;
+
+	/**
+	 * @va: Virtual address to map/unmap.
+	 * MBZ for sync-only operations.
+	 */
+	__u64 va;
+
+	/**
+	 * @size: Size to map/unmap.
+	 * MBZ for sync-only operations.
+	 */
+	__u64 size;
+
+	/**
+	 * @syncs: Array of struct drm_panthor_sync_op synchronization
+	 * operations.
+	 *
+	 * This array must be empty if %DRM_PANTHOR_VM_BIND_ASYNC is not set on
+	 * the drm_panthor_vm_bind object containing this VM bind operation.
+	 *
+	 * This array shall not be empty for sync-only operations.
+	 */
+	struct drm_panthor_obj_array syncs;
+
+};
+
+/**
+ * enum drm_panthor_vm_bind_flags - VM bind flags
+ */
+enum drm_panthor_vm_bind_flags {
+	/**
+	 * @DRM_PANTHOR_VM_BIND_ASYNC: VM bind operations are queued to the VM
+	 * queue instead of being executed synchronously.
+	 */
+	DRM_PANTHOR_VM_BIND_ASYNC = 1 << 0,
+};
+
+/**
+ * struct drm_panthor_vm_bind - Arguments passed to DRM_IOCTL_PANTHOR_VM_BIND
+ */
+struct drm_panthor_vm_bind {
+	/** @vm_id: VM targeted by the bind request. */
+	__u32 vm_id;
+
+	/** @flags: Combination of drm_panthor_vm_bind_flags flags. */
+	__u32 flags;
+
+	/** @ops: Array of struct drm_panthor_vm_bind_op bind operations. */
+	struct drm_panthor_obj_array ops;
+};
+
+/**
+ * enum drm_panthor_bo_flags - Buffer object flags, passed at creation time.
+ */
+enum drm_panthor_bo_flags {
+	/** @DRM_PANTHOR_BO_NO_MMAP: The buffer object will never be CPU-mapped in userspace. */
+	DRM_PANTHOR_BO_NO_MMAP = (1 << 0),
+};
+
+/**
+ * struct drm_panthor_bo_create - Arguments passed to DRM_IOCTL_PANTHOR_BO_CREATE.
+ */
+struct drm_panthor_bo_create {
+	/**
+	 * @size: Requested size for the object
+	 *
+	 * The (page-aligned) allocated size for the object will be returned.
+	 */
+	__u64 size;
+
+	/**
+	 * @flags: Flags. Must be a combination of drm_panthor_bo_flags flags.
+	 */
+	__u32 flags;
+
+	/**
+	 * @exclusive_vm_id: Exclusive VM this buffer object will be mapped to.
+	 *
+	 * If not zero, the field must refer to a valid VM ID, and implies that:
+	 *  - the buffer object will only ever be bound to that VM
+	 *  - cannot be exported as a PRIME fd
+	 */
+	__u32 exclusive_vm_id;
+
+	/**
+	 * @handle: Returned handle for the object.
+	 *
+	 * Object handles are nonzero.
+	 */
+	__u32 handle;
+
+	/** @pad: MBZ. */
+	__u32 pad;
+};
+
+/**
+ * struct drm_panthor_bo_mmap_offset - Arguments passed to DRM_IOCTL_PANTHOR_BO_MMAP_OFFSET.
+ */
+struct drm_panthor_bo_mmap_offset {
+	/** @handle: Handle of the object we want an mmap offset for. */
+	__u32 handle;
+
+	/** @pad: MBZ. */
+	__u32 pad;
+
+	/** @offset: The fake offset to use for subsequent mmap calls. */
+	__u64 offset;
+};
+
+/**
+ * struct drm_panthor_queue_create - Queue creation arguments.
+ */
+struct drm_panthor_queue_create {
+	/**
+	 * @priority: Defines the priority of queues inside a group. Goes from 0 to 15,
+	 * 15 being the highest priority.
+	 */
+	__u8 priority;
+
+	/** @pad: Padding fields, MBZ. */
+	__u8 pad[3];
+
+	/** @ringbuf_size: Size of the ring buffer to allocate to this queue. */
+	__u32 ringbuf_size;
+};
+
+/**
+ * enum drm_panthor_group_priority - Scheduling group priority
+ */
+enum drm_panthor_group_priority {
+	/** @PANTHOR_GROUP_PRIORITY_LOW: Low priority group. */
+	PANTHOR_GROUP_PRIORITY_LOW = 0,
+
+	/** @PANTHOR_GROUP_PRIORITY_MEDIUM: Medium priority group. */
+	PANTHOR_GROUP_PRIORITY_MEDIUM,
+
+	/** @PANTHOR_GROUP_PRIORITY_HIGH: High priority group. */
+	PANTHOR_GROUP_PRIORITY_HIGH,
+};
+
+/**
+ * struct drm_panthor_group_create - Arguments passed to DRM_IOCTL_PANTHOR_GROUP_CREATE
+ */
+struct drm_panthor_group_create {
+	/** @queues: Array of drm_panthor_queue_create elements. */
+	struct drm_panthor_obj_array queues;
+
+	/**
+	 * @max_compute_cores: Maximum number of cores that can be used by compute
+	 * jobs across CS queues bound to this group.
+	 *
+	 * Must be less or equal to the number of bits set in @compute_core_mask.
+	 */
+	__u8 max_compute_cores;
+
+	/**
+	 * @max_fragment_cores: Maximum number of cores that can be used by fragment
+	 * jobs across CS queues bound to this group.
+	 *
+	 * Must be less or equal to the number of bits set in @fragment_core_mask.
+	 */
+	__u8 max_fragment_cores;
+
+	/**
+	 * @max_tiler_cores: Maximum number of tilers that can be used by tiler jobs
+	 * across CS queues bound to this group.
+	 *
+	 * Must be less or equal to the number of bits set in @tiler_core_mask.
+	 */
+	__u8 max_tiler_cores;
+
+	/** @priority: Group priority (see enum drm_panthor_group_priority). */
+	__u8 priority;
+
+	/** @pad: Padding field, MBZ. */
+	__u32 pad;
+
+	/**
+	 * @compute_core_mask: Mask encoding cores that can be used for compute jobs.
+	 *
+	 * This field must have at least @max_compute_cores bits set.
+	 *
+	 * The bits set here should also be set in drm_panthor_gpu_info::shader_present.
+	 */
+	__u64 compute_core_mask;
+
+	/**
+	 * @fragment_core_mask: Mask encoding cores that can be used for fragment jobs.
+	 *
+	 * This field must have at least @max_fragment_cores bits set.
+	 *
+	 * The bits set here should also be set in drm_panthor_gpu_info::shader_present.
+	 */
+	__u64 fragment_core_mask;
+
+	/**
+	 * @tiler_core_mask: Mask encoding cores that can be used for tiler jobs.
+	 *
+	 * This field must have at least @max_tiler_cores bits set.
+	 *
+	 * The bits set here should also be set in drm_panthor_gpu_info::tiler_present.
+	 */
+	__u64 tiler_core_mask;
+
+	/**
+	 * @vm_id: VM ID to bind this group to.
+	 *
+	 * All submission to queues bound to this group will use this VM.
+	 */
+	__u32 vm_id;
+
+	/**
+	 * @group_handle: Returned group handle. Passed back when submitting jobs or
+	 * destroying a group.
+	 */
+	__u32 group_handle;
+};
+
+/**
+ * struct drm_panthor_group_destroy - Arguments passed to DRM_IOCTL_PANTHOR_GROUP_DESTROY
+ */
+struct drm_panthor_group_destroy {
+	/** @group_handle: Group to destroy */
+	__u32 group_handle;
+
+	/** @pad: Padding field, MBZ. */
+	__u32 pad;
+};
+
+/**
+ * struct drm_panthor_queue_submit - Job submission arguments.
+ *
+ * This is describing the userspace command stream to call from the kernel
+ * command stream ring-buffer. Queue submission is always part of a group
+ * submission, taking one or more jobs to submit to the underlying queues.
+ */
+struct drm_panthor_queue_submit {
+	/** @queue_index: Index of the queue inside a group. */
+	__u32 queue_index;
+
+	/**
+	 * @stream_size: Size of the command stream to execute.
+	 *
+	 * Must be 64-bit/8-byte aligned (the size of a CS instruction)
+	 *
+	 * Can be zero if stream_addr is zero too.
+	 */
+	__u32 stream_size;
+
+	/**
+	 * @stream_addr: GPU address of the command stream to execute.
+	 *
+	 * Must be aligned on 64-byte.
+	 *
+	 * Can be zero is stream_size is zero too.
+	 */
+	__u64 stream_addr;
+
+	/**
+	 * @latest_flush: FLUSH_ID read at the time the stream was built.
+	 *
+	 * This allows cache flush elimination for the automatic
+	 * flush+invalidate(all) done at submission time, which is needed to
+	 * ensure the GPU doesn't get garbage when reading the indirect command
+	 * stream buffers. If you want the cache flush to happen
+	 * unconditionally, pass a zero here.
+	 */
+	__u32 latest_flush;
+
+	/** @pad: MBZ. */
+	__u32 pad;
+
+	/** @syncs: Array of struct drm_panthor_sync_op sync operations. */
+	struct drm_panthor_obj_array syncs;
+};
+
+/**
+ * struct drm_panthor_group_submit - Arguments passed to DRM_IOCTL_PANTHOR_VM_BIND
+ */
+struct drm_panthor_group_submit {
+	/** @group_handle: Handle of the group to queue jobs to. */
+	__u32 group_handle;
+
+	/** @pad: MBZ. */
+	__u32 pad;
+
+	/** @queue_submits: Array of drm_panthor_queue_submit objects. */
+	struct drm_panthor_obj_array queue_submits;
+};
+
+/**
+ * enum drm_panthor_group_state_flags - Group state flags
+ */
+enum drm_panthor_group_state_flags {
+	/**
+	 * @DRM_PANTHOR_GROUP_STATE_TIMEDOUT: Group had unfinished jobs.
+	 *
+	 * When a group ends up with this flag set, no jobs can be submitted to its queues.
+	 */
+	DRM_PANTHOR_GROUP_STATE_TIMEDOUT = 1 << 0,
+
+	/**
+	 * @DRM_PANTHOR_GROUP_STATE_FATAL_FAULT: Group had fatal faults.
+	 *
+	 * When a group ends up with this flag set, no jobs can be submitted to its queues.
+	 */
+	DRM_PANTHOR_GROUP_STATE_FATAL_FAULT = 1 << 1,
+};
+
+/**
+ * struct drm_panthor_group_get_state - Arguments passed to DRM_IOCTL_PANTHOR_GROUP_GET_STATE
+ *
+ * Used to query the state of a group and decide whether a new group should be created to
+ * replace it.
+ */
+struct drm_panthor_group_get_state {
+	/** @group_handle: Handle of the group to query state on */
+	__u32 group_handle;
+
+	/**
+	 * @state: Combination of DRM_PANTHOR_GROUP_STATE_* flags encoding the
+	 * group state.
+	 */
+	__u32 state;
+
+	/** @fatal_queues: Bitmask of queues that faced fatal faults. */
+	__u32 fatal_queues;
+
+	/** @pad: MBZ */
+	__u32 pad;
+};
+
+/**
+ * struct drm_panthor_tiler_heap_create - Arguments passed to DRM_IOCTL_PANTHOR_TILER_HEAP_CREATE
+ */
+struct drm_panthor_tiler_heap_create {
+	/** @vm_id: VM ID the tiler heap should be mapped to */
+	__u32 vm_id;
+
+	/** @initial_chunk_count: Initial number of chunks to allocate. */
+	__u32 initial_chunk_count;
+
+	/** @chunk_size: Chunk size. Must be a power of two at least 256KB large. */
+	__u32 chunk_size;
+
+	/** @max_chunks: Maximum number of chunks that can be allocated. */
+	__u32 max_chunks;
+
+	/**
+	 * @target_in_flight: Maximum number of in-flight render passes.
+	 *
+	 * If the heap has more than tiler jobs in-flight, the FW will wait for render
+	 * passes to finish before queuing new tiler jobs.
+	 */
+	__u32 target_in_flight;
+
+	/** @handle: Returned heap handle. Passed back to DESTROY_TILER_HEAP. */
+	__u32 handle;
+
+	/** @tiler_heap_ctx_gpu_va: Returned heap GPU virtual address returned */
+	__u64 tiler_heap_ctx_gpu_va;
+
+	/**
+	 * @first_heap_chunk_gpu_va: First heap chunk.
+	 *
+	 * The tiler heap is formed of heap chunks forming a single-link list. This
+	 * is the first element in the list.
+	 */
+	__u64 first_heap_chunk_gpu_va;
+};
+
+/**
+ * struct drm_panthor_tiler_heap_destroy - Arguments passed to DRM_IOCTL_PANTHOR_TILER_HEAP_DESTROY
+ */
+struct drm_panthor_tiler_heap_destroy {
+	/** @handle: Handle of the tiler heap to destroy */
+	__u32 handle;
+
+	/** @pad: Padding field, MBZ. */
+	__u32 pad;
+};
+
+#if defined(__cplusplus)
+}
+#endif
+
+#endif /* _PANTHOR_DRM_H_ */
-- 
GitLab


From adc2efee935f22c9807a7a8408f2598544809718 Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Mon, 4 Dec 2023 17:40:27 +0100
Subject: [PATCH 12/32] fixup! drm-uapi: Add panthor uAPI

---
 include/drm-uapi/panthor_drm.h | 47 ++++++++++++++++++++++++++++++++++
 1 file changed, 47 insertions(+)

diff --git a/include/drm-uapi/panthor_drm.h b/include/drm-uapi/panthor_drm.h
index 6d815df5e8291..c952c8cc562e8 100644
--- a/include/drm-uapi/panthor_drm.h
+++ b/include/drm-uapi/panthor_drm.h
@@ -95,6 +95,9 @@ enum drm_panthor_ioctl_id {
 	/** @DRM_PANTHOR_VM_BIND: Bind/unbind memory to a VM. */
 	DRM_PANTHOR_VM_BIND,
 
+	/** @DRM_PANTHOR_VM_GET_STATE: Get VM state. */
+	DRM_PANTHOR_VM_GET_STATE,
+
 	/** @DRM_PANTHOR_BO_CREATE: Create a buffer object. */
 	DRM_PANTHOR_BO_CREATE,
 
@@ -149,6 +152,8 @@ enum drm_panthor_ioctl_id {
 	DRM_IOCTL_PANTHOR(WR, VM_DESTROY, vm_destroy)
 #define DRM_IOCTL_PANTHOR_VM_BIND \
 	DRM_IOCTL_PANTHOR(WR, VM_BIND, vm_bind)
+#define DRM_IOCTL_PANTHOR_VM_GET_STATE \
+	DRM_IOCTL_PANTHOR(WR, VM_GET_STATE, vm_get_state)
 #define DRM_IOCTL_PANTHOR_BO_CREATE \
 	DRM_IOCTL_PANTHOR(WR, BO_CREATE, bo_create)
 #define DRM_IOCTL_PANTHOR_BO_MMAP_OFFSET \
@@ -554,6 +559,48 @@ struct drm_panthor_vm_bind {
 	struct drm_panthor_obj_array ops;
 };
 
+/**
+ * enum drm_panthor_vm_state - VM states.
+ */
+enum drm_panthor_vm_state {
+	/**
+	 * @DRM_PANTHOR_VM_STATE_USABLE: VM is usable.
+	 *
+	 * New VM operations will be accepted on this VM.
+	 */
+	DRM_PANTHOR_VM_STATE_USABLE,
+
+	/**
+	 * @DRM_PANTHOR_VM_STATE_UNSABLE: VM is unsable.
+	 *
+	 * Something put the VM in an unusable state (like an asynchronous
+	 * VM_BIND request failing for any reason).
+	 *
+	 * Once the VM is in this state, all new MAP operations will be
+	 * rejected, and any GPU job targeting this VM will fail.
+	 * UNMAP operations are still accepted.
+	 *
+	 * The only way to recover from an unusable VM is to create a new
+	 * VM, and destroy the old one.
+	 */
+	DRM_PANTHOR_VM_STATE_UNUSABLE,
+};
+
+/**
+ * struct drm_panthor_vm_get_state - Get VM state.
+ */
+struct drm_panthor_vm_get_state {
+	/** @vm_id: VM targeted by the get_state request. */
+	__u32 vm_id;
+
+	/**
+	 * @state: state returned by the driver.
+	 *
+	 * Must be one of the enum drm_panthor_vm_state values.
+	 */
+	__u32 state;
+};
+
 /**
  * enum drm_panthor_bo_flags - Buffer object flags, passed at creation time.
  */
-- 
GitLab


From 5676592b4d88047d30e6056ff2bea62220cc4dee Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Tue, 21 Nov 2023 10:06:04 +0100
Subject: [PATCH 13/32] pan/kmod: Add a backend for panthor

Panthor is a new kernel driver handling CSF-based GPUs. It's designed
around the new VM model where:

- VM management is all explicit (you get to choose where objects are
  mapped in the GPU VA space)
- synchronization is explicit too (there's not BO_WAIT, and we don't
  pass BOs around to serve as implicit deps)

We add a few panthor specific helpers (those exposed in panthor_kmod.h)
too:

- panthor_kmod_xxx_sync_point() are needed to make pan_kmod_bo_wait()
  work with the new synchronization/VM model
- panthor_kmod_get_flush_id() is exposing the LATEST_FLUSH_ID register
- panthor_kmod_vm_handle() is providing a way to query the VM handle
  attached to the pan_kmod_vm object (needed for a few panthor ioctls)

Signed-off-by: Boris Brezillon <boris.brezillon@collabora.com>
---
 src/panfrost/lib/kmod/meson.build    |   1 +
 src/panfrost/lib/kmod/pan_kmod.c     |   5 +
 src/panfrost/lib/kmod/pan_kmod.h     |   2 +
 src/panfrost/lib/kmod/panthor_kmod.c | 924 +++++++++++++++++++++++++++
 src/panfrost/lib/kmod/panthor_kmod.h |  29 +
 5 files changed, 961 insertions(+)
 create mode 100644 src/panfrost/lib/kmod/panthor_kmod.c
 create mode 100644 src/panfrost/lib/kmod/panthor_kmod.h

diff --git a/src/panfrost/lib/kmod/meson.build b/src/panfrost/lib/kmod/meson.build
index 1278dc6f394b1..bc1962e10b011 100644
--- a/src/panfrost/lib/kmod/meson.build
+++ b/src/panfrost/lib/kmod/meson.build
@@ -21,6 +21,7 @@
 libpankmod_lib_files = files(
   'pan_kmod.c',
   'panfrost_kmod.c',
+  'panthor_kmod.c',
 )
 
 libpankmod_lib = static_library(
diff --git a/src/panfrost/lib/kmod/pan_kmod.c b/src/panfrost/lib/kmod/pan_kmod.c
index be1fddca252cf..554d230838e10 100644
--- a/src/panfrost/lib/kmod/pan_kmod.c
+++ b/src/panfrost/lib/kmod/pan_kmod.c
@@ -11,6 +11,7 @@
 #include "pan_kmod.h"
 
 extern const struct pan_kmod_ops panfrost_kmod_ops;
+extern const struct pan_kmod_ops panthor_kmod_ops;
 
 static const struct {
    const char *name;
@@ -20,6 +21,10 @@ static const struct {
       "panfrost",
       &panfrost_kmod_ops,
    },
+   {
+      "panthor",
+      &panthor_kmod_ops,
+   },
 };
 
 static void *
diff --git a/src/panfrost/lib/kmod/pan_kmod.h b/src/panfrost/lib/kmod/pan_kmod.h
index 81071beb59880..db9441c461f65 100644
--- a/src/panfrost/lib/kmod/pan_kmod.h
+++ b/src/panfrost/lib/kmod/pan_kmod.h
@@ -32,6 +32,8 @@
 #include "util/sparse_array.h"
 #include "util/u_atomic.h"
 
+#include "kmod/panthor_kmod.h"
+
 struct pan_kmod_dev;
 
 /* GPU VM creation flags. */
diff --git a/src/panfrost/lib/kmod/panthor_kmod.c b/src/panfrost/lib/kmod/panthor_kmod.c
new file mode 100644
index 0000000000000..2ee07b772df57
--- /dev/null
+++ b/src/panfrost/lib/kmod/panthor_kmod.c
@@ -0,0 +1,924 @@
+/*
+ * Copyright © 2023 Collabora, Ltd.
+ *
+ * SPDX-License-Identifier: MIT
+ */
+
+#include <errno.h>
+#include <fcntl.h>
+#include <string.h>
+#include <xf86drm.h>
+
+#include "util/hash_table.h"
+#include "util/libsync.h"
+#include "util/macros.h"
+#include "util/os_time.h"
+#include "util/simple_mtx.h"
+#include "util/u_debug.h"
+#include "util/vma.h"
+
+#include "drm-uapi/dma-buf.h"
+#include "drm-uapi/panthor_drm.h"
+
+#include "pan_kmod_backend.h"
+
+const struct pan_kmod_ops panthor_kmod_ops;
+
+/* Objects used to asynchronously GPU unmap objects that
+ * were destroyed.
+ */
+struct panthor_kmod_async_unmap {
+   struct list_head node;
+
+   /* VM sync point at which the VA range should be released. */
+   uint64_t sync_point;
+
+   /* Start of the VA range to release. */
+   uint64_t va;
+
+   /* Size of the VA range to release. */
+   size_t size;
+};
+
+struct panthor_kmod_vm {
+   struct pan_kmod_vm base;
+
+   /* We manage the VA space ourselves. */
+   struct util_vma_heap vma;
+
+   /* List of async unmaps operations and the associated lock. */
+   struct {
+      struct list_head list;
+      simple_mtx_t lock;
+   } async_unmaps;
+
+   struct {
+      /* VM sync handle. */
+      uint32_t handle;
+
+      /* Current VM sync point. Incremented every time a GPU job or VM
+       * operation is issued.
+       */
+      uint64_t point;
+   } sync;
+};
+
+struct panthor_kmod_dev {
+   struct pan_kmod_dev base;
+
+   /* Userspace mapping of the LATEST_FLUSH_ID register page. */
+   uint32_t *flush_id;
+
+   /* Cached device properties. Filled at device creation time. */
+   struct {
+      struct drm_panthor_gpu_info gpu;
+      struct drm_panthor_csif_info csif;
+   } props;
+};
+
+struct panthor_kmod_bo {
+   struct pan_kmod_bo base;
+   struct {
+      /* BO sync handle. Will point to the VM BO if the object is not shared. */
+      uint32_t handle;
+
+      /* BO read sync point. Zero when the object is shared. */
+      uint64_t read_point;
+
+      /* BO write sync point. Zero when the object is shared. */
+      uint64_t write_point;
+   } sync;
+};
+
+static struct pan_kmod_dev *
+panthor_kmod_dev_create(int fd, drmVersionPtr version,
+                        const struct pan_kmod_allocator *allocator)
+{
+   struct panthor_kmod_dev *panthor_dev =
+      pan_kmod_alloc(allocator, sizeof(*panthor_dev));
+   if (!panthor_dev) {
+      mesa_loge("failed to allocate a panthor_kmod_dev object");
+      return NULL;
+   }
+
+   /* Cache GPU and CSIF information. */
+   struct drm_panthor_dev_query query = {
+      .type = DRM_PANTHOR_DEV_QUERY_GPU_INFO,
+      .size = sizeof(panthor_dev->props.gpu),
+      .pointer = (uint64_t)(uintptr_t)&panthor_dev->props.gpu,
+   };
+
+   int ret = drmIoctl(fd, DRM_IOCTL_PANTHOR_DEV_QUERY, &query);
+   if (ret) {
+      mesa_loge("DRM_IOCTL_PANTHOR_DEV_QUERY failed (err=%d)", errno);
+      goto err_free_dev;
+   }
+
+   query = (struct drm_panthor_dev_query){
+      .type = DRM_PANTHOR_DEV_QUERY_CSIF_INFO,
+      .size = sizeof(panthor_dev->props.csif),
+      .pointer = (uint64_t)(uintptr_t)&panthor_dev->props.csif,
+   };
+
+   ret = drmIoctl(fd, DRM_IOCTL_PANTHOR_DEV_QUERY, &query);
+   if (ret) {
+      mesa_loge("DRM_IOCTL_PANTHOR_DEV_QUERY failed (err=%d)", errno);
+      goto err_free_dev;
+   }
+
+   /* Map the LATEST_FLUSH_ID register at device creation time. */
+   panthor_dev->flush_id = os_mmap(0, getpagesize(), PROT_READ, MAP_SHARED, fd,
+                                   DRM_PANTHOR_USER_FLUSH_ID_MMIO_OFFSET);
+   if (panthor_dev->flush_id == MAP_FAILED) {
+      mesa_loge("failed to mmap the LATEST_FLUSH_ID register (err=%d)", errno);
+      goto err_free_dev;
+   }
+
+   assert(!ret);
+   pan_kmod_dev_init(&panthor_dev->base, fd, version, &panthor_kmod_ops,
+                     allocator);
+   return &panthor_dev->base;
+
+err_free_dev:
+   pan_kmod_free(allocator, panthor_dev);
+   return NULL;
+}
+
+static void
+panthor_kmod_dev_destroy(struct pan_kmod_dev *dev)
+{
+   struct panthor_kmod_dev *panthor_dev =
+      container_of(dev, struct panthor_kmod_dev, base);
+
+   os_munmap(panthor_dev->flush_id, getpagesize());
+   pan_kmod_dev_cleanup(dev);
+   pan_kmod_free(dev->allocator, panthor_dev);
+}
+
+static void
+panthor_dev_query_props(struct pan_kmod_dev *dev,
+                        struct pan_kmod_dev_props *props)
+{
+   struct panthor_kmod_dev *panthor_dev =
+      container_of(dev, struct panthor_kmod_dev, base);
+
+   *props = (struct pan_kmod_dev_props){
+      .gpu_prod_id = panthor_dev->props.gpu.gpu_id >> 16,
+      .gpu_revision = panthor_dev->props.gpu.gpu_id & 0xffff,
+      .shader_present = panthor_dev->props.gpu.shader_present,
+      .tiler_features = panthor_dev->props.gpu.tiler_features,
+      .mem_features = panthor_dev->props.gpu.mem_features,
+      .mmu_features = panthor_dev->props.gpu.mmu_features,
+
+      /* Different register name, but the meaning is the same: MAX_THREADS
+       * encodes the maximum number of threads per-core, which directly
+       * impacts how much memory should be reserved for TLS.
+       */
+      .thread_tls_alloc = panthor_dev->props.gpu.max_threads,
+
+      /* This register does not exist because AFBC is no longer optional. */
+      .afbc_features = 0,
+   };
+
+   static_assert(sizeof(props->texture_features) ==
+                    sizeof(panthor_dev->props.gpu.texture_features),
+                 "Mismatch in texture_features array size");
+
+   memcpy(props->texture_features, panthor_dev->props.gpu.texture_features,
+          sizeof(props->texture_features));
+}
+
+static uint32_t
+to_panthor_bo_flags(uint32_t flags)
+{
+   uint32_t panthor_flags = 0;
+
+   if (flags & PAN_KMOD_BO_FLAG_NO_MMAP)
+      panthor_flags |= DRM_PANTHOR_BO_NO_MMAP;
+
+   return panthor_flags;
+}
+
+static struct pan_kmod_bo *
+panthor_kmod_bo_alloc(struct pan_kmod_dev *dev,
+                      struct pan_kmod_vm *exclusive_vm, size_t size,
+                      uint32_t flags)
+{
+   /* We don't support allocating on-fault. */
+   if (flags & PAN_KMOD_BO_FLAG_ALLOC_ON_FAULT) {
+      mesa_loge("panthor_kmod doesn't support PAN_KMOD_BO_FLAG_ALLOC_ON_FAULT");
+      return NULL;
+   }
+
+   struct panthor_kmod_vm *panthor_vm =
+      exclusive_vm ? container_of(exclusive_vm, struct panthor_kmod_vm, base)
+                   : NULL;
+   struct panthor_kmod_bo *bo = pan_kmod_dev_alloc(dev, sizeof(*bo));
+   if (!bo) {
+      mesa_loge("failed to allocate a panthor_kmod_vm object");
+      return NULL;
+   }
+
+   struct drm_panthor_bo_create req = {
+      .size = size,
+      .flags = to_panthor_bo_flags(flags),
+      .exclusive_vm_id = panthor_vm ? panthor_vm->base.handle : 0,
+   };
+
+   int ret = drmIoctl(dev->fd, DRM_IOCTL_PANTHOR_BO_CREATE, &req);
+   if (ret) {
+      mesa_loge("DRM_IOCTL_PANTHOR_BO_CREATE failed (err=%d)", errno);
+      goto err_free_bo;
+   }
+
+   if (!exclusive_vm) {
+      /* For buffers we know will be shared, create our own syncobj. */
+      int ret = drmSyncobjCreate(dev->fd, DRM_SYNCOBJ_CREATE_SIGNALED,
+                                 &bo->sync.handle);
+      if (ret) {
+         mesa_loge("drmSyncobjCreate() failed (err=%d)", errno);
+         goto err_destroy_bo;
+      }
+   } else {
+      /* If the buffer is private to the VM, we just use the VM syncobj. */
+      bo->sync.handle = panthor_vm->sync.handle;
+   }
+
+   bo->sync.read_point = bo->sync.write_point = 0;
+
+   pan_kmod_bo_init(&bo->base, dev, exclusive_vm, req.size, flags, req.handle);
+   return &bo->base;
+
+err_destroy_bo:
+   drmCloseBufferHandle(dev->fd, bo->base.handle);
+err_free_bo:
+   pan_kmod_dev_free(dev, bo);
+   return NULL;
+}
+
+static void
+panthor_kmod_bo_free(struct pan_kmod_bo *bo)
+{
+   drmCloseBufferHandle(bo->dev->fd, bo->handle);
+   pan_kmod_dev_free(bo->dev, bo);
+}
+
+static struct pan_kmod_bo *
+panthor_kmod_bo_import(struct pan_kmod_dev *dev, uint32_t handle, size_t size,
+                       uint32_t flags)
+{
+   struct panthor_kmod_bo *panthor_bo =
+      pan_kmod_dev_alloc(dev, sizeof(*panthor_bo));
+   if (!panthor_bo) {
+      mesa_loge("failed to allocate a panthor_kmod_bo object");
+      return NULL;
+   }
+
+   /* Create a unsignalled syncobj on import. Will serve as a
+    * temporary container for the exported dmabuf sync file.
+    */
+   int ret = drmSyncobjCreate(dev->fd, 0, &panthor_bo->sync.handle);
+   if (ret) {
+      mesa_loge("drmSyncobjCreate() failed (err=%d)", errno);
+      goto err_free_bo;
+   }
+
+   pan_kmod_bo_init(&panthor_bo->base, dev, NULL, size,
+                    flags | PAN_KMOD_BO_FLAG_IMPORTED, handle);
+   return &panthor_bo->base;
+
+err_free_bo:
+   pan_kmod_dev_free(dev, panthor_bo);
+   return NULL;
+}
+
+static int
+panthor_kmod_bo_export(struct pan_kmod_bo *bo, int dmabuf_fd)
+{
+   struct panthor_kmod_bo *panthor_bo =
+      container_of(bo, struct panthor_kmod_bo, base);
+
+   bool shared =
+      bo->flags & (PAN_KMOD_BO_FLAG_EXPORTED | PAN_KMOD_BO_FLAG_IMPORTED);
+
+   /* If the BO wasn't already shared, we migrate our internal sync points to
+    * the dmabuf itself, so implicit sync can work correctly after this point.
+    */
+   if (!shared) {
+      if (panthor_bo->sync.read_point || panthor_bo->sync.write_point) {
+         struct dma_buf_import_sync_file isync = {
+            .flags = DMA_BUF_SYNC_RW,
+         };
+         int ret = drmSyncobjExportSyncFile(bo->dev->fd,
+                                            panthor_bo->sync.handle, &isync.fd);
+         if (ret) {
+            mesa_loge("drmSyncobjExportSyncFile() failed (err=%d)", errno);
+            return -1;
+         }
+
+         ret = drmIoctl(dmabuf_fd, DMA_BUF_IOCTL_IMPORT_SYNC_FILE, &isync);
+         close(isync.fd);
+         if (ret) {
+            mesa_loge("DMA_BUF_IOCTL_IMPORT_SYNC_FILE failed (err=%d)", errno);
+            return -1;
+         }
+      }
+
+      /* Make sure we reset the syncobj on export. We will use it as a
+       * temporary binary syncobj to import sync_file FD from now on.
+       */
+      int ret = drmSyncobjReset(bo->dev->fd, &panthor_bo->sync.handle, 1);
+      if (ret) {
+         mesa_loge("drmSyncobjReset() failed (err=%d)", errno);
+         return -1;
+      }
+
+      panthor_bo->sync.read_point = 0;
+      panthor_bo->sync.write_point = 0;
+   }
+
+   bo->flags |= PAN_KMOD_BO_FLAG_EXPORTED;
+   return dmabuf_fd;
+}
+
+static off_t
+panthor_kmod_bo_get_mmap_offset(struct pan_kmod_bo *bo)
+{
+   struct drm_panthor_bo_mmap_offset req = {.handle = bo->handle};
+   int ret = drmIoctl(bo->dev->fd, DRM_IOCTL_PANTHOR_BO_MMAP_OFFSET, &req);
+
+   if (ret) {
+      mesa_loge("DRM_IOCTL_PANTHOR_BO_MMAP_OFFSET failed (err=%d)", errno);
+      return -1;
+   }
+
+   return req.offset;
+}
+
+static bool
+panthor_kmod_bo_wait(struct pan_kmod_bo *bo, int64_t timeout_ns,
+                     bool for_read_only_access)
+{
+   struct panthor_kmod_bo *panthor_bo =
+      container_of(bo, struct panthor_kmod_bo, base);
+   bool shared =
+      bo->flags & (PAN_KMOD_BO_FLAG_EXPORTED | PAN_KMOD_BO_FLAG_IMPORTED);
+
+   if (shared) {
+      /* If the object is shared, we have to do this export sync-file dance
+       * to reconcile with the implicit sync model. This implies exporting
+       * our GEM object as a dma-buf and closing it right after the
+       * EXPORT_SYNC_FILE, unfortunately.
+       */
+      int dmabuf_fd;
+      int ret =
+         drmPrimeHandleToFD(bo->dev->fd, bo->handle, DRM_CLOEXEC, &dmabuf_fd);
+
+      if (ret) {
+         mesa_loge("drmPrimeHandleToFD() failed (err=%d)", errno);
+         return false;
+      }
+
+      struct dma_buf_export_sync_file esync = {
+         .flags = for_read_only_access ? DMA_BUF_SYNC_READ : DMA_BUF_SYNC_RW,
+      };
+
+      ret = drmIoctl(dmabuf_fd, DMA_BUF_IOCTL_EXPORT_SYNC_FILE, &esync);
+      close(dmabuf_fd);
+
+      if (ret) {
+         mesa_loge("DMA_BUF_IOCTL_EXPORT_SYNC_FILE failed (err=%d)", errno);
+         return false;
+      }
+
+      ret = sync_wait(esync.fd, timeout_ns / 1000000);
+      close(esync.fd);
+      return ret == 0;
+   } else {
+      /* Waiting on non-shared object is much simpler. We just pick the
+       * right sync point based on for_read_only_access and call
+       * drmSyncobjTimelineWait().
+       */
+      uint64_t sync_point =
+         for_read_only_access
+            ? panthor_bo->sync.write_point
+            : MAX2(panthor_bo->sync.write_point, panthor_bo->sync.read_point);
+
+      if (!sync_point)
+         return true;
+
+      int64_t abs_timeout_ns = timeout_ns < INT64_MAX - os_time_get_nano()
+                                  ? timeout_ns + os_time_get_nano()
+                                  : INT64_MAX;
+      int ret = drmSyncobjTimelineWait(bo->dev->fd, &panthor_bo->sync.handle,
+                                       &sync_point, 1, abs_timeout_ns,
+                                       DRM_SYNCOBJ_WAIT_FLAGS_WAIT_ALL, NULL);
+      if (ret >= 0)
+         return true;
+
+      if (ret != -ETIME)
+         mesa_loge("DMA_BUF_IOCTL_EXPORT_SYNC_FILE failed (err=%d)", ret);
+
+      return false;
+   }
+}
+
+/* Attach a sync to a buffer object. */
+int
+panthor_kmod_bo_attach_sync_point(struct pan_kmod_bo *bo, uint32_t sync_handle,
+                                  uint64_t sync_point, bool read_only)
+{
+   struct panthor_kmod_bo *panthor_bo =
+      container_of(bo, struct panthor_kmod_bo, base);
+   struct panthor_kmod_vm *panthor_vm =
+      bo->exclusive_vm
+         ? container_of(bo->exclusive_vm, struct panthor_kmod_vm, base)
+         : NULL;
+   bool shared =
+      bo->flags & (PAN_KMOD_BO_FLAG_EXPORTED | PAN_KMOD_BO_FLAG_IMPORTED);
+
+   if (shared) {
+      /* Reconciling explicit/implicit sync again: we need to import the
+       * new sync point in the dma-buf, so other parties can rely on
+       * implicit deps.
+       */
+      struct dma_buf_import_sync_file isync = {
+         .flags = read_only ? DMA_BUF_SYNC_READ : DMA_BUF_SYNC_RW,
+      };
+      int dmabuf_fd;
+      int ret = drmSyncobjExportSyncFile(bo->dev->fd, sync_handle, &isync.fd);
+      if (ret) {
+         mesa_loge("drmSyncobjExportSyncFile() failed (err=%d)", errno);
+         return -1;
+      }
+
+      ret =
+         drmPrimeHandleToFD(bo->dev->fd, bo->handle, DRM_CLOEXEC, &dmabuf_fd);
+      if (ret) {
+         mesa_loge("drmPrimeHandleToFD() failed (err=%d)", errno);
+         close(isync.fd);
+         return -1;
+      }
+
+      ret = drmIoctl(dmabuf_fd, DMA_BUF_IOCTL_IMPORT_SYNC_FILE, &isync);
+      close(dmabuf_fd);
+      close(isync.fd);
+      if (ret) {
+         mesa_loge("DMA_BUF_IOCTL_IMPORT_SYNC_FILE failed (err=%d)", errno);
+         return -1;
+      }
+   } else if (panthor_vm) {
+      /* Private BOs should be passed the VM syncobj. */
+      assert(sync_handle == panthor_vm->sync.handle);
+
+      panthor_bo->sync.write_point =
+         MAX2(sync_point, panthor_bo->sync.write_point);
+      if (!read_only) {
+         panthor_bo->sync.read_point =
+            MAX2(sync_point, panthor_bo->sync.read_point);
+      }
+   } else {
+      /* For non-private BOs that are not shared yet, we add a new sync point
+       * to our timeline syncobj, and push the sync there.
+       */
+      uint32_t new_sync_point =
+         MAX2(panthor_bo->sync.write_point, panthor_bo->sync.read_point) + 1;
+
+      int ret = drmSyncobjTransfer(bo->dev->fd, panthor_bo->sync.handle,
+                                   new_sync_point, sync_handle, sync_point, 0);
+      if (ret) {
+         mesa_loge("drmSyncobjTransfer() failed (err=%d)", errno);
+         return -1;
+      }
+
+      /* Write always have to wait for previous read/write operations.
+       * Reads just have to wait for previous writes.
+       */
+      panthor_bo->sync.write_point = new_sync_point;
+      if (!read_only)
+         panthor_bo->sync.read_point = new_sync_point;
+   }
+
+   return 0;
+}
+
+/* Get the sync point for a read or write operation on a buffer object. */
+int
+panthor_kmod_bo_get_sync_point(struct pan_kmod_bo *bo, uint32_t *sync_handle,
+                               uint64_t *sync_point, bool for_read_only_access)
+{
+   struct panthor_kmod_bo *panthor_bo =
+      container_of(bo, struct panthor_kmod_bo, base);
+   bool shared =
+      bo->flags & (PAN_KMOD_BO_FLAG_EXPORTED | PAN_KMOD_BO_FLAG_IMPORTED);
+
+   if (shared) {
+      /* Explicit/implicit sync reconciliation point. We need to export
+       * a sync-file from the dmabuf and make it a syncobj.
+       */
+      int dmabuf_fd;
+      int ret =
+         drmPrimeHandleToFD(bo->dev->fd, bo->handle, DRM_CLOEXEC, &dmabuf_fd);
+      if (ret) {
+         mesa_loge("drmPrimeHandleToFD() failed (err=%d)\n", errno);
+         return -1;
+      }
+
+      struct dma_buf_export_sync_file esync = {
+         .flags = for_read_only_access ? DMA_BUF_SYNC_READ : DMA_BUF_SYNC_RW,
+      };
+
+      ret = drmIoctl(dmabuf_fd, DMA_BUF_IOCTL_EXPORT_SYNC_FILE, &esync);
+      close(dmabuf_fd);
+      if (ret) {
+         mesa_loge("DMA_BUF_IOCTL_EXPORT_SYNC_FILE failed (err=%d)", errno);
+         return -1;
+      }
+
+      /* We store the resulting sync in our BO syncobj, which will be assigned
+       * a new sync next time we enter this function.
+       */
+      ret = drmSyncobjImportSyncFile(bo->dev->fd, panthor_bo->sync.handle,
+                                     esync.fd);
+      close(esync.fd);
+      if (ret) {
+         mesa_loge("drmSyncobjImportSyncFile() failed (err=%d)", errno);
+         return -1;
+      }
+
+      /* The syncobj is a binary syncobj in that case. */
+      *sync_handle = panthor_bo->sync.handle;
+      *sync_point = 0;
+   } else {
+      /* Fortunately, the non-shared path is much simpler, we just return
+       * the read/write sync point depending on the access type. The syncobj
+       * is a timeline syncobj in that case.
+       */
+      *sync_handle = panthor_bo->sync.handle;
+      *sync_point = for_read_only_access ? panthor_bo->sync.write_point
+                                         : MAX2(panthor_bo->sync.read_point,
+                                                panthor_bo->sync.write_point);
+   }
+   return 0;
+}
+
+static struct pan_kmod_vm *
+panthor_kmod_vm_create(struct pan_kmod_dev *dev, uint32_t flags,
+                       uint64_t user_va_start, uint64_t user_va_range)
+{
+   struct pan_kmod_dev_props props;
+
+   panthor_dev_query_props(dev, &props);
+
+   struct panthor_kmod_vm *panthor_vm =
+      pan_kmod_dev_alloc(dev, sizeof(*panthor_vm));
+   if (!panthor_vm) {
+      mesa_loge("failed to allocate a panthor_kmod_vm object");
+      return NULL;
+   }
+
+   if (flags & PAN_KMOD_VM_FLAG_AUTO_VA) {
+      list_inithead(&panthor_vm->async_unmaps.list);
+      simple_mtx_init(&panthor_vm->async_unmaps.lock, mtx_plain);
+      util_vma_heap_init(&panthor_vm->vma, user_va_start, user_va_range);
+   }
+
+   panthor_vm->sync.point = 0;
+   int ret = drmSyncobjCreate(dev->fd, DRM_SYNCOBJ_CREATE_SIGNALED,
+                              &panthor_vm->sync.handle);
+   if (ret) {
+      mesa_loge("drmSyncobjCreate() failed (err=%d)", errno);
+      goto err_free_vm;
+   }
+
+   struct drm_panthor_vm_create req = {
+      .user_va_range = user_va_start + user_va_range,
+   };
+
+   ret = drmIoctl(dev->fd, DRM_IOCTL_PANTHOR_VM_CREATE, &req);
+   if (ret) {
+      mesa_loge("DRM_IOCTL_PANTHOR_VM_CREATE failed (err=%d)", errno);
+      goto err_destroy_sync;
+   }
+
+   pan_kmod_vm_init(&panthor_vm->base, dev, req.id, flags);
+   return &panthor_vm->base;
+
+err_destroy_sync:
+   drmSyncobjDestroy(dev->fd, panthor_vm->sync.handle);
+
+err_free_vm:
+   if (flags & PAN_KMOD_VM_FLAG_AUTO_VA)
+      util_vma_heap_finish(&panthor_vm->vma);
+
+   pan_kmod_dev_free(dev, panthor_vm);
+   return NULL;
+}
+
+static void
+panthor_kmod_vm_collect_async_unmaps(struct panthor_kmod_vm *vm)
+{
+   if (!(vm->base.flags & PAN_KMOD_VM_FLAG_AUTO_VA))
+      return;
+
+   bool done = false;
+
+   simple_mtx_lock(&vm->async_unmaps.lock);
+   list_for_each_entry_safe_rev(struct panthor_kmod_async_unmap, req,
+                                &vm->async_unmaps.list, node)
+   {
+      /* Unmaps are queued in order of execution */
+      if (!done) {
+         int ret = drmSyncobjTimelineWait(
+            vm->base.dev->fd, &vm->sync.handle, &req->sync_point, 1, 0,
+            DRM_SYNCOBJ_WAIT_FLAGS_WAIT_ALL, NULL);
+         if (ret >= 0)
+            done = true;
+         else
+            continue;
+      }
+
+      list_del(&req->node);
+      util_vma_heap_free(&vm->vma, req->va, req->size);
+      pan_kmod_dev_free(vm->base.dev, req);
+   }
+   simple_mtx_unlock(&vm->async_unmaps.lock);
+}
+
+static void
+panthor_kmod_vm_destroy(struct pan_kmod_vm *vm)
+{
+   struct panthor_kmod_vm *panthor_vm =
+      container_of(vm, struct panthor_kmod_vm, base);
+   struct drm_panthor_vm_destroy req = {.id = vm->handle};
+   int ret = drmIoctl(vm->dev->fd, DRM_IOCTL_PANTHOR_VM_DESTROY, &req);
+   if (ret)
+      mesa_loge("DRM_IOCTL_PANTHOR_VM_DESTROY failed (err=%d)", errno);
+
+   assert(!ret);
+
+   drmSyncobjDestroy(vm->dev->fd, panthor_vm->sync.handle);
+
+   if (panthor_vm->base.flags & PAN_KMOD_VM_FLAG_AUTO_VA) {
+      simple_mtx_lock(&panthor_vm->async_unmaps.lock);
+      list_for_each_entry_safe(struct panthor_kmod_async_unmap, req,
+                               &panthor_vm->async_unmaps.list, node) {
+         list_del(&req->node);
+         util_vma_heap_free(&panthor_vm->vma, req->va, req->size);
+         pan_kmod_dev_free(vm->dev, req);
+      }
+      simple_mtx_unlock(&panthor_vm->async_unmaps.lock);
+      util_vma_heap_finish(&panthor_vm->vma);
+   }
+
+   pan_kmod_dev_free(vm->dev, panthor_vm);
+}
+
+static inline uint64_t
+panthor_kmod_vm_alloc_va(struct panthor_kmod_vm *panthor_vm, size_t size)
+{
+   panthor_kmod_vm_collect_async_unmaps(panthor_vm);
+   return util_vma_heap_alloc(&panthor_vm->vma, size,
+                              size > 0x200000 ? 0x200000 : 0x1000);
+}
+
+static int
+panthor_kmod_vm_bind(struct pan_kmod_vm *vm, enum pan_kmod_vm_op_mode mode,
+                     struct pan_kmod_vm_op *ops, uint32_t op_count)
+{
+   struct panthor_kmod_vm *panthor_vm =
+      container_of(vm, struct panthor_kmod_vm, base);
+   uint64_t wait_vm_point = 0, signal_vm_point = 0;
+   struct drm_panthor_vm_bind_op *bind_ops = NULL;
+   struct drm_panthor_sync_op *sync_ops = NULL;
+   uint32_t syncop_cnt = 0, syncop_ptr = 0;
+   int ret = -1;
+
+   /* For any asynchronous VM bind, we assume the user is managing the VM
+    * address space, so we don't have to collect VMAs in that case.
+    */
+   if (mode == PAN_KMOD_VM_OP_MODE_ASYNC &&
+       (vm->flags & PAN_KMOD_VM_FLAG_AUTO_VA)) {
+      mesa_loge(
+         "auto-VA allocation is incompatible with PAN_KMOD_VM_OP_MODE_ASYNC");
+      return -1;
+   }
+
+   /* With PAN_KMOD_VM_OP_MODE_DEFER_TO_NEXT_IDLE_POINT, we need to push our
+    * wait+signal VM syncobj in one of the submissions, hence the two extra
+    * slots reserved here.
+    */
+   if (mode == PAN_KMOD_VM_OP_MODE_DEFER_TO_NEXT_IDLE_POINT)
+      syncop_cnt += 2;
+
+   for (uint32_t i = 0; i < op_count; i++)
+      syncop_cnt += ops[i].syncs.count;
+
+   /* If there's no bind or sync operation, we can return directly. */
+   if (syncop_cnt == 0 && op_count == 0)
+      return 0;
+
+   if (syncop_cnt) {
+      sync_ops =
+         pan_kmod_dev_alloc_transient(vm->dev, sizeof(*sync_ops) * syncop_cnt);
+      if (!sync_ops) {
+         mesa_loge("drm_panthor_sync_op[%d] array allocation failed",
+                   syncop_cnt);
+         return -1;
+      }
+   }
+
+   /* If there is no bind operation, but we are passed sync operation, we
+    * need a sync-only bind op.
+    */
+   if (op_count || syncop_cnt) {
+      bind_ops = pan_kmod_dev_alloc_transient(
+         vm->dev, sizeof(*bind_ops) * MIN2(op_count, 1));
+      if (!bind_ops) {
+         mesa_loge("drm_panthor_vm_bind_op[%d] array allocation failed",
+                   MIN2(op_count, 1));
+         goto out_free_sync_ops;
+      }
+   }
+
+   struct drm_panthor_vm_bind req = {
+      .vm_id = vm->handle,
+      .flags =
+         mode != PAN_KMOD_VM_OP_MODE_IMMEDIATE ? DRM_PANTHOR_VM_BIND_ASYNC : 0,
+      .ops = DRM_PANTHOR_OBJ_ARRAY(MIN2(op_count, 1), bind_ops),
+   };
+
+   if (mode == PAN_KMOD_VM_OP_MODE_DEFER_TO_NEXT_IDLE_POINT) {
+      wait_vm_point = panthor_vm->sync.point++;
+      signal_vm_point = panthor_vm->sync.point;
+   }
+
+   /* If there's no bind operation, we create a sync-only one,
+    * so we can attach our sync operations to it.
+    */
+   if (!op_count) {
+      bind_ops[0] = (struct drm_panthor_vm_bind_op){
+         .flags = DRM_PANTHOR_VM_BIND_OP_TYPE_SYNC_ONLY,
+         .syncs = DRM_PANTHOR_OBJ_ARRAY(syncop_cnt, sync_ops),
+      };
+
+      sync_ops[syncop_ptr++] = (struct drm_panthor_sync_op){
+         .flags = DRM_PANTHOR_SYNC_OP_WAIT |
+                  DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_TIMELINE_SYNCOBJ,
+         .handle = panthor_vm->sync.handle,
+         .timeline_value = wait_vm_point,
+      };
+      sync_ops[syncop_ptr++] = (struct drm_panthor_sync_op){
+         .flags = DRM_PANTHOR_SYNC_OP_SIGNAL |
+                  DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_TIMELINE_SYNCOBJ,
+         .handle = panthor_vm->sync.handle,
+         .timeline_value = signal_vm_point,
+      };
+   }
+
+   for (uint32_t i = 0; i < op_count; i++) {
+      if (pank_kmod_vm_op_check(vm, mode, &ops[i]))
+         goto out_update_vas;
+
+      bind_ops[i].syncs = (struct drm_panthor_obj_array)DRM_PANTHOR_OBJ_ARRAY(
+         ops[i].syncs.count, ops[i].syncs.count ? &sync_ops[syncop_ptr] : NULL);
+
+      if (mode == PAN_KMOD_VM_OP_MODE_DEFER_TO_NEXT_IDLE_POINT && i == 0) {
+         if (!bind_ops[i].syncs.array)
+            bind_ops[i].syncs.array = (uintptr_t)&sync_ops[syncop_ptr];
+
+         bind_ops[i].syncs.count++;
+         sync_ops[syncop_ptr++] = (struct drm_panthor_sync_op){
+            .flags = DRM_PANTHOR_SYNC_OP_WAIT |
+                     DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_TIMELINE_SYNCOBJ,
+            .handle = panthor_vm->sync.handle,
+            .timeline_value = wait_vm_point,
+         };
+      }
+
+      if (mode == PAN_KMOD_VM_OP_MODE_DEFER_TO_NEXT_IDLE_POINT &&
+          i == op_count - 1) {
+         if (!bind_ops[i].syncs.array)
+            bind_ops[i].syncs.array = (uintptr_t)&sync_ops[syncop_ptr];
+
+         bind_ops[i].syncs.count++;
+         sync_ops[syncop_ptr++] = (struct drm_panthor_sync_op){
+            .flags = DRM_PANTHOR_SYNC_OP_SIGNAL |
+                     DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_TIMELINE_SYNCOBJ,
+            .handle = panthor_vm->sync.handle,
+            .timeline_value = signal_vm_point,
+         };
+      }
+
+      for (uint32_t j = 0; j < ops[i].syncs.count; j++) {
+         sync_ops[syncop_ptr++] = (struct drm_panthor_sync_op){
+            .flags = (ops[i].syncs.array[j].type == PAN_KMOD_SYNC_TYPE_WAIT
+                         ? DRM_PANTHOR_SYNC_OP_WAIT
+                         : DRM_PANTHOR_SYNC_OP_SIGNAL) |
+                     DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_TIMELINE_SYNCOBJ,
+            .handle = ops[i].syncs.array[j].handle,
+            .timeline_value = ops[i].syncs.array[j].point,
+         };
+      }
+
+      if (ops[i].type == PAN_KMOD_VM_OP_TYPE_MAP) {
+         bind_ops[i].flags = DRM_PANTHOR_VM_BIND_OP_TYPE_MAP;
+         bind_ops[i].size = ops[i].va.size;
+         bind_ops[i].bo_handle = ops[i].map.bo->handle;
+         bind_ops[i].bo_offset = ops[i].map.bo_offset;
+
+         if (ops[i].va.start == PAN_KMOD_VM_MAP_AUTO_VA) {
+            bind_ops[i].va =
+               panthor_kmod_vm_alloc_va(panthor_vm, bind_ops[i].size);
+            if (!bind_ops[i].va) {
+               mesa_loge("VA allocation failed");
+               ret = -1;
+               goto out_update_vas;
+            }
+         } else {
+            bind_ops[i].va = ops[i].va.start;
+         }
+
+         if (ops[i].map.bo->flags & PAN_KMOD_BO_FLAG_EXECUTABLE)
+            bind_ops[i].flags |= DRM_PANTHOR_VM_BIND_OP_MAP_READONLY;
+         else
+            bind_ops[i].flags |= DRM_PANTHOR_VM_BIND_OP_MAP_NOEXEC;
+
+         if (ops[i].map.bo->flags & PAN_KMOD_BO_FLAG_GPU_UNCACHED)
+            bind_ops[i].flags |= DRM_PANTHOR_VM_BIND_OP_MAP_UNCACHED;
+
+      } else if (ops[i].type == PAN_KMOD_VM_OP_TYPE_UNMAP) {
+         bind_ops[i].flags = DRM_PANTHOR_VM_BIND_OP_TYPE_UNMAP;
+         bind_ops[i].va = ops[i].va.start;
+         bind_ops[i].size = ops[i].va.size;
+      } else {
+         assert(ops[i].type == PAN_KMOD_VM_OP_TYPE_SYNC_ONLY);
+         bind_ops[i].flags = DRM_PANTHOR_VM_BIND_OP_TYPE_SYNC_ONLY;
+      }
+   }
+
+   ret = drmIoctl(vm->dev->fd, DRM_IOCTL_PANTHOR_VM_BIND, &req);
+
+out_update_vas:
+   for (uint32_t i = 0; i < op_count; i++) {
+      if (ops[i].type == PAN_KMOD_VM_OP_TYPE_MAP &&
+          ops[i].va.start == PAN_KMOD_VM_MAP_AUTO_VA) {
+         if (!ret) {
+            ops[i].va.start = bind_ops[i].va;
+         } else if (bind_ops[i].va != 0) {
+            util_vma_heap_free(&panthor_vm->vma, bind_ops[i].va,
+                               bind_ops[i].size);
+         }
+      }
+   }
+
+   pan_kmod_dev_free(vm->dev, bind_ops);
+
+out_free_sync_ops:
+   pan_kmod_dev_free(vm->dev, sync_ops);
+   return ret;
+}
+
+void
+panthor_kmod_vm_new_sync_point(struct pan_kmod_vm *vm, uint32_t *sync_handle,
+                               uint64_t *sync_point)
+{
+   struct panthor_kmod_vm *panthor_vm =
+      container_of(vm, struct panthor_kmod_vm, base);
+
+   *sync_handle = panthor_vm->sync.handle;
+   *sync_point = ++panthor_vm->sync.point;
+}
+
+uint32_t
+panthor_kmod_get_flush_id(const struct pan_kmod_dev *dev)
+{
+   struct panthor_kmod_dev *panthor_dev =
+      container_of(dev, struct panthor_kmod_dev, base);
+
+   return *(panthor_dev->flush_id);
+}
+
+const struct drm_panthor_csif_info *
+panthor_kmod_get_csif_props(const struct pan_kmod_dev *dev)
+{
+   struct panthor_kmod_dev *panthor_dev =
+      container_of(dev, struct panthor_kmod_dev, base);
+
+   return &panthor_dev->props.csif;
+}
+
+const struct pan_kmod_ops panthor_kmod_ops = {
+   .dev_create = panthor_kmod_dev_create,
+   .dev_destroy = panthor_kmod_dev_destroy,
+   .dev_query_props = panthor_dev_query_props,
+   .bo_alloc = panthor_kmod_bo_alloc,
+   .bo_free = panthor_kmod_bo_free,
+   .bo_import = panthor_kmod_bo_import,
+   .bo_export = panthor_kmod_bo_export,
+   .bo_get_mmap_offset = panthor_kmod_bo_get_mmap_offset,
+   .bo_wait = panthor_kmod_bo_wait,
+   .vm_create = panthor_kmod_vm_create,
+   .vm_destroy = panthor_kmod_vm_destroy,
+   .vm_bind = panthor_kmod_vm_bind,
+};
diff --git a/src/panfrost/lib/kmod/panthor_kmod.h b/src/panfrost/lib/kmod/panthor_kmod.h
new file mode 100644
index 0000000000000..5401eb03d2fd8
--- /dev/null
+++ b/src/panfrost/lib/kmod/panthor_kmod.h
@@ -0,0 +1,29 @@
+/*
+ * Copyright © 2023 Collabora, Ltd.
+ *
+ * SPDX-License-Identifier: MIT
+ */
+
+#pragma once
+
+#include <stdint.h>
+
+struct drm_panthor_csif_info;
+
+struct pan_kmod_bo;
+struct pan_kmod_dev;
+struct pan_kmod_vm;
+
+int panthor_kmod_bo_attach_sync_point(struct pan_kmod_bo *bo,
+                                      uint32_t sync_handle,
+                                      uint64_t sync_point, bool read_only);
+int panthor_kmod_bo_get_sync_point(struct pan_kmod_bo *bo,
+                                   uint32_t *sync_handle, uint64_t *sync_point,
+                                   bool read_only);
+void panthor_kmod_vm_new_sync_point(struct pan_kmod_vm *vm,
+                                    uint32_t *sync_handle,
+                                    uint64_t *sync_point);
+uint32_t panthor_kmod_get_flush_id(const struct pan_kmod_dev *dev);
+
+const struct drm_panthor_csif_info *
+panthor_kmod_get_csif_props(const struct pan_kmod_dev *dev);
-- 
GitLab


From dce00b617e036435c960bcf554e595d380a547ae Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Mon, 4 Dec 2023 17:46:57 +0100
Subject: [PATCH 14/32] fixup! pan/kmod: Add a backend for panthor

---
 src/panfrost/lib/kmod/panthor_kmod.c | 12 ++++++++++++
 1 file changed, 12 insertions(+)

diff --git a/src/panfrost/lib/kmod/panthor_kmod.c b/src/panfrost/lib/kmod/panthor_kmod.c
index 2ee07b772df57..7cb5702fec0c4 100644
--- a/src/panfrost/lib/kmod/panthor_kmod.c
+++ b/src/panfrost/lib/kmod/panthor_kmod.c
@@ -879,6 +879,18 @@ out_free_sync_ops:
    return ret;
 }
 
+static enum pan_kmod_vm_state
+panthor_kmod_vm_query_state(struct pan_kmod_vm *vm)
+{
+   struct drm_panthor_vm_get_state query = {.vm_id = vm->handle};
+   int ret = drmIoctl(vm->dev->fd, DRM_IOCTL_PANTHOR_VM_GET_STATE, &query);
+
+   if (ret || query.state == DRM_PANTHOR_VM_STATE_UNUSABLE)
+      return PAN_KMOD_VM_FAULTY;
+
+   return PAN_KMOD_VM_USABLE;
+}
+
 void
 panthor_kmod_vm_new_sync_point(struct pan_kmod_vm *vm, uint32_t *sync_handle,
                                uint64_t *sync_point)
-- 
GitLab


From 5718ef0efeeefc264f2f057ed2f4b61211b4d58f Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Tue, 21 Nov 2023 09:59:46 +0100
Subject: [PATCH 15/32] panfrost: Restrict GPU VA space on panthor + 32bit
 builds

Panthor needs some part of the GPU VA space to be reserved for kernel
buffer objects (heap chunks, mostly). Since our VA space is limited to
32-bit when the kernel is 32-bit, and there's no way we can know if we
are interacting with a 32bit or 64bit kernel when our binary is 32bit,
let's just assume we only have 32bit of address space in that case, and
reserve the upper 1G for kernel use.

Signed-off-by: Boris Brezillon <boris.brezillon@collabora.com>
---
 src/panfrost/lib/pan_props.c | 6 ++++++
 1 file changed, 6 insertions(+)

diff --git a/src/panfrost/lib/pan_props.c b/src/panfrost/lib/pan_props.c
index db08acf9c7c62..6b3beff63c987 100644
--- a/src/panfrost/lib/pan_props.c
+++ b/src/panfrost/lib/pan_props.c
@@ -215,6 +215,12 @@ panfrost_open_device(void *memctx, int fd, struct panfrost_device *dev)
    uint64_t user_va_start = 0x2000000;
    uint64_t user_va_range = (1ull << 32) - user_va_start;
 
+   /* On panthor+32b, we keep part of the VA space to map kernel objects.
+    * We assume a 3G/1G user/kernel split for now.
+    */
+   if (dev->arch >= 10 && sizeof(long) == 4)
+      user_va_range = (3ull << 30) - user_va_start;
+
    dev->kmod.vm = pan_kmod_vm_create(dev->kmod.dev, PAN_KMOD_VM_FLAG_AUTO_VA,
                                      user_va_start, user_va_range);
    if (!dev->kmod.vm)
-- 
GitLab


From 6ca73629a5ccfd6a47c098fb4322fdd546a8fbfb Mon Sep 17 00:00:00 2001
From: Alyssa Rosenzweig <alyssa@rosenzweig.io>
Date: Tue, 14 Nov 2023 13:11:49 +0100
Subject: [PATCH 16/32] panfrost: Patch panfrost_max_thread_count() for v10

Not sure that's correct because some v10 GPUs have less 2k threads
allowed per core. We should probably use THREADS_MAX register value,
and only if it's zero, pick a default value that's the minimum of all
known v10 GPUs.

Signed-off-by: Alyssa Rosenzweig <alyssa@rosenzweig.io>
---
 src/panfrost/util/pan_ir.h | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/src/panfrost/util/pan_ir.h b/src/panfrost/util/pan_ir.h
index 5551fc7526a50..aa96781fa7487 100644
--- a/src/panfrost/util/pan_ir.h
+++ b/src/panfrost/util/pan_ir.h
@@ -452,6 +452,9 @@ panfrost_max_thread_count(unsigned arch, unsigned work_reg_count)
       return work_reg_count > 32 ? 384 : 768;
 
    /* Valhall (for completeness) */
+   case 10:
+      return 2048;
+
    default:
       return work_reg_count > 32 ? 512 : 1024;
    }
-- 
GitLab


From f02f569bc27ff5340609118e14fa69dfe2f34d5f Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Wed, 15 Nov 2023 12:55:56 +0100
Subject: [PATCH 17/32] panfrost: Add v10 support to libpanfrost

The code has already been patched to support v10, we just need to add
v10 to the version array when compiling per-arch files.
---
 src/panfrost/lib/meson.build | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/src/panfrost/lib/meson.build b/src/panfrost/lib/meson.build
index 12b927a2973d7..344d5299deb20 100644
--- a/src/panfrost/lib/meson.build
+++ b/src/panfrost/lib/meson.build
@@ -40,7 +40,7 @@ endforeach
 
 libpanfrost_per_arch = []
 
-foreach ver : ['4', '5', '6', '7', '9']
+foreach ver : ['4', '5', '6', '7', '9', '10']
   libpanfrost_per_arch += static_library(
     'pan-arch-v' + ver,
     [
-- 
GitLab


From d7dacd2001e806b2efb62279829b36b17fbf979f Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Wed, 5 Jul 2023 11:10:11 +0200
Subject: [PATCH 18/32] panfrost/genxml: Add missing 'Progress increment'
 fields

Signed-off-by: Boris Brezillon <boris.brezillon@collabora.com>
---
 src/panfrost/lib/genxml/v10.xml | 4 ++++
 1 file changed, 4 insertions(+)

diff --git a/src/panfrost/lib/genxml/v10.xml b/src/panfrost/lib/genxml/v10.xml
index 249aaff78500d..6a0da30e047d5 100644
--- a/src/panfrost/lib/genxml/v10.xml
+++ b/src/panfrost/lib/genxml/v10.xml
@@ -570,6 +570,7 @@
   <struct name="CEU RUN_COMPUTE" size="2">
     <field name="Task increment" size="14" start="0" type="uint"/>
     <field name="Task axis" size="2" start="14" type="Task Axis"/>
+    <field name="Progress increment" size="1" start="32" type="bool" default="true"/>
     <field name="SRT select" size="2" start="40" type="uint"/>
     <field name="SPD select" size="2" start="42" type="uint"/>
     <field name="TSD select" size="2" start="44" type="uint"/>
@@ -579,6 +580,7 @@
 
   <struct name="CEU RUN_IDVS" size="2">
     <field name="Flags override" size="32" start="0" type="hex"/>
+    <field name="Progress increment" size="1" start="32" type="bool" default="true"/>
     <field name="Malloc enable" size="1" start="33" type="bool"/>
     <field name="Draw ID register enable" size="1" start="34" type="bool"/>
     <field name="Varying SRT select" size="1" start="35" type="bool"/>
@@ -592,10 +594,12 @@
 
   <struct name="CEU RUN_FRAGMENT" size="2">
     <field name="Enable TEM" size="1" start="0" type="bool"/>
+    <field name="Progress increment" size="1" start="32" type="bool" default="true"/>
     <field name="Opcode" size="8" start="56" type="CEU Opcode" default="RUN_FRAGMENT"/>
   </struct>
 
   <struct name="CEU FINISH_TILING" size="2">
+    <field name="Progress increment" size="1" start="32" type="bool" default="true"/>
     <field name="Opcode" size="8" start="56" type="CEU Opcode" default="FINISH_TILING"/>
   </struct>
 
-- 
GitLab


From 9a4931c814b1faaa3c613341c0579485009c3da7 Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Wed, 15 Nov 2023 12:08:24 +0100
Subject: [PATCH 19/32] pandecode/csf: Introduce the concept of usermode queue

This way we allow JUMPs to be decoded when the decode function is
passed an indirect CS buffer that called from a kernel mode queue.

Signed-off-by: Boris Brezillon <boris.brezillon@collabora.com>
---
 src/panfrost/lib/genxml/decode.h     | 6 ++++++
 src/panfrost/lib/genxml/decode_csf.c | 5 +++++
 2 files changed, 11 insertions(+)

diff --git a/src/panfrost/lib/genxml/decode.h b/src/panfrost/lib/genxml/decode.h
index 98166c52c4da1..07339f5c67b51 100644
--- a/src/panfrost/lib/genxml/decode.h
+++ b/src/panfrost/lib/genxml/decode.h
@@ -41,6 +41,12 @@ struct pandecode_context {
    struct util_dynarray ro_mappings;
    int dump_frame_count;
    simple_mtx_t lock;
+
+   /* On CSF context, set to true if the root CS ring buffer
+    * is managed in userspace. The blob does that, and mesa might use
+    * usermode queues too at some point.
+    */
+   bool usermode_queue;
 };
 
 void pandecode_dump_file_open(struct pandecode_context *ctx);
diff --git a/src/panfrost/lib/genxml/decode_csf.c b/src/panfrost/lib/genxml/decode_csf.c
index f713b468554fb..f40498a773c84 100644
--- a/src/panfrost/lib/genxml/decode_csf.c
+++ b/src/panfrost/lib/genxml/decode_csf.c
@@ -758,6 +758,11 @@ GENX(pandecode_cs)(struct pandecode_context *ctx, mali_ptr queue, uint32_t size,
       .ip = cs,
       .end = cs + (size / 8),
       .gpu_id = gpu_id,
+
+      /* If this is a kernel mode queue, we don't see the root ring buffer and
+       * we must adjust the initial call stack depth accordingly.
+       */
+      .call_stack_depth = ctx->usermode_queue ? 0 : 1,
    };
 
    if (size) {
-- 
GitLab


From 110143f5fe4cabd88a2f3a2ba194f176b3e673c4 Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Wed, 15 Nov 2023 11:48:36 +0100
Subject: [PATCH 20/32] panfrost: Don't allocate a tiler heap buffer on v10+

Heap management is completely different on CSF hardware, and the heap
buffer remain unused in that case. Make the tiler heap BO creation
conditional to reflect that.
---
 src/panfrost/lib/pan_props.c | 10 +++++++---
 1 file changed, 7 insertions(+), 3 deletions(-)

diff --git a/src/panfrost/lib/pan_props.c b/src/panfrost/lib/pan_props.c
index 6b3beff63c987..8272cc0c49a07 100644
--- a/src/panfrost/lib/pan_props.c
+++ b/src/panfrost/lib/pan_props.c
@@ -260,8 +260,11 @@ panfrost_open_device(void *memctx, int fd, struct panfrost_device *dev)
     * active for a single job chain at once, so a single heap can be
     * shared across batches/contextes */
 
-   dev->tiler_heap = panfrost_bo_create(
-      dev, 128 * 1024 * 1024, PAN_BO_INVISIBLE | PAN_BO_GROWABLE, "Tiler heap");
+   if (dev->arch < 10) {
+      dev->tiler_heap =
+         panfrost_bo_create(dev, 128 * 1024 * 1024,
+                            PAN_BO_INVISIBLE | PAN_BO_GROWABLE, "Tiler heap");
+   }
 
    pthread_mutex_init(&dev->submit_lock, NULL);
 
@@ -282,7 +285,8 @@ panfrost_close_device(struct panfrost_device *dev)
     */
    if (dev->model) {
       pthread_mutex_destroy(&dev->submit_lock);
-      panfrost_bo_unreference(dev->tiler_heap);
+      if (dev->tiler_heap)
+         panfrost_bo_unreference(dev->tiler_heap);
       panfrost_bo_unreference(dev->sample_positions);
       panfrost_bo_cache_evict_all(dev);
       pthread_mutex_destroy(&dev->bo_cache.lock);
-- 
GitLab


From 236973fa39960c67c117ce73fd310755316c431a Mon Sep 17 00:00:00 2001
From: Alyssa Rosenzweig <alyssa.rosenzweig@collabora.com>
Date: Wed, 5 Jul 2023 11:07:04 +0200
Subject: [PATCH 21/32] panfrost: Add a library to build CSF command streams

Signed-off-by: Alyssa Rosenzweig <alyssa.rosenzweig@collabora.com>
---
 src/.clang-format                     |   1 +
 src/panfrost/lib/genxml/ceu_builder.h | 576 ++++++++++++++++++++++++++
 2 files changed, 577 insertions(+)
 create mode 100644 src/panfrost/lib/genxml/ceu_builder.h

diff --git a/src/.clang-format b/src/.clang-format
index 802b782ed0358..908516b2c5496 100644
--- a/src/.clang-format
+++ b/src/.clang-format
@@ -292,6 +292,7 @@ ForEachMacros:
   - bi_foreach_src
   - bi_foreach_ssa_src
   - bi_foreach_successor
+  - ceu_emit
   - mir_foreach_block
   - mir_foreach_block_from
   - mir_foreach_bundle_in_block
diff --git a/src/panfrost/lib/genxml/ceu_builder.h b/src/panfrost/lib/genxml/ceu_builder.h
new file mode 100644
index 0000000000000..b8abbb8f5cac3
--- /dev/null
+++ b/src/panfrost/lib/genxml/ceu_builder.h
@@ -0,0 +1,576 @@
+/*
+ * Copyright (C) 2022 Collabora Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ */
+
+#pragma once
+
+#include "gen_macros.h"
+
+/*
+ * ceu_builder implements a builder for CSF command queues. It manages the
+ * allocation and overflow behaviour of queues and provides helpers for emitting
+ * commands to run on the CEU.
+ *
+ * Users must implement the ceu_alloc_queue method performing the physical
+ * memory allocation and queue binding. Users must initialize a queue with
+ * ceu_builder_init.
+ */
+
+struct ceu_queue {
+   /* CPU pointer */
+   uint64_t *cpu;
+
+   /* GPU pointer */
+   uint64_t gpu;
+
+   /* Capacity */
+   size_t capacity;
+};
+
+struct ceu_builder_conf {
+   /* Number of 32-bit registers in the hardware register file */
+   uint8_t nr_registers;
+
+   /* Number of 32-bit registers used by the kernel at submission time */
+   uint8_t nr_kernel_registers;
+
+   /* CS chunk allocator. */
+   struct ceu_queue (*alloc)(void *cookie);
+
+   /* Cookie passed back to ceu_alloc_queue for caller use */
+   void *cookie;
+};
+
+typedef struct ceu_builder {
+   /* CEU builder configuration */
+   struct ceu_builder_conf conf;
+
+   /* Initial (root) queue */
+   struct ceu_queue root;
+
+   /* Number of instructions emitted into the root queue */
+   uint32_t root_size;
+
+   /* Current queue */
+   struct ceu_queue queue;
+
+   /* Number of instructions emitted into the current queue so far */
+   uint32_t queue_size;
+
+   /* Move immediate instruction at the end of the last queue that needs to
+    * be patched with the final length of the current queue in order to
+    * facilitate correct overflow behaviour.
+    */
+   uint32_t *length_patch;
+} ceu_builder;
+
+static void
+ceu_builder_init(struct ceu_builder *b, const struct ceu_builder_conf *conf,
+                 struct ceu_queue root)
+{
+   *b = (struct ceu_builder){
+      .conf = *conf,
+      .queue = root,
+      .root = root,
+   };
+}
+
+/*
+ * Wrap the current queue. External users shouldn't call this function
+ * directly, they should call ceu_finish() when they are done building
+ * the command stream, which will in turn call ceu_wrap_queue().
+ *
+ * Internally, this is also used to finalize internal subqueues when
+ * allocating new subqueues. See ceu_alloc for details.
+ *
+ * This notably requires patching the previous queue with the length
+ * we ended up emitting for this queue.
+ */
+static void
+ceu_wrap_queue(ceu_builder *b)
+{
+   if (b->length_patch) {
+      *b->length_patch = (b->queue_size * 8);
+      b->length_patch = NULL;
+   }
+
+   if (b->root.gpu == b->queue.gpu)
+      b->root_size = b->queue_size;
+}
+
+/* Call this when you are done building a command stream and want to prepare
+ * it for submission.
+ */
+static unsigned
+ceu_finish(ceu_builder *b)
+{
+   ceu_wrap_queue(b);
+   return b->root_size;
+}
+
+#if PAN_ARCH >= 10
+enum ceu_index_type { CEU_INDEX_REGISTER = 0, CEU_INDEX_IMMEDIATE = 1 };
+
+typedef struct ceu_index {
+   enum ceu_index_type type;
+
+   /* Number of 32-bit words in the index, must be nonzero */
+   uint8_t size;
+
+   union {
+      uint64_t imm;
+      uint8_t reg;
+   };
+} ceu_index;
+
+static uint8_t
+ceu_to_reg_tuple(ceu_index idx, ASSERTED uint8_t expected_size)
+{
+   assert(idx.type == CEU_INDEX_REGISTER);
+   assert(idx.size == expected_size);
+
+   return idx.reg;
+}
+
+static uint8_t
+ceu_to_reg32(ceu_index idx)
+{
+   return ceu_to_reg_tuple(idx, 1);
+}
+
+static uint8_t
+ceu_to_reg64(ceu_index idx)
+{
+   return ceu_to_reg_tuple(idx, 2);
+}
+
+static ceu_index
+ceu_reg_tuple(ASSERTED ceu_builder *b, uint8_t reg, uint8_t size)
+{
+   assert(reg + size <= b->conf.nr_registers && "overflowed register file");
+   assert(size < 16 && "unsupported");
+
+   return (
+      struct ceu_index){.type = CEU_INDEX_REGISTER, .size = size, .reg = reg};
+}
+
+static inline ceu_index
+ceu_reg32(ceu_builder *b, uint8_t reg)
+{
+   return ceu_reg_tuple(b, reg, 1);
+}
+
+static inline ceu_index
+ceu_reg64(ceu_builder *b, uint8_t reg)
+{
+   assert((reg % 2) == 0 && "unaligned 64-bit reg");
+   return ceu_reg_tuple(b, reg, 2);
+}
+
+/*
+ * The top of the register file is reserved for ceu_builder internal use. We
+ * need 3 spare registers for handling command queue overflow. These are
+ * available here.
+ */
+static inline ceu_index
+ceu_overflow_address(ceu_builder *b)
+{
+   return ceu_reg64(b, b->conf.nr_registers - 2);
+}
+
+static inline ceu_index
+ceu_overflow_length(ceu_builder *b)
+{
+   return ceu_reg32(b, b->conf.nr_registers - 3);
+}
+
+static ceu_index
+ceu_extract32(ceu_builder *b, ceu_index idx, uint8_t word)
+{
+   assert(idx.type == CEU_INDEX_REGISTER && "unsupported");
+   assert(word < idx.size && "overrun");
+
+   return ceu_reg32(b, idx.reg + word);
+}
+
+static inline void *
+ceu_alloc(ceu_builder *b)
+{
+   /* If the current command queue runs out of space, allocate a new one
+    * and jump to it. We actually do this a few instructions before running
+    * out, because the sequence to jump to a new queue takes multiple
+    * instructions.
+    */
+   if (unlikely((b->queue_size + 4) > b->queue.capacity)) {
+      /* Now, allocate a new queue */
+      struct ceu_queue newq = b->conf.alloc(b->conf.cookie);
+
+      uint64_t *ptr = b->queue.cpu + (b->queue_size++);
+
+      pan_pack(ptr, CEU_MOVE, I) {
+         I.destination = ceu_to_reg64(ceu_overflow_address(b));
+         I.immediate = newq.gpu;
+      }
+
+      ptr = b->queue.cpu + (b->queue_size++);
+
+      pan_pack(ptr, CEU_MOVE32, I) {
+         I.destination = ceu_to_reg32(ceu_overflow_length(b));
+      }
+
+      /* The length will be patched in later */
+      uint32_t *length_patch = (uint32_t *)ptr;
+
+      ptr = b->queue.cpu + (b->queue_size++);
+
+      pan_pack(ptr, CEU_JUMP, I) {
+         I.length = ceu_to_reg32(ceu_overflow_length(b));
+         I.address = ceu_to_reg64(ceu_overflow_address(b));
+      }
+
+      /* Now that we've emitted everything, finish up the previous queue */
+      ceu_wrap_queue(b);
+
+      /* And make this one current */
+      b->length_patch = length_patch;
+      b->queue = newq;
+      b->queue_size = 0;
+   }
+
+   assert(b->queue_size < b->queue.capacity);
+   return b->queue.cpu + (b->queue_size++);
+}
+
+/*
+ * Helper to emit a new instruction into the command queue. The allocation needs
+ * to be separated out being pan_pack can evaluate its argument multiple times,
+ * yet ceu_alloc has side effects.
+ */
+#define ceu_emit(b, T, cfg)                                                    \
+   void *_dest = ceu_alloc(b);                                                 \
+   pan_pack(_dest, CEU_##T, cfg)
+
+static inline void
+ceu_move32_to(ceu_builder *b, ceu_index dest, uint32_t imm)
+{
+   ceu_emit(b, MOVE32, I) {
+      I.destination = ceu_to_reg32(dest);
+      I.immediate = imm;
+   }
+}
+
+static inline void
+ceu_move48_to(ceu_builder *b, ceu_index dest, uint64_t imm)
+{
+   ceu_emit(b, MOVE, I) {
+      I.destination = ceu_to_reg64(dest);
+      I.immediate = imm;
+   }
+}
+
+static inline void
+ceu_wait_slots(ceu_builder *b, uint8_t slots)
+{
+   ceu_emit(b, WAIT, I) {
+      I.slots = slots;
+   }
+}
+
+static inline void
+ceu_branch(ceu_builder *b, int16_t offset, enum mali_ceu_condition cond,
+           ceu_index val)
+{
+   ceu_emit(b, BRANCH, I) {
+      I.offset = offset;
+      I.condition = cond;
+      I.value = ceu_to_reg32(val);
+   }
+}
+
+static inline void
+ceu_run_compute(ceu_builder *b, unsigned task_increment,
+                enum mali_task_axis task_axis)
+{
+   ceu_emit(b, RUN_COMPUTE, I) {
+      I.task_increment = task_increment;
+      I.task_axis = task_axis;
+
+      /* We always use the first table for compute jobs */
+   }
+}
+
+static inline void
+ceu_run_idvs(ceu_builder *b, enum mali_draw_mode draw_mode,
+             enum mali_index_type index_type, bool secondary_shader)
+{
+   ceu_emit(b, RUN_IDVS, I) {
+      /* We do not have a use case for traditional IDVS */
+      I.malloc_enable = true;
+
+      /* We hardcode these settings for now, we can revisit this if we
+       * rework how we emit state later.
+       */
+      I.fragment_srt_select = true;
+
+      /* Pack the override we use */
+      pan_pack(&I.flags_override, PRIMITIVE_FLAGS, cfg) {
+         cfg.draw_mode = draw_mode;
+         cfg.index_type = index_type;
+         cfg.secondary_shader = secondary_shader;
+      }
+   }
+}
+
+static inline void
+ceu_run_fragment(ceu_builder *b, bool enable_tem)
+{
+   ceu_emit(b, RUN_FRAGMENT, I) {
+      I.enable_tem = enable_tem;
+   }
+}
+
+static inline void
+ceu_finish_tiling(ceu_builder *b)
+{
+   ceu_emit(b, FINISH_TILING, _)
+      ;
+}
+
+static inline void
+ceu_finish_fragment(ceu_builder *b, bool increment_frag_completed,
+                    ceu_index first_free_heap_chunk,
+                    ceu_index last_free_heap_chunk, uint16_t scoreboard_mask,
+                    uint8_t signal_slot)
+{
+   ceu_emit(b, FINISH_FRAGMENT, I) {
+      I.increment_fragment_completed = increment_frag_completed;
+      I.wait_mask = scoreboard_mask;
+      I.first_heap_chunk = ceu_to_reg64(first_free_heap_chunk);
+      I.last_heap_chunk = ceu_to_reg64(last_free_heap_chunk);
+      I.scoreboard_entry = signal_slot;
+   }
+}
+
+static inline void
+ceu_heap_set(ceu_builder *b, ceu_index address)
+{
+   ceu_emit(b, HEAP_SET, I) {
+      I.address = ceu_to_reg64(address);
+   }
+}
+
+static inline void
+ceu_load_to(ceu_builder *b, ceu_index dest, ceu_index address, uint16_t mask,
+            int16_t offset)
+{
+   ceu_emit(b, LOAD_MULTIPLE, I) {
+      I.base = ceu_to_reg_tuple(dest, util_bitcount(mask));
+      I.address = ceu_to_reg64(address);
+      I.mask = mask;
+      I.offset = offset;
+   }
+}
+
+static inline void
+ceu_store(ceu_builder *b, ceu_index data, ceu_index address, uint16_t mask,
+          int16_t offset)
+{
+   ceu_emit(b, STORE_MULTIPLE, I) {
+      I.base = ceu_to_reg_tuple(data, util_bitcount(mask));
+      I.address = ceu_to_reg64(address);
+      I.mask = mask;
+      I.offset = offset;
+   }
+}
+
+/*
+ * Select which scoreboard entry will track endpoint tasks and other tasks
+ * respectively. Pass to ceu_wait to wait later.
+ */
+static inline void
+ceu_set_scoreboard_entry(ceu_builder *b, uint8_t ep, uint8_t other)
+{
+   assert(ep < 8 && "invalid slot");
+   assert(other < 8 && "invalid slot");
+
+   ceu_emit(b, SET_SB_ENTRY, I) {
+      I.endpoint_entry = ep;
+      I.other_entry = other;
+   }
+}
+
+static inline void
+ceu_require_all(ceu_builder *b)
+{
+   ceu_emit(b, REQ_RESOURCE, I) {
+      I.compute = true;
+      I.tiler = true;
+      I.idvs = true;
+      I.fragment = true;
+   }
+}
+
+static inline void
+ceu_require_compute(ceu_builder *b)
+{
+   ceu_emit(b, REQ_RESOURCE, I)
+      I.compute = true;
+}
+
+static inline void
+ceu_require_fragment(ceu_builder *b)
+{
+   ceu_emit(b, REQ_RESOURCE, I)
+      I.fragment = true;
+}
+
+static inline void
+ceu_require_idvs(ceu_builder *b)
+{
+   ceu_emit(b, REQ_RESOURCE, I) {
+      I.compute = true;
+      I.tiler = true;
+      I.idvs = true;
+   }
+}
+
+static inline void
+ceu_heap_operation(ceu_builder *b, enum mali_ceu_heap_operation operation)
+{
+   ceu_emit(b, HEAP_OPERATION, I)
+      I.operation = operation;
+}
+
+static inline void
+ceu_vt_start(ceu_builder *b)
+{
+   ceu_heap_operation(b, MALI_CEU_HEAP_OPERATION_VERTEX_TILER_STARTED);
+}
+
+static inline void
+ceu_vt_end(ceu_builder *b)
+{
+   ceu_heap_operation(b, MALI_CEU_HEAP_OPERATION_VERTEX_TILER_COMPLETED);
+}
+
+static inline void
+ceu_frag_end(ceu_builder *b)
+{
+   ceu_heap_operation(b, MALI_CEU_HEAP_OPERATION_FRAGMENT_COMPLETED);
+}
+
+static inline void
+ceu_flush_caches(ceu_builder *b, enum mali_ceu_flush_mode l2,
+                 enum mali_ceu_flush_mode lsc, bool other_inv,
+                 ceu_index flush_id, uint16_t scoreboard_mask,
+                 uint8_t signal_slot)
+{
+   ceu_emit(b, FLUSH_CACHE2, I) {
+      I.l2_flush_mode = l2;
+      I.lsc_flush_mode = lsc;
+      I.other_invalidate = other_inv;
+      I.scoreboard_mask = scoreboard_mask;
+      I.latest_flush_id = ceu_to_reg32(flush_id);
+      I.scoreboard_entry = signal_slot;
+   }
+}
+
+/* Pseudoinstructions follow */
+
+static inline void
+ceu_move64_to(ceu_builder *b, ceu_index dest, uint64_t imm)
+{
+   if (imm < (1ull << 48)) {
+      /* Zero extends */
+      ceu_move48_to(b, dest, imm);
+   } else {
+      ceu_move32_to(b, ceu_extract32(b, dest, 0), imm);
+      ceu_move32_to(b, ceu_extract32(b, dest, 1), imm >> 32);
+   }
+}
+
+static inline void
+ceu_load32_to(ceu_builder *b, ceu_index dest, ceu_index address, int16_t offset)
+{
+   ceu_load_to(b, dest, address, BITFIELD_MASK(1), offset);
+}
+
+static inline void
+ceu_load64_to(ceu_builder *b, ceu_index dest, ceu_index address, int16_t offset)
+{
+   ceu_load_to(b, dest, address, BITFIELD_MASK(2), offset);
+}
+
+static inline void
+ceu_store32(ceu_builder *b, ceu_index data, ceu_index address, int16_t offset)
+{
+   ceu_store(b, data, address, BITFIELD_MASK(1), offset);
+}
+
+static inline void
+ceu_store64(ceu_builder *b, ceu_index data, ceu_index address, int16_t offset)
+{
+   ceu_store(b, data, address, BITFIELD_MASK(2), offset);
+}
+
+static inline void
+ceu_wait_slot(ceu_builder *b, uint8_t slot)
+{
+   assert(slot < 8 && "invalid slot");
+
+   ceu_wait_slots(b, BITFIELD_BIT(slot));
+}
+
+static inline void
+ceu_store_state(ceu_builder *b, uint8_t signal_slot, ceu_index address,
+                enum mali_ceu_state state, uint16_t wait_mask, int16_t offset)
+{
+   ceu_emit(b, STORE_STATE, I) {
+      I.offset = offset;
+      I.wait_mask = wait_mask;
+      I.state = state;
+      I.address = ceu_to_reg64(address);
+      I.scoreboard_slot = signal_slot;
+   }
+}
+
+static inline void
+ceu_add64(ceu_builder *b, ceu_index dest, ceu_index src, uint32_t imm)
+{
+   ceu_emit(b, ADD_IMMEDIATE64, I) {
+      I.destination = ceu_to_reg64(dest);
+      I.source = ceu_to_reg64(src);
+      I.immediate = imm;
+   }
+}
+
+static inline void
+ceu_add32(ceu_builder *b, ceu_index dest, ceu_index src, uint32_t imm)
+{
+   ceu_emit(b, ADD_IMMEDIATE32, I) {
+      I.destination = ceu_to_reg32(dest);
+      I.source = ceu_to_reg32(src);
+      I.immediate = imm;
+   }
+}
+
+#endif /* PAN_ARCH >= 10 */
-- 
GitLab


From c7d035bf66becc56ed56d42e6128d5c236d3f4ee Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Wed, 15 Nov 2023 18:03:17 +0100
Subject: [PATCH 22/32] panfrost: Relax position result alignment constraint on
 v10+

Looks like the alignment constraint is gone on v10...

Signed-off-by: Boris Brezillon <boris.brezillon@collabora.com>
---
 src/gallium/drivers/panfrost/pan_cmdstream.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/src/gallium/drivers/panfrost/pan_cmdstream.c b/src/gallium/drivers/panfrost/pan_cmdstream.c
index a047044f73a09..49828883b4b6a 100644
--- a/src/gallium/drivers/panfrost/pan_cmdstream.c
+++ b/src/gallium/drivers/panfrost/pan_cmdstream.c
@@ -2806,7 +2806,7 @@ panfrost_draw_get_vertex_count(struct panfrost_batch *batch,
       panfrost_increase_vertex_count(batch, vertex_count);
    }
 
-   if (info->instance_count > 1) {
+   if (PAN_ARCH <= 9 && info->instance_count > 1) {
       unsigned count = vertex_count;
 
       /* Index-Driven Vertex Shading requires different instances to
-- 
GitLab


From d7c4712daf05119bf47691ace70797a5297d9bf8 Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Thu, 23 Nov 2023 11:04:17 +0100
Subject: [PATCH 23/32] panfrost: Add arch-specific context init/cleanup hooks

This will allow us to defer some CSF-specific initialization to
pan_csf.c, keeping pan_context.c job-frontend agnostic.

Signed-off-by: Boris Brezillon <boris.brezillon@collabora.com>
---
 src/gallium/drivers/panfrost/pan_cmdstream.c | 12 ++++++++++++
 src/gallium/drivers/panfrost/pan_context.c   |  4 ++++
 src/gallium/drivers/panfrost/pan_jm.h        | 10 ++++++++++
 src/gallium/drivers/panfrost/pan_screen.h    |  4 ++++
 4 files changed, 30 insertions(+)

diff --git a/src/gallium/drivers/panfrost/pan_cmdstream.c b/src/gallium/drivers/panfrost/pan_cmdstream.c
index 49828883b4b6a..1a349841f9c75 100644
--- a/src/gallium/drivers/panfrost/pan_cmdstream.c
+++ b/src/gallium/drivers/panfrost/pan_cmdstream.c
@@ -3632,6 +3632,16 @@ context_populate_vtbl(struct pipe_context *pipe)
    pipe->get_sample_position = u_default_get_sample_position;
 }
 
+static void
+context_init(struct panfrost_context *ctx)
+{
+}
+
+static void
+context_cleanup(struct panfrost_context *ctx)
+{
+}
+
 #if PAN_ARCH <= 5
 
 /* Returns the polygon list's GPU address if available, or otherwise allocates
@@ -3718,6 +3728,8 @@ GENX(panfrost_cmdstream_screen_init)(struct panfrost_screen *screen)
    screen->vtbl.prepare_shader = prepare_shader;
    screen->vtbl.screen_destroy = screen_destroy;
    screen->vtbl.context_populate_vtbl = context_populate_vtbl;
+   screen->vtbl.context_init = JOBX(init_context);
+   screen->vtbl.context_cleanup = JOBX(cleanup_context);
    screen->vtbl.init_batch = JOBX(init_batch);
    screen->vtbl.submit_batch = submit_batch;
    screen->vtbl.get_blend_shader = GENX(pan_blend_get_shader_locked);
diff --git a/src/gallium/drivers/panfrost/pan_context.c b/src/gallium/drivers/panfrost/pan_context.c
index e1fed545c4bcf..db12d06ef6d6f 100644
--- a/src/gallium/drivers/panfrost/pan_context.c
+++ b/src/gallium/drivers/panfrost/pan_context.c
@@ -550,6 +550,8 @@ panfrost_destroy(struct pipe_context *pipe)
    struct panfrost_context *panfrost = pan_context(pipe);
    struct panfrost_device *dev = pan_device(pipe->screen);
 
+   pan_screen(pipe->screen)->vtbl.context_cleanup(panfrost);
+
    _mesa_hash_table_destroy(panfrost->writers, NULL);
 
    if (panfrost->blitter)
@@ -982,5 +984,7 @@ panfrost_create_context(struct pipe_screen *screen, void *priv, unsigned flags)
    ret = drmSyncobjCreate(panfrost_device_fd(dev), 0, &ctx->in_sync_obj);
    assert(!ret);
 
+   pan_screen(screen)->vtbl.context_init(ctx);
+
    return gallium;
 }
diff --git a/src/gallium/drivers/panfrost/pan_jm.h b/src/gallium/drivers/panfrost/pan_jm.h
index d41f99d86a114..02fa7eb4190dd 100644
--- a/src/gallium/drivers/panfrost/pan_jm.h
+++ b/src/gallium/drivers/panfrost/pan_jm.h
@@ -49,6 +49,16 @@ struct pipe_draw_info;
 struct pipe_grid_info;
 struct pipe_draw_start_count_bias;
 
+static inline void
+GENX(jm_init_context)(struct panfrost_context *ctx)
+{
+}
+
+static inline void
+GENX(jm_cleanup_context)(struct panfrost_context *ctx)
+{
+}
+
 void GENX(jm_init_batch)(struct panfrost_batch *batch);
 int GENX(jm_submit_batch)(struct panfrost_batch *batch);
 
diff --git a/src/gallium/drivers/panfrost/pan_screen.h b/src/gallium/drivers/panfrost/pan_screen.h
index 3400c0a6cbf64..cb6ee6029048a 100644
--- a/src/gallium/drivers/panfrost/pan_screen.h
+++ b/src/gallium/drivers/panfrost/pan_screen.h
@@ -70,6 +70,10 @@ struct panfrost_vtable {
    /* Populate context vtable */
    void (*context_populate_vtbl)(struct pipe_context *pipe);
 
+   /* Initialize/cleanup a Gallium context */
+   void (*context_init)(struct panfrost_context *ctx);
+   void (*context_cleanup)(struct panfrost_context *ctx);
+
    /* Device-dependent initialization of a panfrost_batch */
    void (*init_batch)(struct panfrost_batch *batch);
 
-- 
GitLab


From 7d9e6a795f8038445e566445249f849ed0b11eb5 Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Mon, 13 Nov 2023 15:39:50 +0100
Subject: [PATCH 24/32] panfrost: Add a panfrost_context_reinit() helper

Will be needed for v10, so we can re-instantiate a context when an
unrecoverable error is reported on a group or VM.

Signed-off-by: Boris Brezillon <boris.brezillon@collabora.com>
---
 src/gallium/drivers/panfrost/pan_context.c | 7 +++++++
 src/gallium/drivers/panfrost/pan_context.h | 2 ++
 2 files changed, 9 insertions(+)

diff --git a/src/gallium/drivers/panfrost/pan_context.c b/src/gallium/drivers/panfrost/pan_context.c
index db12d06ef6d6f..fe73a97d25239 100644
--- a/src/gallium/drivers/panfrost/pan_context.c
+++ b/src/gallium/drivers/panfrost/pan_context.c
@@ -988,3 +988,10 @@ panfrost_create_context(struct pipe_screen *screen, void *priv, unsigned flags)
 
    return gallium;
 }
+
+void
+panfrost_context_reinit(struct panfrost_context *ctx)
+{
+   pan_screen(ctx->base.screen)->vtbl.context_cleanup(ctx);
+   pan_screen(ctx->base.screen)->vtbl.context_init(ctx);
+}
diff --git a/src/gallium/drivers/panfrost/pan_context.h b/src/gallium/drivers/panfrost/pan_context.h
index dbf0d503ae9f3..040143fb94b6f 100644
--- a/src/gallium/drivers/panfrost/pan_context.h
+++ b/src/gallium/drivers/panfrost/pan_context.h
@@ -521,4 +521,6 @@ void panfrost_track_image_access(struct panfrost_batch *batch,
                                  enum pipe_shader_type stage,
                                  struct pipe_image_view *image);
 
+void panfrost_context_reinit(struct panfrost_context *ctx);
+
 #endif
-- 
GitLab


From 45f343bd5e21666a15b676a53ff75966ae42382a Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Wed, 4 Oct 2023 10:49:40 +0200
Subject: [PATCH 25/32] panfrost: Add a cleanup_batch() method to
 panfrost_vtable

We have to do some cleanup on v10+. Let's add a new hook to allow
per-arch batch cleanup procedures.

Signed-off-by: Boris Brezillon <boris.brezillon@collabora.com>
---
 src/gallium/drivers/panfrost/pan_cmdstream.c | 1 +
 src/gallium/drivers/panfrost/pan_jm.h        | 6 ++++++
 src/gallium/drivers/panfrost/pan_job.c       | 3 +++
 src/gallium/drivers/panfrost/pan_screen.h    | 3 ++-
 4 files changed, 12 insertions(+), 1 deletion(-)

diff --git a/src/gallium/drivers/panfrost/pan_cmdstream.c b/src/gallium/drivers/panfrost/pan_cmdstream.c
index 1a349841f9c75..4d9187d0f8208 100644
--- a/src/gallium/drivers/panfrost/pan_cmdstream.c
+++ b/src/gallium/drivers/panfrost/pan_cmdstream.c
@@ -3731,6 +3731,7 @@ GENX(panfrost_cmdstream_screen_init)(struct panfrost_screen *screen)
    screen->vtbl.context_init = JOBX(init_context);
    screen->vtbl.context_cleanup = JOBX(cleanup_context);
    screen->vtbl.init_batch = JOBX(init_batch);
+   screen->vtbl.cleanup_batch = JOBX(cleanup_batch);
    screen->vtbl.submit_batch = submit_batch;
    screen->vtbl.get_blend_shader = GENX(pan_blend_get_shader_locked);
    screen->vtbl.get_compiler_options = GENX(pan_shader_get_compiler_options);
diff --git a/src/gallium/drivers/panfrost/pan_jm.h b/src/gallium/drivers/panfrost/pan_jm.h
index 02fa7eb4190dd..47d99cf3eda0e 100644
--- a/src/gallium/drivers/panfrost/pan_jm.h
+++ b/src/gallium/drivers/panfrost/pan_jm.h
@@ -60,6 +60,12 @@ GENX(jm_cleanup_context)(struct panfrost_context *ctx)
 }
 
 void GENX(jm_init_batch)(struct panfrost_batch *batch);
+
+static inline void
+GENX(jm_cleanup_batch)(struct panfrost_batch *batch)
+{
+}
+
 int GENX(jm_submit_batch)(struct panfrost_batch *batch);
 
 void GENX(jm_preload_fb)(struct panfrost_batch *batch, struct pan_fb_info *fb);
diff --git a/src/gallium/drivers/panfrost/pan_job.c b/src/gallium/drivers/panfrost/pan_job.c
index b48329f52e202..6eddc0b9e645b 100644
--- a/src/gallium/drivers/panfrost/pan_job.c
+++ b/src/gallium/drivers/panfrost/pan_job.c
@@ -113,6 +113,7 @@ static void
 panfrost_batch_cleanup(struct panfrost_context *ctx,
                        struct panfrost_batch *batch)
 {
+   struct panfrost_screen *screen = pan_screen(ctx->base.screen);
    struct panfrost_device *dev = pan_device(ctx->base.screen);
 
    assert(batch->seqnum);
@@ -120,6 +121,8 @@ panfrost_batch_cleanup(struct panfrost_context *ctx,
    if (ctx->batch == batch)
       ctx->batch = NULL;
 
+   screen->vtbl.cleanup_batch(batch);
+
    unsigned batch_idx = panfrost_batch_idx(batch);
 
    pan_bo_access *flags = util_dynarray_begin(&batch->bos);
diff --git a/src/gallium/drivers/panfrost/pan_screen.h b/src/gallium/drivers/panfrost/pan_screen.h
index cb6ee6029048a..56ca5426d605f 100644
--- a/src/gallium/drivers/panfrost/pan_screen.h
+++ b/src/gallium/drivers/panfrost/pan_screen.h
@@ -74,8 +74,9 @@ struct panfrost_vtable {
    void (*context_init)(struct panfrost_context *ctx);
    void (*context_cleanup)(struct panfrost_context *ctx);
 
-   /* Device-dependent initialization of a panfrost_batch */
+   /* Device-dependent initialization/cleanup of a panfrost_batch */
    void (*init_batch)(struct panfrost_batch *batch);
+   void (*cleanup_batch)(struct panfrost_batch *batch);
 
    /* Device-dependent submission of a panfrost_batch */
    int (*submit_batch)(struct panfrost_batch *batch, struct pan_fb_info *fb);
-- 
GitLab


From ad022a31c91b43eafff6fb2e6c371a1c0805721a Mon Sep 17 00:00:00 2001
From: Alyssa Rosenzweig <alyssa@rosenzweig.io>
Date: Wed, 15 Nov 2023 17:58:54 +0100
Subject: [PATCH 26/32] panfrost: Add support for the CSF job frontend

CSF-specific implementation of the job-frontend helpers.

Signed-off-by: Alyssa Rosenzweig <alyssa@rosenzweig.io>
Signed-off-by: Boris Brezillon <boris.brezillon@collabora.com>
---
 src/gallium/drivers/panfrost/meson.build     |   2 +
 src/gallium/drivers/panfrost/pan_cmdstream.c |   3 +
 src/gallium/drivers/panfrost/pan_context.h   |   6 +
 src/gallium/drivers/panfrost/pan_csf.c       | 965 +++++++++++++++++++
 src/gallium/drivers/panfrost/pan_csf.h       |  92 ++
 src/gallium/drivers/panfrost/pan_job.h       |   2 +
 6 files changed, 1070 insertions(+)
 create mode 100644 src/gallium/drivers/panfrost/pan_csf.c
 create mode 100644 src/gallium/drivers/panfrost/pan_csf.h

diff --git a/src/gallium/drivers/panfrost/meson.build b/src/gallium/drivers/panfrost/meson.build
index 58fd5d10b75f7..dff95405f55b1 100644
--- a/src/gallium/drivers/panfrost/meson.build
+++ b/src/gallium/drivers/panfrost/meson.build
@@ -61,6 +61,8 @@ foreach ver : panfrost_versions
   files_panfrost_vx = ['pan_cmdstream.c', pan_packers]
   if ver in ['4', '5', '6', '7', '9']
     files_panfrost_vx += ['pan_jm.c']
+  elif ver in ['10']
+    files_panfrost_vx += ['pan_csf.c']
   endif
   libpanfrost_versions += static_library(
     'panfrost-v' + ver, files_panfrost_vx,
diff --git a/src/gallium/drivers/panfrost/pan_cmdstream.c b/src/gallium/drivers/panfrost/pan_cmdstream.c
index 4d9187d0f8208..59da16bc31edd 100644
--- a/src/gallium/drivers/panfrost/pan_cmdstream.c
+++ b/src/gallium/drivers/panfrost/pan_cmdstream.c
@@ -46,6 +46,7 @@
 #include "pan_bo.h"
 #include "pan_cmdstream.h"
 #include "pan_context.h"
+#include "pan_csf.h"
 #include "pan_indirect_dispatch.h"
 #include "pan_jm.h"
 #include "pan_job.h"
@@ -58,6 +59,8 @@
  * functions. */
 #if PAN_ARCH <= 9
 #define JOBX(__suffix) GENX(jm_##__suffix)
+#elif PAN_ARCH <= 10
+#define JOBX(__suffix) GENX(csf_##__suffix)
 #else
 #error "Unsupported arch"
 #endif
diff --git a/src/gallium/drivers/panfrost/pan_context.h b/src/gallium/drivers/panfrost/pan_context.h
index 040143fb94b6f..dd81f0c809d52 100644
--- a/src/gallium/drivers/panfrost/pan_context.h
+++ b/src/gallium/drivers/panfrost/pan_context.h
@@ -50,6 +50,8 @@
 #include "compiler/shader_enums.h"
 #include "midgard/midgard_compile.h"
 
+#include "pan_csf.h"
+
 #define SET_BIT(lval, bit, cond)                                               \
    if (cond)                                                                   \
       lval |= (bit);                                                           \
@@ -230,6 +232,10 @@ struct panfrost_context {
 
    int in_sync_fd;
    uint32_t in_sync_obj;
+
+   union {
+      struct panfrost_csf_context csf;
+   };
 };
 
 /* Corresponds to the CSO */
diff --git a/src/gallium/drivers/panfrost/pan_csf.c b/src/gallium/drivers/panfrost/pan_csf.c
new file mode 100644
index 0000000000000..726099fa6d971
--- /dev/null
+++ b/src/gallium/drivers/panfrost/pan_csf.c
@@ -0,0 +1,965 @@
+/*
+ * Copyright (C) 2023 Collabora Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "decode.h"
+
+#include "drm-uapi/panthor_drm.h"
+
+#include "genxml/ceu_builder.h"
+
+#include "pan_blitter.h"
+#include "pan_cmdstream.h"
+#include "pan_context.h"
+#include "pan_csf.h"
+#include "pan_job.h"
+
+#if PAN_ARCH < 10
+#error "CSF helpers are only used for gen >= 10"
+#endif
+
+static struct ceu_queue
+csf_alloc_ceu_queue(void *cookie)
+{
+   assert(cookie && "Self-contained queues can't be extended.");
+
+   struct panfrost_batch *batch = cookie;
+   unsigned capacity = 4096;
+   struct panfrost_bo *bo = panfrost_batch_create_bo(
+      batch, capacity * 8, 0, PIPE_SHADER_VERTEX, "Command queue");
+
+   memset(bo->ptr.cpu, 0xFF, capacity * 8);
+
+   return (struct ceu_queue){
+      .cpu = bo->ptr.cpu,
+      .gpu = bo->ptr.gpu,
+      .capacity = capacity,
+   };
+}
+
+void
+GENX(csf_cleanup_batch)(struct panfrost_batch *batch)
+{
+   free(batch->csf.cs.builder);
+}
+
+void
+GENX(csf_init_batch)(struct panfrost_batch *batch)
+{
+   /* Allocate and bind the command queue */
+   struct ceu_queue queue = csf_alloc_ceu_queue(batch);
+   const struct ceu_builder_conf conf = {
+      .nr_registers = 96,
+      .nr_kernel_registers = 4,
+      .alloc = csf_alloc_ceu_queue,
+      .cookie = batch,
+   };
+
+   /* Setup the queue builder */
+   batch->csf.cs.builder = malloc(sizeof(ceu_builder));
+   ceu_builder_init(batch->csf.cs.builder, &conf, queue);
+   ceu_require_all(batch->csf.cs.builder);
+
+   /* Set up entries */
+   ceu_builder *b = batch->csf.cs.builder;
+   ceu_set_scoreboard_entry(b, 2, 0);
+
+   /* Initialize the state vector */
+   for (unsigned i = 0; i < 64; i += 2)
+      ceu_move64_to(b, ceu_reg64(b, i), 0);
+
+   batch->framebuffer = pan_pool_alloc_desc_aggregate(
+      &batch->pool.base, PAN_DESC(FRAMEBUFFER), PAN_DESC(ZS_CRC_EXTENSION),
+      PAN_DESC_ARRAY(MAX2(batch->key.nr_cbufs, 1), RENDER_TARGET));
+   batch->tls = pan_pool_alloc_desc(&batch->pool.base, LOCAL_STORAGE);
+}
+
+static void
+csf_prepare_qsubmit(struct panfrost_context *ctx,
+                    struct drm_panthor_queue_submit *submit, uint8_t queue,
+                    uint64_t cs_start, uint32_t cs_size,
+                    struct drm_panthor_sync_op *syncs, uint32_t sync_count)
+{
+   struct panfrost_device *dev = pan_device(ctx->base.screen);
+
+   *submit = (struct drm_panthor_queue_submit){
+      .queue_index = queue,
+      .stream_addr = cs_start,
+      .stream_size = cs_size,
+      .latest_flush = panthor_kmod_get_flush_id(dev->kmod.dev),
+      .syncs = DRM_PANTHOR_OBJ_ARRAY(sync_count, syncs),
+   };
+}
+
+static void
+csf_prepare_gsubmit(struct panfrost_context *ctx,
+                    struct drm_panthor_group_submit *gsubmit,
+                    struct drm_panthor_queue_submit *qsubmits,
+                    uint32_t qsubmit_count)
+{
+   *gsubmit = (struct drm_panthor_group_submit){
+      .group_handle = ctx->csf.group_handle,
+      .queue_submits = DRM_PANTHOR_OBJ_ARRAY(qsubmit_count, qsubmits),
+   };
+}
+
+static int
+csf_submit_gsubmit(struct panfrost_context *ctx,
+                   struct drm_panthor_group_submit *gsubmit)
+{
+   struct panfrost_device *dev = pan_device(ctx->base.screen);
+   int ret = 0;
+
+   if (!ctx->is_noop) {
+      ret = drmIoctl(panfrost_device_fd(dev), DRM_IOCTL_PANTHOR_GROUP_SUBMIT,
+                     gsubmit);
+   }
+
+   if (ret)
+      return errno;
+
+   if (dev->debug & (PAN_DBG_TRACE | PAN_DBG_SYNC)) {
+      /* Wait so we can get errors reported back */
+      drmSyncobjWait(panfrost_device_fd(dev), &ctx->syncobj, 1, INT64_MAX, 0,
+                     NULL);
+
+      if ((dev->debug & PAN_DBG_TRACE) && dev->arch >= 10) {
+         const struct drm_panthor_queue_submit *qsubmits =
+            (void *)(uintptr_t)gsubmit->queue_submits.array;
+
+         for (unsigned i = 0; i < gsubmit->queue_submits.count; i++) {
+            uint32_t regs[256] = {0};
+            pandecode_cs(dev->decode_ctx, qsubmits[i].stream_addr,
+                         qsubmits[i].stream_size, panfrost_device_gpu_id(dev),
+                         regs);
+         }
+      }
+
+      if (dev->debug & PAN_DBG_DUMP)
+         pandecode_dump_mappings(dev->decode_ctx);
+   }
+
+   return 0;
+}
+
+static void
+csf_emit_batch_end(struct panfrost_batch *batch)
+{
+   ceu_builder *b = batch->csf.cs.builder;
+
+   /* Barrier to let everything finish */
+   ceu_wait_slots(b, BITFIELD_MASK(8));
+
+   /* Get the CS state */
+   batch->csf.cs.state = pan_pool_alloc_aligned(&batch->pool.base, 8, 8);
+   memset(batch->csf.cs.state.cpu, ~0, 8);
+   ceu_move64_to(b, ceu_reg64(b, 90), batch->csf.cs.state.gpu);
+   ceu_store_state(b, 0, ceu_reg64(b, 90), MALI_CEU_STATE_ERROR_STATUS, 0, 0);
+
+   /* Flush caches now that we're done (synchronous) */
+   ceu_index flush_id = ceu_reg32(b, 74);
+   ceu_move32_to(b, flush_id, 0);
+   ceu_flush_caches(b, MALI_CEU_FLUSH_MODE_CLEAN_AND_INVALIDATE,
+                    MALI_CEU_FLUSH_MODE_CLEAN_AND_INVALIDATE, true, flush_id, 0,
+                    0);
+
+   /* Finish the command stream */
+   ceu_finish(batch->csf.cs.builder);
+}
+
+int
+GENX(csf_submit_batch)(struct panfrost_batch *batch)
+{
+   /* Close the batch before submitting. */
+   csf_emit_batch_end(batch);
+
+   uint32_t cs_instr_count = batch->csf.cs.builder->root_size;
+   uint64_t cs_start = batch->csf.cs.builder->root.gpu;
+   uint32_t cs_size = cs_instr_count * 8;
+   uint64_t vm_sync_signal_point, vm_sync_wait_point = 0, bo_sync_point;
+   struct panfrost_context *ctx = batch->ctx;
+   struct panfrost_device *dev = pan_device(ctx->base.screen);
+   uint32_t vm_sync_handle, bo_sync_handle, sync_count = 0;
+   struct drm_panthor_sync_op *syncs = NULL;
+   int ret;
+
+   panthor_kmod_vm_new_sync_point(dev->kmod.vm, &vm_sync_handle,
+                                  &vm_sync_signal_point);
+   assert(vm_sync_handle > 0 && vm_sync_signal_point > 0);
+
+   syncs = calloc(batch->num_bos + 5, sizeof(*syncs));
+   assert(syncs);
+
+   util_dynarray_foreach(&batch->bos, pan_bo_access, ptr) {
+      unsigned i = ptr - util_dynarray_element(&batch->bos, pan_bo_access, 0);
+      pan_bo_access flags = *ptr;
+
+      if (!flags)
+         continue;
+
+      /* Update the BO access flags so that panfrost_bo_wait() knows
+       * about all pending accesses.
+       * We only keep the READ/WRITE info since this is all the BO
+       * wait logic cares about.
+       * We also preserve existing flags as this batch might not
+       * be the first one to access the BO.
+       */
+      struct panfrost_bo *bo = pan_lookup_bo(dev, i);
+
+      bo->gpu_access |= flags & (PAN_BO_ACCESS_RW);
+
+      ret = panthor_kmod_bo_get_sync_point(bo->kmod_bo, &bo_sync_handle,
+                                           &bo_sync_point,
+                                           !(flags & PAN_BO_ACCESS_WRITE));
+      if (ret)
+         return ret;
+
+      if (bo_sync_handle == vm_sync_handle) {
+         vm_sync_wait_point = MAX2(vm_sync_wait_point, bo_sync_point);
+      } else {
+         assert(bo_sync_point == 0 || !bo->kmod_bo->exclusive_vm);
+         syncs[sync_count++] = (struct drm_panthor_sync_op){
+            .flags =
+               DRM_PANTHOR_SYNC_OP_WAIT |
+               (bo_sync_point ? DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_TIMELINE_SYNCOBJ
+                              : DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_SYNCOBJ),
+            .handle = bo_sync_handle,
+            .timeline_value = bo_sync_point,
+         };
+      }
+   }
+
+   util_dynarray_foreach(&batch->pool.bos, struct panfrost_bo *, bo) {
+      (*bo)->gpu_access |= PAN_BO_ACCESS_RW;
+
+      ret = panthor_kmod_bo_get_sync_point((*bo)->kmod_bo, &bo_sync_handle,
+                                           &bo_sync_point, false);
+      if (ret)
+         return ret;
+
+      assert(bo_sync_handle == vm_sync_handle);
+      vm_sync_wait_point = MAX2(vm_sync_wait_point, bo_sync_point);
+   }
+
+   util_dynarray_foreach(&batch->invisible_pool.bos, struct panfrost_bo *, bo) {
+      (*bo)->gpu_access |= PAN_BO_ACCESS_RW;
+
+      ret = panthor_kmod_bo_get_sync_point((*bo)->kmod_bo, &bo_sync_handle,
+                                           &bo_sync_point, false);
+      if (ret)
+         return ret;
+
+      assert(bo_sync_handle == vm_sync_handle);
+      vm_sync_wait_point = MAX2(vm_sync_wait_point, bo_sync_point);
+   }
+
+   /* Always used on Bifrost, occassionally used on Midgard */
+   panthor_kmod_bo_get_sync_point(dev->sample_positions->kmod_bo,
+                                  &bo_sync_handle, &bo_sync_point, true);
+   dev->sample_positions->gpu_access |= PAN_BO_ACCESS_READ;
+   vm_sync_wait_point = MAX2(vm_sync_wait_point, bo_sync_point);
+
+   if (vm_sync_wait_point > 0) {
+      syncs[sync_count++] = (struct drm_panthor_sync_op){
+         .flags = DRM_PANTHOR_SYNC_OP_WAIT |
+                  DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_TIMELINE_SYNCOBJ,
+         .handle = vm_sync_handle,
+         .timeline_value = vm_sync_wait_point,
+      };
+   }
+
+   syncs[sync_count++] = (struct drm_panthor_sync_op){
+      .flags = DRM_PANTHOR_SYNC_OP_SIGNAL |
+               DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_TIMELINE_SYNCOBJ,
+      .handle = vm_sync_handle,
+      .timeline_value = vm_sync_signal_point,
+   };
+
+   syncs[sync_count++] = (struct drm_panthor_sync_op){
+      .flags =
+         DRM_PANTHOR_SYNC_OP_SIGNAL | DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_SYNCOBJ,
+      .handle = ctx->syncobj,
+   };
+
+   if (ctx->in_sync_fd >= 0) {
+      ret = drmSyncobjImportSyncFile(panfrost_device_fd(dev), ctx->in_sync_obj,
+                                     ctx->in_sync_fd);
+      assert(!ret);
+
+      syncs[sync_count++] = (struct drm_panthor_sync_op){
+         .flags =
+            DRM_PANTHOR_SYNC_OP_WAIT | DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_SYNCOBJ,
+         .handle = ctx->in_sync_obj,
+      };
+      close(ctx->in_sync_fd);
+      ctx->in_sync_fd = -1;
+   }
+
+   struct drm_panthor_queue_submit qsubmit;
+   struct drm_panthor_group_submit gsubmit;
+
+   csf_prepare_qsubmit(ctx, &qsubmit, 0, cs_start, cs_size, syncs, sync_count);
+   csf_prepare_gsubmit(ctx, &gsubmit, &qsubmit, 1);
+   ret = csf_submit_gsubmit(ctx, &gsubmit);
+   if (!ret) {
+      util_dynarray_foreach(&batch->bos, pan_bo_access, ptr) {
+         unsigned i =
+            ptr - util_dynarray_element(&batch->bos, pan_bo_access, 0);
+         pan_bo_access flags = *ptr;
+
+         if (!flags)
+            continue;
+
+         struct panfrost_bo *bo = pan_lookup_bo(dev, i);
+
+         ret = panthor_kmod_bo_attach_sync_point(
+            bo->kmod_bo, vm_sync_handle, vm_sync_signal_point,
+            !(flags & PAN_BO_ACCESS_WRITE));
+         if (ret)
+            return ret;
+      }
+
+      util_dynarray_foreach(&batch->pool.bos, struct panfrost_bo *, bo) {
+         ret = panthor_kmod_bo_attach_sync_point((*bo)->kmod_bo, vm_sync_handle,
+                                                 vm_sync_signal_point, false);
+         if (ret)
+            return ret;
+      }
+
+      util_dynarray_foreach(&batch->invisible_pool.bos, struct panfrost_bo *,
+                            bo) {
+         ret = panthor_kmod_bo_attach_sync_point((*bo)->kmod_bo, vm_sync_handle,
+                                                 vm_sync_signal_point, false);
+         if (ret)
+            return ret;
+      }
+
+      ret = panthor_kmod_bo_attach_sync_point(dev->sample_positions->kmod_bo,
+                                              vm_sync_handle,
+                                              vm_sync_signal_point, true);
+      if (ret)
+         return ret;
+   } else {
+      struct drm_panthor_group_get_state state = {
+         .group_handle = ctx->csf.group_handle,
+      };
+
+      ret = drmIoctl(panfrost_device_fd(dev), DRM_IOCTL_PANTHOR_GROUP_GET_STATE,
+                     &state);
+      assert(!ret);
+
+      if (state.state != 0)
+         panfrost_context_reinit(ctx);
+   }
+
+   free(syncs);
+
+   if (ret)
+      return errno;
+
+   /* Jobs won't be complete if blackhole rendering, that's ok */
+   if (!ctx->is_noop && (dev->debug & PAN_DBG_SYNC) &&
+       *((uint64_t *)batch->csf.cs.state.cpu) != 0) {
+      fprintf(stderr, "Incomplete job or timeout\n");
+      fflush(NULL);
+      abort();
+   }
+
+   return 0;
+}
+
+void
+GENX(csf_preload_fb)(struct panfrost_batch *batch, struct pan_fb_info *fb)
+{
+   GENX(pan_preload_fb)
+   (&batch->pool.base, NULL, fb, batch->tls.gpu, batch->tiler_ctx.bifrost,
+    NULL);
+}
+
+void
+GENX(csf_emit_fragment_job)(struct panfrost_batch *batch,
+                            const struct pan_fb_info *pfb)
+{
+   ceu_builder *b = batch->csf.cs.builder;
+
+   if (batch->draw_count > 0) {
+      /* Finish tiling and wait for IDVS and tiling */
+      ceu_finish_tiling(b);
+      ceu_wait_slot(b, 2);
+      ceu_vt_end(b);
+   }
+
+   /* Set up the fragment job */
+   ceu_move64_to(b, ceu_reg64(b, 40), batch->framebuffer.gpu);
+   ceu_move32_to(b, ceu_reg32(b, 42), (batch->miny << 16) | batch->minx);
+   ceu_move32_to(b, ceu_reg32(b, 43),
+                 ((batch->maxy - 1) << 16) | (batch->maxx - 1));
+
+   /* Run the fragment job and wait */
+   ceu_run_fragment(b, false);
+   ceu_wait_slot(b, 2);
+
+   /* Gather freed heap chunks and add them to the heap context free list
+    * so they can be re-used next time the tiler heap runs out of chunks.
+    * That's what ceu_finish_fragment() is all about. The list of freed
+    * chunks is in the tiler context descriptor
+    * (completed_{top,bottom fields}). */
+   if (batch->tiler_ctx.bifrost) {
+      ceu_move64_to(b, ceu_reg64(b, 94), batch->tiler_ctx.bifrost);
+      ceu_load_to(b, ceu_reg_tuple(b, 90, 4), ceu_reg64(b, 94),
+                  BITFIELD_MASK(4), 40);
+      ceu_wait_slot(b, 0);
+      ceu_finish_fragment(b, true, ceu_reg64(b, 90), ceu_reg64(b, 92), 0x0, 1);
+      ceu_wait_slot(b, 1);
+   }
+}
+
+static void
+csf_emit_shader_regs(struct panfrost_batch *batch, enum pipe_shader_type stage,
+                     mali_ptr shader)
+{
+   mali_ptr resources = panfrost_emit_resources(batch, stage);
+
+   assert(stage == PIPE_SHADER_VERTEX || stage == PIPE_SHADER_FRAGMENT ||
+          stage == PIPE_SHADER_COMPUTE);
+
+   unsigned offset = (stage == PIPE_SHADER_FRAGMENT) ? 4 : 0;
+   unsigned fau_count = DIV_ROUND_UP(batch->nr_push_uniforms[stage], 2);
+
+   ceu_builder *b = batch->csf.cs.builder;
+   ceu_move64_to(b, ceu_reg64(b, 0 + offset), resources);
+   ceu_move64_to(b, ceu_reg64(b, 8 + offset),
+                 batch->push_uniforms[stage] | ((uint64_t)fau_count << 56));
+   ceu_move64_to(b, ceu_reg64(b, 16 + offset), shader);
+}
+
+void
+GENX(csf_launch_grid)(struct panfrost_batch *batch,
+                      const struct pipe_grid_info *info)
+{
+   /* Empty compute programs are invalid and don't make sense */
+   if (batch->rsd[PIPE_SHADER_COMPUTE] == 0)
+      return;
+
+   struct panfrost_context *ctx = batch->ctx;
+   struct panfrost_compiled_shader *cs = ctx->prog[PIPE_SHADER_COMPUTE];
+   ceu_builder *b = batch->csf.cs.builder;
+
+   csf_emit_shader_regs(batch, PIPE_SHADER_COMPUTE,
+                        batch->rsd[PIPE_SHADER_COMPUTE]);
+
+   ceu_move64_to(b, ceu_reg64(b, 24), batch->tls.gpu);
+
+   /* Global attribute offset */
+   ceu_move32_to(b, ceu_reg32(b, 32), 0);
+
+   /* Compute workgroup size */
+   uint32_t wg_size[4];
+   pan_pack(wg_size, COMPUTE_SIZE_WORKGROUP, cfg) {
+      cfg.workgroup_size_x = info->block[0];
+      cfg.workgroup_size_y = info->block[1];
+      cfg.workgroup_size_z = info->block[2];
+
+      /* Workgroups may be merged if the shader does not use barriers
+       * or shared memory. This condition is checked against the
+       * static shared_size at compile-time. We need to check the
+       * variable shared size at launch_grid time, because the
+       * compiler doesn't know about that.
+       */
+      cfg.allow_merging_workgroups = cs->info.cs.allow_merging_workgroups &&
+                                     (info->variable_shared_mem == 0);
+   }
+
+   ceu_move32_to(b, ceu_reg32(b, 33), wg_size[0]);
+
+   /* Offset */
+   for (unsigned i = 0; i < 3; ++i)
+      ceu_move32_to(b, ceu_reg32(b, 34 + i), 0);
+
+   if (info->indirect) {
+      /* Load size in workgroups per dimension from memory */
+      ceu_index address = ceu_reg64(b, 64);
+      ceu_move64_to(b, address,
+                    pan_resource(info->indirect)->image.data.bo->ptr.gpu +
+                       info->indirect_offset);
+
+      ceu_index grid_xyz = ceu_reg_tuple(b, 37, 3);
+      ceu_load_to(b, grid_xyz, address, BITFIELD_MASK(3), 0);
+
+      /* Wait for the load */
+      ceu_wait_slot(b, 0);
+
+      /* Copy to FAU */
+      for (unsigned i = 0; i < 3; ++i) {
+         if (batch->num_wg_sysval[i]) {
+            ceu_move64_to(b, address, batch->num_wg_sysval[i]);
+            ceu_store(b, ceu_extract32(b, grid_xyz, i), address,
+                      BITFIELD_MASK(1), 0);
+         }
+      }
+
+      /* Wait for the stores */
+      ceu_wait_slot(b, 0);
+   } else {
+      /* Set size in workgroups per dimension immediately */
+      for (unsigned i = 0; i < 3; ++i)
+         ceu_move32_to(b, ceu_reg32(b, 37 + i), info->grid[i]);
+   }
+
+   /* Dispatch. We could be much smarter choosing task size..
+    *
+    * TODO: How to choose correctly?
+    *
+    * XXX: Why are compute kernels failing if I make this smaller? Race
+    * condition maybe? Cache badnesss?
+    */
+   ceu_run_compute(b, 10, MALI_TASK_AXIS_Z);
+}
+
+void
+GENX(csf_launch_xfb)(struct panfrost_batch *batch,
+                     const struct pipe_draw_info *info, unsigned count)
+{
+   ceu_builder *b = batch->csf.cs.builder;
+
+   ceu_move64_to(b, ceu_reg64(b, 24), batch->tls.gpu);
+
+   /* TODO: Indexing. Also, attribute_offset is a legacy feature.. */
+   ceu_move32_to(b, ceu_reg32(b, 32), batch->ctx->offset_start);
+
+   /* Compute workgroup size */
+   uint32_t wg_size[4];
+   pan_pack(wg_size, COMPUTE_SIZE_WORKGROUP, cfg) {
+      cfg.workgroup_size_x = 1;
+      cfg.workgroup_size_y = 1;
+      cfg.workgroup_size_z = 1;
+
+      /* Transform feedback shaders do not use barriers or
+       * shared memory, so we may merge workgroups.
+       */
+      cfg.allow_merging_workgroups = true;
+   }
+   ceu_move32_to(b, ceu_reg32(b, 33), wg_size[0]);
+
+   /* Offset */
+   for (unsigned i = 0; i < 3; ++i)
+      ceu_move32_to(b, ceu_reg32(b, 34 + i), 0);
+
+   ceu_move32_to(b, ceu_reg32(b, 37), count);
+   ceu_move32_to(b, ceu_reg32(b, 38), info->instance_count);
+   ceu_move32_to(b, ceu_reg32(b, 39), 1);
+
+   csf_emit_shader_regs(batch, PIPE_SHADER_VERTEX,
+                        batch->rsd[PIPE_SHADER_VERTEX]);
+   /* XXX: Choose correctly */
+   ceu_run_compute(b, 1, MALI_TASK_AXIS_Z);
+
+   /* Reset registers expected to be 0 for IDVS */
+   ceu_move32_to(b, ceu_reg32(b, 31), 0);
+   ceu_move32_to(b, ceu_reg32(b, 32), 0);
+   ceu_move32_to(b, ceu_reg32(b, 37), 0);
+   ceu_move32_to(b, ceu_reg32(b, 38), 0);
+}
+
+static mali_ptr
+csf_get_tiler_desc(struct panfrost_batch *batch)
+{
+   struct panfrost_context *ctx = batch->ctx;
+   struct panfrost_device *dev = pan_device(ctx->base.screen);
+
+   if (batch->tiler_ctx.bifrost)
+      return batch->tiler_ctx.bifrost;
+
+   struct panfrost_ptr t =
+      pan_pool_alloc_desc(&batch->pool.base, TILER_CONTEXT);
+   pan_pack(t.cpu, TILER_CONTEXT, tiler) {
+      unsigned max_levels = dev->tiler_features.max_levels;
+      assert(max_levels >= 2);
+
+      /* TODO: Select hierarchy mask more effectively */
+      tiler.hierarchy_mask = (max_levels >= 8) ? 0xFF : 0x28;
+
+      /* For large framebuffers, disable the smallest bin size to
+       * avoid pathological tiler memory usage. Required to avoid OOM
+       * on dEQP-GLES31.functional.fbo.no_attachments.maximums.all on
+       * Mali-G57.
+       */
+      if (MAX2(batch->key.width, batch->key.height) >= 4096)
+         tiler.hierarchy_mask &= ~1;
+
+      tiler.fb_width = batch->key.width;
+      tiler.fb_height = batch->key.height;
+      tiler.heap = batch->ctx->csf.heap.desc_bo->ptr.gpu;
+      tiler.sample_pattern =
+         pan_sample_pattern(util_framebuffer_get_num_samples(&batch->key));
+#if PAN_ARCH >= 9
+      tiler.first_provoking_vertex =
+         pan_tristate_get(batch->first_provoking_vertex);
+#endif
+
+#if PAN_ARCH >= 10
+      tiler.geometry_buffer = ctx->csf.tmp_geom_bo->ptr.gpu;
+      tiler.geometry_buffer_size = ctx->csf.tmp_geom_bo->kmod_bo->size;
+#endif
+   }
+
+   batch->tiler_ctx.bifrost = t.gpu;
+   return batch->tiler_ctx.bifrost;
+}
+
+void
+GENX(csf_launch_draw)(struct panfrost_batch *batch,
+                      const struct pipe_draw_info *info, unsigned drawid_offset,
+                      const struct pipe_draw_start_count_bias *draw,
+                      unsigned vertex_count)
+{
+   struct panfrost_context *ctx = batch->ctx;
+   struct panfrost_compiled_shader *vs = ctx->prog[PIPE_SHADER_VERTEX];
+   struct panfrost_compiled_shader *fs = ctx->prog[PIPE_SHADER_FRAGMENT];
+   bool idvs = vs->info.vs.idvs;
+   bool fs_required = panfrost_fs_required(
+      fs, ctx->blend, &ctx->pipe_framebuffer, ctx->depth_stencil);
+   bool secondary_shader = vs->info.vs.secondary_enable && fs_required;
+
+   assert(idvs && "IDVS required for CSF");
+
+   ceu_builder *b = batch->csf.cs.builder;
+
+   if (batch->draw_count == 0)
+      ceu_vt_start(batch->csf.cs.builder);
+
+   csf_emit_shader_regs(batch, PIPE_SHADER_VERTEX,
+                        panfrost_get_position_shader(batch, info));
+
+   if (fs_required) {
+      csf_emit_shader_regs(batch, PIPE_SHADER_FRAGMENT,
+                           batch->rsd[PIPE_SHADER_FRAGMENT]);
+   } else {
+      ceu_move64_to(b, ceu_reg64(b, 4), 0);
+      ceu_move64_to(b, ceu_reg64(b, 12), 0);
+      ceu_move64_to(b, ceu_reg64(b, 20), 0);
+   }
+
+   if (secondary_shader) {
+      ceu_move64_to(b, ceu_reg64(b, 18), panfrost_get_varying_shader(batch));
+   }
+
+   ceu_move64_to(b, ceu_reg64(b, 24), batch->tls.gpu);
+   ceu_move64_to(b, ceu_reg64(b, 30), batch->tls.gpu);
+   ceu_move32_to(b, ceu_reg32(b, 33), draw->count);
+   ceu_move32_to(b, ceu_reg32(b, 34), info->instance_count);
+   ceu_move32_to(b, ceu_reg32(b, 35), 0);
+
+   /* Base vertex offset on Valhall is used for both indexed and
+    * non-indexed draws, in a simple way for either. Handle both cases.
+    */
+   if (info->index_size) {
+      ceu_move32_to(b, ceu_reg32(b, 36), draw->index_bias);
+      ceu_move32_to(b, ceu_reg32(b, 39), info->index_size * draw->count);
+   } else {
+      ceu_move32_to(b, ceu_reg32(b, 36), draw->start);
+      ceu_move32_to(b, ceu_reg32(b, 39), 0);
+   }
+
+   ceu_move64_to(b, ceu_reg64(b, 40), csf_get_tiler_desc(batch));
+
+   STATIC_ASSERT(sizeof(batch->scissor) == pan_size(SCISSOR));
+   STATIC_ASSERT(sizeof(uint64_t) == pan_size(SCISSOR));
+   uint64_t *sbd = (uint64_t *)&batch->scissor[0];
+   ceu_move64_to(b, ceu_reg64(b, 42), *sbd);
+
+   ceu_move32_to(b, ceu_reg32(b, 44), fui(batch->minimum_z));
+   ceu_move32_to(b, ceu_reg32(b, 45), fui(batch->maximum_z));
+
+   if (ctx->occlusion_query && ctx->active_queries) {
+      struct panfrost_resource *rsrc = pan_resource(ctx->occlusion_query->rsrc);
+      ceu_move64_to(b, ceu_reg64(b, 46), rsrc->image.data.bo->ptr.gpu);
+      panfrost_batch_write_rsrc(ctx->batch, rsrc, PIPE_SHADER_FRAGMENT);
+   }
+
+   ceu_move32_to(b, ceu_reg32(b, 48), panfrost_vertex_attribute_stride(vs, fs));
+   ceu_move64_to(b, ceu_reg64(b, 50),
+                 batch->blend | MAX2(batch->key.nr_cbufs, 1));
+   ceu_move64_to(b, ceu_reg64(b, 52), batch->depth_stencil);
+
+   if (info->index_size)
+      ceu_move64_to(b, ceu_reg64(b, 54), batch->indices);
+
+   uint32_t primitive_flags = 0;
+   pan_pack(&primitive_flags, PRIMITIVE_FLAGS, cfg) {
+      if (panfrost_writes_point_size(ctx))
+         cfg.point_size_array_format = MALI_POINT_SIZE_ARRAY_FORMAT_FP16;
+
+      cfg.allow_rotating_primitives = allow_rotating_primitives(fs, info);
+
+      /* Non-fixed restart indices should have been lowered */
+      assert(!cfg.primitive_restart || panfrost_is_implicit_prim_restart(info));
+      cfg.primitive_restart = info->primitive_restart;
+
+      cfg.position_fifo_format = panfrost_writes_point_size(ctx)
+                                    ? MALI_FIFO_FORMAT_EXTENDED
+                                    : MALI_FIFO_FORMAT_BASIC;
+   }
+
+   ceu_move32_to(b, ceu_reg32(b, 56), primitive_flags);
+
+   struct pipe_rasterizer_state *rast = &ctx->rasterizer->base;
+
+   uint32_t dcd_flags0 = 0, dcd_flags1 = 0;
+   pan_pack(&dcd_flags0, DCD_FLAGS_0, cfg) {
+      bool polygon = (u_reduced_prim(info->mode) == MESA_PRIM_TRIANGLES);
+
+      /*
+       * From the Gallium documentation,
+       * pipe_rasterizer_state::cull_face "indicates which faces of
+       * polygons to cull". Points and lines are not considered
+       * polygons and should be drawn even if all faces are culled.
+       * The hardware does not take primitive type into account when
+       * culling, so we need to do that check ourselves.
+       */
+      cfg.cull_front_face = polygon && (rast->cull_face & PIPE_FACE_FRONT);
+      cfg.cull_back_face = polygon && (rast->cull_face & PIPE_FACE_BACK);
+      cfg.front_face_ccw = rast->front_ccw;
+
+      cfg.multisample_enable = rast->multisample;
+
+      /* Use per-sample shading if required by API Also use it when a
+       * blend shader is used with multisampling, as this is handled
+       * by a single ST_TILE in the blend shader with the current
+       * sample ID, requiring per-sample shading.
+       */
+      cfg.evaluate_per_sample =
+         (rast->multisample &&
+          ((ctx->min_samples > 1) || ctx->valhall_has_blend_shader));
+
+      cfg.single_sampled_lines = !rast->multisample;
+
+      bool has_oq = ctx->occlusion_query && ctx->active_queries;
+      if (has_oq) {
+         if (ctx->occlusion_query->type == PIPE_QUERY_OCCLUSION_COUNTER)
+            cfg.occlusion_query = MALI_OCCLUSION_MODE_COUNTER;
+         else
+            cfg.occlusion_query = MALI_OCCLUSION_MODE_PREDICATE;
+      }
+
+      if (fs_required) {
+         struct pan_earlyzs_state earlyzs = pan_earlyzs_get(
+            fs->earlyzs, ctx->depth_stencil->writes_zs || has_oq,
+            ctx->blend->base.alpha_to_coverage,
+            ctx->depth_stencil->zs_always_passes);
+
+         cfg.pixel_kill_operation = earlyzs.kill;
+         cfg.zs_update_operation = earlyzs.update;
+
+         cfg.allow_forward_pixel_to_kill =
+            pan_allow_forward_pixel_to_kill(ctx, fs);
+         cfg.allow_forward_pixel_to_be_killed = !fs->info.writes_global;
+
+         cfg.overdraw_alpha0 = panfrost_overdraw_alpha(ctx, 0);
+         cfg.overdraw_alpha1 = panfrost_overdraw_alpha(ctx, 1);
+
+         /* Also use per-sample shading if required by the shader
+          */
+         cfg.evaluate_per_sample |= fs->info.fs.sample_shading;
+
+         /* Unlike Bifrost, alpha-to-coverage must be included in
+          * this identically-named flag. Confusing, isn't it?
+          */
+         cfg.shader_modifies_coverage = fs->info.fs.writes_coverage ||
+                                        fs->info.fs.can_discard ||
+                                        ctx->blend->base.alpha_to_coverage;
+
+         cfg.alpha_to_coverage = ctx->blend->base.alpha_to_coverage;
+      } else {
+         /* These operations need to be FORCE to benefit from the
+          * depth-only pass optimizations.
+          */
+         cfg.pixel_kill_operation = MALI_PIXEL_KILL_FORCE_EARLY;
+         cfg.zs_update_operation = MALI_PIXEL_KILL_FORCE_EARLY;
+
+         /* No shader and no blend => no shader or blend
+          * reasons to disable FPK. The only FPK-related state
+          * not covered is alpha-to-coverage which we don't set
+          * without blend.
+          */
+         cfg.allow_forward_pixel_to_kill = true;
+
+         /* No shader => no shader side effects */
+         cfg.allow_forward_pixel_to_be_killed = true;
+
+         /* Alpha isn't written so these are vacuous */
+         cfg.overdraw_alpha0 = true;
+         cfg.overdraw_alpha1 = true;
+      }
+   }
+
+   pan_pack(&dcd_flags1, DCD_FLAGS_1, cfg) {
+      cfg.sample_mask = rast->multisample ? ctx->sample_mask : 0xFFFF;
+
+      if (fs_required) {
+         /* See JM Valhall equivalent code */
+         cfg.render_target_mask =
+            (fs->info.outputs_written >> FRAG_RESULT_DATA0) & ctx->fb_rt_mask;
+      }
+   }
+
+   ceu_move32_to(b, ceu_reg32(b, 57), dcd_flags0);
+   ceu_move32_to(b, ceu_reg32(b, 58), dcd_flags1);
+
+   uint64_t primsize = 0;
+   panfrost_emit_primitive_size(ctx, info->mode == MESA_PRIM_POINTS, 0,
+                                &primsize);
+   ceu_move64_to(b, ceu_reg64(b, 60), primsize);
+
+   ceu_run_idvs(b, pan_draw_mode(info->mode),
+                panfrost_translate_index_size(info->index_size),
+                secondary_shader);
+}
+
+#define POSITION_FIFO_SIZE (64 * 1024)
+
+void
+GENX(csf_init_context)(struct panfrost_context *ctx)
+{
+   struct panfrost_device *dev = pan_device(ctx->base.screen);
+   struct drm_panthor_queue_create qc[] = {{
+      .priority = 1,
+      .ringbuf_size = 64 * 1024,
+   }};
+
+   struct drm_panthor_group_create gc = {
+      .compute_core_mask = dev->kmod.props.shader_present,
+      .fragment_core_mask = dev->kmod.props.shader_present,
+      .tiler_core_mask = 1,
+      .max_compute_cores = util_bitcount64(dev->kmod.props.shader_present),
+      .max_fragment_cores = util_bitcount64(dev->kmod.props.shader_present),
+      .max_tiler_cores = 1,
+      .priority = PANTHOR_GROUP_PRIORITY_MEDIUM,
+      .queues = DRM_PANTHOR_OBJ_ARRAY(ARRAY_SIZE(qc), qc),
+      .vm_id = pan_kmod_vm_handle(dev->kmod.vm),
+   };
+
+   int ret =
+      drmIoctl(panfrost_device_fd(dev), DRM_IOCTL_PANTHOR_GROUP_CREATE, &gc);
+
+   assert(!ret);
+
+   ctx->csf.group_handle = gc.group_handle;
+
+   /* Get tiler heap */
+   struct drm_panthor_tiler_heap_create thc = {
+      .vm_id = pan_kmod_vm_handle(dev->kmod.vm),
+      .chunk_size = 2 * 1024 * 1024,
+      .initial_chunk_count = 5,
+      .max_chunks = 64 * 1024,
+      .target_in_flight = 65535,
+   };
+   ret = drmIoctl(panfrost_device_fd(dev), DRM_IOCTL_PANTHOR_TILER_HEAP_CREATE,
+                  &thc);
+
+   assert(!ret);
+
+   ctx->csf.heap.handle = thc.handle;
+
+   ctx->csf.heap.desc_bo =
+      panfrost_bo_create(dev, pan_size(TILER_HEAP), 0, "Tiler Heap");
+   pan_pack(ctx->csf.heap.desc_bo->ptr.cpu, TILER_HEAP, heap) {
+      heap.size = 2 * 1024 * 1024;
+      heap.base = thc.first_heap_chunk_gpu_va;
+      heap.bottom = heap.base + 64;
+      heap.top = heap.base + heap.size;
+   }
+
+   ctx->csf.tmp_geom_bo = panfrost_bo_create(
+      dev, POSITION_FIFO_SIZE, PAN_BO_INVISIBLE, "Temporary Geometry buffer");
+   assert(ctx->csf.tmp_geom_bo);
+
+   /* Setup the tiler heap */
+   struct panfrost_bo *cs_bo =
+      panfrost_bo_create(dev, 4096, 0, "Temporary CS buffer");
+   assert(cs_bo);
+
+   struct ceu_queue init_queue = {
+      .cpu = cs_bo->ptr.cpu,
+      .gpu = cs_bo->ptr.gpu,
+      .capacity = panfrost_bo_size(cs_bo) / sizeof(uint64_t),
+   };
+   const struct ceu_builder_conf bconf = {
+      .nr_registers = 96,
+      .nr_kernel_registers = 4,
+   };
+   ceu_builder b;
+   ceu_builder_init(&b, &bconf, init_queue);
+   ceu_index heap = ceu_reg64(&b, 72);
+   ceu_move64_to(&b, heap, thc.tiler_heap_ctx_gpu_va);
+   ceu_heap_set(&b, heap);
+
+   struct drm_panthor_queue_submit qsubmit;
+   struct drm_panthor_group_submit gsubmit;
+   struct drm_panthor_sync_op sync = {
+      .flags =
+         DRM_PANTHOR_SYNC_OP_SIGNAL | DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_SYNCOBJ,
+      .handle = ctx->syncobj,
+   };
+   uint32_t cs_instr_count = ceu_finish(&b);
+   uint64_t cs_start = b.root.gpu;
+   uint32_t cs_size = cs_instr_count * 8;
+
+   csf_prepare_qsubmit(ctx, &qsubmit, 0, cs_start, cs_size, &sync, 1);
+   csf_prepare_gsubmit(ctx, &gsubmit, &qsubmit, 1);
+   ret = csf_submit_gsubmit(ctx, &gsubmit);
+   assert(!ret);
+
+   /* Wait before freeing the buffer. */
+   drmSyncobjWait(panfrost_device_fd(dev), &ctx->syncobj, 1, INT64_MAX, 0,
+                  NULL);
+   panfrost_bo_unreference(cs_bo);
+}
+
+void
+GENX(csf_cleanup_context)(struct panfrost_context *ctx)
+{
+   struct panfrost_device *dev = pan_device(ctx->base.screen);
+   struct drm_panthor_tiler_heap_destroy thd = {
+      .handle = ctx->csf.heap.handle,
+   };
+   int ret;
+
+   /* Make sure all jobs are done before destroying the heap. */
+   ret = drmSyncobjWait(panfrost_device_fd(dev), &ctx->syncobj, 1, INT64_MAX, 0,
+                        NULL);
+   assert(!ret);
+
+   ret = drmIoctl(panfrost_device_fd(dev), DRM_IOCTL_PANTHOR_TILER_HEAP_DESTROY,
+                  &thd);
+   assert(!ret);
+
+   struct drm_panthor_group_destroy gd = {
+      .group_handle = ctx->csf.group_handle,
+   };
+
+   ret =
+      drmIoctl(panfrost_device_fd(dev), DRM_IOCTL_PANTHOR_GROUP_DESTROY, &gd);
+   assert(!ret);
+
+   panfrost_bo_unreference(ctx->csf.heap.desc_bo);
+}
diff --git a/src/gallium/drivers/panfrost/pan_csf.h b/src/gallium/drivers/panfrost/pan_csf.h
new file mode 100644
index 0000000000000..0f2b3673073c4
--- /dev/null
+++ b/src/gallium/drivers/panfrost/pan_csf.h
@@ -0,0 +1,92 @@
+/*
+ * Copyright (C) 2023 Collabora Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ *
+ */
+
+#ifndef __PAN_CSF_H__
+#define __PAN_CSF_H__
+
+#include "compiler/shader_enums.h"
+
+#include "pan_bo.h"
+
+struct ceu_builder;
+
+struct panfrost_csf_batch {
+   /* CS related fields. */
+   struct {
+      /* CS builder. */
+      struct ceu_builder *builder;
+
+      /* CS state, written through the CS, and checked when PAN_MESA_DEBUG=sync.
+       */
+      struct panfrost_ptr state;
+   } cs;
+};
+
+struct panfrost_csf_context {
+   uint32_t group_handle;
+
+   struct {
+      uint32_t handle;
+      struct panfrost_bo *desc_bo;
+   } heap;
+
+   /* Temporary geometry buffer. Used as a FIFO by the tiler. */
+   struct panfrost_bo *tmp_geom_bo;
+};
+
+#if defined(PAN_ARCH) && PAN_ARCH >= 10
+
+#include "genxml/gen_macros.h"
+
+struct panfrost_batch;
+struct panfrost_context;
+struct pan_fb_info;
+struct pipe_draw_info;
+struct pipe_grid_info;
+struct pipe_draw_start_count_bias;
+
+void GENX(csf_init_context)(struct panfrost_context *ctx);
+void GENX(csf_cleanup_context)(struct panfrost_context *ctx);
+
+void GENX(csf_init_batch)(struct panfrost_batch *batch);
+void GENX(csf_cleanup_batch)(struct panfrost_batch *batch);
+int GENX(csf_submit_batch)(struct panfrost_batch *batch);
+
+void GENX(csf_preload_fb)(struct panfrost_batch *batch, struct pan_fb_info *fb);
+void GENX(csf_emit_fragment_job)(struct panfrost_batch *batch,
+                                 const struct pan_fb_info *pfb);
+void GENX(csf_emit_batch_end)(struct panfrost_batch *batch);
+void GENX(csf_launch_xfb)(struct panfrost_batch *batch,
+                          const struct pipe_draw_info *info, unsigned count);
+void GENX(csf_launch_grid)(struct panfrost_batch *batch,
+                           const struct pipe_grid_info *info);
+void GENX(csf_launch_draw)(struct panfrost_batch *batch,
+                           const struct pipe_draw_info *info,
+                           unsigned drawid_offset,
+                           const struct pipe_draw_start_count_bias *draw,
+                           unsigned vertex_count);
+
+#endif /* PAN_ARCH >= 10 */
+
+#endif /* __PAN_CSF_H__ */
diff --git a/src/gallium/drivers/panfrost/pan_job.h b/src/gallium/drivers/panfrost/pan_job.h
index 6856bc723ea76..f10da3d1ec930 100644
--- a/src/gallium/drivers/panfrost/pan_job.h
+++ b/src/gallium/drivers/panfrost/pan_job.h
@@ -28,6 +28,7 @@
 
 #include "pipe/p_state.h"
 #include "util/u_dynarray.h"
+#include "pan_csf.h"
 #include "pan_desc.h"
 #include "pan_jm.h"
 #include "pan_mempool.h"
@@ -212,6 +213,7 @@ struct panfrost_batch {
    /* Job frontend specific fields. */
    union {
       struct panfrost_jm_batch jm;
+      struct panfrost_csf_batch csf;
    };
 };
 
-- 
GitLab


From 82b16815c5f8f791901468dd85c0241f91226b98 Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Thu, 30 Nov 2023 16:26:43 +0100
Subject: [PATCH 27/32] fixup! panfrost: Add support for the CSF job frontend

---
 src/gallium/drivers/panfrost/pan_csf.c | 27 ++++++++++++++------------
 1 file changed, 15 insertions(+), 12 deletions(-)

diff --git a/src/gallium/drivers/panfrost/pan_csf.c b/src/gallium/drivers/panfrost/pan_csf.c
index 726099fa6d971..49cdb95b229b1 100644
--- a/src/gallium/drivers/panfrost/pan_csf.c
+++ b/src/gallium/drivers/panfrost/pan_csf.c
@@ -231,7 +231,7 @@ GENX(csf_submit_batch)(struct panfrost_batch *batch)
                                            &bo_sync_point,
                                            !(flags & PAN_BO_ACCESS_WRITE));
       if (ret)
-         return ret;
+         goto out_free_syncs;
 
       if (bo_sync_handle == vm_sync_handle) {
          vm_sync_wait_point = MAX2(vm_sync_wait_point, bo_sync_point);
@@ -254,7 +254,7 @@ GENX(csf_submit_batch)(struct panfrost_batch *batch)
       ret = panthor_kmod_bo_get_sync_point((*bo)->kmod_bo, &bo_sync_handle,
                                            &bo_sync_point, false);
       if (ret)
-         return ret;
+         goto out_free_syncs;
 
       assert(bo_sync_handle == vm_sync_handle);
       vm_sync_wait_point = MAX2(vm_sync_wait_point, bo_sync_point);
@@ -266,7 +266,7 @@ GENX(csf_submit_batch)(struct panfrost_batch *batch)
       ret = panthor_kmod_bo_get_sync_point((*bo)->kmod_bo, &bo_sync_handle,
                                            &bo_sync_point, false);
       if (ret)
-         return ret;
+         goto out_free_syncs;
 
       assert(bo_sync_handle == vm_sync_handle);
       vm_sync_wait_point = MAX2(vm_sync_wait_point, bo_sync_point);
@@ -335,14 +335,14 @@ GENX(csf_submit_batch)(struct panfrost_batch *batch)
             bo->kmod_bo, vm_sync_handle, vm_sync_signal_point,
             !(flags & PAN_BO_ACCESS_WRITE));
          if (ret)
-            return ret;
+            goto out_free_syncs;
       }
 
       util_dynarray_foreach(&batch->pool.bos, struct panfrost_bo *, bo) {
          ret = panthor_kmod_bo_attach_sync_point((*bo)->kmod_bo, vm_sync_handle,
                                                  vm_sync_signal_point, false);
          if (ret)
-            return ret;
+            goto out_free_syncs;
       }
 
       util_dynarray_foreach(&batch->invisible_pool.bos, struct panfrost_bo *,
@@ -350,14 +350,14 @@ GENX(csf_submit_batch)(struct panfrost_batch *batch)
          ret = panthor_kmod_bo_attach_sync_point((*bo)->kmod_bo, vm_sync_handle,
                                                  vm_sync_signal_point, false);
          if (ret)
-            return ret;
+            goto out_free_syncs;
       }
 
       ret = panthor_kmod_bo_attach_sync_point(dev->sample_positions->kmod_bo,
                                               vm_sync_handle,
                                               vm_sync_signal_point, true);
       if (ret)
-         return ret;
+         goto out_free_syncs;
    } else {
       struct drm_panthor_group_get_state state = {
          .group_handle = ctx->csf.group_handle,
@@ -365,16 +365,17 @@ GENX(csf_submit_batch)(struct panfrost_batch *batch)
 
       ret = drmIoctl(panfrost_device_fd(dev), DRM_IOCTL_PANTHOR_GROUP_GET_STATE,
                      &state);
-      assert(!ret);
+      if (ret) {
+         mesa_loge("DRM_IOCTL_PANTHOR_GROUP_GET_STATE failed (err=%d)", errno);
+         goto out_free_syncs;
+      }
 
       if (state.state != 0)
          panfrost_context_reinit(ctx);
    }
 
-   free(syncs);
-
    if (ret)
-      return errno;
+      goto out_free_syncs;
 
    /* Jobs won't be complete if blackhole rendering, that's ok */
    if (!ctx->is_noop && (dev->debug & PAN_DBG_SYNC) &&
@@ -384,7 +385,9 @@ GENX(csf_submit_batch)(struct panfrost_batch *batch)
       abort();
    }
 
-   return 0;
+out_free_syncs:
+   free(syncs);
+   return ret;
 }
 
 void
-- 
GitLab


From 0a2dcf892ab046b989344dc10e1ed9e8658eab35 Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Wed, 15 Nov 2023 12:58:50 +0100
Subject: [PATCH 28/32] panfrost: Enable v10 in the gallium driver

Now that everything is in place to support v10, add it to the
panfrost_versions array in meson.build and patch panfrost_create_screen()
to hook up pipe_screen initialization.
---
 src/gallium/drivers/panfrost/meson.build  | 2 +-
 src/gallium/drivers/panfrost/pan_screen.c | 4 ++++
 src/gallium/drivers/panfrost/pan_screen.h | 1 +
 3 files changed, 6 insertions(+), 1 deletion(-)

diff --git a/src/gallium/drivers/panfrost/meson.build b/src/gallium/drivers/panfrost/meson.build
index dff95405f55b1..820d84794685b 100644
--- a/src/gallium/drivers/panfrost/meson.build
+++ b/src/gallium/drivers/panfrost/meson.build
@@ -54,7 +54,7 @@ compile_args_panfrost = [
   '-Wno-pointer-arith'
 ]
 
-panfrost_versions = ['4', '5', '6', '7', '9']
+panfrost_versions = ['4', '5', '6', '7', '9', '10']
 libpanfrost_versions = []
 
 foreach ver : panfrost_versions
diff --git a/src/gallium/drivers/panfrost/pan_screen.c b/src/gallium/drivers/panfrost/pan_screen.c
index 876833e799192..f775b9c321277 100644
--- a/src/gallium/drivers/panfrost/pan_screen.c
+++ b/src/gallium/drivers/panfrost/pan_screen.c
@@ -43,6 +43,8 @@
 #include "drm-uapi/drm_fourcc.h"
 #include "drm-uapi/panfrost_drm.h"
 
+#include "genxml/ceu_builder.h"
+
 #include "decode.h"
 #include "pan_bo.h"
 #include "pan_fence.h"
@@ -902,6 +904,8 @@ panfrost_create_screen(int fd, const struct pipe_screen_config *config,
       panfrost_cmdstream_screen_init_v7(screen);
    else if (dev->arch == 9)
       panfrost_cmdstream_screen_init_v9(screen);
+   else if (dev->arch == 10)
+      panfrost_cmdstream_screen_init_v10(screen);
    else
       unreachable("Unhandled architecture major");
 
diff --git a/src/gallium/drivers/panfrost/pan_screen.h b/src/gallium/drivers/panfrost/pan_screen.h
index 56ca5426d605f..164fe4b931f6a 100644
--- a/src/gallium/drivers/panfrost/pan_screen.h
+++ b/src/gallium/drivers/panfrost/pan_screen.h
@@ -139,6 +139,7 @@ void panfrost_cmdstream_screen_init_v5(struct panfrost_screen *screen);
 void panfrost_cmdstream_screen_init_v6(struct panfrost_screen *screen);
 void panfrost_cmdstream_screen_init_v7(struct panfrost_screen *screen);
 void panfrost_cmdstream_screen_init_v9(struct panfrost_screen *screen);
+void panfrost_cmdstream_screen_init_v10(struct panfrost_screen *screen);
 
 #define perf_debug(dev, ...)                                                   \
    do {                                                                        \
-- 
GitLab


From 9ee5478d98c07afc6f278d9ea136e5b862a1be31 Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Wed, 15 Nov 2023 11:50:22 +0100
Subject: [PATCH 29/32] panfrost: Add a panfrost_model entry for G610

---
 src/panfrost/lib/pan_props.c | 2 ++
 1 file changed, 2 insertions(+)

diff --git a/src/panfrost/lib/pan_props.c b/src/panfrost/lib/pan_props.c
index 8272cc0c49a07..0ac085a6e77ed 100644
--- a/src/panfrost/lib/pan_props.c
+++ b/src/panfrost/lib/pan_props.c
@@ -71,6 +71,8 @@ const struct panfrost_model panfrost_model_list[] = {
         MODEL(0x7402, "G52 r1", "TGOx", HAS_ANISO,         16384, {}),
         MODEL(0x9091, "G57",    "TNAx", HAS_ANISO,         16384, {}),
         MODEL(0x9093, "G57",    "TNAx", HAS_ANISO,         16384, {}),
+
+        MODEL(0xa867, "G610",   "TNAx", HAS_ANISO,         16384, {}), // TODO
 };
 /* clang-format on */
 
-- 
GitLab


From f200d733e422fcbccbab838455dd5c02a9c3fc52 Mon Sep 17 00:00:00 2001
From: Boris Brezillon <boris.brezillon@collabora.com>
Date: Tue, 5 Sep 2023 12:35:55 +0200
Subject: [PATCH 30/32] panfrost: Add G310 to the list of supported GPUs

Signed-off-by: Boris Brezillon <boris.brezillon@collabora.com>
---
 src/panfrost/lib/pan_props.c | 1 +
 1 file changed, 1 insertion(+)

diff --git a/src/panfrost/lib/pan_props.c b/src/panfrost/lib/pan_props.c
index 0ac085a6e77ed..94b348c96e134 100644
--- a/src/panfrost/lib/pan_props.c
+++ b/src/panfrost/lib/pan_props.c
@@ -73,6 +73,7 @@ const struct panfrost_model panfrost_model_list[] = {
         MODEL(0x9093, "G57",    "TNAx", HAS_ANISO,         16384, {}),
 
         MODEL(0xa867, "G610",   "TNAx", HAS_ANISO,         16384, {}), // TODO
+        MODEL(0xac74, "G310",   "TNAx", HAS_ANISO,         16384, {}), // TODO
 };
 /* clang-format on */
 
-- 
GitLab


From 5a66069f04a4f898349e54aa69f9f6d5d78bcf03 Mon Sep 17 00:00:00 2001
From: Bob Beckett <bob.beckett@collabora.com>
Date: Tue, 14 Nov 2023 11:22:47 +0100
Subject: [PATCH 31/32] panfrost: Add an entry for panthor in the
 renderonly_drivers[] array

Same as panfrost, but the kernel driver has a different name.
---
 src/gallium/winsys/kmsro/drm/kmsro_drm_winsys.c | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/src/gallium/winsys/kmsro/drm/kmsro_drm_winsys.c b/src/gallium/winsys/kmsro/drm/kmsro_drm_winsys.c
index 3d14bce1ce7ff..259e876dc8b29 100644
--- a/src/gallium/winsys/kmsro/drm/kmsro_drm_winsys.c
+++ b/src/gallium/winsys/kmsro/drm/kmsro_drm_winsys.c
@@ -101,7 +101,8 @@ struct pipe_screen *kmsro_drm_screen_create(int kms_fd,
       ro->create_for_resource = renderonly_create_kms_dumb_buffer_for_resource;
       screen = lima_drm_screen_create_renderonly(ro->gpu_fd, ro, config);
 #endif
-   } else if (strcmp(render_dev_name, "panfrost") == 0) {
+   } else if (strcmp(render_dev_name, "panfrost") == 0 ||
+              strcmp(render_dev_name, "panthor") == 0) {
 #if defined(GALLIUM_PANFROST)
       ro->create_for_resource = panfrost_create_kms_dumb_buffer_for_resource;
       screen = panfrost_drm_screen_create_renderonly(ro->gpu_fd, ro, config);
-- 
GitLab


From 39173be155e6568114b2395718c473f85b1d8880 Mon Sep 17 00:00:00 2001
From: Bob Beckett <bob.beckett@collabora.com>
Date: Tue, 14 Nov 2023 11:18:23 +0100
Subject: [PATCH 32/32] panfrost: Add the gallium glue to get panfrost loaded
 when panthor is detected

Needed if we don't want to force users to pass GALLIUM_DRIVER=panfrost.
---
 src/gallium/auxiliary/pipe-loader/pipe_loader_drm.c      | 2 ++
 src/gallium/auxiliary/target-helpers/drm_helper.h        | 2 ++
 src/gallium/auxiliary/target-helpers/drm_helper_public.h | 1 +
 src/gallium/targets/dri/meson.build                      | 2 +-
 src/gallium/targets/dri/target.c                         | 1 +
 5 files changed, 7 insertions(+), 1 deletion(-)

diff --git a/src/gallium/auxiliary/pipe-loader/pipe_loader_drm.c b/src/gallium/auxiliary/pipe-loader/pipe_loader_drm.c
index 055c637199d07..70b3f3a59d778 100644
--- a/src/gallium/auxiliary/pipe-loader/pipe_loader_drm.c
+++ b/src/gallium/auxiliary/pipe-loader/pipe_loader_drm.c
@@ -82,6 +82,7 @@ static const struct drm_driver_descriptor *driver_descriptors[] = {
    &v3d_driver_descriptor,
    &vc4_driver_descriptor,
    &panfrost_driver_descriptor,
+   &panthor_driver_descriptor,
    &asahi_driver_descriptor,
    &etnaviv_driver_descriptor,
    &tegra_driver_descriptor,
@@ -289,6 +290,7 @@ pipe_loader_get_compatible_render_capable_device_fd(int kms_only_fd)
 #endif
 #if defined GALLIUM_PANFROST
       "panfrost",
+      "panthor",
 #endif
 #if defined GALLIUM_V3D
       "v3d",
diff --git a/src/gallium/auxiliary/target-helpers/drm_helper.h b/src/gallium/auxiliary/target-helpers/drm_helper.h
index be44b8f71df7a..8a06b775cdd41 100644
--- a/src/gallium/auxiliary/target-helpers/drm_helper.h
+++ b/src/gallium/auxiliary/target-helpers/drm_helper.h
@@ -335,9 +335,11 @@ pipe_panfrost_create_screen(int fd, const struct pipe_screen_config *config)
    return screen ? debug_screen_wrap(screen) : NULL;
 }
 DRM_DRIVER_DESCRIPTOR(panfrost, NULL, 0)
+DRM_DRIVER_DESCRIPTOR_ALIAS(panfrost, panthor, NULL, 0)
 
 #else
 DRM_DRIVER_DESCRIPTOR_STUB(panfrost)
+DRM_DRIVER_DESCRIPTOR_STUB(panthor)
 #endif
 
 #ifdef GALLIUM_ASAHI
diff --git a/src/gallium/auxiliary/target-helpers/drm_helper_public.h b/src/gallium/auxiliary/target-helpers/drm_helper_public.h
index 89c0a429967ff..e7fcd6b379f8f 100644
--- a/src/gallium/auxiliary/target-helpers/drm_helper_public.h
+++ b/src/gallium/auxiliary/target-helpers/drm_helper_public.h
@@ -18,6 +18,7 @@ extern const struct drm_driver_descriptor virtio_gpu_driver_descriptor;
 extern const struct drm_driver_descriptor v3d_driver_descriptor;
 extern const struct drm_driver_descriptor vc4_driver_descriptor;
 extern const struct drm_driver_descriptor panfrost_driver_descriptor;
+extern const struct drm_driver_descriptor panthor_driver_descriptor;
 extern const struct drm_driver_descriptor asahi_driver_descriptor;
 extern const struct drm_driver_descriptor etnaviv_driver_descriptor;
 extern const struct drm_driver_descriptor tegra_driver_descriptor;
diff --git a/src/gallium/targets/dri/meson.build b/src/gallium/targets/dri/meson.build
index 83925248b17e6..617eae6220229 100644
--- a/src/gallium/targets/dri/meson.build
+++ b/src/gallium/targets/dri/meson.build
@@ -109,7 +109,7 @@ foreach d : [[with_gallium_kmsro, [
              [with_gallium_softpipe and with_gallium_drisw_kms, 'kms_swrast_dri.so'],
              [with_gallium_v3d, 'v3d_dri.so'],
              [with_gallium_vc4, 'vc4_dri.so'],
-             [with_gallium_panfrost, 'panfrost_dri.so'],
+             [with_gallium_panfrost, ['panfrost_dri.so', 'panthor_dri.so']],
              [with_gallium_etnaviv, 'etnaviv_dri.so'],
              [with_gallium_tegra, 'tegra_dri.so'],
              [with_gallium_crocus, 'crocus_dri.so'],
diff --git a/src/gallium/targets/dri/target.c b/src/gallium/targets/dri/target.c
index 415e49465dfc8..af57ed4eee522 100644
--- a/src/gallium/targets/dri/target.c
+++ b/src/gallium/targets/dri/target.c
@@ -82,6 +82,7 @@ DEFINE_LOADER_DRM_ENTRYPOINT(vc4)
 
 #if defined(GALLIUM_PANFROST)
 DEFINE_LOADER_DRM_ENTRYPOINT(panfrost)
+DEFINE_LOADER_DRM_ENTRYPOINT(panthor)
 #endif
 
 #if defined(GALLIUM_ASAHI)
-- 
GitLab

