commit 092c48e1cbcbb4b35ab0a4d2a52c6ad1eb65ece7
Author: Louis-Francis Ratté-Boulianne <lfrb@collabora.com>
Date:   Thu Aug 31 23:33:45 2023 -0400

    panfrost: Add support for AFBC packing
    
    When the GPU is converting a texture from linear/tiled to compressed
    AFBC, it uses a sparse memory layout. That means that the
    superblocks are stored starting at intervals equal to the size of an
    uncompressed superblock. When memory usage needs to be optimized, it
    is possible to pack the resource by trimming each superblock as much
    as possible. The GPU will still be able to read from these packed
    textures, but won't be able to write directly to them. If the
    layout is AFBC-tiled, the packing process will also de-tile as
    tiled+packed is not supported by Mali GPUs.
    
    No new modifier flag has been added as the absence of the
    `AFBC_FORMAT_MOD_SPARSE` flag means the resource will be packed.
    
    Signed-off-by: Louis-Francis Ratté-Boulianne <lfrb@collabora.com>
    Part-of: <https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/25012>
    [bero@lindev.ch: Port to Panthor]
    Signed-off-by: Bernhard Rosenkränzer <bero@lindev.ch>

diff --git a/src/gallium/drivers/panfrost/pan_afbc_cso.c b/src/gallium/drivers/panfrost/pan_afbc_cso.c
new file mode 100644
index 00000000000..7ea74252b98
--- /dev/null
+++ b/src/gallium/drivers/panfrost/pan_afbc_cso.c
@@ -0,0 +1,334 @@
+/*
+ * Copyright (C) 2023 Amazon.com, Inc. or its affiliates
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "pan_afbc_cso.h"
+#include "nir_builder.h"
+#include "pan_context.h"
+#include "pan_resource.h"
+#include "pan_screen.h"
+
+#define panfrost_afbc_add_info_ubo(name, b)                                    \
+   nir_variable *info_ubo = nir_variable_create(                               \
+      b.shader, nir_var_mem_ubo,                                               \
+      glsl_array_type(glsl_uint_type(),                                        \
+                      sizeof(struct panfrost_afbc_##name##_info) / 4, 0),      \
+      "info_ubo");                                                             \
+   info_ubo->data.driver_location = 0;
+
+#define panfrost_afbc_get_info_field(name, b, field)                           \
+   nir_load_ubo(                                                               \
+      (b), 1, sizeof(((struct panfrost_afbc_##name##_info *)0)->field) * 8,    \
+      nir_imm_int(b, 0),                                                       \
+      nir_imm_int(b, offsetof(struct panfrost_afbc_##name##_info, field)),     \
+      .align_mul = 4, .range = ~0)
+
+static nir_def *
+read_afbc_header(nir_builder *b, nir_def *buf, nir_def *idx)
+{
+   nir_def *offset = nir_imul_imm(b, idx, AFBC_HEADER_BYTES_PER_TILE);
+   return nir_load_global(b, nir_iadd(b, buf, nir_u2u64(b, offset)), 16,
+                          AFBC_HEADER_BYTES_PER_TILE / 4, 32);
+}
+
+static void
+write_afbc_header(nir_builder *b, nir_def *buf, nir_def *idx, nir_def *hdr)
+{
+   nir_def *offset = nir_imul_imm(b, idx, AFBC_HEADER_BYTES_PER_TILE);
+   nir_store_global(b, nir_iadd(b, buf, nir_u2u64(b, offset)), 16, hdr, 0xF);
+}
+
+static nir_def *
+get_morton_index(nir_builder *b, nir_def *idx, nir_def *src_stride,
+                 nir_def *dst_stride)
+{
+   nir_def *x = nir_umod(b, idx, dst_stride);
+   nir_def *y = nir_udiv(b, idx, dst_stride);
+
+   nir_def *offset = nir_imul(b, nir_iand_imm(b, y, ~0x7), src_stride);
+   offset = nir_iadd(b, offset, nir_ishl_imm(b, nir_ushr_imm(b, x, 3), 6));
+
+   x = nir_iand_imm(b, x, 0x7);
+   x = nir_iand_imm(b, nir_ior(b, x, nir_ishl_imm(b, x, 2)), 0x13);
+   x = nir_iand_imm(b, nir_ior(b, x, nir_ishl_imm(b, x, 1)), 0x15);
+   y = nir_iand_imm(b, y, 0x7);
+   y = nir_iand_imm(b, nir_ior(b, y, nir_ishl_imm(b, y, 2)), 0x13);
+   y = nir_iand_imm(b, nir_ior(b, y, nir_ishl_imm(b, y, 1)), 0x15);
+   nir_def *tile_idx = nir_ior(b, x, nir_ishl_imm(b, y, 1));
+
+   return nir_iadd(b, offset, tile_idx);
+}
+
+static nir_def *
+get_superblock_size(nir_builder *b, unsigned arch, nir_def *hdr,
+                    nir_def *uncompressed_size)
+{
+   nir_def *size = nir_imm_int(b, 0);
+
+   unsigned body_base_ptr_len = 32;
+   unsigned nr_subblocks = 16;
+   unsigned sz_len = 6; /* bits */
+   nir_def *words[4];
+   nir_def *mask = nir_imm_int(b, (1 << sz_len) - 1);
+   nir_def *is_solid_color = nir_imm_bool(b, false);
+
+   for (int i = 0; i < 4; i++)
+      words[i] = nir_channel(b, hdr, i);
+
+   /* Sum up all of the subblock sizes */
+   for (int i = 0; i < nr_subblocks; i++) {
+      nir_def *subblock_size;
+      unsigned bitoffset = body_base_ptr_len + (i * sz_len);
+      unsigned start = bitoffset / 32;
+      unsigned end = (bitoffset + (sz_len - 1)) / 32;
+      unsigned offset = bitoffset % 32;
+
+      /* Handle differently if the size field is split between two words
+       * of the header */
+      if (start != end) {
+         subblock_size = nir_ior(b, nir_ushr_imm(b, words[start], offset),
+                                 nir_ishl_imm(b, words[end], 32 - offset));
+         subblock_size = nir_iand(b, subblock_size, mask);
+      } else {
+         subblock_size =
+            nir_ubitfield_extract_imm(b, words[start], offset, sz_len);
+      }
+      subblock_size = nir_bcsel(b, nir_ieq_imm(b, subblock_size, 1),
+                                uncompressed_size, subblock_size);
+      size = nir_iadd(b, size, subblock_size);
+
+      /* When the first subblock size is set to zero, the whole superblock is
+       * filled with a solid color specified in the header */
+      if (arch >= 7 && i == 0)
+         is_solid_color = nir_ieq_imm(b, size, 0);
+   }
+
+   return (arch >= 7)
+             ? nir_bcsel(b, is_solid_color, nir_imm_zero(b, 1, 32), size)
+             : size;
+}
+
+static nir_def *
+get_packed_offset(nir_builder *b, nir_def *metadata, nir_def *idx,
+                  nir_def **out_size)
+{
+   nir_def *metadata_offset =
+      nir_u2u64(b, nir_imul_imm(b, idx, sizeof(struct pan_afbc_block_info)));
+   nir_def *range_ptr = nir_iadd(b, metadata, metadata_offset);
+   nir_def *entry = nir_load_global(b, range_ptr, 4,
+                                    sizeof(struct pan_afbc_block_info) / 4, 32);
+   nir_def *offset =
+      nir_channel(b, entry, offsetof(struct pan_afbc_block_info, offset) / 4);
+
+   if (out_size)
+      *out_size =
+         nir_channel(b, entry, offsetof(struct pan_afbc_block_info, size) / 4);
+
+   return nir_u2u64(b, offset);
+}
+
+#define MAX_LINE_SIZE 16
+
+static void
+copy_superblock(nir_builder *b, nir_def *dst, nir_def *dst_idx, nir_def *hdr_sz,
+                nir_def *src, nir_def *src_idx, nir_def *metadata,
+                nir_def *meta_idx, unsigned align)
+{
+   nir_def *hdr = read_afbc_header(b, src, src_idx);
+   nir_def *src_body_base_ptr = nir_u2u64(b, nir_channel(b, hdr, 0));
+   nir_def *src_bodyptr = nir_iadd(b, src, src_body_base_ptr);
+
+   nir_def *size;
+   nir_def *dst_offset = get_packed_offset(b, metadata, meta_idx, &size);
+   nir_def *dst_body_base_ptr = nir_iadd(b, dst_offset, hdr_sz);
+   nir_def *dst_bodyptr = nir_iadd(b, dst, dst_body_base_ptr);
+
+   /* Replace the `base_body_ptr` field if not zero (solid color) */
+   nir_def *hdr2 =
+      nir_vector_insert_imm(b, hdr, nir_u2u32(b, dst_body_base_ptr), 0);
+   hdr = nir_bcsel(b, nir_ieq_imm(b, src_body_base_ptr, 0), hdr, hdr2);
+   write_afbc_header(b, dst, dst_idx, hdr);
+
+   nir_variable *offset_var =
+      nir_local_variable_create(b->impl, glsl_uint_type(), "offset");
+   nir_store_var(b, offset_var, nir_imm_int(b, 0), 1);
+   nir_loop *loop = nir_push_loop(b);
+   {
+      nir_def *offset = nir_load_var(b, offset_var);
+      nir_if *loop_check = nir_push_if(b, nir_uge(b, offset, size));
+      nir_jump(b, nir_jump_break);
+      nir_push_else(b, loop_check);
+      unsigned line_sz = align <= MAX_LINE_SIZE ? align : MAX_LINE_SIZE;
+      for (unsigned i = 0; i < align / line_sz; ++i) {
+         nir_def *src_line = nir_iadd(b, src_bodyptr, nir_u2u64(b, offset));
+         nir_def *dst_line = nir_iadd(b, dst_bodyptr, nir_u2u64(b, offset));
+         nir_store_global(
+            b, dst_line, line_sz,
+            nir_load_global(b, src_line, line_sz, line_sz / 4, 32), ~0);
+         offset = nir_iadd_imm(b, offset, line_sz);
+      }
+      nir_store_var(b, offset_var, offset, 0x1);
+      nir_pop_if(b, loop_check);
+   }
+   nir_pop_loop(b, loop);
+}
+
+#define panfrost_afbc_size_get_info_field(b, field)                            \
+   panfrost_afbc_get_info_field(size, b, field)
+
+static nir_shader *
+panfrost_afbc_create_size_shader(struct panfrost_screen *screen, unsigned bpp,
+                                 unsigned align)
+{
+   struct panfrost_device *dev = pan_device(&screen->base);
+
+   nir_builder b = nir_builder_init_simple_shader(
+      MESA_SHADER_COMPUTE, screen->vtbl.get_compiler_options(),
+      "panfrost_afbc_size(bpp=%d)", bpp);
+
+   panfrost_afbc_add_info_ubo(size, b);
+
+   nir_def *coord = nir_load_global_invocation_id(&b, 32);
+   nir_def *block_idx = nir_channel(&b, coord, 0);
+   nir_def *src = panfrost_afbc_size_get_info_field(&b, src);
+   nir_def *metadata = panfrost_afbc_size_get_info_field(&b, metadata);
+   nir_def *uncompressed_size = nir_imm_int(&b, 4 * 4 * bpp / 8); /* bytes */
+
+   nir_def *hdr = read_afbc_header(&b, src, block_idx);
+   nir_def *size = get_superblock_size(&b, dev->arch, hdr, uncompressed_size);
+   size = nir_iand(&b, nir_iadd(&b, size, nir_imm_int(&b, align - 1)),
+                   nir_inot(&b, nir_imm_int(&b, align - 1)));
+
+   nir_def *offset = nir_u2u64(
+      &b,
+      nir_iadd(&b,
+               nir_imul_imm(&b, block_idx, sizeof(struct pan_afbc_block_info)),
+               nir_imm_int(&b, offsetof(struct pan_afbc_block_info, size))));
+   nir_store_global(&b, nir_iadd(&b, metadata, offset), 4, size, 0x1);
+
+   return b.shader;
+}
+
+#define panfrost_afbc_pack_get_info_field(b, field)                            \
+   panfrost_afbc_get_info_field(pack, b, field)
+
+static nir_shader *
+panfrost_afbc_create_pack_shader(struct panfrost_screen *screen, unsigned align,
+                                 bool tiled)
+{
+   nir_builder b = nir_builder_init_simple_shader(
+      MESA_SHADER_COMPUTE, screen->vtbl.get_compiler_options(),
+      "panfrost_afbc_pack");
+
+   panfrost_afbc_add_info_ubo(pack, b);
+
+   nir_def *coord = nir_load_global_invocation_id(&b, 32);
+   nir_def *src_stride = panfrost_afbc_pack_get_info_field(&b, src_stride);
+   nir_def *dst_stride = panfrost_afbc_pack_get_info_field(&b, dst_stride);
+   nir_def *dst_idx = nir_channel(&b, coord, 0);
+   nir_def *src_idx =
+      tiled ? get_morton_index(&b, dst_idx, src_stride, dst_stride) : dst_idx;
+   nir_def *src = panfrost_afbc_pack_get_info_field(&b, src);
+   nir_def *dst = panfrost_afbc_pack_get_info_field(&b, dst);
+   nir_def *header_size =
+      nir_u2u64(&b, panfrost_afbc_pack_get_info_field(&b, header_size));
+   nir_def *metadata = panfrost_afbc_pack_get_info_field(&b, metadata);
+
+   copy_superblock(&b, dst, dst_idx, header_size, src, src_idx, metadata,
+                   src_idx, align);
+
+   return b.shader;
+}
+
+struct pan_afbc_shader_data *
+panfrost_afbc_get_shaders(struct panfrost_context *ctx,
+                          struct panfrost_resource *rsrc, unsigned align)
+{
+   struct pipe_context *pctx = &ctx->base;
+   struct panfrost_screen *screen = pan_screen(ctx->base.screen);
+   bool tiled = rsrc->image.layout.modifier & AFBC_FORMAT_MOD_TILED;
+   struct pan_afbc_shader_key key = {
+      .bpp = util_format_get_blocksizebits(rsrc->base.format),
+      .align = align,
+      .tiled = tiled,
+   };
+
+   pthread_mutex_lock(&ctx->afbc_shaders.lock);
+   struct hash_entry *he =
+      _mesa_hash_table_search(ctx->afbc_shaders.shaders, &key);
+   struct pan_afbc_shader_data *shader = he ? he->data : NULL;
+   pthread_mutex_unlock(&ctx->afbc_shaders.lock);
+
+   if (shader)
+      return shader;
+
+   shader = rzalloc(ctx->afbc_shaders.shaders, struct pan_afbc_shader_data);
+   shader->key = key;
+   _mesa_hash_table_insert(ctx->afbc_shaders.shaders, &shader->key, shader);
+
+#define COMPILE_SHADER(name, ...)                                              \
+   {                                                                           \
+      nir_shader *nir =                                                        \
+         panfrost_afbc_create_##name##_shader(screen, __VA_ARGS__);            \
+      nir->info.num_ubos = 1;                                                  \
+      struct pipe_compute_state cso = {PIPE_SHADER_IR_NIR, nir};               \
+      shader->name##_cso = pctx->create_compute_state(pctx, &cso);             \
+   }
+
+   COMPILE_SHADER(size, key.bpp, key.align);
+   COMPILE_SHADER(pack, key.align, key.tiled);
+
+#undef COMPILE_SHADER
+
+   pthread_mutex_lock(&ctx->afbc_shaders.lock);
+   _mesa_hash_table_insert(ctx->afbc_shaders.shaders, &shader->key, shader);
+   pthread_mutex_unlock(&ctx->afbc_shaders.lock);
+
+   return shader;
+}
+
+static uint32_t
+panfrost_afbc_shader_key_hash(const void *key)
+{
+   return _mesa_hash_data(key, sizeof(struct pan_afbc_shader_key));
+}
+
+static bool
+panfrost_afbc_shader_key_equal(const void *a, const void *b)
+{
+   return !memcmp(a, b, sizeof(struct pan_afbc_shader_key));
+}
+
+void
+panfrost_afbc_context_init(struct panfrost_context *ctx)
+{
+   ctx->afbc_shaders.shaders = _mesa_hash_table_create(
+      NULL, panfrost_afbc_shader_key_hash, panfrost_afbc_shader_key_equal);
+   pthread_mutex_init(&ctx->afbc_shaders.lock, NULL);
+}
+
+void
+panfrost_afbc_context_destroy(struct panfrost_context *ctx)
+{
+   _mesa_hash_table_destroy(ctx->afbc_shaders.shaders, NULL);
+   pthread_mutex_destroy(&ctx->afbc_shaders.lock);
+}
diff --git a/src/gallium/drivers/panfrost/pan_afbc_cso.h b/src/gallium/drivers/panfrost/pan_afbc_cso.h
new file mode 100644
index 00000000000..4b9f324ac2d
--- /dev/null
+++ b/src/gallium/drivers/panfrost/pan_afbc_cso.h
@@ -0,0 +1,80 @@
+/*
+ * Copyright (C) 2023 Amazon.com, Inc. or its affiliates
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef __PAN_AFBC_CSO_H__
+#define __PAN_AFBC_CSO_H__
+
+#include "util/hash_table.h"
+
+#include "panfrost/util/pan_ir.h"
+#include "pan_texture.h"
+
+struct panfrost_context;
+struct panfrost_resource;
+struct panfrost_screen;
+
+struct pan_afbc_shader_key {
+   unsigned bpp;
+   unsigned align;
+   bool tiled;
+};
+
+struct pan_afbc_shader_data {
+   struct pan_afbc_shader_key key;
+   void *size_cso;
+   void *pack_cso;
+};
+
+struct pan_afbc_shaders {
+   struct hash_table *shaders;
+   pthread_mutex_t lock;
+};
+
+struct pan_afbc_block_info {
+   uint32_t size;
+   uint32_t offset;
+};
+
+struct panfrost_afbc_size_info {
+   mali_ptr src;
+   mali_ptr metadata;
+} PACKED;
+
+struct panfrost_afbc_pack_info {
+   mali_ptr src;
+   mali_ptr dst;
+   mali_ptr metadata;
+   uint32_t header_size;
+   uint32_t src_stride;
+   uint32_t dst_stride;
+   uint32_t padding[3]; // FIXME
+} PACKED;
+
+void panfrost_afbc_context_init(struct panfrost_context *ctx);
+void panfrost_afbc_context_destroy(struct panfrost_context *ctx);
+
+struct pan_afbc_shader_data *
+panfrost_afbc_get_shaders(struct panfrost_context *ctx,
+                          struct panfrost_resource *rsrc, unsigned align);
+
+#endif
diff --git a/src/gallium/drivers/panfrost/pan_cmdstream.c b/src/gallium/drivers/panfrost/pan_cmdstream.c
index a4a61fb02a3..1eab3425055 100644
--- a/src/gallium/drivers/panfrost/pan_cmdstream.c
+++ b/src/gallium/drivers/panfrost/pan_cmdstream.c
@@ -3042,6 +3042,230 @@ panfrost_draw_vbo(struct pipe_context *pipe, const struct pipe_draw_info *info,
    }
 }
 
+/* Launch grid is the compute equivalent of draw_vbo, so in this routine, we
+ * construct the COMPUTE job and some of its payload.
+ */
+
+static void
+panfrost_launch_grid_on_batch(struct pipe_context *pipe,
+                              struct panfrost_batch *batch,
+                              const struct pipe_grid_info *info)
+{
+   struct panfrost_context *ctx = pan_context(pipe);
+
+   if (info->indirect && !PAN_GPU_INDIRECTS) {
+      struct pipe_transfer *transfer;
+      uint32_t *params =
+         pipe_buffer_map_range(pipe, info->indirect, info->indirect_offset,
+                               3 * sizeof(uint32_t), PIPE_MAP_READ, &transfer);
+
+      struct pipe_grid_info direct = *info;
+      direct.indirect = NULL;
+      direct.grid[0] = params[0];
+      direct.grid[1] = params[1];
+      direct.grid[2] = params[2];
+      pipe_buffer_unmap(pipe, transfer);
+
+      if (params[0] && params[1] && params[2])
+         panfrost_launch_grid_on_batch(pipe, batch, &direct);
+
+      return;
+   }
+
+   ctx->compute_grid = info;
+
+   struct panfrost_ptr t = pan_pool_alloc_desc(&batch->pool.base, COMPUTE_JOB);
+
+   /* Invoke according to the grid info */
+
+   unsigned num_wg[3] = {info->grid[0], info->grid[1], info->grid[2]};
+
+   if (info->indirect)
+      num_wg[0] = num_wg[1] = num_wg[2] = 1;
+
+   /* Conservatively assume workgroup size changes every launch */
+   ctx->dirty |= PAN_DIRTY_PARAMS;
+
+   panfrost_update_shader_state(batch, PIPE_SHADER_COMPUTE);
+
+#if PAN_ARCH <= 7
+   panfrost_pack_work_groups_compute(
+      pan_section_ptr(t.cpu, COMPUTE_JOB, INVOCATION), num_wg[0], num_wg[1],
+      num_wg[2], info->block[0], info->block[1], info->block[2], false,
+      info->indirect != NULL);
+
+   pan_section_pack(t.cpu, COMPUTE_JOB, PARAMETERS, cfg) {
+      cfg.job_task_split = util_logbase2_ceil(info->block[0] + 1) +
+                           util_logbase2_ceil(info->block[1] + 1) +
+                           util_logbase2_ceil(info->block[2] + 1);
+   }
+
+   pan_section_pack(t.cpu, COMPUTE_JOB, DRAW, cfg) {
+      cfg.state = batch->rsd[PIPE_SHADER_COMPUTE];
+      cfg.attributes = panfrost_emit_image_attribs(
+         batch, &cfg.attribute_buffers, PIPE_SHADER_COMPUTE);
+      cfg.thread_storage = panfrost_emit_shared_memory(batch, info);
+      cfg.uniform_buffers = batch->uniform_buffers[PIPE_SHADER_COMPUTE];
+      cfg.push_uniforms = batch->push_uniforms[PIPE_SHADER_COMPUTE];
+      cfg.textures = batch->textures[PIPE_SHADER_COMPUTE];
+      cfg.samplers = batch->samplers[PIPE_SHADER_COMPUTE];
+   }
+#else
+   struct panfrost_compiled_shader *cs = ctx->prog[PIPE_SHADER_COMPUTE];
+
+   pan_section_pack(t.cpu, COMPUTE_JOB, PAYLOAD, cfg) {
+      cfg.workgroup_size_x = info->block[0];
+      cfg.workgroup_size_y = info->block[1];
+      cfg.workgroup_size_z = info->block[2];
+
+      cfg.workgroup_count_x = num_wg[0];
+      cfg.workgroup_count_y = num_wg[1];
+      cfg.workgroup_count_z = num_wg[2];
+
+      panfrost_emit_shader(batch, &cfg.compute, PIPE_SHADER_COMPUTE,
+                           batch->rsd[PIPE_SHADER_COMPUTE],
+                           panfrost_emit_shared_memory(batch, info));
+
+      /* Workgroups may be merged if the shader does not use barriers
+       * or shared memory. This condition is checked against the
+       * static shared_size at compile-time. We need to check the
+       * variable shared size at launch_grid time, because the
+       * compiler doesn't know about that.
+       */
+      cfg.allow_merging_workgroups = cs->info.cs.allow_merging_workgroups &&
+                                     (info->variable_shared_mem == 0);
+
+      cfg.task_increment = 1;
+      cfg.task_axis = MALI_TASK_AXIS_Z;
+   }
+#endif
+
+   unsigned indirect_dep = 0;
+#if PAN_GPU_INDIRECTS
+   if (info->indirect) {
+      struct pan_indirect_dispatch_info indirect = {
+         .job = t.gpu,
+         .indirect_dim = pan_resource(info->indirect)->image.data.bo->ptr.gpu +
+                         info->indirect_offset,
+         .num_wg_sysval =
+            {
+               batch->num_wg_sysval[0],
+               batch->num_wg_sysval[1],
+               batch->num_wg_sysval[2],
+            },
+      };
+
+      indirect_dep = GENX(pan_indirect_dispatch_emit)(
+         &batch->pool.base, &batch->scoreboard, &indirect);
+   }
+#endif
+
+   panfrost_add_job(&batch->pool.base, &batch->scoreboard,
+                    MALI_JOB_TYPE_COMPUTE, true, false, indirect_dep, 0, &t,
+                    false);
+}
+
+static void
+panfrost_launch_grid(struct pipe_context *pipe,
+                     const struct pipe_grid_info *info)
+{
+   struct panfrost_context *ctx = pan_context(pipe);
+
+   /* XXX - shouldn't be necessary with working memory barriers. Affected
+    * test: KHR-GLES31.core.compute_shader.pipeline-post-xfb */
+   panfrost_flush_all_batches(ctx, "Launch grid pre-barrier");
+
+   struct panfrost_batch *batch = panfrost_get_batch_for_fbo(ctx);
+   panfrost_launch_grid_on_batch(pipe, batch, info);
+
+   panfrost_flush_all_batches(ctx, "Launch grid post-barrier");
+}
+
+#define AFBC_BLOCK_ALIGN 16
+
+static void
+panfrost_launch_afbc_shader(struct panfrost_batch *batch, void *cso,
+                            struct pipe_constant_buffer *cbuf,
+                            unsigned nr_blocks)
+{
+   struct pipe_context *pctx = &batch->ctx->base;
+   void *saved_cso = NULL;
+   struct pipe_constant_buffer saved_const = {};
+   struct pipe_grid_info grid = {
+      .block[0] = 1,
+      .block[1] = 1,
+      .block[2] = 1,
+      .grid[0] = nr_blocks,
+      .grid[1] = 1,
+      .grid[2] = 1,
+   };
+
+   struct panfrost_constant_buffer *pbuf =
+      &batch->ctx->constant_buffer[PIPE_SHADER_COMPUTE];
+   saved_cso = batch->ctx->uncompiled[PIPE_SHADER_COMPUTE];
+   util_copy_constant_buffer(&pbuf->cb[0], &saved_const, true);
+
+   pctx->bind_compute_state(pctx, cso);
+   pctx->set_constant_buffer(pctx, PIPE_SHADER_COMPUTE, 0, false, cbuf);
+
+   panfrost_launch_grid_on_batch(pctx, batch, &grid);
+
+   pctx->bind_compute_state(pctx, saved_cso);
+   pctx->set_constant_buffer(pctx, PIPE_SHADER_COMPUTE, 0, true, &saved_const);
+}
+
+#define LAUNCH_AFBC_SHADER(name, batch, rsrc, consts, nr_blocks)               \
+   struct pan_afbc_shader_data *shaders =                                      \
+      panfrost_afbc_get_shaders(batch->ctx, rsrc, AFBC_BLOCK_ALIGN);           \
+   struct pipe_constant_buffer constant_buffer = {                             \
+      .buffer_size = sizeof(consts),                                           \
+      .user_buffer = &consts};                                                 \
+   panfrost_launch_afbc_shader(batch, shaders->name##_cso, &constant_buffer,   \
+                               nr_blocks);
+
+static void
+panfrost_afbc_size(struct panfrost_batch *batch, struct panfrost_resource *src,
+                   struct panfrost_bo *metadata, unsigned offset,
+                   unsigned level)
+{
+   struct pan_image_slice_layout *slice = &src->image.layout.slices[level];
+   struct panfrost_afbc_size_info consts = {
+      .src =
+         src->image.data.bo->ptr.gpu + src->image.data.offset + slice->offset,
+      .metadata = metadata->ptr.gpu + offset,
+   };
+
+   panfrost_batch_read_rsrc(batch, src, PIPE_SHADER_COMPUTE);
+   panfrost_batch_write_bo(batch, metadata, PIPE_SHADER_COMPUTE);
+
+   LAUNCH_AFBC_SHADER(size, batch, src, consts, slice->afbc.nr_blocks);
+}
+
+static void
+panfrost_afbc_pack(struct panfrost_batch *batch, struct panfrost_resource *src,
+                   struct panfrost_bo *dst,
+                   struct pan_image_slice_layout *dst_slice,
+                   struct panfrost_bo *metadata, unsigned metadata_offset,
+                   unsigned level)
+{
+   struct pan_image_slice_layout *src_slice = &src->image.layout.slices[level];
+   struct panfrost_afbc_pack_info consts = {
+      .src = src->image.data.bo->ptr.gpu + src->image.data.offset +
+             src_slice->offset,
+      .dst = dst->ptr.gpu + dst_slice->offset,
+      .metadata = metadata->ptr.gpu + metadata_offset,
+      .header_size = dst_slice->afbc.header_size,
+      .src_stride = src_slice->afbc.stride,
+      .dst_stride = dst_slice->afbc.stride,
+   };
+
+   panfrost_batch_write_rsrc(batch, src, PIPE_SHADER_COMPUTE);
+   panfrost_batch_write_bo(batch, dst, PIPE_SHADER_COMPUTE);
+   panfrost_batch_add_bo(batch, metadata, PIPE_SHADER_COMPUTE);
+
+   LAUNCH_AFBC_SHADER(pack, batch, src, consts, dst_slice->afbc.nr_blocks);
+}
+
 static void *
 panfrost_create_rasterizer_state(struct pipe_context *pctx,
                                  const struct pipe_rasterizer_state *cso)
@@ -3668,6 +3892,8 @@ GENX(panfrost_cmdstream_screen_init)(struct panfrost_screen *screen)
    screen->vtbl.get_blend_shader = GENX(pan_blend_get_shader_locked);
    screen->vtbl.get_compiler_options = GENX(pan_shader_get_compiler_options);
    screen->vtbl.compile_shader = GENX(pan_shader_compile);
+   screen->vtbl.afbc_size = panfrost_afbc_size;
+   screen->vtbl.afbc_pack = panfrost_afbc_pack;
 
    GENX(pan_blitter_init)
    (dev, &screen->blitter.bin_pool.base, &screen->blitter.desc_pool.base);
diff --git a/src/gallium/drivers/panfrost/pan_resource.c b/src/gallium/drivers/panfrost/pan_resource.c
index 9bb4b63116e..db6673b750e 100644
--- a/src/gallium/drivers/panfrost/pan_resource.c
+++ b/src/gallium/drivers/panfrost/pan_resource.c
@@ -1541,13 +1541,13 @@ panfrost_pack_afbc(struct panfrost_context *ctx,
 
    unsigned new_size = ALIGN_POT(total_size, 4096); // FIXME
    unsigned old_size = prsrc->image.data.bo->size;
-   unsigned ratio = 100 * new_size / old_size;
 
-   if (ratio > screen->max_afbc_packing_ratio)
+   if (new_size == old_size)
       return;
 
    if (dev->debug & PAN_DBG_PERF) {
-      printf("%i%%: %i KB -> %i KB\n", ratio, old_size / 1024, new_size / 1024);
+      printf("%i%%: %i KB -> %i KB\n", 100 * new_size / old_size,
+             old_size / 1024, new_size / 1024);
    }
 
    struct panfrost_bo *dst =
diff --git a/src/gallium/drivers/panfrost/pan_screen.h b/src/gallium/drivers/panfrost/pan_screen.h
index 26841f0123a..d95a9a3d968 100644
--- a/src/gallium/drivers/panfrost/pan_screen.h
+++ b/src/gallium/drivers/panfrost/pan_screen.h
@@ -90,6 +90,19 @@ struct panfrost_vtable {
    void (*compile_shader)(nir_shader *s, struct panfrost_compile_inputs *inputs,
                           struct util_dynarray *binary,
                           struct pan_shader_info *info);
+
+   /* Run a compute shader to get the compressed size of each superblock */
+   void (*afbc_size)(struct panfrost_batch *batch,
+                     struct panfrost_resource *src,
+                     struct panfrost_bo *metadata, unsigned offset,
+                     unsigned level);
+
+   /* Run a compute shader to compact a sparse layout afbc resource */
+   void (*afbc_pack)(struct panfrost_batch *batch,
+                     struct panfrost_resource *src, struct panfrost_bo *dst,
+                     struct pan_image_slice_layout *slice,
+                     struct panfrost_bo *metadata, unsigned metadata_offset,
+                     unsigned level);
 };
 
 struct panfrost_screen {
