diff -up mesa-23.2.0-rc3/include/drm-uapi/pancsf_drm.h.8~ mesa-23.2.0-rc3/include/drm-uapi/pancsf_drm.h
--- mesa-23.2.0-rc3/include/drm-uapi/pancsf_drm.h.8~	2023-09-08 01:00:47.830430390 +0200
+++ mesa-23.2.0-rc3/include/drm-uapi/pancsf_drm.h	2023-09-08 01:00:47.830430390 +0200
@@ -0,0 +1,458 @@
+/* SPDX-License-Identifier: MIT */
+/* Copyright (C) 2023 Collabora ltd. */
+#ifndef _PANCSF_DRM_H_
+#define _PANCSF_DRM_H_
+
+#include "drm.h"
+
+#if defined(__cplusplus)
+extern "C" {
+#endif
+
+/*
+ * Userspace driver controls GPU cache flushling through CS instructions, but
+ * the flush reduction mechanism requires a flush_id. This flush_id could be
+ * queried with an ioctl, but Arm provides a well-isolated register page
+ * containing only this read-only register, so let's expose this page through
+ * a static mmap offset and allow direct mapping of this MMIO region so we
+ * can avoid the user <-> kernel round-trip.
+ */
+#define DRM_PANCSF_USER_MMIO_OFFSET		(0xffffull << 48)
+#define DRM_PANCSF_USER_FLUSH_ID_MMIO_OFFSET	(DRM_PANCSF_USER_MMIO_OFFSET | 0)
+
+/* Place new ioctls at the end, don't re-oder. */
+enum drm_pancsf_ioctl_id {
+	DRM_PANCSF_DEV_QUERY = 0,
+	DRM_PANCSF_VM_CREATE,
+	DRM_PANCSF_VM_DESTROY,
+	DRM_PANCSF_BO_CREATE,
+	DRM_PANCSF_BO_MMAP_OFFSET,
+	DRM_PANCSF_VM_MAP,
+	DRM_PANCSF_VM_UNMAP,
+	DRM_PANCSF_GROUP_CREATE,
+	DRM_PANCSF_GROUP_DESTROY,
+	DRM_PANCSF_GROUP_GET_STATE,
+	DRM_PANCSF_TILER_HEAP_CREATE,
+	DRM_PANCSF_TILER_HEAP_DESTROY,
+	DRM_PANCSF_GROUP_SUBMIT,
+};
+
+#define DRM_IOCTL_PANCSF_DEV_QUERY		DRM_IOWR(DRM_COMMAND_BASE + DRM_PANCSF_DEV_QUERY, struct drm_pancsf_dev_query)
+#define DRM_IOCTL_PANCSF_VM_CREATE		DRM_IOWR(DRM_COMMAND_BASE + DRM_PANCSF_VM_CREATE, struct drm_pancsf_vm_create)
+#define DRM_IOCTL_PANCSF_VM_DESTROY		DRM_IOWR(DRM_COMMAND_BASE + DRM_PANCSF_VM_DESTROY, struct drm_pancsf_vm_destroy)
+#define DRM_IOCTL_PANCSF_BO_CREATE		DRM_IOWR(DRM_COMMAND_BASE + DRM_PANCSF_BO_CREATE, struct drm_pancsf_bo_create)
+#define DRM_IOCTL_PANCSF_BO_MMAP_OFFSET		DRM_IOWR(DRM_COMMAND_BASE + DRM_PANCSF_BO_MMAP_OFFSET, struct drm_pancsf_bo_mmap_offset)
+#define DRM_IOCTL_PANCSF_VM_MAP			DRM_IOWR(DRM_COMMAND_BASE + DRM_PANCSF_VM_MAP, struct drm_pancsf_vm_map)
+#define DRM_IOCTL_PANCSF_VM_UNMAP		DRM_IOWR(DRM_COMMAND_BASE + DRM_PANCSF_VM_UNMAP, struct drm_pancsf_vm_unmap)
+#define DRM_IOCTL_PANCSF_GROUP_CREATE		DRM_IOWR(DRM_COMMAND_BASE + DRM_PANCSF_GROUP_CREATE, struct drm_pancsf_group_create)
+#define DRM_IOCTL_PANCSF_GROUP_DESTROY		DRM_IOWR(DRM_COMMAND_BASE + DRM_PANCSF_GROUP_DESTROY, struct drm_pancsf_group_destroy)
+#define DRM_IOCTL_PANCSF_GROUP_GET_STATE	DRM_IOWR(DRM_COMMAND_BASE + DRM_PANCSF_GROUP_GET_STATE, struct drm_pancsf_group_get_state)
+#define DRM_IOCTL_PANCSF_TILER_HEAP_CREATE	DRM_IOWR(DRM_COMMAND_BASE + DRM_PANCSF_TILER_HEAP_CREATE, struct drm_pancsf_tiler_heap_create)
+#define DRM_IOCTL_PANCSF_TILER_HEAP_DESTROY	DRM_IOWR(DRM_COMMAND_BASE + DRM_PANCSF_TILER_HEAP_DESTROY, struct drm_pancsf_tiler_heap_destroy)
+#define DRM_IOCTL_PANCSF_GROUP_SUBMIT		DRM_IOWR(DRM_COMMAND_BASE + DRM_PANCSF_GROUP_SUBMIT, struct drm_pancsf_group_submit)
+
+/* Place new types at the end, don't re-oder. */
+enum drm_pancsf_dev_query_type {
+	DRM_PANCSF_DEV_QUERY_GPU_INFO = 0,
+	DRM_PANCSF_DEV_QUERY_CSIF_INFO,
+};
+
+struct drm_pancsf_gpu_info {
+#define DRM_PANCSF_ARCH_MAJOR(x)		((x) >> 28)
+#define DRM_PANCSF_ARCH_MINOR(x)		(((x) >> 24) & 0xf)
+#define DRM_PANCSF_ARCH_REV(x)			(((x) >> 20) & 0xf)
+#define DRM_PANCSF_PRODUCT_MAJOR(x)		(((x) >> 16) & 0xf)
+#define DRM_PANCSF_VERSION_MAJOR(x)		(((x) >> 12) & 0xf)
+#define DRM_PANCSF_VERSION_MINOR(x)		(((x) >> 4) & 0xff)
+#define DRM_PANCSF_VERSION_STATUS(x)		((x) & 0xf)
+	__u32 gpu_id;
+	__u32 gpu_rev;
+#define DRM_PANCSF_CSHW_MAJOR(x)		(((x) >> 26) & 0x3f)
+#define DRM_PANCSF_CSHW_MINOR(x)		(((x) >> 20) & 0x3f)
+#define DRM_PANCSF_CSHW_REV(x)			(((x) >> 16) & 0xf)
+#define DRM_PANCSF_MCU_MAJOR(x)			(((x) >> 10) & 0x3f)
+#define DRM_PANCSF_MCU_MINOR(x)			(((x) >> 4) & 0x3f)
+#define DRM_PANCSF_MCU_REV(x)			((x) & 0xf)
+	__u32 csf_id;
+	__u32 l2_features;
+	__u32 tiler_features;
+	__u32 mem_features;
+	__u32 mmu_features;
+	__u32 thread_features;
+	__u32 max_threads;
+	__u32 thread_max_workgroup_size;
+	__u32 thread_max_barrier_size;
+	__u32 coherency_features;
+	__u32 texture_features[4];
+	__u32 as_present;
+	__u32 core_group_count;
+	__u32 pad;
+	__u64 shader_present;
+	__u64 l2_present;
+	__u64 tiler_present;
+};
+
+struct drm_pancsf_csif_info {
+	__u32 csg_slot_count;
+	__u32 cs_slot_count;
+	__u32 cs_reg_count;
+	__u32 scoreboard_slot_count;
+	__u32 unpreserved_cs_reg_count;
+	__u32 pad;
+};
+
+struct drm_pancsf_dev_query {
+	/** @type: the query type (see enum drm_pancsf_dev_query_type). */
+	__u32 type;
+
+	/**
+	 * @size: size of the type being queried.
+	 *
+	 * If pointer is NULL, size is updated by the driver to provide the
+	 * output structure size. If pointer is not NULL, the the driver will
+	 * only copy min(size, actual_structure_size) bytes to the pointer,
+	 * and update the size accordingly. This allows us to extend query
+	 * types without breaking userspace.
+	 */
+	__u32 size;
+
+	/**
+	 * @pointer: user pointer to a query type struct.
+	 *
+	 * Pointer can be NULL, in which case, nothing is copied, but the
+	 * actual structure size is returned. If not NULL, it must point to
+	 * a location that's large enough to hold size bytes.
+	 */
+	__u64 pointer;
+};
+
+struct drm_pancsf_vm_create {
+	/** @flags: VM flags, MBZ. */
+	__u32 flags;
+
+	/** @id: Returned VM ID */
+	__u32 id;
+};
+
+struct drm_pancsf_vm_destroy {
+	/** @id: ID of the VM to destroy */
+	__u32 id;
+
+	/** @pad: MBZ. */
+	__u32 pad;
+};
+
+struct drm_pancsf_bo_create {
+	/**
+	 * @size: Requested size for the object
+	 *
+	 * The (page-aligned) allocated size for the object will be returned.
+	 */
+	__u64 size;
+
+	/**
+	 * @flags: Flags, currently unused, MBZ.
+	 */
+	__u32 flags;
+
+	/**
+	 * @vm_id: Attached VM, if any
+	 *
+	 * If a VM is specified, this BO must:
+	 *
+	 *  1. Only ever be bound to that VM.
+	 *
+	 *  2. Cannot be exported as a PRIME fd.
+	 */
+	__u32 vm_id;
+
+	/**
+	 * @handle: Returned handle for the object.
+	 *
+	 * Object handles are nonzero.
+	 */
+	__u32 handle;
+
+	/* @pad: MBZ. */
+	__u32 pad;
+};
+
+struct drm_pancsf_bo_mmap_offset {
+	/** @handle: Handle for the object being mapped. */
+	__u32 handle;
+
+	/** @pad: MBZ. */
+	__u32 pad;
+
+	/** @offset: The fake offset to use for subsequent mmap call */
+	__u64 offset;
+};
+
+#define PANCSF_VMA_MAP_READONLY		0x1
+#define PANCSF_VMA_MAP_NOEXEC		0x2
+#define PANCSF_VMA_MAP_UNCACHED		0x4
+#define PANCSF_VMA_MAP_FRAG_SHADER	0x8
+#define PANCSF_VMA_MAP_ON_FAULT		0x10
+#define PANCSF_VMA_MAP_AUTO_VA		0x20
+
+struct drm_pancsf_vm_map {
+	/** @vm_id: VM to map BO range to */
+	__u32 vm_id;
+
+	/** @flags: Combination of PANCSF_VMA_MAP_ flags */
+	__u32 flags;
+
+	/** @pad: MBZ. */
+	__u32 pad;
+
+	/** @bo_handle: Buffer object to map. */
+	__u32 bo_handle;
+
+	/** @bo_offset: Buffer object offset. */
+	__u64 bo_offset;
+
+	/**
+	 * @va: Virtual address to map the BO to. Mapping address returned here if
+	 *	PANCSF_VMA_MAP_ON_FAULT is set.
+	 */
+	__u64 va;
+
+	/** @size: Size to map. */
+	__u64 size;
+};
+
+struct drm_pancsf_vm_unmap {
+	/** @vm_id: VM to map BO range to */
+	__u32 vm_id;
+
+	/** @flags: MBZ. */
+	__u32 flags;
+
+	/** @va: Virtual address to unmap. */
+	__u64 va;
+
+	/** @size: Size to unmap. */
+	__u64 size;
+};
+
+enum drm_pancsf_sync_op_type {
+	DRM_PANCSF_SYNC_OP_WAIT = 0,
+	DRM_PANCSF_SYNC_OP_SIGNAL,
+};
+
+enum drm_pancsf_sync_handle_type {
+	DRM_PANCSF_SYNC_HANDLE_TYPE_SYNCOBJ = 0,
+	DRM_PANCSF_SYNC_HANDLE_TYPE_TIMELINE_SYNCOBJ,
+};
+
+struct drm_pancsf_sync_op {
+	/** @op_type: Sync operation type. */
+	__u32 op_type;
+
+	/** @handle_type: Sync handle type. */
+	__u32 handle_type;
+
+	/** @handle: Sync handle. */
+	__u32 handle;
+
+	/** @flags: MBZ. */
+	__u32 flags;
+
+	/** @timeline_value: MBZ if handle_type != DRM_PANCSF_SYNC_HANDLE_TYPE_TIMELINE_SYNCOBJ. */
+	__u64 timeline_value;
+};
+
+struct drm_pancsf_obj_array {
+	/** @stride: Stride of object struct. Used for versioning. */
+	__u32 stride;
+
+	/** @count: Number of objects in the array. */
+	__u32 count;
+
+	/** @array: User pointer to an array of objects. */
+	__u64 array;
+};
+
+#define DRM_PANCSF_OBJ_ARRAY(cnt, ptr) \
+	{ .stride = sizeof(ptr[0]), .count = cnt, .array = (__u64)(uintptr_t)ptr }
+
+struct drm_pancsf_queue_submit {
+	/** @queue_index: Index of the queue inside a group. */
+	__u32 queue_index;
+
+	/** @stream_size: Size of the command stream to execute. */
+	__u32 stream_size;
+
+	/** @stream_addr: GPU address of the command stream to execute. */
+	__u64 stream_addr;
+
+	/**
+	 * @lastest_flush: FLUSH_ID read at the time the stream was built.
+	 *
+	 * This allows cache flush elimination for the automatic
+	 * flush+invalidate(all) done at submission time, which is needed to
+	 * ensure the GPU doesn't get garbage when reading the linear CS
+	 * buffers. If you want the cache flush to happen unconditionally,
+	 * pass a zero here.
+	 */
+	__u32 latest_flush;
+
+	/** @pad: MBZ. */
+	__u32 pad;
+
+	/** @syncs: Array of sync operations. */
+	struct drm_pancsf_obj_array syncs;
+};
+
+struct drm_pancsf_group_submit {
+	/** @group_handle: Handle of the group to queue jobs to. */
+	__u32 group_handle;
+
+	/** @pad: MBZ. */
+	__u32 pad;
+
+	/** @syncs: Array of queue submit operations. */
+	struct drm_pancsf_obj_array queue_submits;
+};
+
+struct drm_pancsf_queue_create {
+	/**
+	 * @priority: Defines the priority of queues inside a group. Goes from 0 to 15,
+	 *	      15 being the highest priority.
+	 */
+	__u8 priority;
+
+	/** @pad: Padding fields, MBZ. */
+	__u8 pad[3];
+
+	/** @ringbuf_size: Size of the ring buffer to allocate to this queue. */
+	__u32 ringbuf_size;
+};
+
+enum drm_pancsf_group_priority {
+	PANCSF_GROUP_PRIORITY_LOW = 0,
+	PANCSF_GROUP_PRIORITY_MEDIUM,
+	PANCSF_GROUP_PRIORITY_HIGH,
+};
+
+struct drm_pancsf_group_create {
+	/** @queues: Array of drm_pancsf_create_cs_queue elements. */
+	struct drm_pancsf_obj_array queues;
+
+	/**
+	 * @max_compute_cores: Maximum number of cores that can be
+	 *		       used by compute jobs across CS queues
+	 *		       bound to this group.
+	 */
+	__u8 max_compute_cores;
+
+	/**
+	 * @max_fragment_cores: Maximum number of cores that can be
+	 *			used by fragment jobs across CS queues
+	 *			bound to this group.
+	 */
+	__u8 max_fragment_cores;
+
+	/**
+	 * @max_tiler_cores: Maximum number of tilers that can be
+	 *		     used by tiler jobs across CS queues
+	 *		     bound to this group.
+	 */
+	__u8 max_tiler_cores;
+
+	/** @priority: Group priority (see drm_drm_pancsf_cs_group_priority). */
+	__u8 priority;
+
+	/** @pad: Padding field, MBZ. */
+	__u32 pad;
+
+	/** @compute_core_mask: Mask encoding cores that can be used for compute jobs. */
+	__u64 compute_core_mask;
+
+	/** @fragment_core_mask: Mask encoding cores that can be used for fragment jobs. */
+	__u64 fragment_core_mask;
+
+	/** @tiler_core_mask: Mask encoding cores that can be used for tiler jobs. */
+	__u64 tiler_core_mask;
+
+	/**
+	 * @vm_id: VM ID to bind this group to. All submission to queues bound to this
+	 *	   group will use this VM.
+	 */
+	__u32 vm_id;
+
+	/*
+	 * @group_handle: Returned group handle. Passed back when submitting jobs or
+	 *		  destroying a group.
+	 */
+	__u32 group_handle;
+};
+
+struct drm_pancsf_group_destroy {
+	/** @group_handle: Group to destroy */
+	__u32 group_handle;
+
+	/** @pad: Padding field, MBZ. */
+	__u32 pad;
+};
+
+struct drm_pancsf_group_get_state {
+	/** @group_handle: Handle of the group to query state on */
+	__u32 group_handle;
+
+#define DRM_PANCSF_GROUP_STATE_DESTROYED		0x1
+#define DRM_PANCSF_GROUP_STATE_TIMEDOUT			0x2
+#define DRM_PANCSF_GROUP_STATE_FATAL_FAULT		0x4
+	/** @state: Combination of DRM_PANCSF_GROUP_STATE_* flags encoding the
+	 *	    group state.
+	 */
+	__u32 state;
+
+	/** @fatal_queues: Bitmask of queues that faced fatal faults. */
+	__u32 fatal_queues;
+
+	/** @pad: MBZ */
+	__u32 pad;
+};
+
+
+struct drm_pancsf_tiler_heap_create {
+	/** @vm_id: VM ID the tiler heap should be mapped to */
+	__u32 vm_id;
+
+	/** @initial_chunk_count: Initial number of chunks to allocate. */
+	__u32 initial_chunk_count;
+
+	/** @chunk_size: Chunk size. Must be a power of two at least 256KB large. */
+	__u32 chunk_size;
+
+	/* @max_chunks: Maximum number of chunks that can be allocated. */
+	__u32 max_chunks;
+
+	/** @target_in_flight: Maximum number of in-flight render passes.
+	 * If exceeded the FW will wait for render passes to finish before
+	 * queuing new tiler jobs.
+	 */
+	__u32 target_in_flight;
+
+	/** @handle: Returned heap handle. Passed back to DESTROY_TILER_HEAP. */
+	__u32 handle;
+
+	/** @tiler_heap_ctx_gpu_va: Returned heap GPU virtual address returned */
+	__u64 tiler_heap_ctx_gpu_va;
+	__u64 first_heap_chunk_gpu_va;
+};
+
+struct drm_pancsf_tiler_heap_destroy {
+	/** @handle: Handle of the tiler heap to destroy */
+	__u32 handle;
+
+	/** @pad: Padding field, MBZ. */
+	__u32 pad;
+};
+
+#if defined(__cplusplus)
+}
+#endif
+
+#endif /* _PANCSF_DRM_H_ */
diff -up mesa-23.2.0-rc3/src/gallium/drivers/panfrost/meson.build.8~ mesa-23.2.0-rc3/src/gallium/drivers/panfrost/meson.build
--- mesa-23.2.0-rc3/src/gallium/drivers/panfrost/meson.build.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/gallium/drivers/panfrost/meson.build	2023-09-08 01:00:47.830430390 +0200
@@ -53,7 +53,7 @@ compile_args_panfrost = [
   '-Wno-pointer-arith'
 ]
 
-panfrost_versions = ['4', '5', '6', '7', '9']
+panfrost_versions = ['4', '5', '6', '7', '9', '10']
 libpanfrost_versions = []
 
 foreach ver : panfrost_versions
diff -up mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_cmdstream.c.8~ mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_cmdstream.c
--- mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_cmdstream.c.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_cmdstream.c	2023-09-08 01:01:21.374782116 +0200
@@ -35,8 +35,11 @@
 #include "util/u_vbuf.h"
 #include "util/u_viewport.h"
 
+#include "genxml/ceu_builder.h"
 #include "genxml/gen_macros.h"
 
+#include "drm-uapi/panfrost_drm.h"
+
 #include "pan_blend.h"
 #include "pan_blitter.h"
 #include "pan_bo.h"
@@ -2650,12 +2653,70 @@ panfrost_initialize_surface(struct panfr
    }
 }
 
+static void
+panfrost_emit_heap_set(struct panfrost_batch *batch, bool vt)
+{
+#if PAN_ARCH >= 10
+   struct panfrost_device *dev = pan_device(batch->ctx->base.screen);
+   ceu_builder *b = batch->ceu_builder;
+
+   /* Setup the tiler heap */
+   ceu_index heap = ceu_reg64(b, 72);
+   ceu_move64_to(b, heap, batch->ctx->heap.tiler_heap_ctx_gpu_va);
+   ceu_heap_set(b, heap);
+
+   dev->tiler_heap->ptr.cpu = NULL;
+   dev->tiler_heap->ptr.gpu = batch->ctx->heap.first_heap_chunk_gpu_va;
+   dev->tiler_heap->size = 2097152;
+
+   if (vt) {
+      /* Set up the statistics */
+      ceu_vt_start(b);
+   }
+#endif
+}
+
+static void
+panfrost_emit_batch_end(struct panfrost_batch *batch)
+{
+#if PAN_ARCH >= 10
+   ceu_builder *b = batch->ceu_builder;
+
+   /* Barrier to let everything finish */
+   ceu_wait_slots(b, BITFIELD_MASK(8));
+
+   /* Get the CS state */
+   batch->cs_state = pan_pool_alloc_aligned(&batch->pool.base, 8, 8);
+   memset(batch->cs_state.cpu, ~0, 8);
+   ceu_move64_to(b, ceu_reg64(b, 90), batch->cs_state.gpu);
+   ceu_store_state(b, 0, ceu_reg64(b, 90), MALI_CEU_STATE_ERROR_STATUS, 0, 0);
+
+   /* Flush caches now that we're done (synchronous) */
+   ceu_index flush_id = ceu_reg32(b, 74);
+   ceu_move32_to(b, flush_id, 0);
+   ceu_flush_caches(b, MALI_CEU_FLUSH_MODE_CLEAN_AND_INVALIDATE,
+                    MALI_CEU_FLUSH_MODE_CLEAN_AND_INVALIDATE, true, flush_id, 0,
+                    0);
+#endif
+}
+
 /* Generate a fragment job. This should be called once per frame. (Usually,
  * this corresponds to eglSwapBuffers or one of glFlush, glFinish)
  */
 static mali_ptr
 emit_fragment_job(struct panfrost_batch *batch, const struct pan_fb_info *pfb)
 {
+   if (PAN_ARCH >= 10 && !batch->clear && !batch->draws) {
+      /* Compute only batch */
+      panfrost_emit_batch_end(batch);
+      return 0;
+   }
+
+   if (PAN_ARCH >= 10 && !batch->draws) {
+      /* Clear only batch */
+      panfrost_emit_heap_set(batch, false);
+   }
+
    /* Mark the affected buffers as initialized, since we're writing to it.
     * Also, add the surfaces we're writing to to the batch */
 
@@ -2686,12 +2747,39 @@ emit_fragment_job(struct panfrost_batch
    assert(batch->maxx > batch->minx);
    assert(batch->maxy > batch->miny);
 
+#if PAN_ARCH >= 10
+   ceu_builder *b = batch->ceu_builder;
+
+   if (batch->draws) {
+      /* Finish tiling and wait for IDVS and tiling */
+      ceu_finish_tiling(b);
+      ceu_wait_slots(b, 0xFF);
+      ceu_vt_end(b);
+   }
+
+   /* Set up the fragment job */
+   ceu_move64_to(b, ceu_reg64(b, 40), batch->framebuffer.gpu);
+   ceu_move32_to(b, ceu_reg32(b, 42), (batch->miny << 16) | batch->minx);
+   ceu_move32_to(b, ceu_reg32(b, 43),
+                 ((batch->maxy - 1) << 16) | (batch->maxx - 1));
+
+   /* Run the fragment job and wait */
+   ceu_run_fragment(b, false);
+   ceu_wait_slots(b, 0xff);
+   /* TODO: finish_fragment */
+
+   //        ceu_frag_end(b);
+
+   panfrost_emit_batch_end(batch);
+   return 0;
+#else
    struct panfrost_ptr transfer =
       pan_pool_alloc_desc(&batch->pool.base, FRAGMENT_JOB);
 
    GENX(pan_emit_fragment_job)(pfb, batch->framebuffer.gpu, transfer.cpu);
 
    return transfer.gpu;
+#endif
 }
 
 #define DEFINE_CASE(c)                                                         \
@@ -2950,6 +3038,9 @@ panfrost_update_state_3d(struct panfrost
 }
 
 #if PAN_ARCH >= 6
+
+#define POSITION_FIFO_SIZE (64 * 1024)
+
 static mali_ptr
 panfrost_batch_get_bifrost_tiler(struct panfrost_batch *batch,
                                  unsigned vertex_count)
@@ -2959,14 +3050,23 @@ panfrost_batch_get_bifrost_tiler(struct
    if (!vertex_count)
       return 0;
 
-   if (batch->tiler_ctx.bifrost)
-      return batch->tiler_ctx.bifrost;
+   if (batch->tiler_ctx.bifrost.ctx)
+      return batch->tiler_ctx.bifrost.ctx;
+
+   /* Allocate the position FIFO and the tiler heap descriptor.
+    * Don't know what the extra 4k are for. */
+   size_t size =
+      POSITION_FIFO_SIZE + ALIGN(pan_size(TILER_CONTEXT), 4096) + 4096;
+   struct panfrost_ptr t =
+      pan_pool_alloc_aligned(&batch->pool.base, size, POSITION_FIFO_SIZE);
 
-   struct panfrost_ptr t = pan_pool_alloc_desc(&batch->pool.base, TILER_HEAP);
+   /* XXX: unnecessary */
+   memset(t.cpu, 0, size);
 
-   GENX(pan_emit_tiler_heap)(dev, t.cpu);
+   GENX(pan_emit_tiler_heap)(dev, (uint8_t *)t.cpu + POSITION_FIFO_SIZE);
 
-   mali_ptr heap = t.gpu;
+   mali_ptr heap = t.gpu + POSITION_FIFO_SIZE;
+   batch->tiler_ctx.bifrost.heap = heap;
 
    t = pan_pool_alloc_desc(&batch->pool.base, TILER_CONTEXT);
    GENX(pan_emit_tiler_ctx)
@@ -2974,11 +3074,22 @@ panfrost_batch_get_bifrost_tiler(struct
     util_framebuffer_get_num_samples(&batch->key),
     pan_tristate_get(batch->first_provoking_vertex), heap, t.cpu);
 
-   batch->tiler_ctx.bifrost = t.gpu;
-   return batch->tiler_ctx.bifrost;
+   batch->tiler_ctx.bifrost.ctx = t.gpu;
+   return batch->tiler_ctx.bifrost.ctx;
 }
 #endif
 
+static inline bool
+pan_allow_rotating_primitives(const struct panfrost_compiled_shader *fs,
+                              const struct pipe_draw_info *info)
+{
+   bool lines =
+      (info->mode == PIPE_PRIM_LINES || info->mode == PIPE_PRIM_LINE_LOOP ||
+       info->mode == PIPE_PRIM_LINE_STRIP);
+
+   return !lines && !fs->info.bifrost.uses_flat_shading;
+}
+
 /* Packs a primitive descriptor, mostly common between Midgard/Bifrost tiler
  * jobs and Valhall IDVS jobs
  */
@@ -3020,8 +3131,7 @@ panfrost_emit_primitive(struct panfrost_
 #else
       struct panfrost_compiled_shader *fs = ctx->prog[PIPE_SHADER_FRAGMENT];
 
-      cfg.allow_rotating_primitives =
-         !(lines || fs->info.bifrost.uses_flat_shading);
+      cfg.allow_rotating_primitives = pan_allow_rotating_primitives(fs, info);
       cfg.primitive_restart = info->primitive_restart;
 
       /* Non-fixed restart indices should have been lowered */
@@ -3116,6 +3226,27 @@ panfrost_emit_shader(struct panfrost_bat
 }
 #endif
 
+#if PAN_ARCH >= 10
+static void
+panfrost_emit_shader_regs(struct panfrost_batch *batch,
+                          enum pipe_shader_type stage, mali_ptr shader)
+{
+   mali_ptr resources = panfrost_emit_resources(batch, stage);
+
+   assert(stage == PIPE_SHADER_VERTEX || stage == PIPE_SHADER_FRAGMENT ||
+          stage == PIPE_SHADER_COMPUTE);
+
+   unsigned offset = (stage == PIPE_SHADER_FRAGMENT) ? 4 : 0;
+   unsigned fau_count = DIV_ROUND_UP(batch->nr_push_uniforms[stage], 2);
+
+   ceu_builder *b = batch->ceu_builder;
+   ceu_move64_to(b, ceu_reg64(b, 0 + offset), resources);
+   ceu_move64_to(b, ceu_reg64(b, 8 + offset),
+                 batch->push_uniforms[stage] | ((uint64_t)fau_count << 56));
+   ceu_move64_to(b, ceu_reg64(b, 16 + offset), shader);
+}
+#endif
+
 static void
 panfrost_emit_draw(void *out, struct panfrost_batch *batch, bool fs_required,
                    enum mesa_prim prim, mali_ptr pos, mali_ptr fs_vary,
@@ -3272,6 +3403,42 @@ panfrost_emit_draw(void *out, struct pan
 }
 
 #if PAN_ARCH >= 9
+static mali_ptr
+panfrost_get_position_shader(struct panfrost_batch *batch,
+                             const struct pipe_draw_info *info)
+{
+   /* IDVS/points vertex shader */
+   mali_ptr vs_ptr = batch->rsd[PIPE_SHADER_VERTEX];
+
+   /* IDVS/triangle vertex shader */
+   if (vs_ptr && info->mode != PIPE_PRIM_POINTS)
+      vs_ptr += pan_size(SHADER_PROGRAM);
+
+   return vs_ptr;
+}
+
+static mali_ptr
+panfrost_get_varying_shader(struct panfrost_batch *batch)
+{
+   return batch->rsd[PIPE_SHADER_VERTEX] + (2 * pan_size(SHADER_PROGRAM));
+}
+
+static unsigned
+panfrost_vertex_attribute_stride(struct panfrost_compiled_shader *vs,
+                                 struct panfrost_compiled_shader *fs)
+{
+   unsigned v = vs->info.varyings.output_count;
+   unsigned f = fs->info.varyings.input_count;
+   unsigned slots = MAX2(v, f);
+   slots += util_bitcount(fs->key.fs.fixed_varying_mask);
+
+   /* Assumes 16 byte slots. We could do better. */
+   return slots * 16;
+}
+
+#endif
+
+#if PAN_ARCH == 9
 static void
 panfrost_emit_malloc_vertex(struct panfrost_batch *batch,
                             const struct pipe_draw_info *info,
@@ -3299,15 +3466,9 @@ panfrost_emit_malloc_vertex(struct panfr
 
    pan_section_pack(job, MALLOC_VERTEX_JOB, ALLOCATION, cfg) {
       if (secondary_shader) {
-         unsigned v = vs->info.varyings.output_count;
-         unsigned f = fs->info.varyings.input_count;
-         unsigned slots = MAX2(v, f);
-         slots += util_bitcount(fs->key.fs.fixed_varying_mask);
-         unsigned size = slots * 16;
-
-         /* Assumes 16 byte slots. We could do better. */
-         cfg.vertex_packet_stride = size + 16;
-         cfg.vertex_attribute_stride = size;
+         unsigned sz = panfrost_vertex_attribute_stride(vs, fs);
+         cfg.vertex_packet_stride = sz + 16;
+         cfg.vertex_attribute_stride = sz;
       } else {
          /* Hardware requirement for "no varyings" */
          cfg.vertex_packet_stride = 16;
@@ -3335,14 +3496,8 @@ panfrost_emit_malloc_vertex(struct panfr
                       fs_required, u_reduced_prim(info->mode), 0, 0, 0);
 
    pan_section_pack(job, MALLOC_VERTEX_JOB, POSITION, cfg) {
-      /* IDVS/points vertex shader */
-      mali_ptr vs_ptr = batch->rsd[PIPE_SHADER_VERTEX];
-
-      /* IDVS/triangle vertex shader */
-      if (vs_ptr && info->mode != MESA_PRIM_POINTS)
-         vs_ptr += pan_size(SHADER_PROGRAM);
-
-      panfrost_emit_shader(batch, &cfg, PIPE_SHADER_VERTEX, vs_ptr,
+      panfrost_emit_shader(batch, &cfg, PIPE_SHADER_VERTEX,
+                           panfrost_get_position_shader(batch, info),
                            batch->tls.gpu);
    }
 
@@ -3354,11 +3509,8 @@ panfrost_emit_malloc_vertex(struct panfr
       if (!secondary_shader)
          continue;
 
-      mali_ptr ptr =
-         batch->rsd[PIPE_SHADER_VERTEX] + (2 * pan_size(SHADER_PROGRAM));
-
-      panfrost_emit_shader(batch, &cfg, PIPE_SHADER_VERTEX, ptr,
-                           batch->tls.gpu);
+      panfrost_emit_shader(batch, &cfg, PIPE_SHADER_VERTEX,
+                           panfrost_get_varying_shader(batch), batch->tls.gpu);
    }
 }
 #endif
@@ -3399,6 +3551,7 @@ panfrost_draw_emit_tiler(struct panfrost
 }
 #endif
 
+#if PAN_ARCH <= 9
 static void
 panfrost_launch_xfb(struct panfrost_batch *batch,
                     const struct pipe_draw_info *info, mali_ptr attribs,
@@ -3490,6 +3643,7 @@ panfrost_launch_xfb(struct panfrost_batc
    batch->push_uniforms[PIPE_SHADER_VERTEX] = saved_push;
    batch->nr_push_uniforms[PIPE_SHADER_VERTEX] = saved_nr_push_uniforms;
 }
+#endif
 
 /*
  * Increase the vertex count on the batch using a saturating add, and hope the
@@ -3506,21 +3660,39 @@ panfrost_increase_vertex_count(struct pa
       batch->tiler_ctx.vertex_count = UINT32_MAX;
 }
 
-static void
-panfrost_direct_draw(struct panfrost_batch *batch,
-                     const struct pipe_draw_info *info, unsigned drawid_offset,
-                     const struct pipe_draw_start_count_bias *draw)
+static bool
+panfrost_compatible_batch_state(struct panfrost_batch *batch, bool points)
 {
-   if (!draw->count || !info->instance_count)
-      return;
+   /* Only applies on Valhall */
+   if (PAN_ARCH < 9)
+      return true;
 
    struct panfrost_context *ctx = batch->ctx;
+   struct pipe_rasterizer_state *rast = &ctx->rasterizer->base;
+
+   bool coord = (rast->sprite_coord_mode == PIPE_SPRITE_COORD_LOWER_LEFT);
+   bool first = rast->flatshade_first;
 
-   /* If we change whether we're drawing points, or whether point sprites
-    * are enabled (specified in the rasterizer), we may need to rebind
-    * shaders accordingly. This implicitly covers the case of rebinding
-    * framebuffers, because all dirty flags are set there.
+   /* gl_PointCoord orientation only matters when drawing points, but
+    * provoking vertex doesn't matter for points.
     */
+   if (points)
+      return pan_tristate_set(&batch->sprite_coord_origin, coord);
+   else
+      return pan_tristate_set(&batch->first_provoking_vertex, first);
+}
+
+
+/*
+ * If we change whether we're drawing points, or whether point sprites are
+ * enabled (specified in the rasterizer), we may need to rebind shaders
+ * accordingly. This implicitly covers the case of rebinding framebuffers,
+ * because all dirty flags are set there.
+ */
+static void
+panfrost_update_point_sprite_shader(struct panfrost_context *ctx,
+                                    const struct pipe_draw_info *info)
+{
    if ((ctx->dirty & PAN_DIRTY_RASTERIZER) ||
        ((ctx->active_prim == MESA_PRIM_POINTS) ^
         (info->mode == MESA_PRIM_POINTS))) {
@@ -3528,6 +3700,472 @@ panfrost_direct_draw(struct panfrost_bat
       ctx->active_prim = info->mode;
       panfrost_update_shader_variant(ctx, PIPE_SHADER_FRAGMENT);
    }
+}
+
+#if PAN_ARCH >= 10
+/*
+ * Entrypoint for draws with CSF Mali. This is split out from JM as the handling
+ * of indirect draws is completely different, now that we can use the CEU, and
+ * the memory allocation patterns are different.
+ */
+static void
+panfrost_draw(struct panfrost_batch *batch, const struct pipe_draw_info *info,
+              unsigned drawid_offset,
+              const struct pipe_draw_start_count_bias *draw)
+{
+   if (!draw->count || !info->instance_count)
+      return;
+
+   struct panfrost_context *ctx = batch->ctx;
+
+   panfrost_update_point_sprite_shader(ctx, info);
+
+   /* Take into account a negative bias */
+   ctx->vertex_count =
+      draw->count + (info->index_size ? abs(draw->index_bias) : 0);
+   ctx->instance_count = info->instance_count;
+   ctx->base_vertex = info->index_size ? draw->index_bias : 0;
+   ctx->base_instance = info->start_instance;
+   ctx->active_prim = info->mode;
+   ctx->drawid = drawid_offset;
+
+   panfrost_update_state_3d(batch);
+   panfrost_update_shader_state(batch, PIPE_SHADER_VERTEX);
+   panfrost_update_shader_state(batch, PIPE_SHADER_FRAGMENT);
+   panfrost_clean_state_3d(ctx);
+
+   struct panfrost_compiled_shader *vs = ctx->prog[PIPE_SHADER_VERTEX];
+   struct panfrost_compiled_shader *fs = ctx->prog[PIPE_SHADER_FRAGMENT];
+
+   assert(vs->info.vs.idvs && "IDVS required for CSF");
+   bool secondary_shader = vs->info.vs.secondary_enable;
+   mali_ptr indices = 0;
+
+   if (info->index_size) {
+      indices = panfrost_get_index_buffer(batch, info, draw);
+   } else {
+      ctx->offset_start = draw->start;
+   }
+
+   panfrost_statistics_record(ctx, info, draw);
+
+   unsigned count = draw->count;
+   u_trim_pipe_prim(info->mode, &count);
+
+   /* Same register for XFB (compute) and IDVS */
+   ceu_builder *b = batch->ceu_builder;
+   ceu_move64_to(b, ceu_reg64(b, 24), batch->tls.gpu);
+
+   if (ctx->uncompiled[PIPE_SHADER_VERTEX]->xfb &&
+       batch->ctx->streamout.num_targets > 0 && count > 0) {
+      /* TODO: XFB with index buffers */
+      // assert(info->index_size == 0);
+
+      struct panfrost_uncompiled_shader *vs_uncompiled =
+         ctx->uncompiled[PIPE_SHADER_VERTEX];
+      struct panfrost_compiled_shader *vs = ctx->prog[PIPE_SHADER_VERTEX];
+
+      vs_uncompiled->xfb->stream_output = vs->stream_output;
+
+      mali_ptr saved_rsd = batch->rsd[PIPE_SHADER_VERTEX];
+      mali_ptr saved_ubo = batch->uniform_buffers[PIPE_SHADER_VERTEX];
+      mali_ptr saved_push = batch->push_uniforms[PIPE_SHADER_VERTEX];
+
+      ctx->uncompiled[PIPE_SHADER_VERTEX] = NULL; /* should not be read */
+      ctx->prog[PIPE_SHADER_VERTEX] = vs_uncompiled->xfb;
+      batch->rsd[PIPE_SHADER_VERTEX] =
+         panfrost_emit_compute_shader_meta(batch, PIPE_SHADER_VERTEX);
+
+      /* TODO: Indexing. Also, attribute_offset is a legacy feature..
+       */
+      ceu_move32_to(b, ceu_reg32(b, 32), draw->start);
+
+      /* Compute workgroup size */
+      uint32_t wg_size[4];
+      pan_pack(wg_size, COMPUTE_SIZE_WORKGROUP, cfg) {
+         cfg.workgroup_size_x = 1;
+         cfg.workgroup_size_y = 1;
+         cfg.workgroup_size_z = 1;
+
+         /* Transform feedback shaders do not use barriers or
+          * shared memory, so we may merge workgroups.
+          */
+         cfg.allow_merging_workgroups = true;
+      }
+      ceu_move32_to(b, ceu_reg32(b, 33), wg_size[0]);
+
+      /* Offset */
+      for (unsigned i = 0; i < 3; ++i)
+         ceu_move32_to(b, ceu_reg32(b, 34 + i), 0);
+
+      ceu_move32_to(b, ceu_reg32(b, 37), count);
+      ceu_move32_to(b, ceu_reg32(b, 38), info->instance_count);
+      ceu_move32_to(b, ceu_reg32(b, 39), 1);
+
+      panfrost_emit_shader_regs(batch, PIPE_SHADER_VERTEX,
+                                batch->rsd[PIPE_SHADER_VERTEX]);
+      /* XXX: Choose correctly */
+      ceu_run_compute(b, 10, MALI_TASK_AXIS_Z);
+
+      ctx->uncompiled[PIPE_SHADER_VERTEX] = vs_uncompiled;
+      ctx->prog[PIPE_SHADER_VERTEX] = vs;
+      batch->rsd[PIPE_SHADER_VERTEX] = saved_rsd;
+      batch->uniform_buffers[PIPE_SHADER_VERTEX] = saved_ubo;
+      batch->push_uniforms[PIPE_SHADER_VERTEX] = saved_push;
+
+      /* Reset registers expected to be 0 for IDVS */
+      ceu_move32_to(b, ceu_reg32(b, 31), 0);
+      ceu_move32_to(b, ceu_reg32(b, 32), 0);
+      ceu_move32_to(b, ceu_reg32(b, 37), 0);
+      ceu_move32_to(b, ceu_reg32(b, 38), 0);
+   }
+
+   /* Increment transform feedback offsets */
+   panfrost_update_streamout_offsets(ctx);
+
+   if (panfrost_batch_skip_rasterization(batch))
+      return;
+
+   panfrost_emit_shader_regs(batch, PIPE_SHADER_VERTEX,
+                             panfrost_get_position_shader(batch, info));
+
+   panfrost_emit_shader_regs(batch, PIPE_SHADER_FRAGMENT,
+                             batch->rsd[PIPE_SHADER_FRAGMENT]);
+
+   if (secondary_shader) {
+      ceu_move64_to(b, ceu_reg64(b, 18), panfrost_get_varying_shader(batch));
+   }
+
+   ceu_move64_to(b, ceu_reg64(b, 24), batch->tls.gpu);
+   ceu_move64_to(b, ceu_reg64(b, 30), batch->tls.gpu);
+   ceu_move32_to(b, ceu_reg32(b, 33), draw->count);
+   ceu_move32_to(b, ceu_reg32(b, 34), info->instance_count);
+   ceu_move32_to(b, ceu_reg32(b, 35), 0);
+
+   /* Base vertex offset on Valhall is used for both indexed and
+    * non-indexed draws, in a simple way for either. Handle both cases.
+    */
+   ceu_move32_to(b, ceu_reg32(b, 36),
+                 info->index_size ? draw->index_bias : draw->start);
+
+   if (info->index_size)
+      ceu_move32_to(b, ceu_reg32(b, 39), info->index_size * draw->count);
+   else
+      ceu_move32_to(b, ceu_reg32(b, 39), 0);
+
+   ceu_move64_to(b, ceu_reg64(b, 40),
+                 panfrost_batch_get_bifrost_tiler(batch, ~0));
+   ceu_move64_to(b, ceu_reg64(b, 86), batch->tiler_ctx.bifrost.heap);
+
+   STATIC_ASSERT(sizeof(batch->scissor) == pan_size(SCISSOR));
+   STATIC_ASSERT(sizeof(uint64_t) == pan_size(SCISSOR));
+   uint64_t *sbd = (uint64_t *)&batch->scissor[0];
+   ceu_move64_to(b, ceu_reg64(b, 42), *sbd);
+
+   ceu_move32_to(b, ceu_reg32(b, 44), fui(batch->minimum_z));
+   ceu_move32_to(b, ceu_reg32(b, 45), fui(batch->maximum_z));
+
+   if (ctx->occlusion_query && ctx->active_queries) {
+      struct panfrost_resource *rsrc = pan_resource(ctx->occlusion_query->rsrc);
+      ceu_move64_to(b, ceu_reg64(b, 46), rsrc->image.data.bo->ptr.gpu);
+      panfrost_batch_write_rsrc(ctx->batch, rsrc, PIPE_SHADER_FRAGMENT);
+   }
+
+   ceu_move32_to(b, ceu_reg32(b, 48), panfrost_vertex_attribute_stride(vs, fs));
+   ceu_move64_to(b, ceu_reg64(b, 50),
+                 batch->blend | MAX2(batch->key.nr_cbufs, 1));
+   ceu_move64_to(b, ceu_reg64(b, 52), batch->depth_stencil);
+
+   if (info->index_size)
+      ceu_move64_to(b, ceu_reg64(b, 54), indices);
+
+   uint32_t primitive_flags = 0;
+   pan_pack(&primitive_flags, PRIMITIVE_FLAGS, cfg) {
+      if (panfrost_writes_point_size(ctx))
+         cfg.point_size_array_format = MALI_POINT_SIZE_ARRAY_FORMAT_FP16;
+
+      //                cfg.allow_rotating_primitives =
+      //                pan_allow_rotating_primitives(fs, info);
+
+      /* Non-fixed restart indices should have been lowered */
+      assert(!cfg.primitive_restart || panfrost_is_implicit_prim_restart(info));
+      cfg.primitive_restart = info->primitive_restart;
+
+      cfg.position_fifo_format = panfrost_writes_point_size(ctx)
+                                    ? MALI_FIFO_FORMAT_EXTENDED
+                                    : MALI_FIFO_FORMAT_BASIC;
+   }
+
+   ceu_move32_to(b, ceu_reg32(b, 56), primitive_flags);
+
+   struct pipe_rasterizer_state *rast = &ctx->rasterizer->base;
+
+   uint32_t dcd_flags0 = 0, dcd_flags1 = 0;
+   pan_pack(&dcd_flags0, DCD_FLAGS_0, cfg) {
+      bool polygon = (u_reduced_prim(info->mode) == PIPE_PRIM_TRIANGLES);
+
+      /*
+       * From the Gallium documentation,
+       * pipe_rasterizer_state::cull_face "indicates which faces of
+       * polygons to cull". Points and lines are not considered
+       * polygons and should be drawn even if all faces are culled.
+       * The hardware does not take primitive type into account when
+       * culling, so we need to do that check ourselves.
+       */
+      cfg.cull_front_face = polygon && (rast->cull_face & PIPE_FACE_FRONT);
+      cfg.cull_back_face = polygon && (rast->cull_face & PIPE_FACE_BACK);
+      cfg.front_face_ccw = rast->front_ccw;
+
+      cfg.multisample_enable = rast->multisample;
+
+      /* Use per-sample shading if required by API Also use it when a
+       * blend shader is used with multisampling, as this is handled
+       * by a single ST_TILE in the blend shader with the current
+       * sample ID, requiring per-sample shading.
+       */
+      cfg.evaluate_per_sample =
+         (rast->multisample &&
+          ((ctx->min_samples > 1) || ctx->valhall_has_blend_shader));
+
+      cfg.single_sampled_lines = !rast->multisample;
+
+      bool has_oq = ctx->occlusion_query && ctx->active_queries;
+
+      if (has_oq) {
+         if (ctx->occlusion_query->type == PIPE_QUERY_OCCLUSION_COUNTER)
+            cfg.occlusion_query = MALI_OCCLUSION_MODE_COUNTER;
+         else
+            cfg.occlusion_query = MALI_OCCLUSION_MODE_PREDICATE;
+      }
+
+      struct pan_earlyzs_state earlyzs =
+         pan_earlyzs_get(fs->earlyzs, ctx->depth_stencil->writes_zs || has_oq,
+                         ctx->blend->base.alpha_to_coverage,
+                         ctx->depth_stencil->zs_always_passes);
+
+      cfg.pixel_kill_operation = earlyzs.kill;
+      cfg.zs_update_operation = earlyzs.update;
+
+      cfg.allow_forward_pixel_to_kill =
+         pan_allow_forward_pixel_to_kill(ctx, fs);
+      cfg.allow_forward_pixel_to_be_killed = !fs->info.writes_global;
+
+      /* Also use per-sample shading if required by the shader
+       */
+      cfg.evaluate_per_sample |= fs->info.fs.sample_shading;
+
+      /* Unlike Bifrost, alpha-to-coverage must be included in
+       * this identically-named flag. Confusing, isn't it?
+       */
+      cfg.shader_modifies_coverage = fs->info.fs.writes_coverage ||
+                                     fs->info.fs.can_discard ||
+                                     ctx->blend->base.alpha_to_coverage;
+
+      cfg.alpha_to_coverage = ctx->blend->base.alpha_to_coverage;
+
+      cfg.overdraw_alpha0 = panfrost_overdraw_alpha(ctx, 0);
+      cfg.overdraw_alpha1 = panfrost_overdraw_alpha(ctx, 1);
+   }
+
+   pan_pack(&dcd_flags1, DCD_FLAGS_1, cfg) {
+      cfg.sample_mask = rast->multisample ? ctx->sample_mask : 0xFFFF;
+
+      /* See JM Valhall equivalent code */
+      cfg.render_target_mask =
+         (fs->info.outputs_written >> FRAG_RESULT_DATA0) & ctx->fb_rt_mask;
+   }
+
+   ceu_move32_to(b, ceu_reg32(b, 57), dcd_flags0);
+   ceu_move32_to(b, ceu_reg32(b, 58), dcd_flags1);
+
+   uint64_t primsize = 0;
+   panfrost_emit_primitive_size(ctx, info->mode == PIPE_PRIM_POINTS, 0,
+                                &primsize);
+   ceu_move64_to(b, ceu_reg64(b, 60), primsize);
+
+   ceu_run_idvs(b, pan_draw_mode(info->mode),
+                panfrost_translate_index_size(info->index_size),
+                secondary_shader);
+}
+
+static void
+panfrost_draw_vbo(struct pipe_context *pipe, const struct pipe_draw_info *info,
+                  unsigned drawid_offset,
+                  const struct pipe_draw_indirect_info *indirect,
+                  const struct pipe_draw_start_count_bias *draws,
+                  unsigned num_draws)
+{
+   struct panfrost_context *ctx = pan_context(pipe);
+   struct panfrost_device *dev = pan_device(pipe->screen);
+
+   if (!panfrost_render_condition_check(ctx))
+      return;
+
+   assert(!(indirect && indirect->buffer) && "TODO: Indirects with CSF");
+
+   /* Do some common setup */
+   struct panfrost_batch *batch = panfrost_get_batch_for_fbo(ctx);
+
+   bool points = (info->mode == PIPE_PRIM_POINTS);
+
+   if (unlikely(!panfrost_compatible_batch_state(batch, points))) {
+      batch = panfrost_get_fresh_batch_for_fbo(ctx, "State change");
+
+      ASSERTED bool succ = panfrost_compatible_batch_state(batch, points);
+      assert(succ && "must be able to set state for a fresh batch");
+   }
+
+   if (batch->draws == 0)
+      panfrost_emit_heap_set(batch, true);
+
+   /* panfrost_batch_skip_rasterization reads
+    * batch->scissor_culls_everything, which is set by
+    * panfrost_emit_viewport, so call that first.
+    */
+   if (ctx->dirty & (PAN_DIRTY_VIEWPORT | PAN_DIRTY_SCISSOR))
+      batch->viewport = panfrost_emit_viewport(batch);
+
+   /* Mark everything dirty when debugging */
+   if (unlikely(dev->debug & PAN_DBG_DIRTY))
+      panfrost_dirty_state_all(ctx);
+
+   /* Conservatively assume draw parameters always change */
+   ctx->dirty |= PAN_DIRTY_PARAMS | PAN_DIRTY_DRAWID;
+
+   struct pipe_draw_info tmp_info = *info;
+   unsigned drawid = drawid_offset;
+
+   for (unsigned i = 0; i < num_draws; i++) {
+      panfrost_draw(batch, &tmp_info, drawid, &draws[i]);
+
+      if (tmp_info.increment_draw_id) {
+         ctx->dirty |= PAN_DIRTY_DRAWID;
+         drawid++;
+      }
+      batch->draw_count++;
+   }
+}
+
+/*
+ * Launch grid is the compute equivalent of draw_vbo. Set up the registers for a
+ * compute kernel and emit the run_compute command.
+ */
+static void
+panfrost_launch_grid(struct pipe_context *pipe,
+                     const struct pipe_grid_info *info)
+{
+   struct panfrost_context *ctx = pan_context(pipe);
+
+   /* XXX - shouldn't be necessary with working memory barriers. Affected
+    * test: KHR-GLES31.core.compute_shader.pipeline-post-xfb */
+   panfrost_flush_all_batches(ctx, "Launch grid pre-barrier");
+
+   struct panfrost_batch *batch = panfrost_get_batch_for_fbo(ctx);
+
+   ctx->compute_grid = info;
+
+   /* Conservatively assume workgroup size changes every launch */
+   ctx->dirty |= PAN_DIRTY_PARAMS;
+
+   panfrost_update_shader_state(batch, PIPE_SHADER_COMPUTE);
+
+   /* Empty compute programs are invalid and don't make sense */
+   if (batch->rsd[PIPE_SHADER_COMPUTE] == 0)
+      return;
+
+   struct panfrost_compiled_shader *cs = ctx->prog[PIPE_SHADER_COMPUTE];
+   ceu_builder *b = batch->ceu_builder;
+
+   panfrost_emit_shader_regs(batch, PIPE_SHADER_COMPUTE,
+                             batch->rsd[PIPE_SHADER_COMPUTE]);
+
+   ceu_move64_to(b, ceu_reg64(b, 24), panfrost_emit_shared_memory(batch, info));
+
+   /* Global attribute offset */
+   ceu_move32_to(b, ceu_reg32(b, 32), 0);
+
+   /* Compute workgroup size */
+   uint32_t wg_size[4];
+   pan_pack(wg_size, COMPUTE_SIZE_WORKGROUP, cfg) {
+      cfg.workgroup_size_x = info->block[0];
+      cfg.workgroup_size_y = info->block[1];
+      cfg.workgroup_size_z = info->block[2];
+
+      /* Workgroups may be merged if the shader does not use barriers
+       * or shared memory. This condition is checked against the
+       * static shared_size at compile-time. We need to check the
+       * variable shared size at launch_grid time, because the
+       * compiler doesn't know about that.
+       */
+      cfg.allow_merging_workgroups = cs->info.cs.allow_merging_workgroups &&
+                                     (info->variable_shared_mem == 0);
+   }
+
+   ceu_move32_to(b, ceu_reg32(b, 33), wg_size[0]);
+
+   /* Offset */
+   for (unsigned i = 0; i < 3; ++i)
+      ceu_move32_to(b, ceu_reg32(b, 34 + i), 0);
+
+   if (info->indirect) {
+      /* Load size in workgroups per dimension from memory */
+      ceu_index address = ceu_reg64(b, 64);
+      ceu_move64_to(b, address,
+                    pan_resource(info->indirect)->image.data.bo->ptr.gpu +
+                       info->indirect_offset);
+
+      ceu_index grid_xyz = ceu_reg_tuple(b, 37, 3);
+      ceu_load_to(b, grid_xyz, address, BITFIELD_MASK(3), 0);
+
+      /* Wait for the load */
+      ceu_wait_slot(b, 0);
+
+      /* Copy to FAU */
+      for (unsigned i = 0; i < 3; ++i) {
+         if (batch->num_wg_sysval[i]) {
+            ceu_move64_to(b, address, batch->num_wg_sysval[i]);
+            ceu_store(b, ceu_extract32(b, grid_xyz, i), address,
+                      BITFIELD_MASK(1), 0);
+         }
+      }
+
+      /* Wait for the stores */
+      ceu_wait_slot(b, 0);
+   } else {
+      /* Set size in workgroups per dimension immediately */
+      for (unsigned i = 0; i < 3; ++i)
+         ceu_move32_to(b, ceu_reg32(b, 37 + i), info->grid[i]);
+   }
+
+   /* Dispatch. We could be much smarter choosing task size..
+    *
+    * TODO: How to choose correctly?
+    *
+    * XXX: Why are compute kernels failing if I make this smaller? Race
+    * condition maybe? Cache badnesss?
+    */
+   ceu_run_compute(b, 10, MALI_TASK_AXIS_Z);
+   batch->any_compute = true;
+
+   panfrost_flush_all_batches(ctx, "Launch grid post-barrier");
+}
+#else
+/*
+ * Entrypoint for draws on JM Mali. Depending on generation, this requires
+ * emitting jobs for indirect drawing, transform feedback, vertex shading, and
+ * tiling.
+ */
+static void
+panfrost_direct_draw(struct panfrost_batch *batch,
+                     const struct pipe_draw_info *info, unsigned drawid_offset,
+                     const struct pipe_draw_start_count_bias *draw)
+{
+   if (!draw->count || !info->instance_count)
+      return;
+
+   struct panfrost_context *ctx = batch->ctx;
+
+   panfrost_update_point_sprite_shader(ctx, info);
 
    /* Take into account a negative bias */
    ctx->vertex_count =
@@ -3648,7 +4286,7 @@ panfrost_direct_draw(struct panfrost_bat
    if (panfrost_batch_skip_rasterization(batch))
       return;
 
-#if PAN_ARCH >= 9
+#if PAN_ARCH == 9
    assert(idvs && "Memory allocated IDVS required on Valhall");
 
    panfrost_emit_malloc_vertex(batch, info, draw, indices, secondary_shader,
@@ -3679,28 +4317,6 @@ panfrost_direct_draw(struct panfrost_bat
 #endif
 }
 
-static bool
-panfrost_compatible_batch_state(struct panfrost_batch *batch, bool points)
-{
-   /* Only applies on Valhall */
-   if (PAN_ARCH < 9)
-      return true;
-
-   struct panfrost_context *ctx = batch->ctx;
-   struct pipe_rasterizer_state *rast = &ctx->rasterizer->base;
-
-   bool coord = (rast->sprite_coord_mode == PIPE_SPRITE_COORD_LOWER_LEFT);
-   bool first = rast->flatshade_first;
-
-   /* gl_PointCoord orientation only matters when drawing points, but
-    * provoking vertex doesn't matter for points.
-    */
-   if (points)
-      return pan_tristate_set(&batch->sprite_coord_origin, coord);
-   else
-      return pan_tristate_set(&batch->first_provoking_vertex, first);
-}
-
 static void
 panfrost_draw_vbo(struct pipe_context *pipe, const struct pipe_draw_info *info,
                   unsigned drawid_offset,
@@ -3730,7 +4346,7 @@ panfrost_draw_vbo(struct pipe_context *p
    /* Don't add too many jobs to a single batch. Hardware has a hard limit
     * of 65536 jobs, but we choose a smaller soft limit (arbitrary) to
     * avoid the risk of timeouts. This might not be a good idea. */
-   if (unlikely(batch->scoreboard.job_index > 10000))
+   if (unlikely(batch->draw_count > 10000))
       batch = panfrost_get_fresh_batch_for_fbo(ctx, "Too many draws");
 
    bool points = (info->mode == MESA_PRIM_POINTS);
@@ -3766,13 +4382,14 @@ panfrost_draw_vbo(struct pipe_context *p
          ctx->dirty |= PAN_DIRTY_DRAWID;
          drawid++;
       }
+      batch->draw_count++;
    }
 }
 
-/* Launch grid is the compute equivalent of draw_vbo, so in this routine, we
- * construct the COMPUTE job and some of its payload.
+/*
+ * Launch grid is the compute equivalent of draw_vbo, so in this routine, we
+ * construct the COMPUTE job and add it to the job chain.
  */
-
 static void
 panfrost_launch_grid(struct pipe_context *pipe,
                      const struct pipe_grid_info *info)
@@ -3897,6 +4514,7 @@ panfrost_launch_grid(struct pipe_context
                     false);
    panfrost_flush_all_batches(ctx, "Launch grid post-barrier");
 }
+#endif
 
 static void *
 panfrost_create_rasterizer_state(struct pipe_context *pctx,
@@ -4328,7 +4946,10 @@ prepare_shader(struct panfrost_compiled_
    /* Generic, or IDVS/points */
    pan_pack(ptr.cpu, SHADER_PROGRAM, cfg) {
       cfg.stage = pan_shader_stage(&state->info);
-      cfg.primary_shader = true;
+
+      if (PAN_ARCH == 9 || cfg.stage == MALI_SHADER_STAGE_FRAGMENT)
+         cfg.primary_shader = true;
+
       cfg.register_allocation =
          pan_register_allocation(state->info.work_reg_count);
       cfg.binary = state->bin.gpu;
@@ -4345,7 +4966,7 @@ prepare_shader(struct panfrost_compiled_
    /* IDVS/triangles */
    pan_pack(ptr.cpu + pan_size(SHADER_PROGRAM), SHADER_PROGRAM, cfg) {
       cfg.stage = pan_shader_stage(&state->info);
-      cfg.primary_shader = true;
+      cfg.primary_shader = (PAN_ARCH == 9);
       cfg.register_allocation =
          pan_register_allocation(state->info.work_reg_count);
       cfg.binary = state->bin.gpu + state->info.vs.no_psiz_offset;
@@ -4383,8 +5004,8 @@ static void
 preload(struct panfrost_batch *batch, struct pan_fb_info *fb)
 {
    GENX(pan_preload_fb)
-   (&batch->pool.base, &batch->scoreboard, fb, batch->tls.gpu,
-    PAN_ARCH >= 6 ? batch->tiler_ctx.bifrost : 0, NULL);
+   (&batch->pool.base, PAN_ARCH < 10 ? &batch->scoreboard : NULL, fb, batch->tls.gpu,
+    PAN_ARCH >= 6 ? batch->tiler_ctx.bifrost.ctx : 0, NULL);
 }
 
 static void
@@ -4417,6 +5038,24 @@ init_batch(struct panfrost_batch *batch)
    batch->tls.gpu = ptr.opaque[0];
 #endif
 #endif
+
+#if PAN_ARCH >= 10
+   /* Allocate and bind the command queue */
+   struct ceu_queue queue = ceu_alloc_queue(batch);
+
+   /* Setup the queue builder */
+   batch->ceu_builder = malloc(sizeof(ceu_builder));
+   ceu_builder_init(batch->ceu_builder, 96, batch, queue);
+   ceu_require_all(batch->ceu_builder);
+
+   /* Set up entries */
+   ceu_builder *b = batch->ceu_builder;
+   ceu_set_scoreboard_entry(b, 2, 0);
+
+   /* Initialize the state vector */
+   for (unsigned i = 0; i < 64; i += 2)
+      ceu_move64_to(b, ceu_reg64(b, i), 0);
+#endif
 }
 
 static void
diff -up mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_context.c.8~ mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_context.c
--- mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_context.c.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_context.c	2023-09-08 01:00:47.831430400 +0200
@@ -54,6 +54,9 @@
 #include "pan_screen.h"
 #include "pan_util.h"
 
+#include "drm-uapi/panfrost_drm.h"
+#include "drm-uapi/pancsf_drm.h"
+
 static void
 panfrost_clear(struct pipe_context *pipe, unsigned buffers,
                const struct pipe_scissor_state *scissor_state,
@@ -70,7 +73,7 @@ panfrost_clear(struct pipe_context *pipe
    struct panfrost_batch *batch = panfrost_get_batch_for_fbo(ctx);
 
    /* At the start of the batch, we can clear for free */
-   if (!batch->scoreboard.first_job) {
+   if (batch->draw_count == 0) {
       panfrost_batch_clear(batch, buffers, color, depth, stencil);
       return;
    }
@@ -546,6 +549,28 @@ panfrost_render_condition(struct pipe_co
 }
 
 static void
+panfrost_cleanup_cs_queue(struct panfrost_context *ctx)
+{
+   struct panfrost_device *dev = pan_device(ctx->base.screen);
+
+   if (dev->arch < 10)
+      return;
+
+   struct drm_pancsf_tiler_heap_destroy thd = {
+      .handle = ctx->heap.handle,
+   };
+   int ret = drmIoctl(dev->fd, DRM_IOCTL_PANCSF_TILER_HEAP_DESTROY, &thd);
+   assert(!ret);
+
+   struct drm_pancsf_group_destroy gd = {
+      .group_handle = ctx->group.handle,
+   };
+
+   ret = drmIoctl(dev->fd, DRM_IOCTL_PANCSF_GROUP_DESTROY, &gd);
+   assert(!ret);
+}
+
+static void
 panfrost_destroy(struct pipe_context *pipe)
 {
    struct panfrost_context *panfrost = pan_context(pipe);
@@ -567,6 +592,7 @@ panfrost_destroy(struct pipe_context *pi
       close(panfrost->in_sync_fd);
 
    drmSyncobjDestroy(dev->fd, panfrost->syncobj);
+   panfrost_cleanup_cs_queue(panfrost);
    ralloc_free(pipe);
 }
 
@@ -845,6 +871,56 @@ panfrost_memory_barrier(struct pipe_cont
 }
 
 static void
+panfrost_init_cs_queue(struct panfrost_context *ctx)
+{
+   struct panfrost_device *dev = pan_device(ctx->base.screen);
+
+   if (dev->arch < 10)
+      return;
+
+   struct drm_pancsf_queue_create qc[] = {
+      {
+         .priority = 1,
+         .ringbuf_size = 64 * 1024,
+      }
+   };
+
+   struct drm_pancsf_group_create gc = {
+      .compute_core_mask = ~0,
+      .fragment_core_mask = ~0,
+      .tiler_core_mask = ~0,
+      .max_compute_cores = 64,
+      .max_fragment_cores = 64,
+      .max_tiler_cores = 1,
+      .priority = PANCSF_GROUP_PRIORITY_MEDIUM,
+      .queues = DRM_PANCSF_OBJ_ARRAY(ARRAY_SIZE(qc), qc),
+      .vm_id = dev->vm_id,
+   };
+
+   int ret = drmIoctl(dev->fd, DRM_IOCTL_PANCSF_GROUP_CREATE, &gc);
+
+   assert(!ret);
+
+   ctx->group.handle = gc.group_handle;
+
+   /* Get tiler heap */
+   struct drm_pancsf_tiler_heap_create thc = {
+      .vm_id = dev->vm_id,
+      .chunk_size = 2 * 1024 * 1024,
+      .initial_chunk_count = 5,
+      .max_chunks = 64 * 1024,
+      .target_in_flight = 65535,
+   };
+   ret = drmIoctl(dev->fd, DRM_IOCTL_PANCSF_TILER_HEAP_CREATE, &thc);
+
+   assert(!ret);
+
+   ctx->heap.handle = thc.handle;
+   ctx->heap.tiler_heap_ctx_gpu_va = thc.tiler_heap_ctx_gpu_va;
+   ctx->heap.first_heap_chunk_gpu_va = thc.first_heap_chunk_gpu_va;
+}
+
+static void
 panfrost_create_fence_fd(struct pipe_context *pctx,
                          struct pipe_fence_handle **pfence, int fd,
                          enum pipe_fd_type type)
@@ -980,5 +1056,7 @@ panfrost_create_context(struct pipe_scre
    ret = drmSyncobjCreate(dev->fd, 0, &ctx->in_sync_obj);
    assert(!ret);
 
+   panfrost_init_cs_queue(ctx);
+
    return gallium;
 }
diff -up mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_context.h.8~ mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_context.h
--- mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_context.h.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_context.h	2023-09-08 01:00:47.831430400 +0200
@@ -227,6 +227,16 @@ struct panfrost_context {
 
    int in_sync_fd;
    uint32_t in_sync_obj;
+
+   struct {
+      uint32_t handle;
+   } group;
+
+   struct {
+      uint32_t handle;
+      mali_ptr tiler_heap_ctx_gpu_va;
+      mali_ptr first_heap_chunk_gpu_va;
+   } heap;
 };
 
 /* Corresponds to the CSO */
diff -up mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_job.c.8~ mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_job.c
--- mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_job.c.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_job.c	2023-09-08 01:00:47.831430400 +0200
@@ -26,6 +26,7 @@
 
 #include <assert.h>
 
+#include "drm-uapi/pancsf_drm.h"
 #include "drm-uapi/panfrost_drm.h"
 
 #include "util/format/u_format.h"
@@ -39,6 +40,8 @@
 #include "pan_context.h"
 #include "pan_util.h"
 
+#include "genxml/ceu_builder.h"
+
 #define foreach_batch(ctx, idx)                                                \
    BITSET_FOREACH_SET(idx, ctx->batches.active, PAN_MAX_BATCHES)
 
@@ -228,7 +231,7 @@ panfrost_get_fresh_batch_for_fbo(struct
    /* We only need to submit and get a fresh batch if there is no
     * draw/clear queued. Otherwise we may reuse the batch. */
 
-   if (batch->scoreboard.first_job) {
+   if (batch->draw_count) {
       perf_debug_ctx(ctx, "Flushing the current FBO due to: %s", reason);
       panfrost_batch_submit(ctx, batch);
       batch = panfrost_get_batch(ctx, &ctx->pipe_framebuffer);
@@ -438,6 +441,21 @@ panfrost_batch_get_shared_memory(struct
    return batch->shared_memory;
 }
 
+struct ceu_queue
+ceu_alloc_queue(void *cookie)
+{
+   struct panfrost_batch *batch = cookie;
+   unsigned capacity = 4096;
+   struct panfrost_bo *bo = panfrost_batch_create_bo(
+      batch, capacity * 8, 0,
+      PIPE_SHADER_VERTEX, "Command queue");
+   memset(bo->ptr.cpu, 0xFF, capacity * 8);
+
+   return (struct ceu_queue){.cpu = bo->ptr.cpu,
+                             .gpu = bo->ptr.gpu,
+                             .capacity = capacity};
+}
+
 static void
 panfrost_batch_to_fb_info(const struct panfrost_batch *batch,
                           struct pan_fb_info *fb, struct pan_image_view *rts,
@@ -701,7 +719,7 @@ panfrost_batch_submit_ioctl(struct panfr
 static bool
 panfrost_has_fragment_job(struct panfrost_batch *batch)
 {
-   return batch->scoreboard.first_tiler || batch->clear;
+   return batch->draw_count > 0 || batch->clear;
 }
 
 /* Submit both vertex/tiler and fragment jobs for a batch, possibly with an
@@ -751,6 +769,237 @@ done:
    return ret;
 }
 
+static int
+panfrost_batch_submit_cs_ioctl(struct panfrost_batch *batch, mali_ptr cs_start,
+                               uint32_t cs_size, uint32_t in_sync,
+                               uint32_t out_sync)
+{
+   struct panfrost_context *ctx = batch->ctx;
+   struct pipe_context *gallium = (struct pipe_context *)ctx;
+   struct panfrost_device *dev = pan_device(gallium->screen);
+   struct drm_pancsf_sync_op *syncs = NULL;
+   uint32_t *bo_handles;
+   uint32_t bo_handle_count = 0;
+   int ret;
+
+   /* If we trace, we always need a syncobj, so make one of our own if we
+    * weren't given one to use. Remember that we did so, so we can free it
+    * after we're done but preventing double-frees if we were given a
+    * syncobj */
+
+   if (!out_sync && dev->debug & (PAN_DBG_TRACE | PAN_DBG_SYNC))
+      out_sync = ctx->syncobj;
+
+   bo_handles = calloc(panfrost_pool_num_bos(&batch->pool) +
+                          panfrost_pool_num_bos(&batch->invisible_pool) +
+                          batch->num_bos + 2,
+                       sizeof(*bo_handles));
+   assert(bo_handles);
+   syncs = calloc(panfrost_pool_num_bos(&batch->pool) +
+                  panfrost_pool_num_bos(&batch->invisible_pool) +
+                  (batch->num_bos * 2) + 2 + 2,
+                  sizeof(*syncs));
+   assert(syncs);
+
+   struct drm_pancsf_queue_submit qsubmits[] = {
+      {
+         .queue_index = 0,
+         .stream_addr = cs_start,
+         .stream_size = cs_size,
+         .latest_flush = *dev->flush_id,
+         .syncs = DRM_PANCSF_OBJ_ARRAY(0, syncs),
+      },
+   };
+   struct drm_pancsf_group_submit gsubmit = {
+      .group_handle = ctx->group.handle,
+      .queue_submits = DRM_PANCSF_OBJ_ARRAY(ARRAY_SIZE(qsubmits), qsubmits),
+   };
+
+   pan_bo_access *flags = util_dynarray_begin(&batch->bos);
+   unsigned end_bo = util_dynarray_num_elements(&batch->bos, pan_bo_access);
+
+   for (int i = 0; i < end_bo; ++i) {
+      if (!flags[i])
+         continue;
+
+      assert(bo_handle_count < batch->num_bos);
+      bo_handles[bo_handle_count++] = i;
+
+      /* Update the BO access flags so that panfrost_bo_wait() knows
+       * about all pending accesses.
+       * We only keep the READ/WRITE info since this is all the BO
+       * wait logic cares about.
+       * We also preserve existing flags as this batch might not
+       * be the first one to access the BO.
+       */
+      struct panfrost_bo *bo = pan_lookup_bo(dev, i);
+
+      bo->gpu_access |= flags[i] & (PAN_BO_ACCESS_RW);
+   }
+
+   panfrost_pool_get_bo_handles(&batch->pool, bo_handles + bo_handle_count);
+   bo_handle_count += panfrost_pool_num_bos(&batch->pool);
+   panfrost_pool_get_bo_handles(&batch->invisible_pool,
+                                bo_handles + bo_handle_count);
+   bo_handle_count += panfrost_pool_num_bos(&batch->invisible_pool);
+
+   /* Add the tiler heap to the list of accessed BOs if the batch has at
+    * least one tiler job. Tiler heap is written by tiler jobs and read
+    * by fragment jobs (the polygon list is coming from this heap).
+    */
+   if (batch->draw_count > 0)
+      bo_handles[bo_handle_count++] = dev->tiler_heap->gem_handle;
+
+   /* Always used on Bifrost, occassionally used on Midgard */
+   bo_handles[bo_handle_count++] = dev->sample_positions->gem_handle;
+
+   if (in_sync) {
+      syncs[qsubmits[0].syncs.count++] = (struct drm_pancsf_sync_op){
+         .op_type = DRM_PANCSF_SYNC_OP_WAIT,
+         .handle_type = DRM_PANCSF_SYNC_HANDLE_TYPE_SYNCOBJ,
+         .handle = in_sync,
+      };
+   }
+
+   for (unsigned i = 0; i < bo_handle_count; i++) {
+      struct panfrost_bo *bo = pan_lookup_bo(dev, bo_handles[i]);
+
+      if (!(bo->flags & PAN_BO_SHARED)) {
+         /* We assume a single queue and jobs are serialized, so we don't
+          * need to wait on private BOs. Things might be different if we split
+          * the vertex/tiling and fragment queues.
+          */
+         syncs[qsubmits[0].syncs.count++] = (struct drm_pancsf_sync_op){
+            .op_type = DRM_PANCSF_SYNC_OP_SIGNAL,
+            .handle_type = DRM_PANCSF_SYNC_HANDLE_TYPE_TIMELINE_SYNCOBJ,
+            .handle = bo->sync.handle,
+            .timeline_value = ++bo->sync.point,
+         };
+      } else {
+         ret = panfrost_shared_bo_import_sync_handle(bo, flags[bo_handles[i]]);
+         assert(!ret);
+
+         syncs[qsubmits[0].syncs.count++] = (struct drm_pancsf_sync_op){
+            .op_type = DRM_PANCSF_SYNC_OP_WAIT,
+            .handle_type = DRM_PANCSF_SYNC_HANDLE_TYPE_SYNCOBJ,
+            .handle = bo->sync.handle,
+         };
+
+         if (!out_sync)
+            out_sync = ctx->syncobj;
+      }
+   }
+
+   if (out_sync) {
+      syncs[qsubmits[0].syncs.count++] = (struct drm_pancsf_sync_op){
+         .op_type = DRM_PANCSF_SYNC_OP_SIGNAL,
+         .handle_type = DRM_PANCSF_SYNC_HANDLE_TYPE_SYNCOBJ,
+         .handle = out_sync,
+      };
+   }
+
+   if (ctx->is_noop)
+      ret = 0;
+   else
+      ret = drmIoctl(dev->fd, DRM_IOCTL_PANCSF_GROUP_SUBMIT, &gsubmit);
+
+   if (!ret) {
+      for (unsigned i = 0; i < bo_handle_count; i++) {
+         struct panfrost_bo *bo = pan_lookup_bo(dev, bo_handles[i]);
+
+         if (bo->flags & PAN_BO_SHARED) {
+            assert(out_sync);
+            ret = panfrost_shared_bo_attach_sync_handle(bo, out_sync, flags[bo_handles[i]]);
+            assert(!ret);
+         }
+      }
+   } else {
+      struct drm_pancsf_group_get_state state = {
+         .group_handle = ctx->group.handle,
+      };
+
+      ret = drmIoctl(dev->fd, DRM_IOCTL_PANCSF_GROUP_GET_STATE, &state);
+      assert(!ret);
+      if (state.state != 0) {
+         struct drm_pancsf_group_destroy gd = {
+            .group_handle = ctx->group.handle,
+         };
+
+         ret = drmIoctl(dev->fd, DRM_IOCTL_PANCSF_GROUP_DESTROY, &gd);
+         assert(!ret);
+
+         struct drm_pancsf_queue_create qc[] = {
+            {
+               .priority = 1,
+               .ringbuf_size = 64 * 1024,
+            }
+         };
+
+         struct drm_pancsf_group_create gc = {
+            .compute_core_mask = ~0,
+            .fragment_core_mask = ~0,
+            .tiler_core_mask = ~0,
+            .max_compute_cores = 64,
+            .max_fragment_cores = 64,
+            .max_tiler_cores = 1,
+            .priority = PANCSF_GROUP_PRIORITY_MEDIUM,
+            .queues = DRM_PANCSF_OBJ_ARRAY(ARRAY_SIZE(qc), qc),
+            .vm_id = dev->vm_id,
+         };
+
+         ret = drmIoctl(dev->fd, DRM_IOCTL_PANCSF_GROUP_CREATE, &gc);
+         assert(!ret);
+         ctx->group.handle = gc.group_handle;
+      }
+   }
+
+   free(bo_handles);
+   free(syncs);
+
+   if (ret)
+      return errno;
+
+   /* Trace the job late for JM */
+   if (dev->debug & (PAN_DBG_TRACE | PAN_DBG_SYNC)) {
+      /* Wait so we can get errors reported back */
+      drmSyncobjWait(dev->fd, &out_sync, 1, INT64_MAX, 0, NULL);
+
+      if ((dev->debug & PAN_DBG_TRACE) && dev->arch >= 10) {
+         uint32_t regs[256] = {};
+         pandecode_cs(qsubmits[0].stream_addr, qsubmits[0].stream_size,
+                      dev->gpu_id, regs);
+      }
+
+      if (dev->debug & PAN_DBG_DUMP)
+         pandecode_dump_mappings();
+
+      /* Jobs won't be complete if blackhole rendering, that's ok */
+      if (!ctx->is_noop && dev->debug & PAN_DBG_SYNC &&
+          *((uint64_t *)batch->cs_state.cpu) != 0) {
+         fprintf(stderr, "Incomplete job or timeout\n");
+	 fflush(NULL);
+	 abort();
+      }
+   }
+
+   return 0;
+}
+
+static int
+panfrost_batch_submit_csf(struct panfrost_batch *batch,
+                          const struct pan_fb_info *fb, uint32_t in_sync,
+                          uint32_t out_sync)
+{
+   struct panfrost_screen *screen = pan_screen(batch->ctx->base.screen);
+
+   screen->vtbl.emit_fragment_job(batch, fb);
+
+   unsigned count = ceu_finish(batch->ceu_builder);
+
+   return panfrost_batch_submit_cs_ioctl(batch, batch->ceu_builder->root.gpu,
+                                         count * 8, in_sync, out_sync);
+}
+
 static void
 panfrost_emit_tile_map(struct panfrost_batch *batch, struct pan_fb_info *fb)
 {
@@ -774,10 +1023,11 @@ panfrost_batch_submit(struct panfrost_co
 {
    struct pipe_screen *pscreen = ctx->base.screen;
    struct panfrost_screen *screen = pan_screen(pscreen);
+   struct panfrost_device *dev = &screen->dev;
    int ret;
 
    /* Nothing to do! */
-   if (!batch->scoreboard.first_job && !batch->clear)
+   if (!batch->clear && !batch->draws && !batch->any_compute)
       goto out;
 
    if (batch->key.zsbuf && panfrost_has_fragment_job(batch)) {
@@ -814,10 +1064,13 @@ panfrost_batch_submit(struct panfrost_co
    screen->vtbl.emit_tls(batch);
    panfrost_emit_tile_map(batch, &fb);
 
-   if (batch->scoreboard.first_tiler || batch->clear)
+   if (batch->draw_count > 0 || batch->clear)
       screen->vtbl.emit_fbd(batch, &fb);
 
-   ret = panfrost_batch_submit_jobs(batch, &fb, 0, ctx->syncobj);
+   if (dev->arch >= 10)
+      ret = panfrost_batch_submit_csf(batch, &fb, 0, ctx->syncobj);
+   else
+      ret = panfrost_batch_submit_jobs(batch, &fb, 0, ctx->syncobj);
 
    if (ret)
       fprintf(stderr, "panfrost_batch_submit failed: %d\n", ret);
diff -up mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_job.h.8~ mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_job.h
--- mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_job.h.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_job.h	2023-09-08 01:00:47.831430400 +0200
@@ -82,6 +82,8 @@ pan_tristate_get(struct pan_tristate sta
 /* A panfrost_batch corresponds to a bound FBO we're rendering to,
  * collecting over multiple draws. */
 
+struct ceu_builder;
+
 struct panfrost_batch {
    struct panfrost_context *ctx;
    struct pipe_framebuffer_state key;
@@ -102,6 +104,8 @@ struct panfrost_batch {
    /* Buffers needing resolve to memory */
    unsigned resolve;
 
+   bool any_compute;
+
    /* Packed clear values, indexed by both render target as well as word.
     * Essentially, a single pixel is packed, with some padding to bring it
     * up to a 32-bit interval; that pixel is then duplicated over to fill
@@ -142,6 +146,11 @@ struct panfrost_batch {
    /* Job scoreboarding state */
    struct pan_scoreboard scoreboard;
 
+   struct ceu_builder *ceu_builder;
+
+   /* CSF stream state BO. */
+   struct panfrost_ptr cs_state;
+
    /* Polygon list bound to the batch, or NULL if none bound yet */
    struct panfrost_bo *polygon_list;
 
@@ -192,6 +201,8 @@ struct panfrost_batch {
     */
    struct pan_tristate sprite_coord_origin;
    struct pan_tristate first_provoking_vertex;
+
+   uint32_t draw_count;
 };
 
 /* Functions for managing the above */
diff -up mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_screen.c.8~ mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_screen.c
--- mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_screen.c.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_screen.c	2023-09-08 01:00:47.831430400 +0200
@@ -43,6 +43,8 @@
 #include "drm-uapi/drm_fourcc.h"
 #include "drm-uapi/panfrost_drm.h"
 
+#include "genxml/ceu_builder.h"
+
 #include "decode.h"
 #include "pan_bo.h"
 #include "pan_fence.h"
@@ -895,6 +897,8 @@ panfrost_create_screen(int fd, const str
       panfrost_cmdstream_screen_init_v7(screen);
    else if (dev->arch == 9)
       panfrost_cmdstream_screen_init_v9(screen);
+   else if (dev->arch == 10)
+      panfrost_cmdstream_screen_init_v10(screen);
    else
       unreachable("Unhandled architecture major");
 
diff -up mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_screen.h.8~ mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_screen.h
--- mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_screen.h.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/gallium/drivers/panfrost/pan_screen.h	2023-09-08 01:00:47.832430410 +0200
@@ -132,6 +132,7 @@ void panfrost_cmdstream_screen_init_v5(s
 void panfrost_cmdstream_screen_init_v6(struct panfrost_screen *screen);
 void panfrost_cmdstream_screen_init_v7(struct panfrost_screen *screen);
 void panfrost_cmdstream_screen_init_v9(struct panfrost_screen *screen);
+void panfrost_cmdstream_screen_init_v10(struct panfrost_screen *screen);
 
 #define perf_debug(dev, ...)                                                   \
    do {                                                                        \
diff -up mesa-23.2.0-rc3/src/gallium/winsys/kmsro/drm/kmsro_drm_winsys.c.8~ mesa-23.2.0-rc3/src/gallium/winsys/kmsro/drm/kmsro_drm_winsys.c
--- mesa-23.2.0-rc3/src/gallium/winsys/kmsro/drm/kmsro_drm_winsys.c.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/gallium/winsys/kmsro/drm/kmsro_drm_winsys.c	2023-09-08 01:00:47.832430410 +0200
@@ -110,6 +110,11 @@ struct pipe_screen *kmsro_drm_screen_cre
          .create_screen = panfrost_drm_screen_create_renderonly,
          .create_for_resource = panfrost_create_kms_dumb_buffer_for_resource,
       },
+      {
+         .name = "pancsf",
+         .create_screen = panfrost_drm_screen_create_renderonly,
+         .create_for_resource = panfrost_create_kms_dumb_buffer_for_resource,
+      },
 #endif
 
 #if defined(GALLIUM_V3D)
diff -up mesa-23.2.0-rc3/src/panfrost/compiler/bifrost_compile.c.8~ mesa-23.2.0-rc3/src/panfrost/compiler/bifrost_compile.c
--- mesa-23.2.0-rc3/src/panfrost/compiler/bifrost_compile.c.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/panfrost/compiler/bifrost_compile.c	2023-09-08 01:00:47.832430410 +0200
@@ -1012,7 +1012,7 @@ bi_emit_store_vary(bi_builder *b, nir_in
    } else if (b->shader->arch >= 9 && b->shader->idvs != BI_IDVS_NONE) {
       bi_index index = bi_preload(b, 59);
 
-      if (psiz) {
+      if (psiz /*&& b->shader->arch == 9*/) {
          assert(T_size == 16 && "should've been lowered");
          index = bi_iadd_imm_i32(b, index, 4);
       }
@@ -1024,7 +1024,9 @@ bi_emit_store_vary(bi_builder *b, nir_in
 
       bi_store(b, nr * nir_src_bit_size(instr->src[0]), data, a[0], a[1],
                varying ? BI_SEG_VARY : BI_SEG_POS,
-               varying ? bi_varying_offset(b->shader, instr) : 0);
+               varying ? bi_varying_offset(b->shader, instr) :
+                       //(psiz && b->shader->arch >= 10) ? 64 :
+                  0);
    } else if (immediate) {
       bi_index address = bi_lea_attr_imm(b, bi_vertex_id(b), bi_instance_id(b),
                                          regfmt, imm_index);
diff -up mesa-23.2.0-rc3/src/panfrost/lib/genxml/ceu_builder.h.8~ mesa-23.2.0-rc3/src/panfrost/lib/genxml/ceu_builder.h
--- mesa-23.2.0-rc3/src/panfrost/lib/genxml/ceu_builder.h.8~	2023-09-08 01:00:47.832430410 +0200
+++ mesa-23.2.0-rc3/src/panfrost/lib/genxml/ceu_builder.h	2023-09-08 01:00:47.832430410 +0200
@@ -0,0 +1,528 @@
+/*
+ * Copyright (C) 2022 Collabora Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ */
+
+#if !defined(CEU_BUILDER_H)
+#define CEU_BUILDER_H
+#include "gen_macros.h"
+
+/*
+ * ceu_builder implements a builder for CSF command queues. It manages the
+ * allocation and overflow behaviour of queues and provides helpers for emitting
+ * commands to run on the CEU.
+ *
+ * Users must implement the ceu_alloc_queue method performing the physical
+ * memory allocation and queue binding. Users must initialize a queue with
+ * ceu_builder_init.
+ */
+
+struct ceu_queue {
+   /* CPU pointer */
+   uint64_t *cpu;
+
+   /* GPU pointer */
+   uint64_t gpu;
+
+   /* Capacity */
+   size_t capacity;
+};
+
+struct ceu_queue ceu_alloc_queue(void *cookie);
+
+typedef struct ceu_builder {
+   /* Initial (root) queue */
+   struct ceu_queue root;
+
+   /* Number of instructions emitted into the root queue */
+   uint32_t root_size;
+
+   /* Current queue */
+   struct ceu_queue queue;
+
+   /* Number of instructions emitted into the current queue so far */
+   uint32_t queue_size;
+
+   /* Number of 32-bit registers in the hardware register file */
+   uint8_t nr_registers;
+
+   /* Move immediate instruction at the end of the last queue that needs to
+    * be patched with the final length of the current queue in order to
+    * facilitate correct overflow behaviour.
+    */
+   uint32_t *length_patch;
+
+   /* Cookie passed back to ceu_alloc_queue for caller use */
+   void *cookie;
+} ceu_builder;
+
+static void
+ceu_builder_init(struct ceu_builder *b, uint8_t nr_registers, void *cookie,
+                 struct ceu_queue root)
+{
+   *b = (struct ceu_builder){.nr_registers = nr_registers,
+                             .cookie = cookie,
+                             .queue = root,
+                             .root = root};
+}
+
+/*
+ * Finish building a queue. External users should call this once when they are
+ * done with the builder. Returns the number of instructions of the emitted
+ * command queue.
+ *
+ * Internally, this is also used to finalize internal subqueues when allocating
+ * new subqueues. See ceu_alloc for details.
+ *
+ * This notably requires patching the previous queue with the length we ended up
+ * emitting for this queue.
+ */
+static unsigned
+ceu_finish(ceu_builder *b)
+{
+   if (b->length_patch) {
+      *b->length_patch = (b->queue_size * 8);
+      b->length_patch = NULL;
+   }
+
+   if (b->root.gpu == b->queue.gpu)
+      b->root_size = b->queue_size;
+
+   return b->root_size;
+}
+
+#if PAN_ARCH >= 10
+enum ceu_index_type { CEU_INDEX_REGISTER = 0, CEU_INDEX_IMMEDIATE = 1 };
+
+typedef struct ceu_index {
+   enum ceu_index_type type;
+
+   /* Number of 32-bit words in the index, must be nonzero */
+   uint8_t size;
+
+   union {
+      uint64_t imm;
+      uint8_t reg;
+   };
+} ceu_index;
+
+static uint8_t
+ceu_to_reg_tuple(ceu_index idx, ASSERTED uint8_t expected_size)
+{
+   assert(idx.type == CEU_INDEX_REGISTER);
+   assert(idx.size == expected_size);
+
+   return idx.reg;
+}
+
+static uint8_t
+ceu_to_reg32(ceu_index idx)
+{
+   return ceu_to_reg_tuple(idx, 1);
+}
+
+static uint8_t
+ceu_to_reg64(ceu_index idx)
+{
+   return ceu_to_reg_tuple(idx, 2);
+}
+
+static ceu_index
+ceu_reg_tuple(ASSERTED ceu_builder *b, uint8_t reg, uint8_t size)
+{
+   assert(reg + size <= b->nr_registers && "overflowed register file");
+   assert(size < 16 && "unsupported");
+
+   return (
+      struct ceu_index){.type = CEU_INDEX_REGISTER, .size = size, .reg = reg};
+}
+
+static inline ceu_index
+ceu_reg32(ceu_builder *b, uint8_t reg)
+{
+   return ceu_reg_tuple(b, reg, 1);
+}
+
+static inline ceu_index
+ceu_reg64(ceu_builder *b, uint8_t reg)
+{
+   assert((reg % 2) == 0 && "unaligned 64-bit reg");
+   return ceu_reg_tuple(b, reg, 2);
+}
+
+/*
+ * The top of the register file is reserved for ceu_builder internal use. We
+ * need 3 spare registers for handling command queue overflow. These are
+ * available here.
+ */
+static inline ceu_index
+ceu_overflow_address(ceu_builder *b)
+{
+   return ceu_reg64(b, b->nr_registers - 2);
+}
+
+static inline ceu_index
+ceu_overflow_length(ceu_builder *b)
+{
+   return ceu_reg32(b, b->nr_registers - 3);
+}
+
+static ceu_index
+ceu_extract32(ceu_builder *b, ceu_index idx, uint8_t word)
+{
+   assert(idx.type == CEU_INDEX_REGISTER && "unsupported");
+   assert(word < idx.size && "overrun");
+
+   return ceu_reg32(b, idx.reg + word);
+}
+
+static inline void *
+ceu_alloc(ceu_builder *b)
+{
+   /* If the current command queue runs out of space, allocate a new one
+    * and jump to it. We actually do this a few instructions before running
+    * out, because the sequence to jump to a new queue takes multiple
+    * instructions.
+    *
+    * The root queue cannot jump to other queues, so we call instead of
+    * jumping.
+    */
+   if (unlikely((b->queue_size + 4) > b->queue.capacity)) {
+      bool is_root = (b->root.gpu == b->queue.gpu);
+
+      /* Now, allocate a new queue */
+      struct ceu_queue newq = ceu_alloc_queue(b->cookie);
+
+      uint64_t *ptr = b->queue.cpu + (b->queue_size++);
+
+      pan_pack(ptr, CEU_MOVE, I) {
+         I.destination = ceu_to_reg64(ceu_overflow_address(b));
+         I.immediate = newq.gpu;
+      }
+
+      ptr = b->queue.cpu + (b->queue_size++);
+
+      pan_pack(ptr, CEU_MOVE32, I) {
+         I.destination = ceu_to_reg32(ceu_overflow_length(b));
+      }
+
+      /* The length will be patched in later */
+      uint32_t *length_patch = (uint32_t *)ptr;
+
+      ptr = b->queue.cpu + (b->queue_size++);
+
+      pan_pack(ptr, CEU_JUMP, I) {
+         I.length = ceu_to_reg32(ceu_overflow_length(b));
+         I.address = ceu_to_reg64(ceu_overflow_address(b));
+      }
+
+      /* Now that we've emitted everything, finish up the previous queue */
+      ceu_finish(b);
+
+      /* And make this one current */
+      b->length_patch = length_patch;
+      b->queue = newq;
+      b->queue_size = 0;
+   }
+
+   assert(b->queue_size < b->queue.capacity);
+   return b->queue.cpu + (b->queue_size++);
+}
+
+/*
+ * Helper to emit a new instruction into the command queue. The allocation needs
+ * to be separated out being pan_pack can evaluate its argument multiple times,
+ * yet ceu_alloc has side effects.
+ */
+#define ceu_emit(b, T, cfg)                                                    \
+   void *_dest = ceu_alloc(b);                                                 \
+   pan_pack(_dest, CEU_##T, cfg)
+
+static inline void
+ceu_move32_to(ceu_builder *b, ceu_index dest, uint32_t imm)
+{
+   ceu_emit(b, MOVE32, I)
+   {
+      I.destination = ceu_to_reg32(dest);
+      I.immediate = imm;
+   }
+}
+
+static inline void
+ceu_move48_to(ceu_builder *b, ceu_index dest, uint64_t imm)
+{
+   ceu_emit(b, MOVE, I)
+   {
+      I.destination = ceu_to_reg64(dest);
+      I.immediate = imm;
+   }
+}
+
+static inline void
+ceu_wait_slots(ceu_builder *b, uint8_t slots)
+{
+   ceu_emit(b, WAIT, I)
+   {
+      I.slots = slots;
+   }
+}
+
+static inline void
+ceu_run_compute(ceu_builder *b, unsigned task_increment,
+                enum mali_task_axis task_axis)
+{
+   ceu_emit(b, RUN_COMPUTE, I)
+   {
+      I.task_increment = task_increment;
+      I.task_axis = task_axis;
+
+      /* We always use the first table for compute jobs */
+   }
+}
+
+static inline void
+ceu_run_idvs(ceu_builder *b, enum mali_draw_mode draw_mode,
+             enum mali_index_type index_type, bool secondary_shader)
+{
+   ceu_emit(b, RUN_IDVS, I)
+   {
+      /* We do not have a use case for traditional IDVS */
+      I.malloc_enable = true;
+
+      /* We hardcode these settings for now, we can revisit this if we
+       * rework how we emit state later.
+       */
+      I.fragment_srt_select = true;
+
+      /* Pack the override we use */
+      pan_pack(&I.flags_override, PRIMITIVE_FLAGS, cfg) {
+         cfg.draw_mode = draw_mode;
+         cfg.index_type = index_type;
+         cfg.secondary_shader = secondary_shader;
+      }
+   }
+}
+
+static inline void
+ceu_run_fragment(ceu_builder *b, bool enable_tem)
+{
+   ceu_emit(b, RUN_FRAGMENT, I)
+   {
+      I.enable_tem = enable_tem;
+   }
+}
+
+static inline void
+ceu_finish_tiling(ceu_builder *b)
+{
+   ceu_emit(b, FINISH_TILING, _);
+}
+
+static inline void
+ceu_heap_set(ceu_builder *b, ceu_index address)
+{
+   ceu_emit(b, HEAP_SET, I)
+   {
+      I.address = ceu_to_reg64(address);
+   }
+}
+
+static inline void
+ceu_load_to(ceu_builder *b, ceu_index dest, ceu_index address, uint16_t mask,
+            int16_t offset)
+{
+   ceu_emit(b, LOAD_MULTIPLE, I)
+   {
+      I.base = ceu_to_reg_tuple(dest, util_bitcount(mask));
+      I.address = ceu_to_reg64(address);
+      I.mask = mask;
+      I.offset = offset;
+   }
+}
+
+static inline void
+ceu_store(ceu_builder *b, ceu_index data, ceu_index address, uint16_t mask,
+          int16_t offset)
+{
+   ceu_emit(b, STORE_MULTIPLE, I)
+   {
+      I.base = ceu_to_reg_tuple(data, util_bitcount(mask));
+      I.address = ceu_to_reg64(address);
+      I.mask = mask;
+      I.offset = offset;
+   }
+}
+
+/*
+ * Select which scoreboard entry will track endpoint tasks and other tasks
+ * respectively. Pass to ceu_wait to wait later.
+ */
+static inline void
+ceu_set_scoreboard_entry(ceu_builder *b, uint8_t ep, uint8_t other)
+{
+   assert(ep < 8 && "invalid slot");
+   assert(other < 8 && "invalid slot");
+
+   ceu_emit(b, SET_SB_ENTRY, I)
+   {
+      I.endpoint_entry = ep;
+      I.other_entry = other;
+   }
+}
+
+static inline void
+ceu_require_all(ceu_builder *b)
+{
+   ceu_emit(b, REQ_RESOURCE, I)
+   {
+      I.compute = true;
+      I.tiler = true;
+      I.idvs = true;
+      I.fragment = true;
+   }
+}
+
+static inline void
+ceu_require_compute(ceu_builder *b)
+{
+   ceu_emit(b, REQ_RESOURCE, I) I.compute = true;
+}
+
+static inline void
+ceu_require_fragment(ceu_builder *b)
+{
+   ceu_emit(b, REQ_RESOURCE, I) I.fragment = true;
+}
+
+static inline void
+ceu_require_idvs(ceu_builder *b)
+{
+   ceu_emit(b, REQ_RESOURCE, I)
+   {
+      I.compute = true;
+      I.tiler = true;
+      I.idvs = true;
+   }
+}
+
+static inline void
+ceu_heap_operation(ceu_builder *b, enum mali_ceu_heap_operation operation)
+{
+   ceu_emit(b, HEAP_OPERATION, I) I.operation = operation;
+}
+
+static inline void
+ceu_vt_start(ceu_builder *b)
+{
+   ceu_heap_operation(b, MALI_CEU_HEAP_OPERATION_VERTEX_TILER_STARTED);
+}
+
+static inline void
+ceu_vt_end(ceu_builder *b)
+{
+   ceu_heap_operation(b, MALI_CEU_HEAP_OPERATION_VERTEX_TILER_COMPLETED);
+}
+
+static inline void
+ceu_frag_end(ceu_builder *b)
+{
+   ceu_heap_operation(b, MALI_CEU_HEAP_OPERATION_FRAGMENT_COMPLETED);
+}
+
+static inline void
+ceu_flush_caches(ceu_builder *b, enum mali_ceu_flush_mode l2,
+                 enum mali_ceu_flush_mode lsc, bool other_inv,
+                 ceu_index flush_id, uint16_t scoreboard_mask,
+                 uint8_t signal_slot)
+{
+   ceu_emit(b, FLUSH_CACHE2, I)
+   {
+      I.l2_flush_mode = l2;
+      I.lsc_flush_mode = lsc;
+      I.other_invalidate = other_inv;
+      I.scoreboard_mask = scoreboard_mask;
+      I.latest_flush_id = ceu_to_reg32(flush_id);
+      I.scoreboard_entry = signal_slot;
+   }
+}
+
+/* Pseudoinstructions follow */
+
+static inline void
+ceu_move64_to(ceu_builder *b, ceu_index dest, uint64_t imm)
+{
+   if (imm < (1ull << 48)) {
+      /* Zero extends */
+      ceu_move48_to(b, dest, imm);
+   } else {
+      ceu_move32_to(b, ceu_extract32(b, dest, 0), imm);
+      ceu_move32_to(b, ceu_extract32(b, dest, 1), imm >> 32);
+   }
+}
+
+static inline void
+ceu_load32_to(ceu_builder *b, ceu_index dest, ceu_index address, int16_t offset)
+{
+   ceu_load_to(b, dest, address, BITFIELD_MASK(1), offset);
+}
+
+static inline void
+ceu_load64_to(ceu_builder *b, ceu_index dest, ceu_index address, int16_t offset)
+{
+   ceu_load_to(b, dest, address, BITFIELD_MASK(2), offset);
+}
+
+static inline void
+ceu_store32(ceu_builder *b, ceu_index data, ceu_index address, int16_t offset)
+{
+   ceu_store(b, data, address, BITFIELD_MASK(1), offset);
+}
+
+static inline void
+ceu_store64(ceu_builder *b, ceu_index data, ceu_index address, int16_t offset)
+{
+   ceu_store(b, data, address, BITFIELD_MASK(2), offset);
+}
+
+static inline void
+ceu_wait_slot(ceu_builder *b, uint8_t slot)
+{
+   assert(slot < 8 && "invalid slot");
+
+   ceu_wait_slots(b, BITFIELD_BIT(slot));
+}
+
+static inline void
+ceu_store_state(ceu_builder *b, uint8_t signal_slot, ceu_index address,
+		enum mali_ceu_state state, uint16_t wait_mask, int16_t offset)
+{
+   ceu_emit(b, STORE_STATE, I)
+   {
+      I.offset = offset;
+      I.wait_mask = wait_mask;
+      I.state = state;
+      I.address = ceu_to_reg64(address);
+      I.scoreboard_slot = signal_slot;
+   }
+}
+
+#endif
+#endif
diff -up mesa-23.2.0-rc3/src/panfrost/lib/genxml/decode.c.8~ mesa-23.2.0-rc3/src/panfrost/lib/genxml/decode.c
--- mesa-23.2.0-rc3/src/panfrost/lib/genxml/decode.c.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/panfrost/lib/genxml/decode.c	2023-09-08 01:00:47.832430410 +0200
@@ -330,6 +330,72 @@ GENX(pandecode_tiler)(mali_ptr gpu_va, u
    }
 
    DUMP_UNPACKED(TILER_CONTEXT, t, "Tiler Context @%" PRIx64 ":\n", gpu_va);
+
+#if PAN_ARCH >= 10
+   if (t.heap) {
+      mali_ptr heap = t.heap;
+      mali_ptr heap_prefix_ptr = heap - 0x10000;
+      const uint32_t *PANDECODE_PTR_VAR(heap_prefix, heap_prefix_ptr);
+      bool repeat_pattern_printed = false;
+
+      pandecode_log("Position FIFO %lx:\n", heap_prefix_ptr);
+      pandecode_indent++;
+      for (unsigned i = 0; i < 0x10000 / 4; i += 4) {
+         bool first = i == 0;
+         bool last = i == ((0x10000 / 4) - 4);
+
+         if (!first && !last &&
+             !memcmp(&heap_prefix[i], &heap_prefix[i - 4],
+                     sizeof(*heap_prefix) * 4)) {
+            if (!repeat_pattern_printed) {
+               pandecode_log("*\n");
+               repeat_pattern_printed = true;
+            }
+         } else {
+            pandecode_log("0x%x: %08x %08x %08x %08x\n", i * 4, heap_prefix[i],
+                          heap_prefix[i + 1], heap_prefix[i + 2],
+                          heap_prefix[i + 3]);
+            repeat_pattern_printed = false;
+         }
+      }
+      pandecode_indent--;
+
+      mali_ptr heap_postfix_ptr = heap + 0x40;
+      const uint32_t *PANDECODE_PTR_VAR(heap_postfix, heap_postfix_ptr);
+      pandecode_log("Pointer Array %lx:\n", heap_postfix_ptr);
+      pandecode_indent++;
+      repeat_pattern_printed = false;
+      for (unsigned i = 0; i < (0x2000 - 0x40) / 4; i += 4) {
+         bool first = i == 0;
+         bool last = i == (((0x2000 - 0x40) / 4) - 4);
+
+         if (!first && !last &&
+             !memcmp(&heap_postfix[i], &heap_postfix[i - 4],
+                     sizeof(*heap_postfix) * 4)) {
+            if (!repeat_pattern_printed) {
+               pandecode_log("*\n");
+               repeat_pattern_printed = true;
+            }
+         } else {
+            pandecode_log("0x%x: %08x %08x %08x %08x\n", i * 4, heap_postfix[i],
+                          heap_postfix[i + 1], heap_postfix[i + 2],
+                          heap_postfix[i + 3]);
+            repeat_pattern_printed = false;
+         }
+      }
+      pandecode_indent--;
+
+      //		pandecode_print_refs_to(heap_prefix_ptr);
+   }
+
+   // pandecode_print_refs_to(gpu_va + 0x40);
+/*
+        uint32_t *words = PANDECODE_PTR(gpu_va, void) + 64;
+        mprotect((void*)((uintptr_t)words & ~4095ull), 4096, PROT_READ |
+   PROT_WRITE); for (unsigned i = 0; i < 16; i++) words[i] = 0;
+        mprotect((void*)((uintptr_t)words & ~4095ull), 4096, PROT_READ);
+*/
+#endif
 }
 #endif
 
diff -up mesa-23.2.0-rc3/src/panfrost/lib/genxml/decode.h.8~ mesa-23.2.0-rc3/src/panfrost/lib/genxml/decode.h
--- mesa-23.2.0-rc3/src/panfrost/lib/genxml/decode.h.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/panfrost/lib/genxml/decode.h	2023-09-08 01:00:47.832430410 +0200
@@ -87,6 +87,7 @@ __pandecode_fetch_gpu_mem(uint64_t gpu_v
    name = __pandecode_fetch_gpu_mem(gpu_va, sizeof(*name), __LINE__, __FILE__)
 
 void pandecode_validate_buffer(mali_ptr addr, size_t sz);
+void pandecode_print_refs_to(mali_ptr addr);
 
 /* Forward declare for all supported gens to permit thunking */
 void pandecode_jc_v4(mali_ptr jc_gpu_va, unsigned gpu_id);
diff -up mesa-23.2.0-rc3/src/panfrost/lib/genxml/decode_common.c.8~ mesa-23.2.0-rc3/src/panfrost/lib/genxml/decode_common.c
--- mesa-23.2.0-rc3/src/panfrost/lib/genxml/decode_common.c.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/panfrost/lib/genxml/decode_common.c	2023-09-08 01:00:47.832430410 +0200
@@ -144,6 +144,48 @@ pandecode_validate_buffer(mali_ptr addr,
 }
 
 void
+pandecode_print_refs_to(mali_ptr addr)
+{
+   if (!addr)
+      return;
+
+   uint64_t unused_lo = ffs(addr);
+   uint64_t mask = 0xffffffffffff >> unused_lo;
+
+   pandecode_log("searching refs to %lx\n", addr);
+   rb_tree_foreach(struct pandecode_mapped_memory, mem, &mmap_tree, node) {
+      if (!mem->addr)
+         continue;
+
+      if (mem->gpu_va == 0x7fdfffe21000 || mem->gpu_va == 0x7fdffdc00000 ||
+          mem->gpu_va == 0x7fdffbc00000 || mem->gpu_va == 0x7fdff9c00000 ||
+          mem->gpu_va == 0x7fdff7c00000)
+         continue;
+
+      uint64_t *ptr = mem->addr;
+      for (uint64_t i = 0; i < mem->length; i += 8) {
+         if (ptr[i / 8] == addr) {
+            pandecode_log("%lx referenced at %lx\n", addr, mem->gpu_va + i);
+            break;
+         }
+
+         uint64_t test_mask = mask;
+         uint64_t test_addr = addr >> unused_lo;
+
+         do {
+            if ((ptr[i / 8] & test_mask) == test_addr) {
+               pandecode_log("%lx referenced at %lx (value %lx)\n", addr,
+                             mem->gpu_va + i, ptr[i / 8]);
+               break;
+            }
+            test_addr <<= 1;
+            test_mask <<= 1;
+         } while (!(test_mask & 0x8000000000000000ull));
+      }
+   }
+}
+
+void
 pandecode_map_read_write(void)
 {
    simple_mtx_assert_locked(&pandecode_lock);
diff -up mesa-23.2.0-rc3/src/panfrost/lib/genxml/decode_csf.c.8~ mesa-23.2.0-rc3/src/panfrost/lib/genxml/decode_csf.c
--- mesa-23.2.0-rc3/src/panfrost/lib/genxml/decode_csf.c.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/panfrost/lib/genxml/decode_csf.c	2023-09-08 01:00:47.832430410 +0200
@@ -249,7 +249,7 @@ pandecode_run_fragment(struct queue_ctx
    DUMP_CL(SCISSOR, &ctx->regs[42], "Scissor\n");
 
    /* TODO: Tile enable map */
-   GENX(pandecode_fbd)(cs_get_u64(ctx, 40), true, ctx->gpu_id);
+   GENX(pandecode_fbd)(cs_get_u64(ctx, 40) & ~0x3full, true, ctx->gpu_id);
 
    pandecode_indent--;
 }
@@ -698,12 +698,7 @@ interpret_ceu_instr(struct queue_ctx *ct
    }
 
    case MALI_CEU_OPCODE_JUMP: {
-      pan_unpack(bytes, CEU_CALL, I);
-
-      if (ctx->call_stack_depth == 0) {
-         fprintf(stderr, "Cannot jump from the entrypoint\n");
-         return false;
-      }
+      pan_unpack(bytes, CEU_JUMP, I);
 
       return interpret_ceu_jump(ctx, I.address, I.length);
    }
diff -up mesa-23.2.0-rc3/src/panfrost/lib/meson.build.8~ mesa-23.2.0-rc3/src/panfrost/lib/meson.build
--- mesa-23.2.0-rc3/src/panfrost/lib/meson.build.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/panfrost/lib/meson.build	2023-09-08 01:00:47.832430410 +0200
@@ -39,7 +39,7 @@ endforeach
 
 libpanfrost_per_arch = []
 
-foreach ver : ['4', '5', '6', '7', '9']
+foreach ver : ['4', '5', '6', '7', '9', '10']
   libpanfrost_per_arch += static_library(
     'pan-arch-v' + ver,
     [
diff -up mesa-23.2.0-rc3/src/panfrost/lib/pan_bo.c.8~ mesa-23.2.0-rc3/src/panfrost/lib/pan_bo.c
--- mesa-23.2.0-rc3/src/panfrost/lib/pan_bo.c.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/panfrost/lib/pan_bo.c	2023-09-08 01:00:47.833430421 +0200
@@ -28,6 +28,7 @@
 #include <pthread.h>
 #include <stdio.h>
 #include <xf86drm.h>
+#include "drm-uapi/pancsf_drm.h"
 #include "drm-uapi/panfrost_drm.h"
 
 #include "pan_bo.h"
@@ -36,6 +37,7 @@
 #include "wrap.h"
 
 #include "util/os_mman.h"
+#include "util/os_time.h"
 
 #include "util/u_inlines.h"
 #include "util/u_math.h"
@@ -59,30 +61,92 @@ static struct panfrost_bo *
 panfrost_bo_alloc(struct panfrost_device *dev, size_t size, uint32_t flags,
                   const char *label)
 {
-   struct drm_panfrost_create_bo create_bo = {.size = size};
    struct panfrost_bo *bo;
+   uint32_t bo_handle;
+   uint32_t sync_handle = 0;
+   uint64_t gpu_va;
    int ret;
 
-   if (dev->kernel_version->version_major > 1 ||
-       dev->kernel_version->version_minor >= 1) {
-      if (flags & PAN_BO_GROWABLE)
-         create_bo.flags |= PANFROST_BO_HEAP;
+   if (dev->arch < 10) {
+      struct drm_panfrost_create_bo create_bo = {.size = size};
+
+      if (dev->kernel_version->version_major > 1 ||
+          dev->kernel_version->version_minor >= 1) {
+         if (flags & PAN_BO_GROWABLE)
+            create_bo.flags |= PANFROST_BO_HEAP;
+         if (!(flags & PAN_BO_EXECUTE))
+            create_bo.flags |= PANFROST_BO_NOEXEC;
+      }
+
+      ret = drmIoctl(dev->fd, DRM_IOCTL_PANFROST_CREATE_BO, &create_bo);
+      if (ret) {
+         fprintf(stderr, "DRM_IOCTL_PANFROST_CREATE_BO failed: %m\n");
+         return NULL;
+      }
+
+      bo_handle = create_bo.handle;
+      gpu_va = create_bo.offset;
+      size = create_bo.size;
+   } else {
+      struct drm_pancsf_bo_create boc = {
+         .size = size,
+         .vm_id = (flags & PAN_BO_ACCESS_SHARED) ? dev->vm_id : 0,
+      };
+
+      ret = drmIoctl(dev->fd, DRM_IOCTL_PANCSF_BO_CREATE, &boc);
+      if (ret) {
+         fprintf(stderr, "DRM_IOCTL_PANCSF_BO_CREATE failed: %m\n");
+         return NULL;
+      }
+
+      bo_handle = boc.handle;
+      size = boc.size;
+
+      ret = drmSyncobjCreate(dev->fd, 0, &sync_handle);
+      if (ret) {
+         fprintf(stderr, "drmSyncobjCreate failed: %s\n", strerror(-ret));
+         drmCloseBufferHandle(dev->fd, bo_handle);
+         return NULL;
+      }
+
+      struct drm_pancsf_vm_map map = {
+         .vm_id = dev->vm_id,
+         .bo_handle = bo_handle,
+         .bo_offset = 0,
+         .size = size,
+      };
+
+      /* Align on 2MB for bufs > 2MB. */
+      map.va = util_vma_heap_alloc(&dev->vma_heap, size,
+                                   size > 0x200000 ? 0x200000 : 0x1000);
+      assert(map.va);
+
       if (!(flags & PAN_BO_EXECUTE))
-         create_bo.flags |= PANFROST_BO_NOEXEC;
-   }
+         map.flags |= PANCSF_VMA_MAP_NOEXEC;
+      else
+         map.flags |= PANCSF_VMA_MAP_READONLY;
+
+      if (flags & PAN_BO_GPU_UNCACHED)
+         map.flags |= PANCSF_VMA_MAP_UNCACHED;
+
+      ret = drmIoctl(dev->fd, DRM_IOCTL_PANCSF_VM_MAP, &map);
+      if (ret) {
+         drmSyncobjDestroy(dev->fd, sync_handle);
+         drmCloseBufferHandle(dev->fd, bo_handle);
+         fprintf(stderr, "DRM_IOCTL_PANCSF_VM_MAP failed: %m\n");
+         return NULL;
+      }
 
-   ret = drmIoctl(dev->fd, DRM_IOCTL_PANFROST_CREATE_BO, &create_bo);
-   if (ret) {
-      fprintf(stderr, "DRM_IOCTL_PANFROST_CREATE_BO failed: %m\n");
-      return NULL;
+      gpu_va = map.va;
    }
 
-   bo = pan_lookup_bo(dev, create_bo.handle);
+   bo = pan_lookup_bo(dev, bo_handle);
    assert(!memcmp(bo, &((struct panfrost_bo){}), sizeof(*bo)));
 
-   bo->size = create_bo.size;
-   bo->ptr.gpu = create_bo.offset;
-   bo->gem_handle = create_bo.handle;
+   bo->size = size;
+   bo->ptr.gpu = gpu_va;
+   bo->gem_handle = bo_handle;
+   bo->sync.handle = sync_handle;
    bo->flags = flags;
    bo->dev = dev;
    bo->label = label;
@@ -93,9 +157,32 @@ static void
 panfrost_bo_free(struct panfrost_bo *bo)
 {
    struct drm_gem_close gem_close = {.handle = bo->gem_handle};
+   struct panfrost_device *dev = bo->dev;
    int fd = bo->dev->fd;
    int ret;
 
+   if (bo->dev->arch >= 10) {
+      if (bo->sync.handle && bo->sync.point) {
+         ret = drmSyncobjTimelineWait(bo->dev->fd, &bo->sync.handle,
+                                      &bo->sync.point, 1, INT64_MAX,
+                                      DRM_SYNCOBJ_WAIT_FLAGS_WAIT_ALL, NULL);
+      }
+
+      struct drm_pancsf_vm_unmap unmap = {
+         .vm_id = bo->dev->vm_id,
+         .va = bo->ptr.gpu,
+         .size = bo->size,
+      };
+
+      ret = drmIoctl(dev->fd, DRM_IOCTL_PANCSF_VM_UNMAP, &unmap);
+      assert(!ret);
+
+      util_vma_heap_free(&dev->vma_heap, bo->ptr.gpu, bo->size);
+
+      drmSyncobjDestroy(dev->fd, bo->sync.handle);
+
+   }
+
    /* BO will be freed with the sparse array, but zero to indicate free */
    memset(bo, 0, sizeof(*bo));
 
@@ -106,6 +193,100 @@ panfrost_bo_free(struct panfrost_bo *bo)
    }
 }
 
+struct dma_buf_export_sync_file {
+   __u32 flags;
+   __s32 fd;
+};
+
+struct dma_buf_import_sync_file {
+   __u32 flags;
+   __s32 fd;
+};
+
+#define DMA_BUF_SYNC_READ  (1 << 0)
+#define DMA_BUF_SYNC_WRITE (2 << 0)
+
+#define DMA_BUF_BASE 'b'
+#define DMA_BUF_IOCTL_EXPORT_SYNC_FILE                                         \
+   _IOWR(DMA_BUF_BASE, 2, struct dma_buf_export_sync_file)
+#define DMA_BUF_IOCTL_IMPORT_SYNC_FILE                                         \
+   _IOW(DMA_BUF_BASE, 3, struct dma_buf_import_sync_file)
+
+int
+panfrost_shared_bo_import_sync_handle(struct panfrost_bo *bo, uint32_t flags)
+{
+   if (!(bo->flags & PAN_BO_SHARED)) {
+      return -EINVAL;
+   }
+
+   int dmabuf_fd;
+   int ret = drmPrimeHandleToFD(bo->dev->fd, bo->gem_handle,
+                                DRM_CLOEXEC | DRM_RDWR, &dmabuf_fd);
+   if (ret) {
+      return ret;
+   }
+
+   struct dma_buf_export_sync_file esync = {
+      .flags =
+         (flags & PAN_BO_ACCESS_WRITE) ? DMA_BUF_SYNC_WRITE : DMA_BUF_SYNC_READ,
+   };
+
+   ret = drmIoctl(dmabuf_fd, DMA_BUF_IOCTL_EXPORT_SYNC_FILE, &esync);
+   if (ret) {
+      goto out_close_dmabuf_fd;
+   }
+
+   ret = drmSyncobjImportSyncFile(bo->dev->fd, bo->sync.handle, esync.fd);
+   if (ret) {
+      goto out_close_sync_fd;
+   }
+
+   bo->sync.point = 0;
+
+out_close_sync_fd:
+   close(esync.fd);
+out_close_dmabuf_fd:
+   close(dmabuf_fd);
+   return ret;
+}
+
+int
+panfrost_shared_bo_attach_sync_handle(struct panfrost_bo *bo, uint32_t handle,
+                                      uint32_t flags)
+{
+   if (!(bo->flags & PAN_BO_SHARED))
+      return -EINVAL;
+
+   int dmabuf_fd;
+   int ret = drmPrimeHandleToFD(bo->dev->fd, bo->gem_handle,
+                                DRM_CLOEXEC | DRM_RDWR, &dmabuf_fd);
+   if (ret) {
+      return ret;
+   }
+
+   int sync_fd;
+   ret = drmSyncobjExportSyncFile(bo->dev->fd, handle, &sync_fd);
+   if (ret) {
+      goto out_close_dmabuf_fd;
+   }
+
+   struct dma_buf_import_sync_file isync = {
+      .flags =
+         (flags & PAN_BO_ACCESS_WRITE) ? DMA_BUF_SYNC_WRITE : DMA_BUF_SYNC_READ,
+      .fd = sync_fd,
+   };
+   ret = drmIoctl(dmabuf_fd, DMA_BUF_IOCTL_IMPORT_SYNC_FILE, &isync);
+   if (ret) {
+      goto out_close_sync_fd;
+   }
+
+out_close_sync_fd:
+   close(sync_fd);
+out_close_dmabuf_fd:
+   close(dmabuf_fd);
+   return ret;
+}
+
 /* Returns true if the BO is ready, false otherwise.
  * access_type is encoding the type of access one wants to ensure is done.
  * Waiting is always done for writers, but if wait_readers is set then readers
@@ -114,10 +295,6 @@ panfrost_bo_free(struct panfrost_bo *bo)
 bool
 panfrost_bo_wait(struct panfrost_bo *bo, int64_t timeout_ns, bool wait_readers)
 {
-   struct drm_panfrost_wait_bo req = {
-      .handle = bo->gem_handle,
-      .timeout_ns = timeout_ns,
-   };
    int ret;
 
    /* If the BO has been exported or imported we can't rely on the cached
@@ -138,8 +315,35 @@ panfrost_bo_wait(struct panfrost_bo *bo,
    /* The ioctl returns >= 0 value when the BO we are waiting for is ready
     * -1 otherwise.
     */
-   ret = drmIoctl(bo->dev->fd, DRM_IOCTL_PANFROST_WAIT_BO, &req);
-   if (ret != -1) {
+   if (bo->dev->arch < 10) {
+      struct drm_panfrost_wait_bo req = {
+         .handle = bo->gem_handle,
+         .timeout_ns = timeout_ns,
+      };
+
+      ret = drmIoctl(bo->dev->fd, DRM_IOCTL_PANFROST_WAIT_BO, &req);
+      if (ret == -1)
+         ret = -errno;
+   } else {
+      int64_t abs_timeout_ns;
+
+      if (timeout_ns < INT64_MAX - os_time_get_nano())
+         abs_timeout_ns = timeout_ns + os_time_get_nano();
+      else
+         abs_timeout_ns = INT64_MAX;
+
+      if (bo->flags & PAN_BO_SHARED) {
+         ret = panfrost_shared_bo_import_sync_handle(bo,
+                                                     bo->gpu_access & PAN_BO_ACCESS_WRITE);
+         assert(!ret);
+      }
+
+      ret = drmSyncobjTimelineWait(bo->dev->fd, &bo->sync.handle,
+                                   &bo->sync.point, 1, abs_timeout_ns,
+                                   DRM_SYNCOBJ_WAIT_FLAGS_WAIT_ALL, NULL);
+   }
+
+   if (ret >= 0) {
       /* Set gpu_access to 0 so that the next call to bo_wait()
        * doesn't have to call the WAIT_BO ioctl.
        */
@@ -150,7 +354,7 @@ panfrost_bo_wait(struct panfrost_bo *bo,
    /* If errno is not ETIMEDOUT or EBUSY that means the handle we passed
     * is invalid, which shouldn't happen here.
     */
-   assert(errno == ETIMEDOUT || errno == EBUSY);
+   assert(ret == -ETIMEDOUT || ret == -EBUSY || ret == -ETIME);
    return false;
 }
 
@@ -198,24 +402,32 @@ panfrost_bo_cache_fetch(struct panfrost_
 
       /* If the oldest BO in the cache is busy, likely so is
        * everything newer, so bail. */
-      if (!panfrost_bo_wait(entry, dontwait ? 0 : INT64_MAX, PAN_BO_ACCESS_RW))
+      if (!panfrost_bo_wait(entry, dontwait ? 0 : INT64_MAX, true))
          break;
 
-      struct drm_panfrost_madvise madv = {
-         .handle = entry->gem_handle,
-         .madv = PANFROST_MADV_WILLNEED,
-      };
-      int ret;
-
       /* This one works, splice it out of the cache */
       list_del(&entry->bucket_link);
       list_del(&entry->lru_link);
 
-      ret = drmIoctl(dev->fd, DRM_IOCTL_PANFROST_MADVISE, &madv);
-      if (!ret && !madv.retained) {
+      bool retained = true;
+
+      if (dev->arch < 10) {
+         struct drm_panfrost_madvise madv = {
+            .handle = entry->gem_handle,
+            .madv = PANFROST_MADV_WILLNEED,
+         };
+         int ret;
+
+         ret = drmIoctl(dev->fd, DRM_IOCTL_PANFROST_MADVISE, &madv);
+         if (!ret && !madv.retained)
+            retained = false;
+      }
+
+      if (!retained) {
          panfrost_bo_free(entry);
          continue;
       }
+
       /* Let's go! */
       bo = entry;
       bo->label = label;
@@ -266,14 +478,16 @@ panfrost_bo_cache_put(struct panfrost_bo
    pthread_mutex_lock(&dev->bo_cache.lock);
 
    struct list_head *bucket = pan_bucket(dev, MAX2(bo->size, 4096));
-   struct drm_panfrost_madvise madv;
    struct timespec time;
 
-   madv.handle = bo->gem_handle;
-   madv.madv = PANFROST_MADV_DONTNEED;
-   madv.retained = 0;
+   if (dev->arch < 10) {
+      struct drm_panfrost_madvise madv = {
+         .handle = bo->gem_handle,
+         .madv = PANFROST_MADV_DONTNEED,
+      };
 
-   drmIoctl(dev->fd, DRM_IOCTL_PANFROST_MADVISE, &madv);
+      drmIoctl(dev->fd, DRM_IOCTL_PANFROST_MADVISE, &madv);
+   }
 
    /* Add us to the bucket */
    list_addtail(&bo->bucket_link, bucket);
@@ -321,34 +535,47 @@ panfrost_bo_cache_evict_all(struct panfr
 void
 panfrost_bo_mmap(struct panfrost_bo *bo)
 {
-   struct drm_panfrost_mmap_bo mmap_bo = {.handle = bo->gem_handle};
+   off_t offset;
    int ret;
 
    if (bo->ptr.cpu)
       return;
 
-   ret = drmIoctl(bo->dev->fd, DRM_IOCTL_PANFROST_MMAP_BO, &mmap_bo);
-   if (ret) {
-      fprintf(stderr, "DRM_IOCTL_PANFROST_MMAP_BO failed: %m\n");
-      assert(0);
+   if (bo->dev->arch < 10) {
+      struct drm_panfrost_mmap_bo mmap_bo = {.handle = bo->gem_handle};
+
+      ret = drmIoctl(bo->dev->fd, DRM_IOCTL_PANFROST_MMAP_BO, &mmap_bo);
+      if (ret) {
+         fprintf(stderr, "DRM_IOCTL_PANFROST_MMAP_BO failed: %m\n");
+         assert(0);
+      }
+      offset = mmap_bo.offset;
+   } else {
+      struct drm_pancsf_bo_mmap_offset mmapoffs = {.handle = bo->gem_handle};
+
+      ret = drmIoctl(bo->dev->fd, DRM_IOCTL_PANCSF_BO_MMAP_OFFSET, &mmapoffs);
+      if (ret) {
+         fprintf(stderr, "DRM_IOCTL_PANCSF_BO_GET_OFFSET failed: %m\n");
+         assert(0);
+      }
+      offset = mmapoffs.offset;
    }
 
    bo->ptr.cpu = os_mmap(NULL, bo->size, PROT_READ | PROT_WRITE, MAP_SHARED,
-                         bo->dev->fd, mmap_bo.offset);
+                         bo->dev->fd, offset);
    if (bo->ptr.cpu == MAP_FAILED) {
       bo->ptr.cpu = NULL;
       fprintf(stderr,
               "mmap failed: result=%p size=0x%llx fd=%i offset=0x%llx %m\n",
-              bo->ptr.cpu, (long long)bo->size, bo->dev->fd,
-              (long long)mmap_bo.offset);
+              bo->ptr.cpu, (long long)bo->size, bo->dev->fd, (long long)offset);
    }
 }
 
 static void
 panfrost_bo_munmap(struct panfrost_bo *bo)
 {
-   if (!bo->ptr.cpu)
-      return;
+   //        if (!bo->ptr.cpu)
+   return;
 
    if (os_munmap((void *)(uintptr_t)bo->ptr.cpu, bo->size)) {
       perror("munmap");
@@ -459,9 +686,6 @@ struct panfrost_bo *
 panfrost_bo_import(struct panfrost_device *dev, int fd)
 {
    struct panfrost_bo *bo;
-   struct drm_panfrost_get_bo_offset get_bo_offset = {
-      0,
-   };
    ASSERTED int ret;
    unsigned gem_handle;
 
@@ -473,13 +697,37 @@ panfrost_bo_import(struct panfrost_devic
    bo = pan_lookup_bo(dev, gem_handle);
 
    if (!bo->dev) {
-      get_bo_offset.handle = gem_handle;
-      ret = drmIoctl(dev->fd, DRM_IOCTL_PANFROST_GET_BO_OFFSET, &get_bo_offset);
-      assert(!ret);
+      bo->size = lseek(fd, 0, SEEK_END);
+
+      if (dev->arch < 10) {
+         struct drm_panfrost_get_bo_offset get_bo_offset = {.handle =
+                                                               gem_handle};
+
+         ret =
+            drmIoctl(dev->fd, DRM_IOCTL_PANFROST_GET_BO_OFFSET, &get_bo_offset);
+         assert(!ret);
+         bo->ptr.gpu = (mali_ptr)get_bo_offset.offset;
+      } else {
+         struct drm_pancsf_vm_map map = {
+            .vm_id = dev->vm_id,
+            .bo_handle = gem_handle,
+            .bo_offset = 0,
+            .size = bo->size,
+         };
+
+         map.va = util_vma_heap_alloc(&dev->vma_heap, bo->size,
+                                      bo->size > 0x200000 ? 0x200000 : 0x1000);
+         assert(map.va);
+
+         ret = drmIoctl(dev->fd, DRM_IOCTL_PANCSF_VM_MAP, &map);
+         assert(!ret);
+
+         bo->ptr.gpu = map.va;
+         ret = drmSyncobjCreate(dev->fd, 0, &bo->sync.handle);
+         assert(!ret);
+      }
 
       bo->dev = dev;
-      bo->ptr.gpu = (mali_ptr)get_bo_offset.offset;
-      bo->size = lseek(fd, 0, SEEK_END);
       /* Sometimes this can fail and return -1. size of -1 is not
        * a nice thing for mmap to try mmap. Be more robust also
        * for zero sized maps and fail nicely too
diff -up mesa-23.2.0-rc3/src/panfrost/lib/pan_bo.h.8~ mesa-23.2.0-rc3/src/panfrost/lib/pan_bo.h
--- mesa-23.2.0-rc3/src/panfrost/lib/pan_bo.h.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/panfrost/lib/pan_bo.h	2023-09-08 01:00:47.833430421 +0200
@@ -50,6 +50,8 @@
  * cached locally */
 #define PAN_BO_SHARED (1 << 4)
 
+#define PAN_BO_GPU_UNCACHED (1 << 5)
+
 /* GPU access flags */
 
 /* BO is either shared (can be accessed by more than one GPU batch) or private
@@ -115,6 +117,12 @@ struct panfrost_bo {
 
    /* Human readable description of the BO for debugging. */
    const char *label;
+
+   /* Syncobject to wait on before re-using the buffer. */
+   struct {
+      uint32_t handle;
+      uint64_t point;
+   } sync;
 };
 
 bool panfrost_bo_wait(struct panfrost_bo *bo, int64_t timeout_ns,
@@ -127,5 +135,9 @@ void panfrost_bo_mmap(struct panfrost_bo
 struct panfrost_bo *panfrost_bo_import(struct panfrost_device *dev, int fd);
 int panfrost_bo_export(struct panfrost_bo *bo);
 void panfrost_bo_cache_evict_all(struct panfrost_device *dev);
+int panfrost_shared_bo_import_sync_handle(struct panfrost_bo *bo,
+                                          uint32_t flags);
+int panfrost_shared_bo_attach_sync_handle(struct panfrost_bo *bo,
+                                          uint32_t handle, uint32_t flags);
 
 #endif /* __PAN_BO_H__ */
diff -up mesa-23.2.0-rc3/src/panfrost/lib/pan_cs.c.8~ mesa-23.2.0-rc3/src/panfrost/lib/pan_cs.c
--- mesa-23.2.0-rc3/src/panfrost/lib/pan_cs.c.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/panfrost/lib/pan_cs.c	2023-09-08 01:00:47.833430421 +0200
@@ -442,7 +442,7 @@ pan_rt_init_format(const struct pan_imag
 
 #if PAN_ARCH >= 9
 enum mali_afbc_compression_mode
-pan_afbc_compression_mode(enum pipe_format format)
+GENX(pan_afbc_compression_mode)(enum pipe_format format)
 {
    /* There's a special case for texturing the stencil part from a combined
     * depth/stencil texture, handle it separately.
@@ -532,7 +532,7 @@ pan_prepare_rt(const struct pan_fb_info
       cfg->afbc.body_offset = surf.afbc.body - surf.afbc.header;
       assert(surf.afbc.body >= surf.afbc.header);
 
-      cfg->afbc.compression_mode = pan_afbc_compression_mode(rt->format);
+      cfg->afbc.compression_mode = GENX(pan_afbc_compression_mode)(rt->format);
       cfg->afbc.row_stride = row_stride;
 #else
       const struct pan_image_slice_layout *slice =
@@ -747,6 +747,7 @@ GENX(pan_emit_fbd)(const struct panfrost
 
       cfg.sample_locations =
          panfrost_sample_positions(dev, pan_sample_pattern(fb->nr_samples));
+      assert(cfg.sample_locations != 0);
       cfg.pre_frame_0 = pan_fix_frame_shader_mode(fb->bifrost.pre_post.modes[0],
                                                   force_clean_write);
       cfg.pre_frame_1 = pan_fix_frame_shader_mode(fb->bifrost.pre_post.modes[1],
@@ -754,7 +755,7 @@ GENX(pan_emit_fbd)(const struct panfrost
       cfg.post_frame = pan_fix_frame_shader_mode(fb->bifrost.pre_post.modes[2],
                                                  force_clean_write);
       cfg.frame_shader_dcds = fb->bifrost.pre_post.dcds.gpu;
-      cfg.tiler = tiler_ctx->bifrost;
+      cfg.tiler = tiler_ctx->bifrost.ctx;
 #endif
       cfg.width = fb->width;
       cfg.height = fb->height;
@@ -959,7 +960,7 @@ GENX(pan_emit_tiler_heap)(const struct p
    pan_pack(out, TILER_HEAP, heap) {
       heap.size = dev->tiler_heap->size;
       heap.base = dev->tiler_heap->ptr.gpu;
-      heap.bottom = dev->tiler_heap->ptr.gpu;
+      heap.bottom = dev->tiler_heap->ptr.gpu + 64;
       heap.top = dev->tiler_heap->ptr.gpu + dev->tiler_heap->size;
    }
 }
@@ -974,7 +975,7 @@ GENX(pan_emit_tiler_ctx)(const struct pa
 
    pan_pack(out, TILER_CONTEXT, tiler) {
       /* TODO: Select hierarchy mask more effectively */
-      tiler.hierarchy_mask = (max_levels >= 8) ? 0xFF : 0x28;
+      tiler.hierarchy_mask = (max_levels >= 8) ? 0xFE : 0x28;
 
       /* For large framebuffers, disable the smallest bin size to
        * avoid pathological tiler memory usage. Required to avoid OOM
@@ -987,14 +988,25 @@ GENX(pan_emit_tiler_ctx)(const struct pa
       tiler.fb_width = fb_width;
       tiler.fb_height = fb_height;
       tiler.heap = heap;
+      tiler.heap = heap;
       tiler.sample_pattern = pan_sample_pattern(nr_samples);
 #if PAN_ARCH >= 9
       tiler.first_provoking_vertex = first_provoking_vertex;
 #endif
+#if PAN_ARCH >= 10
+      /* Temporary geometry buffer is placed just before the HEAP
+       * descriptor and is 64KB large.
+       *
+       * Note: DDK assigns this pointer in the CS.
+       */
+#define POSITION_FIFO_SIZE (64 * 1024)
+      tiler.geometry_buffer = (tiler.heap - POSITION_FIFO_SIZE);
+#endif
    }
 }
 #endif
 
+#if PAN_ARCH <= 9
 void
 GENX(pan_emit_fragment_job)(const struct pan_fb_info *fb, mali_ptr fbd,
                             void *out)
@@ -1020,3 +1032,4 @@ GENX(pan_emit_fragment_job)(const struct
 #endif
    }
 }
+#endif
diff -up mesa-23.2.0-rc3/src/panfrost/lib/pan_cs.h.8~ mesa-23.2.0-rc3/src/panfrost/lib/pan_cs.h
--- mesa-23.2.0-rc3/src/panfrost/lib/pan_cs.h.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/panfrost/lib/pan_cs.h	2023-09-08 01:00:47.833430421 +0200
@@ -76,11 +76,14 @@ struct pan_tiler_context {
    uint32_t vertex_count;
 
    union {
-      mali_ptr bifrost;
       struct {
          bool disable;
          struct panfrost_bo *polygon_list;
       } midgard;
+      struct {
+         mali_ptr ctx;
+         mali_ptr heap;
+      } bifrost;
    };
 };
 
@@ -171,8 +174,11 @@ void GENX(pan_emit_tiler_ctx)(const stru
                               mali_ptr heap, void *out);
 #endif
 
+#if PAN_ARCH <= 9
 void GENX(pan_emit_fragment_job)(const struct pan_fb_info *fb, mali_ptr fbd,
                                  void *out);
+#endif
+
 #endif /* ifdef PAN_ARCH */
 
 #endif
diff -up mesa-23.2.0-rc3/src/panfrost/lib/pan_device.h.8~ mesa-23.2.0-rc3/src/panfrost/lib/pan_device.h
--- mesa-23.2.0-rc3/src/panfrost/lib/pan_device.h.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/panfrost/lib/pan_device.h	2023-09-08 01:00:47.833430421 +0200
@@ -36,6 +36,7 @@
 #include "util/list.h"
 #include "util/sparse_array.h"
 #include "util/u_dynarray.h"
+#include "util/vma.h"
 
 #include "panfrost/util/pan_ir.h"
 #include "pan_pool.h"
@@ -209,6 +210,13 @@ struct panfrost_device {
     * unconditionally on Bifrost, and useful for sharing with Midgard */
 
    struct panfrost_bo *sample_positions;
+
+   /* Default VM for pancsf devices. */
+   uint32_t vm_id;
+   struct util_vma_heap vma_heap;
+
+   /* FLUSH_ID register. */
+   uint32_t *flush_id;
 };
 
 void panfrost_open_device(void *memctx, int fd, struct panfrost_device *dev);
diff -up mesa-23.2.0-rc3/src/panfrost/lib/pan_props.c.8~ mesa-23.2.0-rc3/src/panfrost/lib/pan_props.c
--- mesa-23.2.0-rc3/src/panfrost/lib/pan_props.c.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/panfrost/lib/pan_props.c	2023-09-08 01:00:47.833430421 +0200
@@ -27,8 +27,10 @@
 #include <xf86drm.h>
 
 #include "drm-uapi/panfrost_drm.h"
+#include "drm-uapi/pancsf_drm.h"
 #include "util/hash_table.h"
 #include "util/macros.h"
+#include "util/os_mman.h"
 #include "util/u_math.h"
 #include "util/u_thread.h"
 #include "pan_bo.h"
@@ -70,6 +72,8 @@ const struct panfrost_model panfrost_mod
         MODEL(0x7212, "G52",    "TGOx", HAS_ANISO,         16384, {}),
         MODEL(0x7402, "G52 r1", "TGOx", HAS_ANISO,         16384, {}),
         MODEL(0x9093, "G57",    "TNAx", HAS_ANISO,         16384, {}),
+
+        MODEL(0xa867, "G610",   "TNAx", HAS_ANISO,         16384, {}), // TODO
 };
 /* clang-format on */
 
@@ -92,6 +96,16 @@ panfrost_get_model(uint32_t gpu_id)
    return NULL;
 }
 
+static bool is_pancsf_fd(int fd)
+{
+        drmVersionPtr version = drmGetVersion(fd);
+        assert(version);
+        bool is_pancsf = !strcmp(version->name, "pancsf");
+        drmFreeVersion(version);
+
+        return is_pancsf;
+}
+
 /* Abstraction over the raw drm_panfrost_get_param ioctl for fetching
  * information about devices */
 
@@ -99,20 +113,76 @@ static __u64
 panfrost_query_raw(int fd, enum drm_panfrost_param param, bool required,
                    unsigned default_value)
 {
-   struct drm_panfrost_get_param get_param = {
-      0,
-   };
    ASSERTED int ret;
 
-   get_param.param = param;
-   ret = drmIoctl(fd, DRM_IOCTL_PANFROST_GET_PARAM, &get_param);
+   if (is_pancsf_fd(fd)) {
+      struct drm_pancsf_gpu_info gpuinfo = {};
+      struct drm_pancsf_dev_query dq = {
+         .type = DRM_PANCSF_DEV_QUERY_GPU_INFO,
+         .size = sizeof(gpuinfo),
+         .pointer = (uint64_t)(uintptr_t)&gpuinfo,
+      };
+
+      ret = drmIoctl(fd, DRM_IOCTL_PANCSF_DEV_QUERY, &dq);
+      assert(!ret);
+
+      switch (param) {
+      case DRM_PANFROST_PARAM_GPU_PROD_ID:
+         return gpuinfo.gpu_id >> 16;
+      case DRM_PANFROST_PARAM_GPU_REVISION:
+         return gpuinfo.gpu_id & 0xffff;
+      case DRM_PANFROST_PARAM_SHADER_PRESENT:
+         return gpuinfo.shader_present;
+      case DRM_PANFROST_PARAM_TILER_PRESENT:
+         return gpuinfo.tiler_present;
+      case DRM_PANFROST_PARAM_L2_PRESENT:
+         return gpuinfo.l2_present;
+      case DRM_PANFROST_PARAM_AS_PRESENT:
+         return gpuinfo.as_present;
+      case DRM_PANFROST_PARAM_L2_FEATURES:
+         return gpuinfo.l2_features;
+      case DRM_PANFROST_PARAM_TILER_FEATURES:
+         return gpuinfo.tiler_features;
+      case DRM_PANFROST_PARAM_MEM_FEATURES:
+         return gpuinfo.mem_features;
+      case DRM_PANFROST_PARAM_MMU_FEATURES:
+         return gpuinfo.mmu_features;
+      case DRM_PANFROST_PARAM_THREAD_FEATURES:
+         return gpuinfo.thread_features;
+      case DRM_PANFROST_PARAM_MAX_THREADS:
+         return gpuinfo.thread_features;
+      case DRM_PANFROST_PARAM_THREAD_MAX_WORKGROUP_SZ:
+         return gpuinfo.thread_max_workgroup_size;
+      case DRM_PANFROST_PARAM_THREAD_MAX_BARRIER_SZ:
+         return gpuinfo.thread_max_barrier_size;
+      case DRM_PANFROST_PARAM_COHERENCY_FEATURES:
+         return gpuinfo.coherency_features;
+      case DRM_PANFROST_PARAM_TEXTURE_FEATURES0 ... DRM_PANFROST_PARAM_TEXTURE_FEATURES3:
+         return gpuinfo
+            .texture_features[param - DRM_PANFROST_PARAM_TEXTURE_FEATURES0];
+      case DRM_PANFROST_PARAM_NR_CORE_GROUPS:
+         return gpuinfo.core_group_count;
+      case DRM_PANFROST_PARAM_THREAD_TLS_ALLOC:
+      case DRM_PANFROST_PARAM_AFBC_FEATURES:
+      case DRM_PANFROST_PARAM_STACK_PRESENT:
+      case DRM_PANFROST_PARAM_JS_PRESENT:
+      case DRM_PANFROST_PARAM_CORE_FEATURES:
+      case DRM_PANFROST_PARAM_JS_FEATURES0 ... DRM_PANFROST_PARAM_JS_FEATURES15:
+      default:
+         assert(!required);
+         return default_value;
+      }
+   } else {
+      struct drm_panfrost_get_param get_param = {.param = param};
+
+      ret = drmIoctl(fd, DRM_IOCTL_PANFROST_GET_PARAM, &get_param);
+      if (ret) {
+         assert(!required);
+         return default_value;
+      }
 
-   if (ret) {
-      assert(!required);
-      return default_value;
+      return get_param.value;
    }
-
-   return get_param.value;
 }
 
 static unsigned
@@ -288,6 +358,24 @@ panfrost_open_device(void *memctx, int f
    if (dev->debug & (PAN_DBG_TRACE | PAN_DBG_SYNC))
       pandecode_initialize(!(dev->debug & PAN_DBG_TRACE));
 
+   if (dev->arch >= 10) {
+      struct drm_pancsf_vm_create vmc = {
+         .flags = 0,
+      };
+
+      int ret = drmIoctl(dev->fd, DRM_IOCTL_PANCSF_VM_CREATE, &vmc);
+
+      assert(!ret && vmc.id > 0);
+      dev->vm_id = vmc.id;
+
+      /* 4G range, with the top/bottom 32M reserved. */
+      util_vma_heap_init(&dev->vma_heap, 0x2000000, 0xfe000000);
+
+      dev->flush_id = os_mmap(0, 4096, PROT_READ, MAP_SHARED, dev->fd,
+			      DRM_PANCSF_USER_FLUSH_ID_MMIO_OFFSET);
+      assert(dev->flush_id != MAP_FAILED);
+   }
+
    /* Tiler heap is internally required by the tiler, which can only be
     * active for a single job chain at once, so a single heap can be
     * shared across batches/contextes */
@@ -318,4 +406,7 @@ panfrost_close_device(struct panfrost_de
 
    drmFreeVersion(dev->kernel_version);
    close(dev->fd);
+
+   if (dev->arch >= 10)
+      util_vma_heap_finish(&dev->vma_heap);
 }
diff -up mesa-23.2.0-rc3/src/panfrost/lib/pan_scoreboard.h.8~ mesa-23.2.0-rc3/src/panfrost/lib/pan_scoreboard.h
--- mesa-23.2.0-rc3/src/panfrost/lib/pan_scoreboard.h.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/panfrost/lib/pan_scoreboard.h	2023-09-08 01:00:47.833430421 +0200
@@ -54,7 +54,7 @@ struct pan_scoreboard {
    unsigned write_value_index;
 };
 
-#ifdef PAN_ARCH
+#if defined(PAN_ARCH) && PAN_ARCH <= 9
 /*
  * There are various types of Mali jobs:
  *
diff -up mesa-23.2.0-rc3/src/panfrost/lib/pan_texture.c.8~ mesa-23.2.0-rc3/src/panfrost/lib/pan_texture.c
--- mesa-23.2.0-rc3/src/panfrost/lib/pan_texture.c.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/panfrost/lib/pan_texture.c	2023-09-08 01:00:47.833430421 +0200
@@ -438,7 +438,7 @@ panfrost_emit_plane(const struct pan_ima
          cfg.afbc.ytr = (layout->modifier & AFBC_FORMAT_MOD_YTR);
          cfg.afbc.tiled_header = (layout->modifier & AFBC_FORMAT_MOD_TILED);
          cfg.afbc.prefetch = true;
-         cfg.afbc.compression_mode = pan_afbc_compression_mode(format);
+         cfg.afbc.compression_mode = GENX(pan_afbc_compression_mode)(format);
          cfg.afbc.header_stride = layout->slices[level].afbc.header_size;
       } else {
          cfg.plane_type = MALI_PLANE_TYPE_GENERIC;
diff -up mesa-23.2.0-rc3/src/panfrost/lib/pan_texture.h.8~ mesa-23.2.0-rc3/src/panfrost/lib/pan_texture.h
--- mesa-23.2.0-rc3/src/panfrost/lib/pan_texture.h.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/panfrost/lib/pan_texture.h	2023-09-08 01:00:47.833430421 +0200
@@ -260,7 +260,7 @@ void pan_iview_get_surface(const struct
 
 #if PAN_ARCH >= 9
 enum mali_afbc_compression_mode
-pan_afbc_compression_mode(enum pipe_format format);
+   GENX(pan_afbc_compression_mode)(enum pipe_format format);
 #endif
 
 #ifdef __cplusplus
diff -up mesa-23.2.0-rc3/src/panfrost/lib/wrap.h.8~ mesa-23.2.0-rc3/src/panfrost/lib/wrap.h
--- mesa-23.2.0-rc3/src/panfrost/lib/wrap.h.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/panfrost/lib/wrap.h	2023-09-08 01:00:47.833430421 +0200
@@ -60,4 +60,6 @@ void pandecode_cs(mali_ptr queue_gpu_va,
 
 void pandecode_abort_on_fault(uint64_t jc_gpu_va, unsigned gpu_id);
 
+void pandecode_dump_mappings(void);
+
 #endif /* __MMAP_TRACE_H__ */
diff -up mesa-23.2.0-rc3/src/panfrost/util/pan_ir.h.8~ mesa-23.2.0-rc3/src/panfrost/util/pan_ir.h
--- mesa-23.2.0-rc3/src/panfrost/util/pan_ir.h.8~	2023-09-05 18:48:17.000000000 +0200
+++ mesa-23.2.0-rc3/src/panfrost/util/pan_ir.h	2023-09-08 01:00:47.833430421 +0200
@@ -481,6 +481,9 @@ panfrost_max_thread_count(unsigned arch,
       return work_reg_count > 32 ? 384 : 768;
 
    /* Valhall (for completeness) */
+   case 10:
+      return 2048;
+
    default:
       return work_reg_count > 32 ? 512 : 1024;
    }
