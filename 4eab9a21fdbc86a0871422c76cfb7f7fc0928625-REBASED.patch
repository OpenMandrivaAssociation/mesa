diff -up mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_cs.c.16~ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_cs.c
--- mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_cs.c.16~	2023-07-29 15:51:45.093980518 +0100
+++ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_cs.c	2023-07-29 15:51:45.083980485 +0100
@@ -0,0 +1,67 @@
+/*
+ * Copyright (C) 2021 Collabora Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#include "compiler/shader_enums.h"
+#include "util/macros.h"
+
+#include "pan_cs.h"
+#include "pan_pool.h"
+
+#include "panvk_cs.h"
+#include "panvk_private.h"
+
+/*
+ * Upload the viewport scale. Defined as (px/2, py/2, pz) at the start of
+ * section 24.5 ("Controlling the Viewport") of the Vulkan spec. At the end of
+ * the section, the spec defines:
+ *
+ * px = width
+ * py = height
+ * pz = maxDepth - minDepth
+ */
+void
+panvk_sysval_upload_viewport_scale(const VkViewport *viewport,
+                                   union panvk_sysval_vec4 *data)
+{
+   data->f32[0] = 0.5f * viewport->width;
+   data->f32[1] = 0.5f * viewport->height;
+   data->f32[2] = (viewport->maxDepth - viewport->minDepth);
+}
+
+/*
+ * Upload the viewport offset. Defined as (ox, oy, oz) at the start of section
+ * 24.5 ("Controlling the Viewport") of the Vulkan spec. At the end of the
+ * section, the spec defines:
+ *
+ * ox = x + width/2
+ * oy = y + height/2
+ * oz = minDepth
+ */
+void
+panvk_sysval_upload_viewport_offset(const VkViewport *viewport,
+                                    union panvk_sysval_vec4 *data)
+{
+   data->f32[0] = (0.5f * viewport->width) + viewport->x;
+   data->f32[1] = (0.5f * viewport->height) + viewport->y;
+   data->f32[2] = viewport->minDepth;
+}
diff -up mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_descriptor_set.c.16~ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_descriptor_set.c
--- mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_descriptor_set.c.16~	2023-07-29 15:51:45.093980518 +0100
+++ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_descriptor_set.c	2023-07-29 15:51:45.083980485 +0100
@@ -0,0 +1,342 @@
+/*
+ * Copyright © 2021 Collabora Ltd.
+ *
+ * Derived from:
+ * Copyright © 2016 Red Hat.
+ * Copyright © 2016 Bas Nieuwenhuizen
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+#include "panvk_private.h"
+
+#include <assert.h>
+#include <fcntl.h>
+#include <stdbool.h>
+#include <string.h>
+#include <unistd.h>
+
+#include "util/mesa-sha1.h"
+#include "vk_descriptors.h"
+#include "vk_util.h"
+
+#include "pan_bo.h"
+
+/* FIXME: make sure those values are correct */
+#define PANVK_MAX_TEXTURES (1 << 16)
+#define PANVK_MAX_IMAGES   (1 << 8)
+#define PANVK_MAX_SAMPLERS (1 << 16)
+#define PANVK_MAX_UBOS     255
+
+void
+panvk_bifrost_GetDescriptorSetLayoutSupport(
+   VkDevice _device, const VkDescriptorSetLayoutCreateInfo *pCreateInfo,
+   VkDescriptorSetLayoutSupport *pSupport)
+{
+   VK_FROM_HANDLE(panvk_device, device, _device);
+
+   pSupport->supported = false;
+
+   VkDescriptorSetLayoutBinding *bindings;
+   VkResult result = vk_create_sorted_bindings(
+      pCreateInfo->pBindings, pCreateInfo->bindingCount, &bindings);
+   if (result != VK_SUCCESS) {
+      vk_error(device, result);
+      return;
+   }
+
+   unsigned sampler_idx = 0, tex_idx = 0, ubo_idx = 0;
+   unsigned img_idx = 0;
+   UNUSED unsigned dynoffset_idx = 0;
+
+   for (unsigned i = 0; i < pCreateInfo->bindingCount; i++) {
+      const VkDescriptorSetLayoutBinding *binding = &bindings[i];
+
+      switch (binding->descriptorType) {
+      case VK_DESCRIPTOR_TYPE_SAMPLER:
+         sampler_idx += binding->descriptorCount;
+         break;
+      case VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER:
+         sampler_idx += binding->descriptorCount;
+         tex_idx += binding->descriptorCount;
+         break;
+      case VK_DESCRIPTOR_TYPE_SAMPLED_IMAGE:
+      case VK_DESCRIPTOR_TYPE_INPUT_ATTACHMENT:
+      case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
+         tex_idx += binding->descriptorCount;
+         break;
+      case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC:
+         dynoffset_idx += binding->descriptorCount;
+         FALLTHROUGH;
+      case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER:
+         ubo_idx += binding->descriptorCount;
+         break;
+      case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER_DYNAMIC:
+         dynoffset_idx += binding->descriptorCount;
+         FALLTHROUGH;
+      case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER:
+         break;
+      case VK_DESCRIPTOR_TYPE_STORAGE_IMAGE:
+      case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
+         img_idx += binding->descriptorCount;
+         break;
+      default:
+         unreachable("Invalid descriptor type");
+      }
+   }
+
+   /* The maximum values apply to all sets attached to a pipeline since all
+    * sets descriptors have to be merged in a single array.
+    */
+   if (tex_idx > PANVK_MAX_TEXTURES / MAX_SETS ||
+       sampler_idx > PANVK_MAX_SAMPLERS / MAX_SETS ||
+       ubo_idx > PANVK_MAX_UBOS / MAX_SETS ||
+       img_idx > PANVK_MAX_IMAGES / MAX_SETS)
+      return;
+
+   pSupport->supported = true;
+}
+
+/*
+ * Pipeline layouts.  These have nothing to do with the pipeline.  They are
+ * just multiple descriptor set layouts pasted together.
+ */
+
+VkResult
+panvk_bifrost_CreatePipelineLayout(VkDevice _device,
+                                   const VkPipelineLayoutCreateInfo *pCreateInfo,
+                                   const VkAllocationCallbacks *pAllocator,
+                                   VkPipelineLayout *pPipelineLayout)
+{
+   VK_FROM_HANDLE(panvk_device, device, _device);
+   struct panvk_pipeline_layout *layout;
+   struct mesa_sha1 ctx;
+
+   layout =
+      vk_pipeline_layout_zalloc(&device->vk, sizeof(*layout), pCreateInfo);
+   if (layout == NULL)
+      return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
+
+   _mesa_sha1_init(&ctx);
+
+   unsigned sampler_idx = 0, tex_idx = 0, ubo_idx = 0;
+   unsigned dyn_ubo_idx = 0, dyn_ssbo_idx = 0, img_idx = 0;
+   for (unsigned set = 0; set < pCreateInfo->setLayoutCount; set++) {
+      const struct panvk_descriptor_set_layout *set_layout =
+         vk_to_panvk_descriptor_set_layout(layout->vk.set_layouts[set]);
+
+      layout->sets[set].sampler_offset = sampler_idx;
+      layout->sets[set].tex_offset = tex_idx;
+      layout->sets[set].ubo_offset = ubo_idx;
+      layout->sets[set].dyn_ubo_offset = dyn_ubo_idx;
+      layout->sets[set].dyn_ssbo_offset = dyn_ssbo_idx;
+      layout->sets[set].img_offset = img_idx;
+      sampler_idx += set_layout->num_samplers;
+      tex_idx += set_layout->num_textures;
+      ubo_idx += set_layout->num_ubos;
+      dyn_ubo_idx += set_layout->num_dyn_ubos;
+      dyn_ssbo_idx += set_layout->num_dyn_ssbos;
+      img_idx += set_layout->num_imgs;
+
+      for (unsigned b = 0; b < set_layout->binding_count; b++) {
+         const struct panvk_descriptor_set_binding_layout *binding_layout =
+            &set_layout->bindings[b];
+
+         if (binding_layout->immutable_samplers) {
+            for (unsigned s = 0; s < binding_layout->array_size; s++) {
+               struct panvk_sampler *sampler =
+                  binding_layout->immutable_samplers[s];
+
+               _mesa_sha1_update(&ctx, &sampler->desc, sizeof(sampler->desc));
+            }
+         }
+         _mesa_sha1_update(&ctx, &binding_layout->type,
+                           sizeof(binding_layout->type));
+         _mesa_sha1_update(&ctx, &binding_layout->array_size,
+                           sizeof(binding_layout->array_size));
+         _mesa_sha1_update(&ctx, &binding_layout->shader_stages,
+                           sizeof(binding_layout->shader_stages));
+      }
+   }
+
+   for (unsigned range = 0; range < pCreateInfo->pushConstantRangeCount;
+        range++) {
+      layout->push_constants.size =
+         MAX2(pCreateInfo->pPushConstantRanges[range].offset +
+                 pCreateInfo->pPushConstantRanges[range].size,
+              layout->push_constants.size);
+   }
+
+   layout->num_samplers = sampler_idx;
+   layout->num_textures = tex_idx;
+   layout->num_ubos = ubo_idx;
+   layout->num_dyn_ubos = dyn_ubo_idx;
+   layout->num_dyn_ssbos = dyn_ssbo_idx;
+   layout->num_imgs = img_idx;
+
+   /* Some NIR texture operations don't require a sampler, but Bifrost/Midgard
+    * ones always expect one. Add a dummy sampler to deal with this limitation.
+    */
+   if (layout->num_textures) {
+      layout->num_samplers++;
+      for (unsigned set = 0; set < pCreateInfo->setLayoutCount; set++)
+         layout->sets[set].sampler_offset++;
+   }
+
+   _mesa_sha1_final(&ctx, layout->sha1);
+
+   *pPipelineLayout = panvk_pipeline_layout_to_handle(layout);
+   return VK_SUCCESS;
+}
+
+VkResult
+panvk_bifrost_CreateDescriptorPool(VkDevice _device,
+                                  const VkDescriptorPoolCreateInfo *pCreateInfo,
+                                  const VkAllocationCallbacks *pAllocator,
+                                  VkDescriptorPool *pDescriptorPool)
+{
+   VK_FROM_HANDLE(panvk_device, device, _device);
+   struct panvk_descriptor_pool *pool;
+
+   pool = vk_object_zalloc(&device->vk, pAllocator,
+                           sizeof(struct panvk_descriptor_pool),
+                           VK_OBJECT_TYPE_DESCRIPTOR_POOL);
+   if (!pool)
+      return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
+
+   pool->max.sets = pCreateInfo->maxSets;
+
+   for (unsigned i = 0; i < pCreateInfo->poolSizeCount; ++i) {
+      unsigned desc_count = pCreateInfo->pPoolSizes[i].descriptorCount;
+
+      switch (pCreateInfo->pPoolSizes[i].type) {
+      case VK_DESCRIPTOR_TYPE_SAMPLER:
+         pool->max.samplers += desc_count;
+         break;
+      case VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER:
+         pool->max.combined_image_samplers += desc_count;
+         break;
+      case VK_DESCRIPTOR_TYPE_SAMPLED_IMAGE:
+         pool->max.sampled_images += desc_count;
+         break;
+      case VK_DESCRIPTOR_TYPE_STORAGE_IMAGE:
+         pool->max.storage_images += desc_count;
+         break;
+      case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
+         pool->max.uniform_texel_bufs += desc_count;
+         break;
+      case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
+         pool->max.storage_texel_bufs += desc_count;
+         break;
+      case VK_DESCRIPTOR_TYPE_INPUT_ATTACHMENT:
+         pool->max.input_attachments += desc_count;
+         break;
+      case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER:
+         pool->max.uniform_bufs += desc_count;
+         break;
+      case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER:
+         pool->max.storage_bufs += desc_count;
+         break;
+      case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC:
+         pool->max.uniform_dyn_bufs += desc_count;
+         break;
+      case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER_DYNAMIC:
+         pool->max.storage_dyn_bufs += desc_count;
+         break;
+      default:
+         unreachable("Invalid descriptor type");
+      }
+   }
+
+   *pDescriptorPool = panvk_descriptor_pool_to_handle(pool);
+   return VK_SUCCESS;
+}
+
+void
+panvk_bifrost_DestroyDescriptorPool(VkDevice _device, VkDescriptorPool _pool,
+                                    const VkAllocationCallbacks *pAllocator)
+{
+   VK_FROM_HANDLE(panvk_device, device, _device);
+   VK_FROM_HANDLE(panvk_descriptor_pool, pool, _pool);
+
+   if (pool)
+      vk_object_free(&device->vk, pAllocator, pool);
+}
+
+VkResult
+panvk_bifrost_ResetDescriptorPool(VkDevice _device, VkDescriptorPool _pool,
+                                  VkDescriptorPoolResetFlags flags)
+{
+   VK_FROM_HANDLE(panvk_descriptor_pool, pool, _pool);
+   memset(&pool->cur, 0, sizeof(pool->cur));
+   return VK_SUCCESS;
+}
+
+static void
+panvk_bifrost_descriptor_set_destroy(struct panvk_device *device,
+                                     struct panvk_descriptor_pool *pool,
+                                     struct panvk_descriptor_set *set)
+{
+   vk_free(&device->vk.alloc, set->textures);
+   vk_free(&device->vk.alloc, set->samplers);
+   vk_free(&device->vk.alloc, set->ubos);
+   vk_free(&device->vk.alloc, set->dyn_ubos);
+   vk_free(&device->vk.alloc, set->dyn_ssbos);
+   vk_free(&device->vk.alloc, set->img_fmts);
+   vk_free(&device->vk.alloc, set->img_attrib_bufs);
+   if (set->desc_bo)
+      panfrost_bo_unreference(set->desc_bo);
+   vk_object_free(&device->vk, NULL, set);
+}
+
+VkResult
+panvk_bifrost_FreeDescriptorSets(VkDevice _device,
+                                 VkDescriptorPool descriptorPool,
+                                 uint32_t count,
+                                 const VkDescriptorSet *pDescriptorSets)
+{
+   VK_FROM_HANDLE(panvk_device, device, _device);
+   VK_FROM_HANDLE(panvk_descriptor_pool, pool, descriptorPool);
+
+   for (unsigned i = 0; i < count; i++) {
+      VK_FROM_HANDLE(panvk_descriptor_set, set, pDescriptorSets[i]);
+
+      if (set)
+         panvk_bifrost_descriptor_set_destroy(device, pool, set);
+   }
+   return VK_SUCCESS;
+}
+
+VkResult
+panvk_bifrost_CreateSamplerYcbcrConversion(
+   VkDevice device, const VkSamplerYcbcrConversionCreateInfo *pCreateInfo,
+   const VkAllocationCallbacks *pAllocator,
+   VkSamplerYcbcrConversion *pYcbcrConversion)
+{
+   panvk_stub();
+   return VK_SUCCESS;
+}
+
+void
+panvk_bifrost_DestroySamplerYcbcrConversion(VkDevice device,
+                                            VkSamplerYcbcrConversion ycbcrConversion,
+                                            const VkAllocationCallbacks *pAllocator)
+{
+   panvk_stub();
+}
diff -up mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_cmd_buffer.c.16~ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_cmd_buffer.c
--- mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_cmd_buffer.c.16~	2023-07-29 15:51:45.093980518 +0100
+++ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_cmd_buffer.c	2023-07-29 15:51:45.083980485 +0100
@@ -0,0 +1,1214 @@
+/*
+ * Copyright © 2021 Collabora Ltd.
+ *
+ * Derived from tu_cmd_buffer.c which is:
+ * Copyright © 2016 Red Hat.
+ * Copyright © 2016 Bas Nieuwenhuizen
+ * Copyright © 2015 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#include "genxml/gen_macros.h"
+
+#include "panvk_cs.h"
+#include "panvk_private.h"
+
+#include "pan_blitter.h"
+#include "pan_cs.h"
+#include "pan_encoder.h"
+
+#include "util/rounding.h"
+#include "util/u_pack_color.h"
+#include "vk_format.h"
+
+static uint32_t
+panvk_debug_adjust_bo_flags(const struct panvk_device *device,
+                            uint32_t bo_flags)
+{
+   uint32_t debug_flags = device->physical_device->instance->debug_flags;
+
+   if (debug_flags & PANVK_DEBUG_DUMP)
+      bo_flags &= ~PAN_BO_INVISIBLE;
+
+   return bo_flags;
+}
+
+static void
+panvk_cmd_prepare_fragment_job(struct panvk_cmd_buffer *cmdbuf)
+{
+   const struct pan_fb_info *fbinfo = &cmdbuf->state.fb.info;
+   struct panvk_batch *batch = cmdbuf->state.batch;
+   struct panfrost_ptr job_ptr =
+      pan_pool_alloc_desc(&cmdbuf->desc_pool.base, FRAGMENT_JOB);
+
+   GENX(pan_emit_fragment_job)
+   (fbinfo, batch->fb.desc.gpu, job_ptr.cpu), batch->fragment_job = job_ptr.gpu;
+   util_dynarray_append(&batch->jobs, void *, job_ptr.cpu);
+}
+
+void
+panvk_per_arch(cmd_close_batch)(struct panvk_cmd_buffer *cmdbuf)
+{
+   struct panvk_batch *batch = cmdbuf->state.batch;
+
+   if (!batch)
+      return;
+
+   const struct pan_fb_info *fbinfo = &cmdbuf->state.fb.info;
+
+   assert(batch);
+
+   bool clear = fbinfo->zs.clear.z | fbinfo->zs.clear.s;
+   for (unsigned i = 0; i < fbinfo->rt_count; i++)
+      clear |= fbinfo->rts[i].clear;
+
+   if (!clear && !batch->scoreboard.first_job) {
+      if (util_dynarray_num_elements(&batch->event_ops,
+                                     struct panvk_event_op) == 0) {
+         /* Content-less batch, let's drop it */
+         vk_free(&cmdbuf->vk.pool->alloc, batch);
+      } else {
+         /* Batch has no jobs but is needed for synchronization, let's add a
+          * NULL job so the SUBMIT ioctl doesn't choke on it.
+          */
+         struct panfrost_ptr ptr =
+            pan_pool_alloc_desc(&cmdbuf->desc_pool.base, JOB_HEADER);
+         util_dynarray_append(&batch->jobs, void *, ptr.cpu);
+         panfrost_add_job(&cmdbuf->desc_pool.base, &batch->scoreboard,
+                          MALI_JOB_TYPE_NULL, false, false, 0, 0, &ptr, false);
+         list_addtail(&batch->node, &cmdbuf->batches);
+      }
+      cmdbuf->state.batch = NULL;
+      return;
+   }
+
+   struct panfrost_device *pdev = &cmdbuf->device->physical_device->pdev;
+
+   list_addtail(&batch->node, &cmdbuf->batches);
+
+   if (batch->scoreboard.first_tiler) {
+      struct panfrost_ptr preload_jobs[2];
+      unsigned num_preload_jobs = GENX(pan_preload_fb)(
+         &cmdbuf->desc_pool.base, &batch->scoreboard, &cmdbuf->state.fb.info,
+         batch->tls.gpu, batch->tiler.descs.gpu, preload_jobs);
+      for (unsigned i = 0; i < num_preload_jobs; i++)
+         util_dynarray_append(&batch->jobs, void *, preload_jobs[i].cpu);
+   }
+
+   if (batch->tlsinfo.tls.size) {
+      unsigned size = panfrost_get_total_stack_size(
+         batch->tlsinfo.tls.size, pdev->thread_tls_alloc, pdev->core_id_range);
+      batch->tlsinfo.tls.ptr =
+         pan_pool_alloc_aligned(&cmdbuf->tls_pool.base, size, 4096).gpu;
+   }
+
+   if (batch->tlsinfo.wls.size) {
+      assert(batch->wls_total_size);
+      batch->tlsinfo.wls.ptr =
+         pan_pool_alloc_aligned(&cmdbuf->tls_pool.base, batch->wls_total_size,
+                                4096)
+            .gpu;
+   }
+
+   if (batch->tls.cpu)
+      GENX(pan_emit_tls)(&batch->tlsinfo, batch->tls.cpu);
+
+   if (batch->fb.desc.cpu) {
+      batch->fb.desc.gpu |=
+         GENX(pan_emit_fbd)(pdev, &cmdbuf->state.fb.info, &batch->tlsinfo,
+                            &batch->tiler.ctx, batch->fb.desc.cpu);
+
+      panvk_cmd_prepare_fragment_job(cmdbuf);
+   }
+
+   cmdbuf->state.batch = NULL;
+}
+
+void
+panvk_per_arch(CmdNextSubpass2)(VkCommandBuffer commandBuffer,
+                                const VkSubpassBeginInfo *pSubpassBeginInfo,
+                                const VkSubpassEndInfo *pSubpassEndInfo)
+{
+   VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
+
+   panvk_per_arch(cmd_close_batch)(cmdbuf);
+
+   cmdbuf->state.subpass++;
+   panvk_cmd_fb_info_set_subpass(cmdbuf);
+   panvk_cmd_open_batch(cmdbuf);
+}
+
+void
+panvk_per_arch(CmdNextSubpass)(VkCommandBuffer cmd, VkSubpassContents contents)
+{
+   VkSubpassBeginInfo binfo = {.sType = VK_STRUCTURE_TYPE_SUBPASS_BEGIN_INFO,
+                               .contents = contents};
+   VkSubpassEndInfo einfo = {
+      .sType = VK_STRUCTURE_TYPE_SUBPASS_END_INFO,
+   };
+
+   panvk_per_arch(CmdNextSubpass2)(cmd, &binfo, &einfo);
+}
+
+void
+panvk_per_arch(cmd_alloc_fb_desc)(struct panvk_cmd_buffer *cmdbuf)
+{
+   struct panvk_batch *batch = cmdbuf->state.batch;
+
+   if (batch->fb.desc.gpu)
+      return;
+
+   const struct pan_fb_info *fbinfo = &cmdbuf->state.fb.info;
+   bool has_zs_ext = fbinfo->zs.view.zs || fbinfo->zs.view.s;
+
+   batch->fb.info = cmdbuf->state.framebuffer;
+   batch->fb.desc = pan_pool_alloc_desc_aggregate(
+      &cmdbuf->desc_pool.base, PAN_DESC(FRAMEBUFFER),
+      PAN_DESC_ARRAY(has_zs_ext ? 1 : 0, ZS_CRC_EXTENSION),
+      PAN_DESC_ARRAY(MAX2(fbinfo->rt_count, 1), RENDER_TARGET));
+
+   memset(&cmdbuf->state.fb.info.bifrost.pre_post.dcds, 0,
+          sizeof(cmdbuf->state.fb.info.bifrost.pre_post.dcds));
+}
+
+void
+panvk_per_arch(cmd_alloc_tls_desc)(struct panvk_cmd_buffer *cmdbuf, bool gfx)
+{
+   struct panvk_batch *batch = cmdbuf->state.batch;
+
+   assert(batch);
+   if (!batch->tls.gpu) {
+      batch->tls = pan_pool_alloc_desc(&cmdbuf->desc_pool.base, LOCAL_STORAGE);
+   }
+}
+
+static void
+panvk_cmd_prepare_draw_sysvals(
+   struct panvk_cmd_buffer *cmdbuf,
+   struct panvk_cmd_bind_point_state *bind_point_state,
+   struct panvk_draw_info *draw)
+{
+   struct panvk_sysvals *sysvals = &bind_point_state->desc_state.sysvals;
+
+   unsigned base_vertex = draw->index_size ? draw->vertex_offset : 0;
+   if (sysvals->first_vertex != draw->offset_start ||
+       sysvals->base_vertex != base_vertex ||
+       sysvals->base_instance != draw->first_instance) {
+      sysvals->first_vertex = draw->offset_start;
+      sysvals->base_vertex = base_vertex;
+      sysvals->base_instance = draw->first_instance;
+      bind_point_state->desc_state.sysvals_ptr = 0;
+   }
+
+   if (cmdbuf->state.dirty & PANVK_DYNAMIC_BLEND_CONSTANTS) {
+      memcpy(&sysvals->blend_constants, cmdbuf->state.blend.constants,
+             sizeof(cmdbuf->state.blend.constants));
+      bind_point_state->desc_state.sysvals_ptr = 0;
+   }
+
+   if (cmdbuf->state.dirty & PANVK_DYNAMIC_VIEWPORT) {
+      panvk_sysval_upload_viewport_scale(&cmdbuf->state.viewport,
+                                         &sysvals->viewport_scale);
+      panvk_sysval_upload_viewport_offset(&cmdbuf->state.viewport,
+                                          &sysvals->viewport_offset);
+      bind_point_state->desc_state.sysvals_ptr = 0;
+   }
+}
+
+static void
+panvk_cmd_prepare_sysvals(struct panvk_cmd_buffer *cmdbuf,
+                          struct panvk_cmd_bind_point_state *bind_point_state)
+{
+   struct panvk_descriptor_state *desc_state = &bind_point_state->desc_state;
+
+   if (desc_state->sysvals_ptr)
+      return;
+
+   struct panfrost_ptr sysvals = pan_pool_alloc_aligned(
+      &cmdbuf->desc_pool.base, sizeof(desc_state->sysvals), 16);
+   memcpy(sysvals.cpu, &desc_state->sysvals, sizeof(desc_state->sysvals));
+   desc_state->sysvals_ptr = sysvals.gpu;
+}
+
+static void
+panvk_cmd_prepare_push_constants(
+   struct panvk_cmd_buffer *cmdbuf,
+   struct panvk_cmd_bind_point_state *bind_point_state)
+{
+   struct panvk_descriptor_state *desc_state = &bind_point_state->desc_state;
+   const struct panvk_pipeline *pipeline = bind_point_state->pipeline;
+
+   if (!pipeline->layout->push_constants.size || desc_state->push_constants)
+      return;
+
+   struct panfrost_ptr push_constants = pan_pool_alloc_aligned(
+      &cmdbuf->desc_pool.base,
+      ALIGN_POT(pipeline->layout->push_constants.size, 16), 16);
+
+   memcpy(push_constants.cpu, cmdbuf->push_constants,
+          pipeline->layout->push_constants.size);
+   desc_state->push_constants = push_constants.gpu;
+}
+
+static void
+panvk_cmd_prepare_ubos(struct panvk_cmd_buffer *cmdbuf,
+                       struct panvk_cmd_bind_point_state *bind_point_state)
+{
+   struct panvk_descriptor_state *desc_state = &bind_point_state->desc_state;
+   const struct panvk_pipeline *pipeline = bind_point_state->pipeline;
+
+   if (!pipeline->num_ubos || desc_state->ubos)
+      return;
+
+   panvk_cmd_prepare_sysvals(cmdbuf, bind_point_state);
+   panvk_cmd_prepare_push_constants(cmdbuf, bind_point_state);
+
+   struct panfrost_ptr ubos = pan_pool_alloc_desc_array(
+      &cmdbuf->desc_pool.base, pipeline->num_ubos, UNIFORM_BUFFER);
+
+   panvk_per_arch(emit_ubos)(pipeline, desc_state, ubos.cpu);
+
+   desc_state->ubos = ubos.gpu;
+}
+
+static void
+panvk_cmd_prepare_textures(struct panvk_cmd_buffer *cmdbuf,
+                           struct panvk_cmd_bind_point_state *bind_point_state)
+{
+   struct panvk_descriptor_state *desc_state = &bind_point_state->desc_state;
+   const struct panvk_pipeline *pipeline = bind_point_state->pipeline;
+   unsigned num_textures = pipeline->layout->num_textures;
+
+   if (!num_textures || desc_state->textures)
+      return;
+
+   struct panfrost_ptr textures = pan_pool_alloc_aligned(
+      &cmdbuf->desc_pool.base, num_textures * pan_size(TEXTURE),
+      pan_size(TEXTURE));
+
+   void *texture = textures.cpu;
+
+   for (unsigned i = 0; i < ARRAY_SIZE(desc_state->sets); i++) {
+      if (!desc_state->sets[i])
+         continue;
+
+      memcpy(texture, desc_state->sets[i]->textures,
+             desc_state->sets[i]->layout->num_textures * pan_size(TEXTURE));
+
+      texture += desc_state->sets[i]->layout->num_textures * pan_size(TEXTURE);
+   }
+
+   desc_state->textures = textures.gpu;
+}
+
+static void
+panvk_cmd_prepare_samplers(struct panvk_cmd_buffer *cmdbuf,
+                           struct panvk_cmd_bind_point_state *bind_point_state)
+{
+   struct panvk_descriptor_state *desc_state = &bind_point_state->desc_state;
+   const struct panvk_pipeline *pipeline = bind_point_state->pipeline;
+   unsigned num_samplers = pipeline->layout->num_samplers;
+
+   if (!num_samplers || desc_state->samplers)
+      return;
+
+   struct panfrost_ptr samplers =
+      pan_pool_alloc_desc_array(&cmdbuf->desc_pool.base, num_samplers, SAMPLER);
+
+   void *sampler = samplers.cpu;
+
+   /* Prepare the dummy sampler */
+   pan_pack(sampler, SAMPLER, cfg) {
+      cfg.seamless_cube_map = false;
+      cfg.magnify_nearest = true;
+      cfg.minify_nearest = true;
+      cfg.normalized_coordinates = false;
+   }
+
+   sampler += pan_size(SAMPLER);
+
+   for (unsigned i = 0; i < ARRAY_SIZE(desc_state->sets); i++) {
+      if (!desc_state->sets[i])
+         continue;
+
+      memcpy(sampler, desc_state->sets[i]->samplers,
+             desc_state->sets[i]->layout->num_samplers * pan_size(SAMPLER));
+
+      sampler += desc_state->sets[i]->layout->num_samplers * pan_size(SAMPLER);
+   }
+
+   desc_state->samplers = samplers.gpu;
+}
+
+static void
+panvk_draw_prepare_fs_rsd(struct panvk_cmd_buffer *cmdbuf,
+                          struct panvk_draw_info *draw)
+{
+   const struct panvk_pipeline *pipeline =
+      panvk_cmd_get_pipeline(cmdbuf, GRAPHICS);
+
+   if (!pipeline->fs.dynamic_rsd) {
+      draw->fs_rsd = pipeline->rsds[MESA_SHADER_FRAGMENT];
+      return;
+   }
+
+   if (!cmdbuf->state.fs_rsd) {
+      struct panfrost_ptr rsd = pan_pool_alloc_desc_aggregate(
+         &cmdbuf->desc_pool.base, PAN_DESC(RENDERER_STATE),
+         PAN_DESC_ARRAY(pipeline->blend.state.rt_count, BLEND));
+
+      struct mali_renderer_state_packed rsd_dyn;
+      struct mali_renderer_state_packed *rsd_templ =
+         (struct mali_renderer_state_packed *)&pipeline->fs.rsd_template;
+
+      STATIC_ASSERT(sizeof(pipeline->fs.rsd_template) >= sizeof(*rsd_templ));
+
+      panvk_per_arch(emit_dyn_fs_rsd)(pipeline, &cmdbuf->state, &rsd_dyn);
+      pan_merge(rsd_dyn, (*rsd_templ), RENDERER_STATE);
+      memcpy(rsd.cpu, &rsd_dyn, sizeof(rsd_dyn));
+
+      void *bd = rsd.cpu + pan_size(RENDERER_STATE);
+      for (unsigned i = 0; i < pipeline->blend.state.rt_count; i++) {
+         if (pipeline->blend.constant[i].index != (uint8_t)~0) {
+            struct mali_blend_packed bd_dyn;
+            struct mali_blend_packed *bd_templ =
+               (struct mali_blend_packed *)&pipeline->blend.bd_template[i];
+
+            STATIC_ASSERT(sizeof(pipeline->blend.bd_template[0]) >=
+                          sizeof(*bd_templ));
+            panvk_per_arch(emit_blend_constant)(cmdbuf->device, pipeline, i,
+                                                cmdbuf->state.blend.constants,
+                                                &bd_dyn);
+            pan_merge(bd_dyn, (*bd_templ), BLEND);
+            memcpy(bd, &bd_dyn, sizeof(bd_dyn));
+         }
+         bd += pan_size(BLEND);
+      }
+
+      cmdbuf->state.fs_rsd = rsd.gpu;
+   }
+
+   draw->fs_rsd = cmdbuf->state.fs_rsd;
+}
+
+void
+panvk_per_arch(cmd_get_tiler_context)(struct panvk_cmd_buffer *cmdbuf,
+                                      unsigned width, unsigned height)
+{
+   struct panvk_batch *batch = cmdbuf->state.batch;
+
+   if (batch->tiler.descs.cpu)
+      return;
+
+   batch->tiler.descs = pan_pool_alloc_desc_aggregate(
+      &cmdbuf->desc_pool.base, PAN_DESC(TILER_CONTEXT), PAN_DESC(TILER_HEAP));
+   STATIC_ASSERT(sizeof(batch->tiler.templ) >=
+                 pan_size(TILER_CONTEXT) + pan_size(TILER_HEAP));
+
+   struct panfrost_ptr desc = {
+      .gpu = batch->tiler.descs.gpu,
+      .cpu = batch->tiler.templ,
+   };
+
+   panvk_per_arch(emit_tiler_context)(cmdbuf->device, width, height, &desc);
+   memcpy(batch->tiler.descs.cpu, batch->tiler.templ,
+          pan_size(TILER_CONTEXT) + pan_size(TILER_HEAP));
+   batch->tiler.ctx.bifrost.ctx = batch->tiler.descs.gpu;
+}
+
+void
+panvk_per_arch(cmd_prepare_tiler_context)(struct panvk_cmd_buffer *cmdbuf)
+{
+   const struct pan_fb_info *fbinfo = &cmdbuf->state.fb.info;
+
+   panvk_per_arch(cmd_get_tiler_context)(cmdbuf, fbinfo->width, fbinfo->height);
+}
+
+static void
+panvk_draw_prepare_tiler_context(struct panvk_cmd_buffer *cmdbuf,
+                                 struct panvk_draw_info *draw)
+{
+   struct panvk_batch *batch = cmdbuf->state.batch;
+
+   panvk_per_arch(cmd_prepare_tiler_context)(cmdbuf);
+   draw->tiler_ctx = &batch->tiler.ctx;
+}
+
+static void
+panvk_draw_prepare_varyings(struct panvk_cmd_buffer *cmdbuf,
+                            struct panvk_draw_info *draw)
+{
+   const struct panvk_pipeline *pipeline =
+      panvk_cmd_get_pipeline(cmdbuf, GRAPHICS);
+   struct panvk_varyings_info *varyings = &cmdbuf->state.varyings;
+
+   panvk_varyings_alloc(varyings, &cmdbuf->varying_pool.base,
+                        draw->padded_vertex_count * draw->instance_count);
+
+   unsigned buf_count = panvk_varyings_buf_count(varyings);
+   struct panfrost_ptr bufs = pan_pool_alloc_desc_array(
+      &cmdbuf->desc_pool.base, buf_count + 1, ATTRIBUTE_BUFFER);
+
+   panvk_per_arch(emit_varying_bufs)(varyings, bufs.cpu);
+
+   /* We need an empty entry to stop prefetching on Bifrost */
+   memset(bufs.cpu + (pan_size(ATTRIBUTE_BUFFER) * buf_count), 0,
+          pan_size(ATTRIBUTE_BUFFER));
+
+   if (BITSET_TEST(varyings->active, VARYING_SLOT_POS)) {
+      draw->position =
+         varyings->buf[varyings->varying[VARYING_SLOT_POS].buf].address +
+         varyings->varying[VARYING_SLOT_POS].offset;
+   }
+
+   if (pipeline->ia.writes_point_size) {
+      draw->psiz =
+         varyings->buf[varyings->varying[VARYING_SLOT_PSIZ].buf].address +
+         varyings->varying[VARYING_SLOT_POS].offset;
+   } else if (pipeline->ia.topology == MALI_DRAW_MODE_LINES ||
+              pipeline->ia.topology == MALI_DRAW_MODE_LINE_STRIP ||
+              pipeline->ia.topology == MALI_DRAW_MODE_LINE_LOOP) {
+      draw->line_width = pipeline->dynamic_state_mask & PANVK_DYNAMIC_LINE_WIDTH
+                            ? cmdbuf->state.rast.line_width
+                            : pipeline->rast.line_width;
+   } else {
+      draw->line_width = 1.0f;
+   }
+   draw->varying_bufs = bufs.gpu;
+
+   for (unsigned s = 0; s < MESA_SHADER_STAGES; s++) {
+      if (!varyings->stage[s].count)
+         continue;
+
+      struct panfrost_ptr attribs = pan_pool_alloc_desc_array(
+         &cmdbuf->desc_pool.base, varyings->stage[s].count, ATTRIBUTE);
+
+      panvk_per_arch(emit_varyings)(cmdbuf->device, varyings, s, attribs.cpu);
+      draw->stages[s].varyings = attribs.gpu;
+   }
+}
+
+static void
+panvk_fill_non_vs_attribs(struct panvk_cmd_buffer *cmdbuf,
+                          struct panvk_cmd_bind_point_state *bind_point_state,
+                          void *attrib_bufs, void *attribs, unsigned first_buf)
+{
+   struct panvk_descriptor_state *desc_state = &bind_point_state->desc_state;
+   const struct panvk_pipeline *pipeline = bind_point_state->pipeline;
+
+   for (unsigned s = 0; s < pipeline->layout->num_sets; s++) {
+      const struct panvk_descriptor_set *set = desc_state->sets[s];
+
+      if (!set)
+         continue;
+
+      const struct panvk_descriptor_set_layout *layout = set->layout;
+      unsigned img_idx = pipeline->layout->sets[s].img_offset;
+      unsigned offset = img_idx * pan_size(ATTRIBUTE_BUFFER) * 2;
+      unsigned size = layout->num_imgs * pan_size(ATTRIBUTE_BUFFER) * 2;
+
+      memcpy(attrib_bufs + offset, desc_state->sets[s]->img_attrib_bufs, size);
+
+      offset = img_idx * pan_size(ATTRIBUTE);
+      for (unsigned i = 0; i < layout->num_imgs; i++) {
+         pan_pack(attribs + offset, ATTRIBUTE, cfg) {
+            cfg.buffer_index = first_buf + (img_idx + i) * 2;
+            cfg.format = desc_state->sets[s]->img_fmts[i];
+         }
+         offset += pan_size(ATTRIBUTE);
+      }
+   }
+}
+
+static void
+panvk_prepare_non_vs_attribs(struct panvk_cmd_buffer *cmdbuf,
+                             struct panvk_cmd_bind_point_state *bind_point_state)
+{
+   struct panvk_descriptor_state *desc_state = &bind_point_state->desc_state;
+   const struct panvk_pipeline *pipeline = bind_point_state->pipeline;
+
+   if (desc_state->non_vs_attribs || !pipeline->img_access_mask)
+      return;
+
+   unsigned attrib_count = pipeline->layout->num_imgs;
+   unsigned attrib_buf_count = (pipeline->layout->num_imgs * 2);
+   struct panfrost_ptr bufs = pan_pool_alloc_desc_array(
+      &cmdbuf->desc_pool.base, attrib_buf_count + 1, ATTRIBUTE_BUFFER);
+   struct panfrost_ptr attribs = pan_pool_alloc_desc_array(
+      &cmdbuf->desc_pool.base, attrib_count, ATTRIBUTE);
+
+   panvk_fill_non_vs_attribs(cmdbuf, bind_point_state, bufs.cpu, attribs.cpu,
+                             0);
+
+   desc_state->non_vs_attrib_bufs = bufs.gpu;
+   desc_state->non_vs_attribs = attribs.gpu;
+}
+
+static void
+panvk_draw_prepare_vs_attribs(struct panvk_cmd_buffer *cmdbuf,
+                              struct panvk_draw_info *draw)
+{
+   struct panvk_cmd_bind_point_state *bind_point_state =
+      panvk_cmd_get_bind_point_state(cmdbuf, GRAPHICS);
+   struct panvk_descriptor_state *desc_state = &bind_point_state->desc_state;
+   const struct panvk_pipeline *pipeline = bind_point_state->pipeline;
+   unsigned num_imgs =
+      pipeline->img_access_mask & BITFIELD_BIT(MESA_SHADER_VERTEX)
+         ? pipeline->layout->num_imgs
+         : 0;
+   unsigned attrib_count = pipeline->attribs.attrib_count + num_imgs;
+
+   if (desc_state->vs_attribs || !attrib_count)
+      return;
+
+   if (!pipeline->attribs.buf_count) {
+      panvk_prepare_non_vs_attribs(cmdbuf, bind_point_state);
+      desc_state->vs_attrib_bufs = desc_state->non_vs_attrib_bufs;
+      desc_state->vs_attribs = desc_state->non_vs_attribs;
+      return;
+   }
+
+   unsigned attrib_buf_count = pipeline->attribs.buf_count * 2;
+   struct panfrost_ptr bufs = pan_pool_alloc_desc_array(
+      &cmdbuf->desc_pool.base, attrib_buf_count + 1, ATTRIBUTE_BUFFER);
+   struct panfrost_ptr attribs = pan_pool_alloc_desc_array(
+      &cmdbuf->desc_pool.base, attrib_count, ATTRIBUTE);
+
+   panvk_per_arch(emit_attrib_bufs)(&pipeline->attribs, cmdbuf->state.vb.bufs,
+                                    cmdbuf->state.vb.count, draw, bufs.cpu);
+   panvk_per_arch(emit_attribs)(cmdbuf->device, draw, &pipeline->attribs,
+                                cmdbuf->state.vb.bufs, cmdbuf->state.vb.count,
+                                attribs.cpu);
+
+   if (attrib_count > pipeline->attribs.buf_count) {
+      unsigned bufs_offset =
+         pipeline->attribs.buf_count * pan_size(ATTRIBUTE_BUFFER) * 2;
+      unsigned attribs_offset =
+         pipeline->attribs.buf_count * pan_size(ATTRIBUTE);
+
+      panvk_fill_non_vs_attribs(
+         cmdbuf, bind_point_state, bufs.cpu + bufs_offset,
+         attribs.cpu + attribs_offset, pipeline->attribs.buf_count * 2);
+   }
+
+   /* A NULL entry is needed to stop prefecting on Bifrost */
+   memset(bufs.cpu + (pan_size(ATTRIBUTE_BUFFER) * attrib_buf_count), 0,
+          pan_size(ATTRIBUTE_BUFFER));
+
+   desc_state->vs_attrib_bufs = bufs.gpu;
+   desc_state->vs_attribs = attribs.gpu;
+}
+
+static void
+panvk_draw_prepare_attributes(struct panvk_cmd_buffer *cmdbuf,
+                              struct panvk_draw_info *draw)
+{
+   struct panvk_cmd_bind_point_state *bind_point_state =
+      panvk_cmd_get_bind_point_state(cmdbuf, GRAPHICS);
+   struct panvk_descriptor_state *desc_state = &bind_point_state->desc_state;
+   const struct panvk_pipeline *pipeline = bind_point_state->pipeline;
+
+   for (unsigned i = 0; i < ARRAY_SIZE(draw->stages); i++) {
+      if (i == MESA_SHADER_VERTEX) {
+         panvk_draw_prepare_vs_attribs(cmdbuf, draw);
+         draw->stages[i].attributes = desc_state->vs_attribs;
+         draw->stages[i].attribute_bufs = desc_state->vs_attrib_bufs;
+      } else if (pipeline->img_access_mask & BITFIELD_BIT(i)) {
+         panvk_prepare_non_vs_attribs(cmdbuf, bind_point_state);
+         draw->stages[i].attributes = desc_state->non_vs_attribs;
+         draw->stages[i].attribute_bufs = desc_state->non_vs_attrib_bufs;
+      }
+   }
+}
+
+static void
+panvk_draw_prepare_viewport(struct panvk_cmd_buffer *cmdbuf,
+                            struct panvk_draw_info *draw)
+{
+   const struct panvk_pipeline *pipeline =
+      panvk_cmd_get_pipeline(cmdbuf, GRAPHICS);
+
+   if (pipeline->vpd) {
+      draw->viewport = pipeline->vpd;
+   } else if (cmdbuf->state.vpd) {
+      draw->viewport = cmdbuf->state.vpd;
+   } else {
+      struct panfrost_ptr vp =
+         pan_pool_alloc_desc(&cmdbuf->desc_pool.base, VIEWPORT);
+
+      const VkViewport *viewport =
+         pipeline->dynamic_state_mask & PANVK_DYNAMIC_VIEWPORT
+            ? &cmdbuf->state.viewport
+            : &pipeline->viewport;
+      const VkRect2D *scissor =
+         pipeline->dynamic_state_mask & PANVK_DYNAMIC_SCISSOR
+            ? &cmdbuf->state.scissor
+            : &pipeline->scissor;
+
+      panvk_per_arch(emit_viewport)(viewport, scissor, vp.cpu);
+      draw->viewport = cmdbuf->state.vpd = vp.gpu;
+   }
+}
+
+static void
+panvk_draw_prepare_vertex_job(struct panvk_cmd_buffer *cmdbuf,
+                              struct panvk_draw_info *draw)
+{
+   const struct panvk_pipeline *pipeline =
+      panvk_cmd_get_pipeline(cmdbuf, GRAPHICS);
+   struct panvk_batch *batch = cmdbuf->state.batch;
+   struct panfrost_ptr ptr =
+      pan_pool_alloc_desc(&cmdbuf->desc_pool.base, COMPUTE_JOB);
+
+   util_dynarray_append(&batch->jobs, void *, ptr.cpu);
+   draw->jobs.vertex = ptr;
+   panvk_per_arch(emit_vertex_job)(pipeline, draw, ptr.cpu);
+}
+
+static void
+panvk_draw_prepare_tiler_job(struct panvk_cmd_buffer *cmdbuf,
+                             struct panvk_draw_info *draw)
+{
+   const struct panvk_pipeline *pipeline =
+      panvk_cmd_get_pipeline(cmdbuf, GRAPHICS);
+   struct panvk_batch *batch = cmdbuf->state.batch;
+   struct panfrost_ptr ptr =
+      pan_pool_alloc_desc(&cmdbuf->desc_pool.base, TILER_JOB);
+
+   util_dynarray_append(&batch->jobs, void *, ptr.cpu);
+   draw->jobs.tiler = ptr;
+   panvk_per_arch(emit_tiler_job)(pipeline, draw, ptr.cpu);
+}
+
+static void
+panvk_cmd_draw(struct panvk_cmd_buffer *cmdbuf, struct panvk_draw_info *draw)
+{
+   struct panvk_batch *batch = cmdbuf->state.batch;
+   struct panvk_cmd_bind_point_state *bind_point_state =
+      panvk_cmd_get_bind_point_state(cmdbuf, GRAPHICS);
+   const struct panvk_pipeline *pipeline =
+      panvk_cmd_get_pipeline(cmdbuf, GRAPHICS);
+
+   /* There are only 16 bits in the descriptor for the job ID, make sure all
+    * the 3 (2 in Bifrost) jobs in this draw are in the same batch.
+    */
+   if (batch->scoreboard.job_index >= (UINT16_MAX - 3)) {
+      panvk_per_arch(cmd_close_batch)(cmdbuf);
+      panvk_cmd_preload_fb_after_batch_split(cmdbuf);
+      batch = panvk_cmd_open_batch(cmdbuf);
+   }
+
+   if (pipeline->rast.enable)
+      panvk_per_arch(cmd_alloc_fb_desc)(cmdbuf);
+
+   panvk_per_arch(cmd_alloc_tls_desc)(cmdbuf, true);
+
+   panvk_cmd_prepare_draw_sysvals(cmdbuf, bind_point_state, draw);
+   panvk_cmd_prepare_ubos(cmdbuf, bind_point_state);
+   panvk_cmd_prepare_textures(cmdbuf, bind_point_state);
+   panvk_cmd_prepare_samplers(cmdbuf, bind_point_state);
+
+   /* TODO: indexed draws */
+   struct panvk_descriptor_state *desc_state =
+      panvk_cmd_get_desc_state(cmdbuf, GRAPHICS);
+
+   draw->tls = batch->tls.gpu;
+   draw->fb = batch->fb.desc.gpu;
+   draw->ubos = desc_state->ubos;
+   draw->textures = desc_state->textures;
+   draw->samplers = desc_state->samplers;
+
+   STATIC_ASSERT(sizeof(draw->invocation) >=
+                 sizeof(struct mali_invocation_packed));
+   panfrost_pack_work_groups_compute(
+      (struct mali_invocation_packed *)&draw->invocation, 1, draw->vertex_range,
+      draw->instance_count, 1, 1, 1, true, false);
+
+   panvk_draw_prepare_fs_rsd(cmdbuf, draw);
+   panvk_draw_prepare_varyings(cmdbuf, draw);
+   panvk_draw_prepare_attributes(cmdbuf, draw);
+   panvk_draw_prepare_viewport(cmdbuf, draw);
+   panvk_draw_prepare_tiler_context(cmdbuf, draw);
+   panvk_draw_prepare_vertex_job(cmdbuf, draw);
+   panvk_draw_prepare_tiler_job(cmdbuf, draw);
+   batch->tlsinfo.tls.size = MAX2(pipeline->tls_size, batch->tlsinfo.tls.size);
+   assert(!pipeline->wls_size);
+
+   unsigned vjob_id = panfrost_add_job(
+      &cmdbuf->desc_pool.base, &batch->scoreboard, MALI_JOB_TYPE_VERTEX, false,
+      false, 0, 0, &draw->jobs.vertex, false);
+
+   if (pipeline->rast.enable) {
+      panfrost_add_job(&cmdbuf->desc_pool.base, &batch->scoreboard,
+                       MALI_JOB_TYPE_TILER, false, false, vjob_id, 0,
+                       &draw->jobs.tiler, false);
+   }
+
+   /* Clear the dirty flags all at once */
+   desc_state->dirty = cmdbuf->state.dirty = 0;
+}
+
+void
+panvk_per_arch(CmdDraw)(VkCommandBuffer commandBuffer, uint32_t vertexCount,
+                        uint32_t instanceCount, uint32_t firstVertex,
+                        uint32_t firstInstance)
+{
+   VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
+
+   if (instanceCount == 0 || vertexCount == 0)
+      return;
+
+   struct panvk_draw_info draw = {
+      .first_vertex = firstVertex,
+      .vertex_count = vertexCount,
+      .vertex_range = vertexCount,
+      .first_instance = firstInstance,
+      .instance_count = instanceCount,
+      .padded_vertex_count = instanceCount > 1
+                                ? panfrost_padded_vertex_count(vertexCount)
+                                : vertexCount,
+      .offset_start = firstVertex,
+   };
+
+   panvk_cmd_draw(cmdbuf, &draw);
+}
+
+static void
+panvk_index_minmax_search(struct panvk_cmd_buffer *cmdbuf, uint32_t start,
+                          uint32_t count, bool restart, uint32_t *min,
+                          uint32_t *max)
+{
+   void *ptr = cmdbuf->state.ib.buffer->bo->ptr.cpu +
+               cmdbuf->state.ib.buffer->bo_offset + cmdbuf->state.ib.offset;
+
+   fprintf(
+      stderr,
+      "WARNING: Crawling index buffers from the CPU isn't valid in Vulkan\n");
+
+   assert(cmdbuf->state.ib.buffer);
+   assert(cmdbuf->state.ib.buffer->bo);
+   assert(cmdbuf->state.ib.buffer->bo->ptr.cpu);
+
+   *max = 0;
+
+   /* TODO: Use panfrost_minmax_cache */
+   /* TODO: Read full cacheline of data to mitigate the uncached
+    * mapping slowness.
+    */
+   switch (cmdbuf->state.ib.index_size) {
+#define MINMAX_SEARCH_CASE(sz)                                                 \
+   case sz: {                                                                  \
+      uint##sz##_t *indices = ptr;                                             \
+      *min = UINT##sz##_MAX;                                                   \
+      for (uint32_t i = 0; i < count; i++) {                                   \
+         if (restart && indices[i + start] == UINT##sz##_MAX)                  \
+            continue;                                                          \
+         *min = MIN2(indices[i + start], *min);                                \
+         *max = MAX2(indices[i + start], *max);                                \
+      }                                                                        \
+      break;                                                                   \
+   }
+      MINMAX_SEARCH_CASE(32)
+      MINMAX_SEARCH_CASE(16)
+      MINMAX_SEARCH_CASE(8)
+#undef MINMAX_SEARCH_CASE
+   default:
+      unreachable("Invalid index size");
+   }
+}
+
+void
+panvk_per_arch(CmdDrawIndexed)(VkCommandBuffer commandBuffer,
+                               uint32_t indexCount, uint32_t instanceCount,
+                               uint32_t firstIndex, int32_t vertexOffset,
+                               uint32_t firstInstance)
+{
+   VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
+   uint32_t min_vertex, max_vertex;
+
+   if (instanceCount == 0 || indexCount == 0)
+      return;
+
+   const struct panvk_pipeline *pipeline =
+      panvk_cmd_get_pipeline(cmdbuf, GRAPHICS);
+   bool primitive_restart = pipeline->ia.primitive_restart;
+
+   panvk_index_minmax_search(cmdbuf, firstIndex, indexCount, primitive_restart,
+                             &min_vertex, &max_vertex);
+
+   unsigned vertex_range = max_vertex - min_vertex + 1;
+   struct panvk_draw_info draw = {
+      .index_size = cmdbuf->state.ib.index_size,
+      .first_index = firstIndex,
+      .index_count = indexCount,
+      .vertex_offset = vertexOffset,
+      .first_instance = firstInstance,
+      .instance_count = instanceCount,
+      .vertex_range = vertex_range,
+      .vertex_count = indexCount + abs(vertexOffset),
+      .padded_vertex_count = instanceCount > 1
+                                ? panfrost_padded_vertex_count(vertex_range)
+                                : vertex_range,
+      .offset_start = min_vertex + vertexOffset,
+      .indices = panvk_buffer_gpu_ptr(cmdbuf->state.ib.buffer,
+                                      cmdbuf->state.ib.offset) +
+                 (firstIndex * (cmdbuf->state.ib.index_size / 8)),
+   };
+
+   panvk_cmd_draw(cmdbuf, &draw);
+}
+
+VkResult
+panvk_per_arch(EndCommandBuffer)(VkCommandBuffer commandBuffer)
+{
+   VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
+
+   panvk_per_arch(cmd_close_batch)(cmdbuf);
+
+   return vk_command_buffer_end(&cmdbuf->vk);
+}
+
+void
+panvk_per_arch(CmdEndRenderPass2)(VkCommandBuffer commandBuffer,
+                                  const VkSubpassEndInfo *pSubpassEndInfo)
+{
+   VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
+
+   panvk_per_arch(cmd_close_batch)(cmdbuf);
+   vk_free(&cmdbuf->vk.pool->alloc, cmdbuf->state.clear);
+   cmdbuf->state.batch = NULL;
+   cmdbuf->state.pass = NULL;
+   cmdbuf->state.subpass = NULL;
+   cmdbuf->state.framebuffer = NULL;
+   cmdbuf->state.clear = NULL;
+}
+
+void
+panvk_per_arch(CmdEndRenderPass)(VkCommandBuffer cmd)
+{
+   VkSubpassEndInfo einfo = {
+      .sType = VK_STRUCTURE_TYPE_SUBPASS_END_INFO,
+   };
+
+   panvk_per_arch(CmdEndRenderPass2)(cmd, &einfo);
+}
+
+void
+panvk_per_arch(CmdPipelineBarrier2)(VkCommandBuffer commandBuffer,
+                                    const VkDependencyInfo *pDependencyInfo)
+{
+   VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
+
+   /* Caches are flushed/invalidated at batch boundaries for now, nothing to do
+    * for memory barriers assuming we implement barriers with the creation of a
+    * new batch.
+    * FIXME: We can probably do better with a CacheFlush job that has the
+    * barrier flag set to true.
+    */
+   if (cmdbuf->state.batch) {
+      panvk_per_arch(cmd_close_batch)(cmdbuf);
+      panvk_cmd_preload_fb_after_batch_split(cmdbuf);
+      panvk_cmd_open_batch(cmdbuf);
+   }
+}
+
+static void
+panvk_add_set_event_operation(struct panvk_cmd_buffer *cmdbuf,
+                              struct panvk_event *event,
+                              enum panvk_event_op_type type)
+{
+   struct panvk_event_op op = {
+      .type = type,
+      .event = event,
+   };
+
+   if (cmdbuf->state.batch == NULL) {
+      /* No open batch, let's create a new one so this operation happens in
+       * the right order.
+       */
+      panvk_cmd_open_batch(cmdbuf);
+      util_dynarray_append(&cmdbuf->state.batch->event_ops,
+                           struct panvk_event_op, op);
+      panvk_per_arch(cmd_close_batch)(cmdbuf);
+   } else {
+      /* Let's close the current batch so the operation executes before any
+       * future commands.
+       */
+      util_dynarray_append(&cmdbuf->state.batch->event_ops,
+                           struct panvk_event_op, op);
+      panvk_per_arch(cmd_close_batch)(cmdbuf);
+      panvk_cmd_preload_fb_after_batch_split(cmdbuf);
+      panvk_cmd_open_batch(cmdbuf);
+   }
+}
+
+static void
+panvk_add_wait_event_operation(struct panvk_cmd_buffer *cmdbuf,
+                               struct panvk_event *event)
+{
+   struct panvk_event_op op = {
+      .type = PANVK_EVENT_OP_WAIT,
+      .event = event,
+   };
+
+   if (cmdbuf->state.batch == NULL) {
+      /* No open batch, let's create a new one and have it wait for this event. */
+      panvk_cmd_open_batch(cmdbuf);
+      util_dynarray_append(&cmdbuf->state.batch->event_ops,
+                           struct panvk_event_op, op);
+   } else {
+      /* Let's close the current batch so any future commands wait on the
+       * event signal operation.
+       */
+      if (cmdbuf->state.batch->fragment_job ||
+          cmdbuf->state.batch->scoreboard.first_job) {
+         panvk_per_arch(cmd_close_batch)(cmdbuf);
+         panvk_cmd_preload_fb_after_batch_split(cmdbuf);
+         panvk_cmd_open_batch(cmdbuf);
+      }
+      util_dynarray_append(&cmdbuf->state.batch->event_ops,
+                           struct panvk_event_op, op);
+   }
+}
+
+void
+panvk_per_arch(CmdSetEvent2)(VkCommandBuffer commandBuffer, VkEvent _event,
+                             const VkDependencyInfo *pDependencyInfo)
+{
+   VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
+   VK_FROM_HANDLE(panvk_event, event, _event);
+
+   /* vkCmdSetEvent cannot be called inside a render pass */
+   assert(cmdbuf->state.pass == NULL);
+
+   panvk_add_set_event_operation(cmdbuf, event, PANVK_EVENT_OP_SET);
+}
+
+void
+panvk_per_arch(CmdResetEvent2)(VkCommandBuffer commandBuffer, VkEvent _event,
+                               VkPipelineStageFlags2 stageMask)
+{
+   VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
+   VK_FROM_HANDLE(panvk_event, event, _event);
+
+   /* vkCmdResetEvent cannot be called inside a render pass */
+   assert(cmdbuf->state.pass == NULL);
+
+   panvk_add_set_event_operation(cmdbuf, event, PANVK_EVENT_OP_RESET);
+}
+
+void
+panvk_per_arch(CmdWaitEvents2)(VkCommandBuffer commandBuffer,
+                               uint32_t eventCount, const VkEvent *pEvents,
+                               const VkDependencyInfo *pDependencyInfos)
+{
+   VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
+
+   assert(eventCount > 0);
+
+   for (uint32_t i = 0; i < eventCount; i++) {
+      VK_FROM_HANDLE(panvk_event, event, pEvents[i]);
+      panvk_add_wait_event_operation(cmdbuf, event);
+   }
+}
+
+static void
+panvk_reset_cmdbuf(struct vk_command_buffer *vk_cmdbuf,
+                   VkCommandBufferResetFlags flags)
+{
+   struct panvk_cmd_buffer *cmdbuf =
+      container_of(vk_cmdbuf, struct panvk_cmd_buffer, vk);
+
+   vk_command_buffer_reset(&cmdbuf->vk);
+
+   list_for_each_entry_safe(struct panvk_batch, batch, &cmdbuf->batches, node) {
+      list_del(&batch->node);
+      util_dynarray_fini(&batch->jobs);
+      util_dynarray_fini(&batch->event_ops);
+
+      vk_free(&cmdbuf->vk.pool->alloc, batch);
+   }
+
+   panvk_pool_reset(&cmdbuf->desc_pool);
+   panvk_pool_reset(&cmdbuf->tls_pool);
+   panvk_pool_reset(&cmdbuf->varying_pool);
+
+   for (unsigned i = 0; i < MAX_BIND_POINTS; i++)
+      memset(&cmdbuf->bind_points[i].desc_state.sets, 0,
+             sizeof(cmdbuf->bind_points[0].desc_state.sets));
+}
+
+static void
+panvk_destroy_cmdbuf(struct vk_command_buffer *vk_cmdbuf)
+{
+   struct panvk_cmd_buffer *cmdbuf =
+      container_of(vk_cmdbuf, struct panvk_cmd_buffer, vk);
+   struct panvk_device *device = cmdbuf->device;
+
+   list_for_each_entry_safe(struct panvk_batch, batch, &cmdbuf->batches, node) {
+      list_del(&batch->node);
+      util_dynarray_fini(&batch->jobs);
+      util_dynarray_fini(&batch->event_ops);
+
+      vk_free(&cmdbuf->vk.pool->alloc, batch);
+   }
+
+   panvk_pool_cleanup(&cmdbuf->desc_pool);
+   panvk_pool_cleanup(&cmdbuf->tls_pool);
+   panvk_pool_cleanup(&cmdbuf->varying_pool);
+   vk_command_buffer_finish(&cmdbuf->vk);
+   vk_free(&device->vk.alloc, cmdbuf);
+}
+
+static VkResult
+panvk_create_cmdbuf(struct vk_command_pool *vk_pool,
+                    struct vk_command_buffer **cmdbuf_out)
+{
+   struct panvk_device *device =
+      container_of(vk_pool->base.device, struct panvk_device, vk);
+   struct panvk_cmd_pool *pool =
+      container_of(vk_pool, struct panvk_cmd_pool, vk);
+   struct panvk_cmd_buffer *cmdbuf;
+
+   cmdbuf = vk_zalloc(&device->vk.alloc, sizeof(*cmdbuf), 8,
+                      VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
+   if (!cmdbuf)
+      return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
+
+   VkResult result = vk_command_buffer_init(&pool->vk, &cmdbuf->vk,
+                                            &panvk_per_arch(cmd_buffer_ops), 0);
+   if (result != VK_SUCCESS) {
+      vk_free(&device->vk.alloc, cmdbuf);
+      return result;
+   }
+
+   cmdbuf->device = device;
+
+   panvk_pool_init(&cmdbuf->desc_pool, &device->physical_device->pdev,
+                   &pool->desc_bo_pool, 0, 64 * 1024,
+                   "Command buffer descriptor pool", true);
+   panvk_pool_init(&cmdbuf->tls_pool, &device->physical_device->pdev,
+                   &pool->tls_bo_pool,
+                   panvk_debug_adjust_bo_flags(device, PAN_BO_INVISIBLE),
+                   64 * 1024, "TLS pool", false);
+   panvk_pool_init(&cmdbuf->varying_pool, &device->physical_device->pdev,
+                   &pool->varying_bo_pool,
+                   panvk_debug_adjust_bo_flags(device, PAN_BO_INVISIBLE),
+                   64 * 1024, "Varyings pool", false);
+   list_inithead(&cmdbuf->batches);
+   *cmdbuf_out = &cmdbuf->vk;
+   return VK_SUCCESS;
+}
+
+const struct vk_command_buffer_ops panvk_per_arch(cmd_buffer_ops) = {
+   .create = panvk_create_cmdbuf,
+   .reset = panvk_reset_cmdbuf,
+   .destroy = panvk_destroy_cmdbuf,
+};
+
+VkResult
+panvk_per_arch(BeginCommandBuffer)(VkCommandBuffer commandBuffer,
+                                   const VkCommandBufferBeginInfo *pBeginInfo)
+{
+   VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
+
+   vk_command_buffer_begin(&cmdbuf->vk, pBeginInfo);
+
+   memset(&cmdbuf->state, 0, sizeof(cmdbuf->state));
+
+   return VK_SUCCESS;
+}
+
+void
+panvk_per_arch(DestroyCommandPool)(VkDevice _device, VkCommandPool commandPool,
+                                   const VkAllocationCallbacks *pAllocator)
+{
+   VK_FROM_HANDLE(panvk_device, device, _device);
+   VK_FROM_HANDLE(panvk_cmd_pool, pool, commandPool);
+
+   vk_command_pool_finish(&pool->vk);
+
+   panvk_bo_pool_cleanup(&pool->desc_bo_pool);
+   panvk_bo_pool_cleanup(&pool->varying_bo_pool);
+   panvk_bo_pool_cleanup(&pool->tls_bo_pool);
+
+   vk_free2(&device->vk.alloc, pAllocator, pool);
+}
+
+void
+panvk_per_arch(CmdDispatch)(VkCommandBuffer commandBuffer, uint32_t x,
+                            uint32_t y, uint32_t z)
+{
+   VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
+   const struct panfrost_device *pdev = &cmdbuf->device->physical_device->pdev;
+   struct panvk_dispatch_info dispatch = {
+      .wg_count = {x, y, z},
+   };
+
+   panvk_per_arch(cmd_close_batch)(cmdbuf);
+   struct panvk_batch *batch = panvk_cmd_open_batch(cmdbuf);
+
+   struct panvk_cmd_bind_point_state *bind_point_state =
+      panvk_cmd_get_bind_point_state(cmdbuf, COMPUTE);
+   struct panvk_descriptor_state *desc_state = &bind_point_state->desc_state;
+   const struct panvk_pipeline *pipeline = bind_point_state->pipeline;
+   struct panfrost_ptr job =
+      pan_pool_alloc_desc(&cmdbuf->desc_pool.base, COMPUTE_JOB);
+
+   struct panvk_sysvals *sysvals = &desc_state->sysvals;
+   sysvals->num_work_groups.u32[0] = x;
+   sysvals->num_work_groups.u32[1] = y;
+   sysvals->num_work_groups.u32[2] = z;
+   sysvals->local_group_size.u32[0] = pipeline->cs.local_size.x;
+   sysvals->local_group_size.u32[1] = pipeline->cs.local_size.y;
+   sysvals->local_group_size.u32[2] = pipeline->cs.local_size.z;
+   desc_state->sysvals_ptr = 0;
+
+   panvk_per_arch(cmd_alloc_tls_desc)(cmdbuf, false);
+   dispatch.tsd = batch->tls.gpu;
+
+   panvk_prepare_non_vs_attribs(cmdbuf, bind_point_state);
+   dispatch.attributes = desc_state->non_vs_attribs;
+   dispatch.attribute_bufs = desc_state->non_vs_attrib_bufs;
+
+   panvk_cmd_prepare_ubos(cmdbuf, bind_point_state);
+   dispatch.ubos = desc_state->ubos;
+
+   panvk_cmd_prepare_textures(cmdbuf, bind_point_state);
+   dispatch.textures = desc_state->textures;
+
+   panvk_cmd_prepare_samplers(cmdbuf, bind_point_state);
+   dispatch.samplers = desc_state->samplers;
+
+   panvk_per_arch(emit_compute_job)(pipeline, &dispatch, job.cpu);
+   panfrost_add_job(&cmdbuf->desc_pool.base, &batch->scoreboard,
+                    MALI_JOB_TYPE_COMPUTE, false, false, 0, 0, &job, false);
+
+   batch->tlsinfo.tls.size = pipeline->tls_size;
+   batch->tlsinfo.wls.size = pipeline->wls_size;
+   if (batch->tlsinfo.wls.size) {
+      batch->wls_total_size =
+         pan_wls_mem_size(pdev, &dispatch.wg_count, batch->tlsinfo.wls.size);
+   }
+
+   panvk_per_arch(cmd_close_batch)(cmdbuf);
+   desc_state->dirty = 0;
+}
diff -up mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_cmd_buffer.h.16~ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_cmd_buffer.h
--- mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_cmd_buffer.h.16~	2023-07-29 15:51:45.093980518 +0100
+++ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_cmd_buffer.h	2023-07-29 15:51:45.083980485 +0100
@@ -0,0 +1,47 @@
+/*
+ * Copyright (C) 2021 Collabora Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#ifndef PANVK_PRIVATE_H
+#error "Must be included from panvk_private.h"
+#endif
+
+#ifndef PAN_ARCH
+#error "no arch"
+#endif
+
+#include "compiler/shader_enums.h"
+#include <vulkan/vulkan.h>
+
+extern const struct vk_command_buffer_ops panvk_per_arch(cmd_buffer_ops);
+
+void panvk_per_arch(cmd_close_batch)(struct panvk_cmd_buffer *cmdbuf);
+
+void panvk_per_arch(cmd_get_tiler_context)(struct panvk_cmd_buffer *cmdbuf,
+                                           unsigned width, unsigned height);
+
+void panvk_per_arch(cmd_alloc_fb_desc)(struct panvk_cmd_buffer *cmdbuf);
+
+void panvk_per_arch(cmd_alloc_tls_desc)(struct panvk_cmd_buffer *cmdbuf,
+                                        bool gfx);
+
+void panvk_per_arch(cmd_prepare_tiler_context)(struct panvk_cmd_buffer *cmdbuf);
diff -up mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_cs.c.16~ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_cs.c
--- mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_cs.c.16~	2023-07-29 15:51:45.093980518 +0100
+++ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_cs.c	2023-07-29 15:51:45.083980485 +0100
@@ -0,0 +1,844 @@
+/*
+ * Copyright (C) 2021 Collabora Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#include "genxml/gen_macros.h"
+
+#include "compiler/shader_enums.h"
+#include "util/macros.h"
+
+#include "vk_util.h"
+
+#include "pan_cs.h"
+#include "pan_earlyzs.h"
+#include "pan_encoder.h"
+#include "pan_pool.h"
+#include "pan_shader.h"
+
+#include "panvk_cs.h"
+#include "panvk_private.h"
+#include "panvk_varyings.h"
+
+#include "vk_sampler.h"
+
+static enum mali_mipmap_mode
+panvk_translate_sampler_mipmap_mode(VkSamplerMipmapMode mode)
+{
+   switch (mode) {
+   case VK_SAMPLER_MIPMAP_MODE_NEAREST:
+      return MALI_MIPMAP_MODE_NEAREST;
+   case VK_SAMPLER_MIPMAP_MODE_LINEAR:
+      return MALI_MIPMAP_MODE_TRILINEAR;
+   default:
+      unreachable("Invalid mipmap mode");
+   }
+}
+
+static unsigned
+panvk_translate_sampler_address_mode(VkSamplerAddressMode mode)
+{
+   switch (mode) {
+   case VK_SAMPLER_ADDRESS_MODE_REPEAT:
+      return MALI_WRAP_MODE_REPEAT;
+   case VK_SAMPLER_ADDRESS_MODE_MIRRORED_REPEAT:
+      return MALI_WRAP_MODE_MIRRORED_REPEAT;
+   case VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE:
+      return MALI_WRAP_MODE_CLAMP_TO_EDGE;
+   case VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_BORDER:
+      return MALI_WRAP_MODE_CLAMP_TO_BORDER;
+   case VK_SAMPLER_ADDRESS_MODE_MIRROR_CLAMP_TO_EDGE:
+      return MALI_WRAP_MODE_MIRRORED_CLAMP_TO_EDGE;
+   default:
+      unreachable("Invalid wrap");
+   }
+}
+
+static mali_pixel_format
+panvk_varying_hw_format(const struct panvk_device *dev,
+                        const struct panvk_varyings_info *varyings,
+                        gl_shader_stage stage, unsigned idx)
+{
+   const struct panfrost_device *pdev = &dev->physical_device->pdev;
+   gl_varying_slot loc = varyings->stage[stage].loc[idx];
+
+   switch (loc) {
+   case VARYING_SLOT_PNTC:
+   case VARYING_SLOT_PSIZ:
+#if PAN_ARCH <= 6
+      return (MALI_R16F << 12) | panfrost_get_default_swizzle(1);
+#else
+      return (MALI_R16F << 12) | MALI_RGB_COMPONENT_ORDER_R000;
+#endif
+   case VARYING_SLOT_POS:
+#if PAN_ARCH <= 6
+      return (MALI_SNAP_4 << 12) | panfrost_get_default_swizzle(4);
+#else
+      return (MALI_SNAP_4 << 12) | MALI_RGB_COMPONENT_ORDER_RGBA;
+#endif
+   default:
+      if (varyings->varying[loc].format != PIPE_FORMAT_NONE)
+         return pdev->formats[varyings->varying[loc].format].hw;
+#if PAN_ARCH >= 7
+      return (MALI_CONSTANT << 12) | MALI_RGB_COMPONENT_ORDER_0000;
+#else
+      return (MALI_CONSTANT << 12) | PAN_V6_SWIZZLE(0, 0, 0, 0);
+#endif
+   }
+}
+
+static void
+panvk_emit_varying(const struct panvk_device *dev,
+                   const struct panvk_varyings_info *varyings,
+                   gl_shader_stage stage, unsigned idx, void *attrib)
+{
+   gl_varying_slot loc = varyings->stage[stage].loc[idx];
+
+   pan_pack(attrib, ATTRIBUTE, cfg) {
+      cfg.buffer_index = varyings->varying[loc].buf;
+      cfg.offset = varyings->varying[loc].offset;
+      cfg.format = panvk_varying_hw_format(dev, varyings, stage, idx);
+   }
+}
+
+void
+panvk_per_arch(emit_varyings)(const struct panvk_device *dev,
+                              const struct panvk_varyings_info *varyings,
+                              gl_shader_stage stage, void *descs)
+{
+   struct mali_attribute_packed *attrib = descs;
+
+   for (unsigned i = 0; i < varyings->stage[stage].count; i++)
+      panvk_emit_varying(dev, varyings, stage, i, attrib++);
+}
+
+static void
+panvk_emit_varying_buf(const struct panvk_varyings_info *varyings,
+                       enum panvk_varying_buf_id id, void *buf)
+{
+   unsigned buf_idx = panvk_varying_buf_index(varyings, id);
+
+   pan_pack(buf, ATTRIBUTE_BUFFER, cfg) {
+      unsigned offset = varyings->buf[buf_idx].address & 63;
+
+      cfg.stride = varyings->buf[buf_idx].stride;
+      cfg.size = varyings->buf[buf_idx].size + offset;
+      cfg.pointer = varyings->buf[buf_idx].address & ~63ULL;
+   }
+}
+
+void
+panvk_per_arch(emit_varying_bufs)(const struct panvk_varyings_info *varyings,
+                                  void *descs)
+{
+   struct mali_attribute_buffer_packed *buf = descs;
+
+   for (unsigned i = 0; i < PANVK_VARY_BUF_MAX; i++) {
+      if (varyings->buf_mask & (1 << i))
+         panvk_emit_varying_buf(varyings, i, buf++);
+   }
+}
+
+static void
+panvk_emit_attrib_buf(const struct panvk_attribs_info *info,
+                      const struct panvk_draw_info *draw,
+                      const struct panvk_attrib_buf *bufs, unsigned buf_count,
+                      unsigned idx, void *desc)
+{
+   const struct panvk_attrib_buf_info *buf_info = &info->buf[idx];
+
+   assert(idx < buf_count);
+   const struct panvk_attrib_buf *buf = &bufs[idx];
+   mali_ptr addr = buf->address & ~63ULL;
+   unsigned size = buf->size + (buf->address & 63);
+   unsigned divisor = draw->padded_vertex_count * buf_info->instance_divisor;
+
+   /* TODO: support instanced arrays */
+   if (draw->instance_count <= 1) {
+      pan_pack(desc, ATTRIBUTE_BUFFER, cfg) {
+         cfg.type = MALI_ATTRIBUTE_TYPE_1D;
+         cfg.stride = buf_info->per_instance ? 0 : buf_info->stride;
+         cfg.pointer = addr;
+         cfg.size = size;
+      }
+   } else if (!buf_info->per_instance) {
+      pan_pack(desc, ATTRIBUTE_BUFFER, cfg) {
+         cfg.type = MALI_ATTRIBUTE_TYPE_1D_MODULUS;
+         cfg.divisor = draw->padded_vertex_count;
+         cfg.stride = buf_info->stride;
+         cfg.pointer = addr;
+         cfg.size = size;
+      }
+   } else if (!divisor) {
+      /* instance_divisor == 0 means all instances share the same value.
+       * Make it a 1D array with a zero stride.
+       */
+      pan_pack(desc, ATTRIBUTE_BUFFER, cfg) {
+         cfg.type = MALI_ATTRIBUTE_TYPE_1D;
+         cfg.stride = 0;
+         cfg.pointer = addr;
+         cfg.size = size;
+      }
+   } else if (util_is_power_of_two_or_zero(divisor)) {
+      pan_pack(desc, ATTRIBUTE_BUFFER, cfg) {
+         cfg.type = MALI_ATTRIBUTE_TYPE_1D_POT_DIVISOR;
+         cfg.stride = buf_info->stride;
+         cfg.pointer = addr;
+         cfg.size = size;
+         cfg.divisor_r = __builtin_ctz(divisor);
+      }
+   } else {
+      unsigned divisor_r = 0, divisor_e = 0;
+      unsigned divisor_num =
+         panfrost_compute_magic_divisor(divisor, &divisor_r, &divisor_e);
+      pan_pack(desc, ATTRIBUTE_BUFFER, cfg) {
+         cfg.type = MALI_ATTRIBUTE_TYPE_1D_NPOT_DIVISOR;
+         cfg.stride = buf_info->stride;
+         cfg.pointer = addr;
+         cfg.size = size;
+         cfg.divisor_r = divisor_r;
+         cfg.divisor_e = divisor_e;
+      }
+
+      desc += pan_size(ATTRIBUTE_BUFFER);
+      pan_pack(desc, ATTRIBUTE_BUFFER_CONTINUATION_NPOT, cfg) {
+         cfg.divisor_numerator = divisor_num;
+         cfg.divisor = buf_info->instance_divisor;
+      }
+   }
+}
+
+void
+panvk_per_arch(emit_attrib_bufs)(const struct panvk_attribs_info *info,
+                                 const struct panvk_attrib_buf *bufs,
+                                 unsigned buf_count,
+                                 const struct panvk_draw_info *draw,
+                                 void *descs)
+{
+   struct mali_attribute_buffer_packed *buf = descs;
+
+   for (unsigned i = 0; i < info->buf_count; i++) {
+      panvk_emit_attrib_buf(info, draw, bufs, buf_count, i, buf);
+      buf += 2;
+   }
+}
+
+void
+panvk_per_arch(emit_sampler)(const VkSamplerCreateInfo *pCreateInfo, void *desc)
+{
+   VkClearColorValue border_color =
+      vk_sampler_border_color_value(pCreateInfo, NULL);
+
+   pan_pack(desc, SAMPLER, cfg) {
+      cfg.magnify_nearest = pCreateInfo->magFilter == VK_FILTER_NEAREST;
+      cfg.minify_nearest = pCreateInfo->minFilter == VK_FILTER_NEAREST;
+      cfg.mipmap_mode =
+         panvk_translate_sampler_mipmap_mode(pCreateInfo->mipmapMode);
+      cfg.normalized_coordinates = !pCreateInfo->unnormalizedCoordinates;
+
+      cfg.lod_bias = pCreateInfo->mipLodBias;
+      cfg.minimum_lod = pCreateInfo->minLod;
+      cfg.maximum_lod = pCreateInfo->maxLod;
+      cfg.wrap_mode_s =
+         panvk_translate_sampler_address_mode(pCreateInfo->addressModeU);
+      cfg.wrap_mode_t =
+         panvk_translate_sampler_address_mode(pCreateInfo->addressModeV);
+      cfg.wrap_mode_r =
+         panvk_translate_sampler_address_mode(pCreateInfo->addressModeW);
+      cfg.compare_function =
+         panvk_per_arch(translate_sampler_compare_func)(pCreateInfo);
+      cfg.border_color_r = border_color.uint32[0];
+      cfg.border_color_g = border_color.uint32[1];
+      cfg.border_color_b = border_color.uint32[2];
+      cfg.border_color_a = border_color.uint32[3];
+   }
+}
+
+static void
+panvk_emit_attrib(const struct panvk_device *dev,
+                  const struct panvk_draw_info *draw,
+                  const struct panvk_attribs_info *attribs,
+                  const struct panvk_attrib_buf *bufs, unsigned buf_count,
+                  unsigned idx, void *attrib)
+{
+   const struct panfrost_device *pdev = &dev->physical_device->pdev;
+   unsigned buf_idx = attribs->attrib[idx].buf;
+   const struct panvk_attrib_buf_info *buf_info = &attribs->buf[buf_idx];
+
+   pan_pack(attrib, ATTRIBUTE, cfg) {
+      cfg.buffer_index = buf_idx * 2;
+      cfg.offset = attribs->attrib[idx].offset + (bufs[buf_idx].address & 63);
+
+      if (buf_info->per_instance)
+         cfg.offset += draw->first_instance * buf_info->stride;
+
+      cfg.format = pdev->formats[attribs->attrib[idx].format].hw;
+   }
+}
+
+void
+panvk_per_arch(emit_attribs)(const struct panvk_device *dev,
+                             const struct panvk_draw_info *draw,
+                             const struct panvk_attribs_info *attribs,
+                             const struct panvk_attrib_buf *bufs,
+                             unsigned buf_count, void *descs)
+{
+   struct mali_attribute_packed *attrib = descs;
+
+   for (unsigned i = 0; i < attribs->attrib_count; i++)
+      panvk_emit_attrib(dev, draw, attribs, bufs, buf_count, i, attrib++);
+}
+
+void
+panvk_per_arch(emit_ubo)(mali_ptr address, size_t size, void *desc)
+{
+   pan_pack(desc, UNIFORM_BUFFER, cfg) {
+      cfg.pointer = address;
+      cfg.entries = DIV_ROUND_UP(size, 16);
+   }
+}
+
+void
+panvk_per_arch(emit_ubos)(const struct panvk_pipeline *pipeline,
+                          const struct panvk_descriptor_state *state,
+                          void *descs)
+{
+   struct mali_uniform_buffer_packed *ubos = descs;
+
+   panvk_per_arch(emit_ubo)(state->sysvals_ptr, sizeof(state->sysvals),
+                            &ubos[PANVK_SYSVAL_UBO_INDEX]);
+
+   if (pipeline->layout->push_constants.size) {
+      panvk_per_arch(emit_ubo)(
+         state->push_constants,
+         ALIGN_POT(pipeline->layout->push_constants.size, 16),
+         &ubos[PANVK_PUSH_CONST_UBO_INDEX]);
+   } else {
+      memset(&ubos[PANVK_PUSH_CONST_UBO_INDEX], 0, sizeof(*ubos));
+   }
+
+   for (unsigned s = 0; s < pipeline->layout->vk.set_count; s++) {
+      const struct panvk_descriptor_set_layout *set_layout =
+         vk_to_panvk_descriptor_set_layout(pipeline->layout->vk.set_layouts[s]);
+      const struct panvk_descriptor_set *set = state->sets[s];
+
+      unsigned ubo_start =
+         panvk_pipeline_layout_ubo_start(pipeline->layout, s, false);
+
+      if (!set) {
+         unsigned all_ubos = set_layout->num_ubos + set_layout->num_dyn_ubos;
+         memset(&ubos[ubo_start], 0, all_ubos * sizeof(*ubos));
+      } else {
+         memcpy(&ubos[ubo_start], set->ubos,
+                set_layout->num_ubos * sizeof(*ubos));
+
+         unsigned dyn_ubo_start =
+            panvk_pipeline_layout_ubo_start(pipeline->layout, s, true);
+
+         for (unsigned i = 0; i < set_layout->num_dyn_ubos; i++) {
+            const struct panvk_buffer_desc *bdesc =
+               &state->dyn.ubos[pipeline->layout->sets[s].dyn_ubo_offset + i];
+
+            mali_ptr address =
+               panvk_buffer_gpu_ptr(bdesc->buffer, bdesc->offset);
+            size_t size =
+               panvk_buffer_range(bdesc->buffer, bdesc->offset, bdesc->size);
+            if (size) {
+               panvk_per_arch(emit_ubo)(address, size,
+                                        &ubos[dyn_ubo_start + i]);
+            } else {
+               memset(&ubos[dyn_ubo_start + i], 0, sizeof(*ubos));
+            }
+         }
+      }
+   }
+}
+
+void
+panvk_per_arch(emit_vertex_job)(const struct panvk_pipeline *pipeline,
+                                const struct panvk_draw_info *draw, void *job)
+{
+   void *section = pan_section_ptr(job, COMPUTE_JOB, INVOCATION);
+
+   memcpy(section, &draw->invocation, pan_size(INVOCATION));
+
+   pan_section_pack(job, COMPUTE_JOB, PARAMETERS, cfg) {
+      cfg.job_task_split = 5;
+   }
+
+   pan_section_pack(job, COMPUTE_JOB, DRAW, cfg) {
+      cfg.state = pipeline->rsds[MESA_SHADER_VERTEX];
+      cfg.attributes = draw->stages[MESA_SHADER_VERTEX].attributes;
+      cfg.attribute_buffers = draw->stages[MESA_SHADER_VERTEX].attribute_bufs;
+      cfg.varyings = draw->stages[MESA_SHADER_VERTEX].varyings;
+      cfg.varying_buffers = draw->varying_bufs;
+      cfg.thread_storage = draw->tls;
+      cfg.offset_start = draw->offset_start;
+      cfg.instance_size =
+         draw->instance_count > 1 ? draw->padded_vertex_count : 1;
+      cfg.uniform_buffers = draw->ubos;
+      cfg.push_uniforms = draw->stages[PIPE_SHADER_VERTEX].push_constants;
+      cfg.textures = draw->textures;
+      cfg.samplers = draw->samplers;
+   }
+}
+
+void
+panvk_per_arch(emit_compute_job)(const struct panvk_pipeline *pipeline,
+                                 const struct panvk_dispatch_info *dispatch,
+                                 void *job)
+{
+   panfrost_pack_work_groups_compute(
+      pan_section_ptr(job, COMPUTE_JOB, INVOCATION), dispatch->wg_count.x,
+      dispatch->wg_count.y, dispatch->wg_count.z, pipeline->cs.local_size.x,
+      pipeline->cs.local_size.y, pipeline->cs.local_size.z, false, false);
+
+   pan_section_pack(job, COMPUTE_JOB, PARAMETERS, cfg) {
+      cfg.job_task_split = util_logbase2_ceil(pipeline->cs.local_size.x + 1) +
+                           util_logbase2_ceil(pipeline->cs.local_size.y + 1) +
+                           util_logbase2_ceil(pipeline->cs.local_size.z + 1);
+   }
+
+   pan_section_pack(job, COMPUTE_JOB, DRAW, cfg) {
+      cfg.state = pipeline->rsds[MESA_SHADER_COMPUTE];
+      cfg.attributes = dispatch->attributes;
+      cfg.attribute_buffers = dispatch->attribute_bufs;
+      cfg.thread_storage = dispatch->tsd;
+      cfg.uniform_buffers = dispatch->ubos;
+      cfg.push_uniforms = dispatch->push_uniforms;
+      cfg.textures = dispatch->textures;
+      cfg.samplers = dispatch->samplers;
+   }
+}
+
+static void
+panvk_emit_tiler_primitive(const struct panvk_pipeline *pipeline,
+                           const struct panvk_draw_info *draw, void *prim)
+{
+   pan_pack(prim, PRIMITIVE, cfg) {
+      cfg.draw_mode = pipeline->ia.topology;
+      if (pipeline->ia.writes_point_size)
+         cfg.point_size_array_format = MALI_POINT_SIZE_ARRAY_FORMAT_FP16;
+
+      cfg.first_provoking_vertex = true;
+      if (pipeline->ia.primitive_restart)
+         cfg.primitive_restart = MALI_PRIMITIVE_RESTART_IMPLICIT;
+      cfg.job_task_split = 6;
+
+      if (draw->index_size) {
+         cfg.index_count = draw->index_count;
+         cfg.indices = draw->indices;
+         cfg.base_vertex_offset = draw->vertex_offset - draw->offset_start;
+
+         switch (draw->index_size) {
+         case 32:
+            cfg.index_type = MALI_INDEX_TYPE_UINT32;
+            break;
+         case 16:
+            cfg.index_type = MALI_INDEX_TYPE_UINT16;
+            break;
+         case 8:
+            cfg.index_type = MALI_INDEX_TYPE_UINT8;
+            break;
+         default:
+            unreachable("Invalid index size");
+         }
+      } else {
+         cfg.index_count = draw->vertex_count;
+         cfg.index_type = MALI_INDEX_TYPE_NONE;
+      }
+   }
+}
+
+static void
+panvk_emit_tiler_primitive_size(const struct panvk_pipeline *pipeline,
+                                const struct panvk_draw_info *draw,
+                                void *primsz)
+{
+   pan_pack(primsz, PRIMITIVE_SIZE, cfg) {
+      if (pipeline->ia.writes_point_size) {
+         cfg.size_array = draw->psiz;
+      } else {
+         cfg.constant = draw->line_width;
+      }
+   }
+}
+
+static void
+panvk_emit_tiler_dcd(const struct panvk_pipeline *pipeline,
+                     const struct panvk_draw_info *draw, void *dcd)
+{
+   pan_pack(dcd, DRAW, cfg) {
+      cfg.front_face_ccw = pipeline->rast.front_ccw;
+      cfg.cull_front_face = pipeline->rast.cull_front_face;
+      cfg.cull_back_face = pipeline->rast.cull_back_face;
+      cfg.position = draw->position;
+      cfg.state = draw->fs_rsd;
+      cfg.attributes = draw->stages[MESA_SHADER_FRAGMENT].attributes;
+      cfg.attribute_buffers = draw->stages[MESA_SHADER_FRAGMENT].attribute_bufs;
+      cfg.viewport = draw->viewport;
+      cfg.varyings = draw->stages[MESA_SHADER_FRAGMENT].varyings;
+      cfg.varying_buffers = cfg.varyings ? draw->varying_bufs : 0;
+      cfg.thread_storage = draw->tls;
+
+      /* For all primitives but lines DRAW.flat_shading_vertex must
+       * be set to 0 and the provoking vertex is selected with the
+       * PRIMITIVE.first_provoking_vertex field.
+       */
+      if (pipeline->ia.topology == MALI_DRAW_MODE_LINES ||
+          pipeline->ia.topology == MALI_DRAW_MODE_LINE_STRIP ||
+          pipeline->ia.topology == MALI_DRAW_MODE_LINE_LOOP) {
+         cfg.flat_shading_vertex = true;
+      }
+
+      cfg.offset_start = draw->offset_start;
+      cfg.instance_size =
+         draw->instance_count > 1 ? draw->padded_vertex_count : 1;
+      cfg.uniform_buffers = draw->ubos;
+      cfg.push_uniforms = draw->stages[PIPE_SHADER_FRAGMENT].push_constants;
+      cfg.textures = draw->textures;
+      cfg.samplers = draw->samplers;
+
+      /* TODO: occlusion queries */
+   }
+}
+
+void
+panvk_per_arch(emit_tiler_job)(const struct panvk_pipeline *pipeline,
+                               const struct panvk_draw_info *draw, void *job)
+{
+   void *section;
+
+   section = pan_section_ptr(job, TILER_JOB, INVOCATION);
+   memcpy(section, &draw->invocation, pan_size(INVOCATION));
+
+   section = pan_section_ptr(job, TILER_JOB, PRIMITIVE);
+   panvk_emit_tiler_primitive(pipeline, draw, section);
+
+   section = pan_section_ptr(job, TILER_JOB, PRIMITIVE_SIZE);
+   panvk_emit_tiler_primitive_size(pipeline, draw, section);
+
+   section = pan_section_ptr(job, TILER_JOB, DRAW);
+   panvk_emit_tiler_dcd(pipeline, draw, section);
+
+   pan_section_pack(job, TILER_JOB, TILER, cfg) {
+      cfg.address = draw->tiler_ctx->bifrost.ctx;
+   }
+   pan_section_pack(job, TILER_JOB, PADDING, padding)
+      ;
+}
+
+void
+panvk_per_arch(emit_viewport)(const VkViewport *viewport,
+                              const VkRect2D *scissor, void *vpd)
+{
+   /* The spec says "width must be greater than 0.0" */
+   assert(viewport->x >= 0);
+   int minx = (int)viewport->x;
+   int maxx = (int)(viewport->x + viewport->width);
+
+   /* Viewport height can be negative */
+   int miny = MIN2((int)viewport->y, (int)(viewport->y + viewport->height));
+   int maxy = MAX2((int)viewport->y, (int)(viewport->y + viewport->height));
+
+   assert(scissor->offset.x >= 0 && scissor->offset.y >= 0);
+   miny = MAX2(scissor->offset.x, minx);
+   miny = MAX2(scissor->offset.y, miny);
+   maxx = MIN2(scissor->offset.x + scissor->extent.width, maxx);
+   maxy = MIN2(scissor->offset.y + scissor->extent.height, maxy);
+
+   /* Make sure we don't end up with a max < min when width/height is 0 */
+   maxx = maxx > minx ? maxx - 1 : maxx;
+   maxy = maxy > miny ? maxy - 1 : maxy;
+
+   assert(viewport->minDepth >= 0.0f && viewport->minDepth <= 1.0f);
+   assert(viewport->maxDepth >= 0.0f && viewport->maxDepth <= 1.0f);
+
+   pan_pack(vpd, VIEWPORT, cfg) {
+      cfg.scissor_minimum_x = minx;
+      cfg.scissor_minimum_y = miny;
+      cfg.scissor_maximum_x = maxx;
+      cfg.scissor_maximum_y = maxy;
+      cfg.minimum_z = MIN2(viewport->minDepth, viewport->maxDepth);
+      cfg.maximum_z = MAX2(viewport->minDepth, viewport->maxDepth);
+   }
+}
+
+static enum mali_register_file_format
+bifrost_blend_type_from_nir(nir_alu_type nir_type)
+{
+   switch (nir_type) {
+   case 0: /* Render target not in use */
+      return 0;
+   case nir_type_float16:
+      return MALI_REGISTER_FILE_FORMAT_F16;
+   case nir_type_float32:
+      return MALI_REGISTER_FILE_FORMAT_F32;
+   case nir_type_int32:
+      return MALI_REGISTER_FILE_FORMAT_I32;
+   case nir_type_uint32:
+      return MALI_REGISTER_FILE_FORMAT_U32;
+   case nir_type_int16:
+      return MALI_REGISTER_FILE_FORMAT_I16;
+   case nir_type_uint16:
+      return MALI_REGISTER_FILE_FORMAT_U16;
+   default:
+      unreachable("Unsupported blend shader type for NIR alu type");
+   }
+}
+
+void
+panvk_per_arch(emit_blend)(const struct panvk_device *dev,
+                           const struct panvk_pipeline *pipeline, unsigned rt,
+                           void *bd)
+{
+   const struct pan_blend_state *blend = &pipeline->blend.state;
+   const struct pan_blend_rt_state *rts = &blend->rts[rt];
+   bool dithered = false;
+
+   pan_pack(bd, BLEND, cfg) {
+      if (!blend->rt_count || !rts->equation.color_mask) {
+         cfg.enable = false;
+         cfg.internal.mode = MALI_BLEND_MODE_OFF;
+         continue;
+      }
+
+      cfg.srgb = util_format_is_srgb(rts->format);
+      cfg.load_destination = pan_blend_reads_dest(blend->rts[rt].equation);
+      cfg.round_to_fb_precision = !dithered;
+
+      const struct panfrost_device *pdev = &dev->physical_device->pdev;
+      const struct util_format_description *format_desc =
+         util_format_description(rts->format);
+      unsigned chan_size = 0;
+      for (unsigned i = 0; i < format_desc->nr_channels; i++)
+         chan_size = MAX2(format_desc->channel[i].size, chan_size);
+
+      pan_blend_to_fixed_function_equation(blend->rts[rt].equation,
+                                           &cfg.equation);
+
+      /* Fixed point constant */
+      float fconst = pan_blend_get_constant(
+         pan_blend_constant_mask(blend->rts[rt].equation), blend->constants);
+      u16 constant = fconst * ((1 << chan_size) - 1);
+      constant <<= 16 - chan_size;
+      cfg.constant = constant;
+
+      if (pan_blend_is_opaque(blend->rts[rt].equation)) {
+         cfg.internal.mode = MALI_BLEND_MODE_OPAQUE;
+      } else {
+         cfg.internal.mode = MALI_BLEND_MODE_FIXED_FUNCTION;
+
+         cfg.internal.fixed_function.alpha_zero_nop =
+            pan_blend_alpha_zero_nop(blend->rts[rt].equation);
+         cfg.internal.fixed_function.alpha_one_store =
+            pan_blend_alpha_one_store(blend->rts[rt].equation);
+      }
+
+      /* If we want the conversion to work properly,
+       * num_comps must be set to 4
+       */
+      cfg.internal.fixed_function.num_comps = 4;
+      cfg.internal.fixed_function.conversion.memory_format =
+         panfrost_format_to_bifrost_blend(pdev, rts->format, dithered);
+      cfg.internal.fixed_function.conversion.register_format =
+         bifrost_blend_type_from_nir(pipeline->fs.info.bifrost.blend[rt].type);
+      cfg.internal.fixed_function.rt = rt;
+   }
+}
+
+void
+panvk_per_arch(emit_blend_constant)(const struct panvk_device *dev,
+                                    const struct panvk_pipeline *pipeline,
+                                    unsigned rt, const float *constants,
+                                    void *bd)
+{
+   float constant = constants[pipeline->blend.constant[rt].index];
+
+   pan_pack(bd, BLEND, cfg) {
+      cfg.enable = false;
+      cfg.constant = constant * pipeline->blend.constant[rt].bifrost_factor;
+   }
+}
+
+void
+panvk_per_arch(emit_dyn_fs_rsd)(const struct panvk_pipeline *pipeline,
+                                const struct panvk_cmd_state *state, void *rsd)
+{
+   pan_pack(rsd, RENDERER_STATE, cfg) {
+      if (pipeline->dynamic_state_mask & (1 << VK_DYNAMIC_STATE_DEPTH_BIAS)) {
+         cfg.depth_units = state->rast.depth_bias.constant_factor * 2.0f;
+         cfg.depth_factor = state->rast.depth_bias.slope_factor;
+         cfg.depth_bias_clamp = state->rast.depth_bias.clamp;
+      }
+
+      if (pipeline->dynamic_state_mask &
+          (1 << VK_DYNAMIC_STATE_STENCIL_COMPARE_MASK)) {
+         cfg.stencil_front.mask = state->zs.s_front.compare_mask;
+         cfg.stencil_back.mask = state->zs.s_back.compare_mask;
+      }
+
+      if (pipeline->dynamic_state_mask &
+          (1 << VK_DYNAMIC_STATE_STENCIL_WRITE_MASK)) {
+         cfg.stencil_mask_misc.stencil_mask_front =
+            state->zs.s_front.write_mask;
+         cfg.stencil_mask_misc.stencil_mask_back = state->zs.s_back.write_mask;
+      }
+
+      if (pipeline->dynamic_state_mask &
+          (1 << VK_DYNAMIC_STATE_STENCIL_REFERENCE)) {
+         cfg.stencil_front.reference_value = state->zs.s_front.ref;
+         cfg.stencil_back.reference_value = state->zs.s_back.ref;
+      }
+   }
+}
+
+void
+panvk_per_arch(emit_base_fs_rsd)(const struct panvk_device *dev,
+                                 const struct panvk_pipeline *pipeline,
+                                 void *rsd)
+{
+   const struct pan_shader_info *info = &pipeline->fs.info;
+
+   pan_pack(rsd, RENDERER_STATE, cfg) {
+      if (pipeline->fs.required) {
+         pan_shader_prepare_rsd(info, pipeline->fs.address, &cfg);
+
+         uint8_t rt_written =
+            pipeline->fs.info.outputs_written >> FRAG_RESULT_DATA0;
+         uint8_t rt_mask = pipeline->fs.rt_mask;
+         cfg.properties.allow_forward_pixel_to_kill =
+            pipeline->fs.info.fs.can_fpk && !(rt_mask & ~rt_written) &&
+            !pipeline->ms.alpha_to_coverage && !pipeline->blend.reads_dest;
+
+         bool writes_zs = pipeline->zs.z_write || pipeline->zs.s_test;
+         bool zs_always_passes = !pipeline->zs.z_test && !pipeline->zs.s_test;
+         bool oq = false; /* TODO: Occlusion queries */
+
+         struct pan_earlyzs_state earlyzs =
+            pan_earlyzs_get(pan_earlyzs_analyze(info), writes_zs || oq,
+                            pipeline->ms.alpha_to_coverage, zs_always_passes);
+
+         cfg.properties.pixel_kill_operation = earlyzs.kill;
+         cfg.properties.zs_update_operation = earlyzs.update;
+      } else {
+         cfg.properties.depth_source = MALI_DEPTH_SOURCE_FIXED_FUNCTION;
+         cfg.properties.allow_forward_pixel_to_kill = true;
+         cfg.properties.allow_forward_pixel_to_be_killed = true;
+         cfg.properties.zs_update_operation = MALI_PIXEL_KILL_STRONG_EARLY;
+      }
+
+      bool msaa = pipeline->ms.rast_samples > 1;
+      cfg.multisample_misc.multisample_enable = msaa;
+      cfg.multisample_misc.sample_mask =
+         msaa ? pipeline->ms.sample_mask : UINT16_MAX;
+
+      cfg.multisample_misc.depth_function =
+         pipeline->zs.z_test ? pipeline->zs.z_compare_func : MALI_FUNC_ALWAYS;
+
+      cfg.multisample_misc.depth_write_mask = pipeline->zs.z_write;
+      cfg.multisample_misc.fixed_function_near_discard =
+         !pipeline->rast.clamp_depth;
+      cfg.multisample_misc.fixed_function_far_discard =
+         !pipeline->rast.clamp_depth;
+      cfg.multisample_misc.shader_depth_range_fixed = true;
+
+      cfg.stencil_mask_misc.stencil_enable = pipeline->zs.s_test;
+      cfg.stencil_mask_misc.alpha_to_coverage = pipeline->ms.alpha_to_coverage;
+      cfg.stencil_mask_misc.alpha_test_compare_function = MALI_FUNC_ALWAYS;
+      cfg.stencil_mask_misc.front_facing_depth_bias =
+         pipeline->rast.depth_bias.enable;
+      cfg.stencil_mask_misc.back_facing_depth_bias =
+         pipeline->rast.depth_bias.enable;
+      cfg.stencil_mask_misc.single_sampled_lines =
+         pipeline->ms.rast_samples <= 1;
+
+      if (!(pipeline->dynamic_state_mask &
+            (1 << VK_DYNAMIC_STATE_DEPTH_BIAS))) {
+         cfg.depth_units = pipeline->rast.depth_bias.constant_factor * 2.0f;
+         cfg.depth_factor = pipeline->rast.depth_bias.slope_factor;
+         cfg.depth_bias_clamp = pipeline->rast.depth_bias.clamp;
+      }
+
+      if (!(pipeline->dynamic_state_mask &
+            (1 << VK_DYNAMIC_STATE_STENCIL_COMPARE_MASK))) {
+         cfg.stencil_front.mask = pipeline->zs.s_front.compare_mask;
+         cfg.stencil_back.mask = pipeline->zs.s_back.compare_mask;
+      }
+
+      if (!(pipeline->dynamic_state_mask &
+            (1 << VK_DYNAMIC_STATE_STENCIL_WRITE_MASK))) {
+         cfg.stencil_mask_misc.stencil_mask_front =
+            pipeline->zs.s_front.write_mask;
+         cfg.stencil_mask_misc.stencil_mask_back =
+            pipeline->zs.s_back.write_mask;
+      }
+
+      if (!(pipeline->dynamic_state_mask &
+            (1 << VK_DYNAMIC_STATE_STENCIL_REFERENCE))) {
+         cfg.stencil_front.reference_value = pipeline->zs.s_front.ref;
+         cfg.stencil_back.reference_value = pipeline->zs.s_back.ref;
+      }
+
+      cfg.stencil_front.compare_function = pipeline->zs.s_front.compare_func;
+      cfg.stencil_front.stencil_fail = pipeline->zs.s_front.fail_op;
+      cfg.stencil_front.depth_fail = pipeline->zs.s_front.z_fail_op;
+      cfg.stencil_front.depth_pass = pipeline->zs.s_front.pass_op;
+      cfg.stencil_back.compare_function = pipeline->zs.s_back.compare_func;
+      cfg.stencil_back.stencil_fail = pipeline->zs.s_back.fail_op;
+      cfg.stencil_back.depth_fail = pipeline->zs.s_back.z_fail_op;
+      cfg.stencil_back.depth_pass = pipeline->zs.s_back.pass_op;
+   }
+}
+
+void
+panvk_per_arch(emit_non_fs_rsd)(const struct panvk_device *dev,
+                                const struct pan_shader_info *shader_info,
+                                mali_ptr shader_ptr, void *rsd)
+{
+   assert(shader_info->stage != MESA_SHADER_FRAGMENT);
+
+   pan_pack(rsd, RENDERER_STATE, cfg) {
+      pan_shader_prepare_rsd(shader_info, shader_ptr, &cfg);
+   }
+}
+
+void
+panvk_per_arch(emit_tiler_context)(const struct panvk_device *dev,
+                                   unsigned width, unsigned height,
+                                   const struct panfrost_ptr *descs)
+{
+   const struct panfrost_device *pdev = &dev->physical_device->pdev;
+
+   pan_pack(descs->cpu + pan_size(TILER_CONTEXT), TILER_HEAP, cfg) {
+      cfg.size = pdev->tiler_heap->size;
+      cfg.base = pdev->tiler_heap->ptr.gpu;
+      cfg.bottom = pdev->tiler_heap->ptr.gpu;
+      cfg.top = pdev->tiler_heap->ptr.gpu + pdev->tiler_heap->size;
+   }
+
+   pan_pack(descs->cpu, TILER_CONTEXT, cfg) {
+      cfg.hierarchy_mask = 0x28;
+      cfg.fb_width = width;
+      cfg.fb_height = height;
+      cfg.heap = descs->gpu + pan_size(TILER_CONTEXT);
+   }
+}
diff -up mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_cs.h.16~ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_cs.h
--- mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_cs.h.16~	2023-07-29 15:51:45.093980518 +0100
+++ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_cs.h	2023-07-29 15:51:45.083980485 +0100
@@ -0,0 +1,108 @@
+/*
+ * Copyright (C) 2021 Collabora Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#ifndef PANVK_PRIVATE_H
+#error "Must be included from panvk_private.h"
+#endif
+
+#ifndef PAN_ARCH
+#error "no arch"
+#endif
+
+#include "compiler/shader_enums.h"
+#include <vulkan/vulkan.h>
+
+void panvk_per_arch(emit_varying)(const struct panvk_device *dev,
+                                  const struct panvk_varyings_info *varyings,
+                                  gl_shader_stage stage, unsigned idx,
+                                  void *attrib);
+
+void panvk_per_arch(emit_varyings)(const struct panvk_device *dev,
+                                   const struct panvk_varyings_info *varyings,
+                                   gl_shader_stage stage, void *descs);
+
+void
+   panvk_per_arch(emit_varying_bufs)(const struct panvk_varyings_info *varyings,
+                                     void *descs);
+
+void panvk_per_arch(emit_attrib_bufs)(const struct panvk_attribs_info *info,
+                                      const struct panvk_attrib_buf *bufs,
+                                      unsigned buf_count,
+                                      const struct panvk_draw_info *draw,
+                                      void *descs);
+
+void panvk_per_arch(emit_attribs)(const struct panvk_device *dev,
+                                  const struct panvk_draw_info *draw,
+                                  const struct panvk_attribs_info *attribs,
+                                  const struct panvk_attrib_buf *bufs,
+                                  unsigned buf_count, void *descs);
+
+void panvk_per_arch(emit_ubo)(mali_ptr address, size_t size, void *desc);
+
+void panvk_per_arch(emit_ubos)(const struct panvk_pipeline *pipeline,
+                               const struct panvk_descriptor_state *state,
+                               void *descs);
+
+void panvk_per_arch(emit_sampler)(const VkSamplerCreateInfo *pCreateInfo,
+                                  void *desc);
+
+void panvk_per_arch(emit_vertex_job)(const struct panvk_pipeline *pipeline,
+                                     const struct panvk_draw_info *draw,
+                                     void *job);
+
+void
+   panvk_per_arch(emit_compute_job)(const struct panvk_pipeline *pipeline,
+                                    const struct panvk_dispatch_info *dispatch,
+                                    void *job);
+
+void panvk_per_arch(emit_tiler_job)(const struct panvk_pipeline *pipeline,
+                                    const struct panvk_draw_info *draw,
+                                    void *job);
+
+void panvk_per_arch(emit_viewport)(const VkViewport *viewport,
+                                   const VkRect2D *scissor, void *vpd);
+
+void panvk_per_arch(emit_blend)(const struct panvk_device *dev,
+                                const struct panvk_pipeline *pipeline,
+                                unsigned rt, void *bd);
+
+void panvk_per_arch(emit_blend_constant)(const struct panvk_device *dev,
+                                         const struct panvk_pipeline *pipeline,
+                                         unsigned rt, const float *constants,
+                                         void *bd);
+
+void panvk_per_arch(emit_dyn_fs_rsd)(const struct panvk_pipeline *pipeline,
+                                     const struct panvk_cmd_state *state,
+                                     void *rsd);
+
+void panvk_per_arch(emit_base_fs_rsd)(const struct panvk_device *dev,
+                                      const struct panvk_pipeline *pipeline,
+                                      void *rsd);
+
+void panvk_per_arch(emit_non_fs_rsd)(const struct panvk_device *dev,
+                                     const struct pan_shader_info *shader_info,
+                                     mali_ptr shader_ptr, void *rsd);
+
+void panvk_per_arch(emit_tiler_context)(const struct panvk_device *dev,
+                                        unsigned width, unsigned height,
+                                        const struct panfrost_ptr *descs);
diff -up mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_descriptor_set.c.16~ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_descriptor_set.c
--- mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_descriptor_set.c.16~	2023-07-29 15:51:45.093980518 +0100
+++ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_descriptor_set.c	2023-07-29 15:51:45.083980485 +0100
@@ -0,0 +1,1004 @@
+/*
+ * Copyright © 2021 Collabora Ltd.
+ *
+ * Derived from:
+ * Copyright © 2016 Red Hat.
+ * Copyright © 2016 Bas Nieuwenhuizen
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#include "genxml/gen_macros.h"
+
+#include "panvk_private.h"
+
+#include <assert.h>
+#include <fcntl.h>
+#include <stdbool.h>
+#include <string.h>
+#include <unistd.h>
+
+#include "util/mesa-sha1.h"
+#include "vk_descriptor_update_template.h"
+#include "vk_descriptors.h"
+#include "vk_util.h"
+
+#include "pan_bo.h"
+#include "panvk_cs.h"
+
+#define PANVK_DESCRIPTOR_ALIGN 8
+
+struct panvk_bview_desc {
+   uint32_t elems;
+};
+
+static void
+panvk_fill_bview_desc(struct panvk_bview_desc *desc,
+                      struct panvk_buffer_view *view)
+{
+   desc->elems = view->elems;
+}
+
+struct panvk_image_desc {
+   uint16_t width;
+   uint16_t height;
+   uint16_t depth;
+   uint8_t levels;
+   uint8_t samples;
+};
+
+static void
+panvk_fill_image_desc(struct panvk_image_desc *desc,
+                      struct panvk_image_view *view)
+{
+   desc->width = view->vk.extent.width - 1;
+   desc->height = view->vk.extent.height - 1;
+   desc->depth = view->vk.extent.depth - 1;
+   desc->levels = view->vk.level_count;
+   desc->samples = view->vk.image->samples;
+
+   /* Stick array layer count after the last valid size component */
+   if (view->vk.image->image_type == VK_IMAGE_TYPE_1D)
+      desc->height = view->vk.layer_count - 1;
+   else if (view->vk.image->image_type == VK_IMAGE_TYPE_2D)
+      desc->depth = view->vk.layer_count - 1;
+}
+
+VkResult
+panvk_per_arch(CreateDescriptorSetLayout)(
+   VkDevice _device, const VkDescriptorSetLayoutCreateInfo *pCreateInfo,
+   const VkAllocationCallbacks *pAllocator, VkDescriptorSetLayout *pSetLayout)
+{
+   VK_FROM_HANDLE(panvk_device, device, _device);
+   struct panvk_descriptor_set_layout *set_layout;
+   VkDescriptorSetLayoutBinding *bindings = NULL;
+   unsigned num_bindings = 0;
+   VkResult result;
+
+   if (pCreateInfo->bindingCount) {
+      result = vk_create_sorted_bindings(pCreateInfo->pBindings,
+                                         pCreateInfo->bindingCount, &bindings);
+      if (result != VK_SUCCESS)
+         return vk_error(device, result);
+
+      num_bindings = bindings[pCreateInfo->bindingCount - 1].binding + 1;
+   }
+
+   unsigned num_immutable_samplers = 0;
+   for (unsigned i = 0; i < pCreateInfo->bindingCount; i++) {
+      if (bindings[i].pImmutableSamplers)
+         num_immutable_samplers += bindings[i].descriptorCount;
+   }
+
+   size_t size =
+      sizeof(*set_layout) +
+      (sizeof(struct panvk_descriptor_set_binding_layout) * num_bindings) +
+      (sizeof(struct panvk_sampler *) * num_immutable_samplers);
+   set_layout = vk_descriptor_set_layout_zalloc(&device->vk, size);
+   if (!set_layout) {
+      result = VK_ERROR_OUT_OF_HOST_MEMORY;
+      goto err_free_bindings;
+   }
+
+   struct panvk_sampler **immutable_samplers =
+      (struct panvk_sampler **)((uint8_t *)set_layout + sizeof(*set_layout) +
+                                (sizeof(
+                                    struct panvk_descriptor_set_binding_layout) *
+                                 num_bindings));
+
+   set_layout->binding_count = num_bindings;
+
+   unsigned sampler_idx = 0, tex_idx = 0, ubo_idx = 0;
+   unsigned dyn_ubo_idx = 0, dyn_ssbo_idx = 0, img_idx = 0;
+   uint32_t desc_ubo_size = 0;
+
+   for (unsigned i = 0; i < pCreateInfo->bindingCount; i++) {
+      const VkDescriptorSetLayoutBinding *binding = &bindings[i];
+      struct panvk_descriptor_set_binding_layout *binding_layout =
+         &set_layout->bindings[binding->binding];
+
+      binding_layout->type = binding->descriptorType;
+      binding_layout->array_size = binding->descriptorCount;
+      binding_layout->shader_stages = binding->stageFlags;
+      binding_layout->desc_ubo_stride = 0;
+      if (binding->pImmutableSamplers) {
+         binding_layout->immutable_samplers = immutable_samplers;
+         immutable_samplers += binding_layout->array_size;
+         for (unsigned j = 0; j < binding_layout->array_size; j++) {
+            VK_FROM_HANDLE(panvk_sampler, sampler,
+                           binding->pImmutableSamplers[j]);
+            binding_layout->immutable_samplers[j] = sampler;
+         }
+      }
+
+      switch (binding_layout->type) {
+      case VK_DESCRIPTOR_TYPE_SAMPLER:
+         binding_layout->sampler_idx = sampler_idx;
+         sampler_idx += binding_layout->array_size;
+         break;
+      case VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER:
+         binding_layout->sampler_idx = sampler_idx;
+         binding_layout->tex_idx = tex_idx;
+         sampler_idx += binding_layout->array_size;
+         tex_idx += binding_layout->array_size;
+         binding_layout->desc_ubo_stride = sizeof(struct panvk_image_desc);
+         break;
+      case VK_DESCRIPTOR_TYPE_SAMPLED_IMAGE:
+      case VK_DESCRIPTOR_TYPE_INPUT_ATTACHMENT:
+         binding_layout->tex_idx = tex_idx;
+         tex_idx += binding_layout->array_size;
+         binding_layout->desc_ubo_stride = sizeof(struct panvk_image_desc);
+         break;
+      case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
+         binding_layout->tex_idx = tex_idx;
+         tex_idx += binding_layout->array_size;
+         binding_layout->desc_ubo_stride = sizeof(struct panvk_bview_desc);
+         break;
+      case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC:
+         binding_layout->dyn_ubo_idx = dyn_ubo_idx;
+         dyn_ubo_idx += binding_layout->array_size;
+         break;
+      case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER:
+         binding_layout->ubo_idx = ubo_idx;
+         ubo_idx += binding_layout->array_size;
+         break;
+      case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER_DYNAMIC:
+         binding_layout->dyn_ssbo_idx = dyn_ssbo_idx;
+         dyn_ssbo_idx += binding_layout->array_size;
+         break;
+      case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER:
+         binding_layout->desc_ubo_stride = sizeof(struct panvk_ssbo_addr);
+         break;
+      case VK_DESCRIPTOR_TYPE_STORAGE_IMAGE:
+         binding_layout->img_idx = img_idx;
+         img_idx += binding_layout->array_size;
+         binding_layout->desc_ubo_stride = sizeof(struct panvk_image_desc);
+         break;
+      case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
+         binding_layout->img_idx = img_idx;
+         img_idx += binding_layout->array_size;
+         binding_layout->desc_ubo_stride = sizeof(struct panvk_bview_desc);
+         break;
+      default:
+         unreachable("Invalid descriptor type");
+      }
+
+      desc_ubo_size = ALIGN_POT(desc_ubo_size, PANVK_DESCRIPTOR_ALIGN);
+      binding_layout->desc_ubo_offset = desc_ubo_size;
+      desc_ubo_size +=
+         binding_layout->desc_ubo_stride * binding_layout->array_size;
+   }
+
+   set_layout->desc_ubo_size = desc_ubo_size;
+   if (desc_ubo_size > 0)
+      set_layout->desc_ubo_index = ubo_idx++;
+
+   set_layout->num_samplers = sampler_idx;
+   set_layout->num_textures = tex_idx;
+   set_layout->num_ubos = ubo_idx;
+   set_layout->num_dyn_ubos = dyn_ubo_idx;
+   set_layout->num_dyn_ssbos = dyn_ssbo_idx;
+   set_layout->num_imgs = img_idx;
+
+   free(bindings);
+   *pSetLayout = panvk_descriptor_set_layout_to_handle(set_layout);
+   return VK_SUCCESS;
+
+err_free_bindings:
+   free(bindings);
+   return vk_error(device, result);
+}
+
+static void panvk_write_sampler_desc_raw(struct panvk_descriptor_set *set,
+                                         uint32_t binding, uint32_t elem,
+                                         struct panvk_sampler *sampler);
+
+static VkResult
+panvk_per_arch(descriptor_set_create)(
+   struct panvk_device *device, struct panvk_descriptor_pool *pool,
+   const struct panvk_descriptor_set_layout *layout,
+   struct panvk_descriptor_set **out_set)
+{
+   struct panvk_descriptor_set *set;
+
+   /* TODO: Allocate from the pool! */
+   set =
+      vk_object_zalloc(&device->vk, NULL, sizeof(struct panvk_descriptor_set),
+                       VK_OBJECT_TYPE_DESCRIPTOR_SET);
+   if (!set)
+      return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
+
+   set->layout = layout;
+
+   if (layout->num_ubos) {
+      set->ubos = vk_zalloc(&device->vk.alloc,
+                            pan_size(UNIFORM_BUFFER) * layout->num_ubos, 8,
+                            VK_OBJECT_TYPE_DESCRIPTOR_SET);
+      if (!set->ubos)
+         goto err_free_set;
+   }
+
+   if (layout->num_dyn_ubos) {
+      set->dyn_ubos = vk_zalloc(&device->vk.alloc,
+                                sizeof(*set->dyn_ubos) * layout->num_dyn_ubos,
+                                8, VK_OBJECT_TYPE_DESCRIPTOR_SET);
+      if (!set->dyn_ubos)
+         goto err_free_set;
+   }
+
+   if (layout->num_dyn_ssbos) {
+      set->dyn_ssbos = vk_zalloc(
+         &device->vk.alloc, sizeof(*set->dyn_ssbos) * layout->num_dyn_ssbos, 8,
+         VK_OBJECT_TYPE_DESCRIPTOR_SET);
+      if (!set->dyn_ssbos)
+         goto err_free_set;
+   }
+
+   if (layout->num_samplers) {
+      set->samplers =
+         vk_zalloc(&device->vk.alloc, pan_size(SAMPLER) * layout->num_samplers,
+                   8, VK_OBJECT_TYPE_DESCRIPTOR_SET);
+      if (!set->samplers)
+         goto err_free_set;
+   }
+
+   if (layout->num_textures) {
+      set->textures =
+         vk_zalloc(&device->vk.alloc, pan_size(TEXTURE) * layout->num_textures,
+                   8, VK_OBJECT_TYPE_DESCRIPTOR_SET);
+      if (!set->textures)
+         goto err_free_set;
+   }
+
+   if (layout->num_imgs) {
+      set->img_fmts =
+         vk_zalloc(&device->vk.alloc, sizeof(*set->img_fmts) * layout->num_imgs,
+                   8, VK_OBJECT_TYPE_DESCRIPTOR_SET);
+      if (!set->img_fmts)
+         goto err_free_set;
+
+      set->img_attrib_bufs = vk_zalloc(
+         &device->vk.alloc, pan_size(ATTRIBUTE_BUFFER) * 2 * layout->num_imgs,
+         8, VK_OBJECT_TYPE_DESCRIPTOR_SET);
+      if (!set->img_attrib_bufs)
+         goto err_free_set;
+   }
+
+   if (layout->desc_ubo_size) {
+      set->desc_bo =
+         panfrost_bo_create(&device->physical_device->pdev,
+                            layout->desc_ubo_size, 0, "Descriptor set");
+      if (!set->desc_bo)
+         goto err_free_set;
+
+      struct mali_uniform_buffer_packed *ubos = set->ubos;
+
+      panvk_per_arch(emit_ubo)(set->desc_bo->ptr.gpu, layout->desc_ubo_size,
+                               &ubos[layout->desc_ubo_index]);
+   }
+
+   for (unsigned i = 0; i < layout->binding_count; i++) {
+      if (!layout->bindings[i].immutable_samplers)
+         continue;
+
+      for (unsigned j = 0; j < layout->bindings[i].array_size; j++) {
+         struct panvk_sampler *sampler =
+            layout->bindings[i].immutable_samplers[j];
+         panvk_write_sampler_desc_raw(set, i, j, sampler);
+      }
+   }
+
+   *out_set = set;
+   return VK_SUCCESS;
+
+err_free_set:
+   vk_free(&device->vk.alloc, set->textures);
+   vk_free(&device->vk.alloc, set->samplers);
+   vk_free(&device->vk.alloc, set->ubos);
+   vk_free(&device->vk.alloc, set->dyn_ubos);
+   vk_free(&device->vk.alloc, set->dyn_ssbos);
+   vk_free(&device->vk.alloc, set->img_fmts);
+   vk_free(&device->vk.alloc, set->img_attrib_bufs);
+   if (set->desc_bo)
+      panfrost_bo_unreference(set->desc_bo);
+   vk_object_free(&device->vk, NULL, set);
+   return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
+}
+
+VkResult
+panvk_per_arch(AllocateDescriptorSets)(
+   VkDevice _device, const VkDescriptorSetAllocateInfo *pAllocateInfo,
+   VkDescriptorSet *pDescriptorSets)
+{
+   VK_FROM_HANDLE(panvk_device, device, _device);
+   VK_FROM_HANDLE(panvk_descriptor_pool, pool, pAllocateInfo->descriptorPool);
+   VkResult result;
+   unsigned i;
+
+   for (i = 0; i < pAllocateInfo->descriptorSetCount; i++) {
+      VK_FROM_HANDLE(panvk_descriptor_set_layout, layout,
+                     pAllocateInfo->pSetLayouts[i]);
+      struct panvk_descriptor_set *set = NULL;
+
+      result =
+         panvk_per_arch(descriptor_set_create)(device, pool, layout, &set);
+      if (result != VK_SUCCESS)
+         goto err_free_sets;
+
+      pDescriptorSets[i] = panvk_descriptor_set_to_handle(set);
+   }
+
+   return VK_SUCCESS;
+
+err_free_sets:
+   panvk_FreeDescriptorSets(_device, pAllocateInfo->descriptorPool, i,
+                            pDescriptorSets);
+   for (i = 0; i < pAllocateInfo->descriptorSetCount; i++)
+      pDescriptorSets[i] = VK_NULL_HANDLE;
+
+   return result;
+}
+
+static void *
+panvk_desc_ubo_data(struct panvk_descriptor_set *set, uint32_t binding,
+                    uint32_t elem)
+{
+   const struct panvk_descriptor_set_binding_layout *binding_layout =
+      &set->layout->bindings[binding];
+
+   return (char *)set->desc_bo->ptr.cpu + binding_layout->desc_ubo_offset +
+          elem * binding_layout->desc_ubo_stride;
+}
+
+static struct mali_sampler_packed *
+panvk_sampler_desc(struct panvk_descriptor_set *set, uint32_t binding,
+                   uint32_t elem)
+{
+   const struct panvk_descriptor_set_binding_layout *binding_layout =
+      &set->layout->bindings[binding];
+
+   uint32_t sampler_idx = binding_layout->sampler_idx + elem;
+
+   return &((struct mali_sampler_packed *)set->samplers)[sampler_idx];
+}
+
+static void
+panvk_write_sampler_desc_raw(struct panvk_descriptor_set *set, uint32_t binding,
+                             uint32_t elem, struct panvk_sampler *sampler)
+{
+   memcpy(panvk_sampler_desc(set, binding, elem), &sampler->desc,
+          sizeof(sampler->desc));
+}
+
+static void
+panvk_write_sampler_desc(UNUSED struct panvk_device *dev,
+                         struct panvk_descriptor_set *set, uint32_t binding,
+                         uint32_t elem,
+                         const VkDescriptorImageInfo *const pImageInfo)
+{
+   const struct panvk_descriptor_set_binding_layout *binding_layout =
+      &set->layout->bindings[binding];
+
+   if (binding_layout->immutable_samplers)
+      return;
+
+   VK_FROM_HANDLE(panvk_sampler, sampler, pImageInfo->sampler);
+   panvk_write_sampler_desc_raw(set, binding, elem, sampler);
+}
+
+static void
+panvk_copy_sampler_desc(struct panvk_descriptor_set *dst_set,
+                        uint32_t dst_binding, uint32_t dst_elem,
+                        struct panvk_descriptor_set *src_set,
+                        uint32_t src_binding, uint32_t src_elem)
+{
+   const struct panvk_descriptor_set_binding_layout *dst_binding_layout =
+      &dst_set->layout->bindings[dst_binding];
+
+   if (dst_binding_layout->immutable_samplers)
+      return;
+
+   memcpy(panvk_sampler_desc(dst_set, dst_binding, dst_elem),
+          panvk_sampler_desc(src_set, src_binding, src_elem),
+          sizeof(struct mali_sampler_packed));
+}
+
+static struct mali_texture_packed *
+panvk_tex_desc(struct panvk_descriptor_set *set, uint32_t binding,
+               uint32_t elem)
+{
+   const struct panvk_descriptor_set_binding_layout *binding_layout =
+      &set->layout->bindings[binding];
+
+   unsigned tex_idx = binding_layout->tex_idx + elem;
+
+   return &((struct mali_texture_packed *)set->textures)[tex_idx];
+}
+
+static void
+panvk_write_tex_desc(UNUSED struct panvk_device *dev,
+                     struct panvk_descriptor_set *set, uint32_t binding,
+                     uint32_t elem,
+                     const VkDescriptorImageInfo *const pImageInfo)
+{
+   VK_FROM_HANDLE(panvk_image_view, view, pImageInfo->imageView);
+
+   memcpy(panvk_tex_desc(set, binding, elem), view->descs.tex,
+          pan_size(TEXTURE));
+
+   panvk_fill_image_desc(panvk_desc_ubo_data(set, binding, elem), view);
+}
+
+static void
+panvk_copy_tex_desc(struct panvk_descriptor_set *dst_set, uint32_t dst_binding,
+                    uint32_t dst_elem, struct panvk_descriptor_set *src_set,
+                    uint32_t src_binding, uint32_t src_elem)
+{
+   *panvk_tex_desc(dst_set, dst_binding, dst_elem) =
+      *panvk_tex_desc(src_set, src_binding, src_elem);
+
+   /* Descriptor UBO data gets copied automatically */
+}
+
+static void
+panvk_write_tex_buf_desc(UNUSED struct panvk_device *dev,
+                         struct panvk_descriptor_set *set, uint32_t binding,
+                         uint32_t elem, const VkBufferView bufferView)
+{
+   VK_FROM_HANDLE(panvk_buffer_view, view, bufferView);
+
+   memcpy(panvk_tex_desc(set, binding, elem), view->descs.tex,
+          pan_size(TEXTURE));
+
+   panvk_fill_bview_desc(panvk_desc_ubo_data(set, binding, elem), view);
+}
+
+static uint32_t
+panvk_img_idx(struct panvk_descriptor_set *set, uint32_t binding, uint32_t elem)
+{
+   const struct panvk_descriptor_set_binding_layout *binding_layout =
+      &set->layout->bindings[binding];
+
+   return binding_layout->img_idx + elem;
+}
+
+static void
+panvk_write_img_desc(struct panvk_device *dev, struct panvk_descriptor_set *set,
+                     uint32_t binding, uint32_t elem,
+                     const VkDescriptorImageInfo *pImageInfo)
+{
+   const struct panfrost_device *pdev = &dev->physical_device->pdev;
+   VK_FROM_HANDLE(panvk_image_view, view, pImageInfo->imageView);
+
+   unsigned img_idx = panvk_img_idx(set, binding, elem);
+   void *attrib_buf = (uint8_t *)set->img_attrib_bufs +
+                      (pan_size(ATTRIBUTE_BUFFER) * 2 * img_idx);
+
+   set->img_fmts[img_idx] = pdev->formats[view->pview.format].hw;
+   memcpy(attrib_buf, view->descs.img_attrib_buf,
+          pan_size(ATTRIBUTE_BUFFER) * 2);
+
+   panvk_fill_image_desc(panvk_desc_ubo_data(set, binding, elem), view);
+}
+
+static void
+panvk_copy_img_desc(struct panvk_descriptor_set *dst_set, uint32_t dst_binding,
+                    uint32_t dst_elem, struct panvk_descriptor_set *src_set,
+                    uint32_t src_binding, uint32_t src_elem)
+{
+   unsigned dst_img_idx = panvk_img_idx(dst_set, dst_binding, dst_elem);
+   unsigned src_img_idx = panvk_img_idx(src_set, src_binding, src_elem);
+
+   void *dst_attrib_buf = (uint8_t *)dst_set->img_attrib_bufs +
+                          (pan_size(ATTRIBUTE_BUFFER) * 2 * dst_img_idx);
+   void *src_attrib_buf = (uint8_t *)src_set->img_attrib_bufs +
+                          (pan_size(ATTRIBUTE_BUFFER) * 2 * src_img_idx);
+
+   dst_set->img_fmts[dst_img_idx] = src_set->img_fmts[src_img_idx];
+   memcpy(dst_attrib_buf, src_attrib_buf, pan_size(ATTRIBUTE_BUFFER) * 2);
+
+   /* Descriptor UBO data gets copied automatically */
+}
+
+static void
+panvk_write_img_buf_desc(struct panvk_device *dev,
+                         struct panvk_descriptor_set *set, uint32_t binding,
+                         uint32_t elem, const VkBufferView bufferView)
+{
+   const struct panfrost_device *pdev = &dev->physical_device->pdev;
+   VK_FROM_HANDLE(panvk_buffer_view, view, bufferView);
+
+   unsigned img_idx = panvk_img_idx(set, binding, elem);
+   void *attrib_buf = (uint8_t *)set->img_attrib_bufs +
+                      (pan_size(ATTRIBUTE_BUFFER) * 2 * img_idx);
+
+   set->img_fmts[img_idx] = pdev->formats[view->fmt].hw;
+   memcpy(attrib_buf, view->descs.img_attrib_buf,
+          pan_size(ATTRIBUTE_BUFFER) * 2);
+
+   panvk_fill_bview_desc(panvk_desc_ubo_data(set, binding, elem), view);
+}
+
+static struct mali_uniform_buffer_packed *
+panvk_ubo_desc(struct panvk_descriptor_set *set, uint32_t binding,
+               uint32_t elem)
+{
+   const struct panvk_descriptor_set_binding_layout *binding_layout =
+      &set->layout->bindings[binding];
+
+   unsigned ubo_idx = binding_layout->ubo_idx + elem;
+
+   return &((struct mali_uniform_buffer_packed *)set->ubos)[ubo_idx];
+}
+
+static void
+panvk_write_ubo_desc(UNUSED struct panvk_device *dev,
+                     struct panvk_descriptor_set *set, uint32_t binding,
+                     uint32_t elem, const VkDescriptorBufferInfo *pBufferInfo)
+{
+   VK_FROM_HANDLE(panvk_buffer, buffer, pBufferInfo->buffer);
+
+   mali_ptr ptr = panvk_buffer_gpu_ptr(buffer, pBufferInfo->offset);
+   size_t size =
+      panvk_buffer_range(buffer, pBufferInfo->offset, pBufferInfo->range);
+
+   panvk_per_arch(emit_ubo)(ptr, size, panvk_ubo_desc(set, binding, elem));
+}
+
+static void
+panvk_copy_ubo_desc(struct panvk_descriptor_set *dst_set, uint32_t dst_binding,
+                    uint32_t dst_elem, struct panvk_descriptor_set *src_set,
+                    uint32_t src_binding, uint32_t src_elem)
+{
+   *panvk_ubo_desc(dst_set, dst_binding, dst_elem) =
+      *panvk_ubo_desc(src_set, src_binding, src_elem);
+}
+
+static struct panvk_buffer_desc *
+panvk_dyn_ubo_desc(struct panvk_descriptor_set *set, uint32_t binding,
+                   uint32_t elem)
+{
+   const struct panvk_descriptor_set_binding_layout *binding_layout =
+      &set->layout->bindings[binding];
+
+   return &set->dyn_ubos[binding_layout->dyn_ubo_idx + elem];
+}
+
+static void
+panvk_write_dyn_ubo_desc(UNUSED struct panvk_device *dev,
+                         struct panvk_descriptor_set *set, uint32_t binding,
+                         uint32_t elem,
+                         const VkDescriptorBufferInfo *pBufferInfo)
+{
+   VK_FROM_HANDLE(panvk_buffer, buffer, pBufferInfo->buffer);
+
+   *panvk_dyn_ubo_desc(set, binding, elem) = (struct panvk_buffer_desc){
+      .buffer = buffer,
+      .offset = pBufferInfo->offset,
+      .size = pBufferInfo->range,
+   };
+}
+
+static void
+panvk_copy_dyn_ubo_desc(struct panvk_descriptor_set *dst_set,
+                        uint32_t dst_binding, uint32_t dst_elem,
+                        struct panvk_descriptor_set *src_set,
+                        uint32_t src_binding, uint32_t src_elem)
+{
+   *panvk_dyn_ubo_desc(dst_set, dst_binding, dst_elem) =
+      *panvk_dyn_ubo_desc(src_set, src_binding, src_elem);
+}
+
+static void
+panvk_write_ssbo_desc(UNUSED struct panvk_device *dev,
+                      struct panvk_descriptor_set *set, uint32_t binding,
+                      uint32_t elem, const VkDescriptorBufferInfo *pBufferInfo)
+{
+   VK_FROM_HANDLE(panvk_buffer, buffer, pBufferInfo->buffer);
+
+   struct panvk_ssbo_addr *desc = panvk_desc_ubo_data(set, binding, elem);
+   *desc = (struct panvk_ssbo_addr){
+      .base_addr = panvk_buffer_gpu_ptr(buffer, pBufferInfo->offset),
+      .size =
+         panvk_buffer_range(buffer, pBufferInfo->offset, pBufferInfo->range),
+   };
+}
+
+static void
+panvk_copy_ssbo_desc(struct panvk_descriptor_set *dst_set, uint32_t dst_binding,
+                     uint32_t dst_elem, struct panvk_descriptor_set *src_set,
+                     uint32_t src_binding, uint32_t src_elem)
+{
+   /* Descriptor UBO data gets copied automatically */
+}
+
+static struct panvk_buffer_desc *
+panvk_dyn_ssbo_desc(struct panvk_descriptor_set *set, uint32_t binding,
+                    uint32_t elem)
+{
+   const struct panvk_descriptor_set_binding_layout *binding_layout =
+      &set->layout->bindings[binding];
+
+   return &set->dyn_ssbos[binding_layout->dyn_ssbo_idx + elem];
+}
+
+static void
+panvk_write_dyn_ssbo_desc(UNUSED struct panvk_device *dev,
+                          struct panvk_descriptor_set *set, uint32_t binding,
+                          uint32_t elem,
+                          const VkDescriptorBufferInfo *pBufferInfo)
+{
+   VK_FROM_HANDLE(panvk_buffer, buffer, pBufferInfo->buffer);
+
+   *panvk_dyn_ssbo_desc(set, binding, elem) = (struct panvk_buffer_desc){
+      .buffer = buffer,
+      .offset = pBufferInfo->offset,
+      .size = pBufferInfo->range,
+   };
+}
+
+static void
+panvk_copy_dyn_ssbo_desc(struct panvk_descriptor_set *dst_set,
+                         uint32_t dst_binding, uint32_t dst_elem,
+                         struct panvk_descriptor_set *src_set,
+                         uint32_t src_binding, uint32_t src_elem)
+{
+   *panvk_dyn_ssbo_desc(dst_set, dst_binding, dst_elem) =
+      *panvk_dyn_ssbo_desc(src_set, src_binding, src_elem);
+}
+
+void
+panvk_per_arch(UpdateDescriptorSets)(
+   VkDevice _device, uint32_t descriptorWriteCount,
+   const VkWriteDescriptorSet *pDescriptorWrites, uint32_t descriptorCopyCount,
+   const VkCopyDescriptorSet *pDescriptorCopies)
+{
+   VK_FROM_HANDLE(panvk_device, dev, _device);
+
+   for (unsigned i = 0; i < descriptorWriteCount; i++) {
+      const VkWriteDescriptorSet *write = &pDescriptorWrites[i];
+      VK_FROM_HANDLE(panvk_descriptor_set, set, write->dstSet);
+
+      switch (write->descriptorType) {
+      case VK_DESCRIPTOR_TYPE_SAMPLER:
+         for (uint32_t j = 0; j < write->descriptorCount; j++) {
+            panvk_write_sampler_desc(dev, set, write->dstBinding,
+                                     write->dstArrayElement + j,
+                                     &write->pImageInfo[j]);
+         }
+         break;
+
+      case VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER:
+         for (uint32_t j = 0; j < write->descriptorCount; j++) {
+            panvk_write_sampler_desc(dev, set, write->dstBinding,
+                                     write->dstArrayElement + j,
+                                     &write->pImageInfo[j]);
+            panvk_write_tex_desc(dev, set, write->dstBinding,
+                                 write->dstArrayElement + j,
+                                 &write->pImageInfo[j]);
+         }
+         break;
+
+      case VK_DESCRIPTOR_TYPE_SAMPLED_IMAGE:
+         for (uint32_t j = 0; j < write->descriptorCount; j++) {
+            panvk_write_tex_desc(dev, set, write->dstBinding,
+                                 write->dstArrayElement + j,
+                                 &write->pImageInfo[j]);
+         }
+         break;
+
+      case VK_DESCRIPTOR_TYPE_STORAGE_IMAGE:
+      case VK_DESCRIPTOR_TYPE_INPUT_ATTACHMENT:
+         for (uint32_t j = 0; j < write->descriptorCount; j++) {
+            panvk_write_img_desc(dev, set, write->dstBinding,
+                                 write->dstArrayElement + j,
+                                 &write->pImageInfo[j]);
+         }
+         break;
+
+      case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
+         for (uint32_t j = 0; j < write->descriptorCount; j++) {
+            panvk_write_tex_buf_desc(dev, set, write->dstBinding,
+                                     write->dstArrayElement + j,
+                                     write->pTexelBufferView[j]);
+         }
+         break;
+
+      case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
+         for (uint32_t j = 0; j < write->descriptorCount; j++) {
+            panvk_write_img_buf_desc(dev, set, write->dstBinding,
+                                     write->dstArrayElement + j,
+                                     write->pTexelBufferView[j]);
+         }
+         break;
+
+      case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER:
+         for (uint32_t j = 0; j < write->descriptorCount; j++) {
+            panvk_write_ubo_desc(dev, set, write->dstBinding,
+                                 write->dstArrayElement + j,
+                                 &write->pBufferInfo[j]);
+         }
+         break;
+
+      case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC:
+         for (uint32_t j = 0; j < write->descriptorCount; j++) {
+            panvk_write_dyn_ubo_desc(dev, set, write->dstBinding,
+                                     write->dstArrayElement + j,
+                                     &write->pBufferInfo[j]);
+         }
+         break;
+
+      case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER:
+         for (uint32_t j = 0; j < write->descriptorCount; j++) {
+            panvk_write_ssbo_desc(dev, set, write->dstBinding,
+                                  write->dstArrayElement + j,
+                                  &write->pBufferInfo[j]);
+         }
+         break;
+
+      case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER_DYNAMIC:
+         for (uint32_t j = 0; j < write->descriptorCount; j++) {
+            panvk_write_dyn_ssbo_desc(dev, set, write->dstBinding,
+                                      write->dstArrayElement + j,
+                                      &write->pBufferInfo[j]);
+         }
+         break;
+
+      default:
+         unreachable("Unsupported descriptor type");
+      }
+   }
+
+   for (unsigned i = 0; i < descriptorCopyCount; i++) {
+      const VkCopyDescriptorSet *copy = &pDescriptorCopies[i];
+      VK_FROM_HANDLE(panvk_descriptor_set, src_set, copy->srcSet);
+      VK_FROM_HANDLE(panvk_descriptor_set, dst_set, copy->dstSet);
+
+      const struct panvk_descriptor_set_binding_layout *dst_binding_layout =
+         &dst_set->layout->bindings[copy->dstBinding];
+      const struct panvk_descriptor_set_binding_layout *src_binding_layout =
+         &src_set->layout->bindings[copy->srcBinding];
+
+      assert(dst_binding_layout->type == src_binding_layout->type);
+
+      if (dst_binding_layout->desc_ubo_stride > 0 &&
+          src_binding_layout->desc_ubo_stride > 0) {
+         for (uint32_t j = 0; j < copy->descriptorCount; j++) {
+            memcpy(panvk_desc_ubo_data(dst_set, copy->dstBinding,
+                                       copy->dstArrayElement + j),
+                   panvk_desc_ubo_data(src_set, copy->srcBinding,
+                                       copy->srcArrayElement + j),
+                   MIN2(dst_binding_layout->desc_ubo_stride,
+                        src_binding_layout->desc_ubo_stride));
+         }
+      }
+
+      switch (src_binding_layout->type) {
+      case VK_DESCRIPTOR_TYPE_SAMPLER:
+         for (uint32_t j = 0; j < copy->descriptorCount; j++) {
+            panvk_copy_sampler_desc(
+               dst_set, copy->dstBinding, copy->dstArrayElement + j, src_set,
+               copy->srcBinding, copy->srcArrayElement + j);
+         }
+         break;
+
+      case VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER:
+         for (uint32_t j = 0; j < copy->descriptorCount; j++) {
+            panvk_copy_sampler_desc(
+               dst_set, copy->dstBinding, copy->dstArrayElement + j, src_set,
+               copy->srcBinding, copy->srcArrayElement + j);
+            panvk_copy_tex_desc(dst_set, copy->dstBinding,
+                                copy->dstArrayElement + j, src_set,
+                                copy->srcBinding, copy->srcArrayElement + j);
+         }
+         break;
+
+      case VK_DESCRIPTOR_TYPE_SAMPLED_IMAGE:
+      case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
+         for (uint32_t j = 0; j < copy->descriptorCount; j++) {
+            panvk_copy_tex_desc(dst_set, copy->dstBinding,
+                                copy->dstArrayElement + j, src_set,
+                                copy->srcBinding, copy->srcArrayElement + j);
+         }
+         break;
+
+      case VK_DESCRIPTOR_TYPE_STORAGE_IMAGE:
+      case VK_DESCRIPTOR_TYPE_INPUT_ATTACHMENT:
+      case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
+         for (uint32_t j = 0; j < copy->descriptorCount; j++) {
+            panvk_copy_img_desc(dst_set, copy->dstBinding,
+                                copy->dstArrayElement + j, src_set,
+                                copy->srcBinding, copy->srcArrayElement + j);
+         }
+         break;
+
+      case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER:
+         for (uint32_t j = 0; j < copy->descriptorCount; j++) {
+            panvk_copy_ubo_desc(dst_set, copy->dstBinding,
+                                copy->dstArrayElement + j, src_set,
+                                copy->srcBinding, copy->srcArrayElement + j);
+         }
+         break;
+
+      case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC:
+         for (uint32_t j = 0; j < copy->descriptorCount; j++) {
+            panvk_copy_dyn_ubo_desc(
+               dst_set, copy->dstBinding, copy->dstArrayElement + j, src_set,
+               copy->srcBinding, copy->srcArrayElement + j);
+         }
+         break;
+
+      case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER:
+         for (uint32_t j = 0; j < copy->descriptorCount; j++) {
+            panvk_copy_ssbo_desc(dst_set, copy->dstBinding,
+                                 copy->dstArrayElement + j, src_set,
+                                 copy->srcBinding, copy->srcArrayElement + j);
+         }
+         break;
+
+      case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER_DYNAMIC:
+         for (uint32_t j = 0; j < copy->descriptorCount; j++) {
+            panvk_copy_dyn_ssbo_desc(
+               dst_set, copy->dstBinding, copy->dstArrayElement + j, src_set,
+               copy->srcBinding, copy->srcArrayElement + j);
+         }
+         break;
+
+      default:
+         unreachable("Unsupported descriptor type");
+      }
+   }
+}
+
+void
+panvk_per_arch(UpdateDescriptorSetWithTemplate)(
+   VkDevice _device, VkDescriptorSet descriptorSet,
+   VkDescriptorUpdateTemplate descriptorUpdateTemplate, const void *data)
+{
+   VK_FROM_HANDLE(panvk_device, dev, _device);
+   VK_FROM_HANDLE(panvk_descriptor_set, set, descriptorSet);
+   VK_FROM_HANDLE(vk_descriptor_update_template, template,
+                  descriptorUpdateTemplate);
+
+   const struct panvk_descriptor_set_layout *layout = set->layout;
+
+   for (uint32_t i = 0; i < template->entry_count; i++) {
+      const struct vk_descriptor_template_entry *entry = &template->entries[i];
+      const struct panvk_descriptor_set_binding_layout *binding_layout =
+         &layout->bindings[entry->binding];
+
+      switch (entry->type) {
+      case VK_DESCRIPTOR_TYPE_SAMPLER:
+      case VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER:
+      case VK_DESCRIPTOR_TYPE_SAMPLED_IMAGE:
+         for (unsigned j = 0; j < entry->array_count; j++) {
+            const VkDescriptorImageInfo *info =
+               data + entry->offset + j * entry->stride;
+
+            if ((entry->type == VK_DESCRIPTOR_TYPE_SAMPLER ||
+                 entry->type == VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER) &&
+                !binding_layout->immutable_samplers) {
+
+               panvk_write_sampler_desc(dev, set, entry->binding,
+                                        entry->array_element + j, info);
+            }
+
+            if (entry->type == VK_DESCRIPTOR_TYPE_SAMPLED_IMAGE ||
+                entry->type == VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER) {
+
+               panvk_write_tex_desc(dev, set, entry->binding,
+                                    entry->array_element + j, info);
+            }
+         }
+         break;
+
+      case VK_DESCRIPTOR_TYPE_STORAGE_IMAGE:
+      case VK_DESCRIPTOR_TYPE_INPUT_ATTACHMENT:
+         for (unsigned j = 0; j < entry->array_count; j++) {
+            const VkDescriptorImageInfo *info =
+               data + entry->offset + j * entry->stride;
+
+            panvk_write_img_desc(dev, set, entry->binding,
+                                 entry->array_element + j, info);
+         }
+         break;
+
+      case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
+         for (unsigned j = 0; j < entry->array_count; j++) {
+            const VkBufferView *view = data + entry->offset + j * entry->stride;
+
+            panvk_write_tex_buf_desc(dev, set, entry->binding,
+                                     entry->array_element + j, *view);
+         }
+         break;
+
+      case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
+         for (unsigned j = 0; j < entry->array_count; j++) {
+            const VkBufferView *view = data + entry->offset + j * entry->stride;
+
+            panvk_write_img_buf_desc(dev, set, entry->binding,
+                                     entry->array_element + j, *view);
+         }
+         break;
+
+      case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER:
+         for (unsigned j = 0; j < entry->array_count; j++) {
+            const VkDescriptorBufferInfo *info =
+               data + entry->offset + j * entry->stride;
+
+            panvk_write_ubo_desc(dev, set, entry->binding,
+                                 entry->array_element + j, info);
+         }
+         break;
+
+      case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC:
+         for (unsigned j = 0; j < entry->array_count; j++) {
+            const VkDescriptorBufferInfo *info =
+               data + entry->offset + j * entry->stride;
+
+            panvk_write_dyn_ubo_desc(dev, set, entry->binding,
+                                     entry->array_element + j, info);
+         }
+         break;
+
+      case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER:
+         for (unsigned j = 0; j < entry->array_count; j++) {
+            const VkDescriptorBufferInfo *info =
+               data + entry->offset + j * entry->stride;
+
+            panvk_write_ssbo_desc(dev, set, entry->binding,
+                                  entry->array_element + j, info);
+         }
+         break;
+
+      case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER_DYNAMIC:
+         for (unsigned j = 0; j < entry->array_count; j++) {
+            const VkDescriptorBufferInfo *info =
+               data + entry->offset + j * entry->stride;
+
+            panvk_write_dyn_ssbo_desc(dev, set, entry->binding,
+                                      entry->array_element + j, info);
+         }
+         break;
+      default:
+         unreachable("Invalid type");
+      }
+   }
+}
diff -up mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_device.c.16~ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_device.c
--- mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_device.c.16~	2023-07-29 15:51:45.093980518 +0100
+++ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_device.c	2023-07-29 15:51:45.083980485 +0100
@@ -0,0 +1,322 @@
+/*
+ * Copyright © 2021 Collabora Ltd.
+ *
+ * Derived from tu_device.c which is:
+ * Copyright © 2016 Red Hat.
+ * Copyright © 2016 Bas Nieuwenhuizen
+ * Copyright © 2015 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#include "genxml/gen_macros.h"
+
+#include "decode.h"
+
+#include "panvk_cs.h"
+#include "panvk_private.h"
+
+#include "vk_drm_syncobj.h"
+
+static void
+panvk_queue_submit_batch(struct panvk_queue *queue, struct panvk_batch *batch,
+                         uint32_t *bos, unsigned nr_bos, uint32_t *in_fences,
+                         unsigned nr_in_fences)
+{
+   const struct panvk_device *dev = queue->device;
+   unsigned debug = dev->physical_device->instance->debug_flags;
+   const struct panfrost_device *pdev = &dev->physical_device->pdev;
+   int ret;
+
+   /* Reset the batch if it's already been issued */
+   if (batch->issued) {
+      util_dynarray_foreach(&batch->jobs, void *, job)
+         memset((*job), 0, 4 * 4);
+
+      /* Reset the tiler before re-issuing the batch */
+      if (batch->tiler.descs.cpu) {
+         memcpy(batch->tiler.descs.cpu, batch->tiler.templ,
+                pan_size(TILER_CONTEXT) + pan_size(TILER_HEAP));
+      }
+   }
+
+   if (batch->scoreboard.first_job) {
+      struct drm_panfrost_submit submit = {
+         .bo_handles = (uintptr_t)bos,
+         .bo_handle_count = nr_bos,
+         .in_syncs = (uintptr_t)in_fences,
+         .in_sync_count = nr_in_fences,
+         .out_sync = queue->sync,
+         .jc = batch->scoreboard.first_job,
+      };
+
+      ret = drmIoctl(pdev->fd, DRM_IOCTL_PANFROST_SUBMIT, &submit);
+      assert(!ret);
+
+      if (debug & (PANVK_DEBUG_TRACE | PANVK_DEBUG_SYNC)) {
+         ret =
+            drmSyncobjWait(pdev->fd, &submit.out_sync, 1, INT64_MAX, 0, NULL);
+         assert(!ret);
+      }
+
+      if (debug & PANVK_DEBUG_TRACE)
+         pandecode_jc(batch->scoreboard.first_job, pdev->gpu_id);
+
+      if (debug & PANVK_DEBUG_DUMP)
+         pandecode_dump_mappings();
+   }
+
+   if (batch->fragment_job) {
+      struct drm_panfrost_submit submit = {
+         .bo_handles = (uintptr_t)bos,
+         .bo_handle_count = nr_bos,
+         .out_sync = queue->sync,
+         .jc = batch->fragment_job,
+         .requirements = PANFROST_JD_REQ_FS,
+      };
+
+      if (batch->scoreboard.first_job) {
+         submit.in_syncs = (uintptr_t)(&queue->sync);
+         submit.in_sync_count = 1;
+      } else {
+         submit.in_syncs = (uintptr_t)in_fences;
+         submit.in_sync_count = nr_in_fences;
+      }
+
+      ret = drmIoctl(pdev->fd, DRM_IOCTL_PANFROST_SUBMIT, &submit);
+      assert(!ret);
+      if (debug & (PANVK_DEBUG_TRACE | PANVK_DEBUG_SYNC)) {
+         ret =
+            drmSyncobjWait(pdev->fd, &submit.out_sync, 1, INT64_MAX, 0, NULL);
+         assert(!ret);
+      }
+
+      if (debug & PANVK_DEBUG_TRACE)
+         pandecode_jc(batch->fragment_job, pdev->gpu_id);
+
+      if (debug & PANVK_DEBUG_DUMP)
+         pandecode_dump_mappings();
+   }
+
+   if (debug & PANVK_DEBUG_TRACE)
+      pandecode_next_frame();
+
+   batch->issued = true;
+}
+
+static void
+panvk_queue_transfer_sync(struct panvk_queue *queue, uint32_t syncobj)
+{
+   const struct panfrost_device *pdev = &queue->device->physical_device->pdev;
+   int ret;
+
+   struct drm_syncobj_handle handle = {
+      .handle = queue->sync,
+      .flags = DRM_SYNCOBJ_HANDLE_TO_FD_FLAGS_EXPORT_SYNC_FILE,
+      .fd = -1,
+   };
+
+   ret = drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_HANDLE_TO_FD, &handle);
+   assert(!ret);
+   assert(handle.fd >= 0);
+
+   handle.handle = syncobj;
+   ret = drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_FD_TO_HANDLE, &handle);
+   assert(!ret);
+
+   close(handle.fd);
+}
+
+static void
+panvk_add_wait_event_syncobjs(struct panvk_batch *batch, uint32_t *in_fences,
+                              unsigned *nr_in_fences)
+{
+   util_dynarray_foreach(&batch->event_ops, struct panvk_event_op, op) {
+      switch (op->type) {
+      case PANVK_EVENT_OP_SET:
+         /* Nothing to do yet */
+         break;
+      case PANVK_EVENT_OP_RESET:
+         /* Nothing to do yet */
+         break;
+      case PANVK_EVENT_OP_WAIT:
+         in_fences[(*nr_in_fences)++] = op->event->syncobj;
+         break;
+      default:
+         unreachable("bad panvk_event_op type\n");
+      }
+   }
+}
+
+static void
+panvk_signal_event_syncobjs(struct panvk_queue *queue,
+                            struct panvk_batch *batch)
+{
+   const struct panfrost_device *pdev = &queue->device->physical_device->pdev;
+
+   util_dynarray_foreach(&batch->event_ops, struct panvk_event_op, op) {
+      switch (op->type) {
+      case PANVK_EVENT_OP_SET: {
+         panvk_queue_transfer_sync(queue, op->event->syncobj);
+         break;
+      }
+      case PANVK_EVENT_OP_RESET: {
+         struct panvk_event *event = op->event;
+
+         struct drm_syncobj_array objs = {
+            .handles = (uint64_t)(uintptr_t)&event->syncobj,
+            .count_handles = 1};
+
+         int ret = drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_RESET, &objs);
+         assert(!ret);
+         break;
+      }
+      case PANVK_EVENT_OP_WAIT:
+         /* Nothing left to do */
+         break;
+      default:
+         unreachable("bad panvk_event_op type\n");
+      }
+   }
+}
+
+VkResult
+panvk_per_arch(queue_submit)(struct vk_queue *vk_queue,
+                             struct vk_queue_submit *submit)
+{
+   struct panvk_queue *queue = container_of(vk_queue, struct panvk_queue, vk);
+   const struct panfrost_device *pdev = &queue->device->physical_device->pdev;
+
+   unsigned nr_semaphores = submit->wait_count + 1;
+   uint32_t semaphores[nr_semaphores];
+
+   semaphores[0] = queue->sync;
+   for (unsigned i = 0; i < submit->wait_count; i++) {
+      assert(vk_sync_type_is_drm_syncobj(submit->waits[i].sync->type));
+      struct vk_drm_syncobj *syncobj =
+         vk_sync_as_drm_syncobj(submit->waits[i].sync);
+
+      semaphores[i + 1] = syncobj->syncobj;
+   }
+
+   for (uint32_t j = 0; j < submit->command_buffer_count; ++j) {
+      struct panvk_cmd_buffer *cmdbuf =
+         container_of(submit->command_buffers[j], struct panvk_cmd_buffer, vk);
+
+      list_for_each_entry(struct panvk_batch, batch, &cmdbuf->batches, node) {
+         /* FIXME: should be done at the batch level */
+         unsigned nr_bos =
+            panvk_pool_num_bos(&cmdbuf->desc_pool) +
+            panvk_pool_num_bos(&cmdbuf->varying_pool) +
+            panvk_pool_num_bos(&cmdbuf->tls_pool) +
+            (batch->fb.info ? batch->fb.info->attachment_count : 0) +
+            (batch->blit.src ? 1 : 0) + (batch->blit.dst ? 1 : 0) +
+            (batch->scoreboard.first_tiler ? 1 : 0) + 1;
+         unsigned bo_idx = 0;
+         uint32_t bos[nr_bos];
+
+         panvk_pool_get_bo_handles(&cmdbuf->desc_pool, &bos[bo_idx]);
+         bo_idx += panvk_pool_num_bos(&cmdbuf->desc_pool);
+
+         panvk_pool_get_bo_handles(&cmdbuf->varying_pool, &bos[bo_idx]);
+         bo_idx += panvk_pool_num_bos(&cmdbuf->varying_pool);
+
+         panvk_pool_get_bo_handles(&cmdbuf->tls_pool, &bos[bo_idx]);
+         bo_idx += panvk_pool_num_bos(&cmdbuf->tls_pool);
+
+         if (batch->fb.info) {
+            for (unsigned i = 0; i < batch->fb.info->attachment_count; i++) {
+               bos[bo_idx++] = batch->fb.info->attachments[i]
+                                  .iview->pview.image->data.bo->gem_handle;
+            }
+         }
+
+         if (batch->blit.src)
+            bos[bo_idx++] = batch->blit.src->gem_handle;
+
+         if (batch->blit.dst)
+            bos[bo_idx++] = batch->blit.dst->gem_handle;
+
+         if (batch->scoreboard.first_tiler)
+            bos[bo_idx++] = pdev->tiler_heap->gem_handle;
+
+         bos[bo_idx++] = pdev->sample_positions->gem_handle;
+         assert(bo_idx == nr_bos);
+
+         /* Merge identical BO entries. */
+         for (unsigned x = 0; x < nr_bos; x++) {
+            for (unsigned y = x + 1; y < nr_bos;) {
+               if (bos[x] == bos[y])
+                  bos[y] = bos[--nr_bos];
+               else
+                  y++;
+            }
+         }
+
+         unsigned nr_in_fences = 0;
+         unsigned max_wait_event_syncobjs = util_dynarray_num_elements(
+            &batch->event_ops, struct panvk_event_op);
+         uint32_t in_fences[nr_semaphores + max_wait_event_syncobjs];
+         memcpy(in_fences, semaphores, nr_semaphores * sizeof(*in_fences));
+         nr_in_fences += nr_semaphores;
+
+         panvk_add_wait_event_syncobjs(batch, in_fences, &nr_in_fences);
+
+         panvk_queue_submit_batch(queue, batch, bos, nr_bos, in_fences,
+                                  nr_in_fences);
+
+         panvk_signal_event_syncobjs(queue, batch);
+      }
+   }
+
+   /* Transfer the out fence to signal semaphores */
+   for (unsigned i = 0; i < submit->signal_count; i++) {
+      assert(vk_sync_type_is_drm_syncobj(submit->signals[i].sync->type));
+      struct vk_drm_syncobj *syncobj =
+         vk_sync_as_drm_syncobj(submit->signals[i].sync);
+
+      panvk_queue_transfer_sync(queue, syncobj->syncobj);
+   }
+
+   return VK_SUCCESS;
+}
+
+VkResult
+panvk_per_arch(CreateSampler)(VkDevice _device,
+                              const VkSamplerCreateInfo *pCreateInfo,
+                              const VkAllocationCallbacks *pAllocator,
+                              VkSampler *pSampler)
+{
+   VK_FROM_HANDLE(panvk_device, device, _device);
+   struct panvk_sampler *sampler;
+
+   assert(pCreateInfo->sType == VK_STRUCTURE_TYPE_SAMPLER_CREATE_INFO);
+
+   sampler = vk_object_alloc(&device->vk, pAllocator, sizeof(*sampler),
+                             VK_OBJECT_TYPE_SAMPLER);
+   if (!sampler)
+      return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
+
+   STATIC_ASSERT(sizeof(sampler->desc) >= pan_size(SAMPLER));
+   panvk_per_arch(emit_sampler)(pCreateInfo, &sampler->desc);
+   *pSampler = panvk_sampler_to_handle(sampler);
+
+   return VK_SUCCESS;
+}
diff -up mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_device.h.16~ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_device.h
--- mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_device.h.16~	2023-07-29 15:51:45.093980518 +0100
+++ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_device.h	2023-07-29 15:51:45.083980485 +0100
@@ -0,0 +1,33 @@
+/*
+ * Copyright (C) 2022 Collabora Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#ifndef PANVK_PRIVATE_H
+#error "Must be included from panvk_private.h"
+#endif
+
+#ifndef PAN_ARCH
+#error "no arch"
+#endif
+
+VkResult panvk_per_arch(queue_submit)(struct vk_queue *queue,
+                                      struct vk_queue_submit *submit);
diff -up mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_image.c.16~ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_image.c
--- mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_image.c.16~	2023-07-29 15:51:45.093980518 +0100
+++ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_image.c	2023-07-29 15:51:45.083980485 +0100
@@ -0,0 +1,240 @@
+/*
+ * Copyright © 2021 Collabora Ltd.
+ *
+ * Derived from tu_image.c which is:
+ * Copyright © 2016 Red Hat.
+ * Copyright © 2016 Bas Nieuwenhuizen
+ * Copyright © 2015 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#include "genxml/gen_macros.h"
+#include "panvk_private.h"
+
+#include "drm-uapi/drm_fourcc.h"
+#include "util/u_atomic.h"
+#include "util/u_debug.h"
+#include "vk_format.h"
+#include "vk_object.h"
+#include "vk_util.h"
+
+static enum mali_texture_dimension
+panvk_view_type_to_mali_tex_dim(VkImageViewType type)
+{
+   switch (type) {
+   case VK_IMAGE_VIEW_TYPE_1D:
+   case VK_IMAGE_VIEW_TYPE_1D_ARRAY:
+      return MALI_TEXTURE_DIMENSION_1D;
+   case VK_IMAGE_VIEW_TYPE_2D:
+   case VK_IMAGE_VIEW_TYPE_2D_ARRAY:
+      return MALI_TEXTURE_DIMENSION_2D;
+   case VK_IMAGE_VIEW_TYPE_3D:
+      return MALI_TEXTURE_DIMENSION_3D;
+   case VK_IMAGE_VIEW_TYPE_CUBE:
+   case VK_IMAGE_VIEW_TYPE_CUBE_ARRAY:
+      return MALI_TEXTURE_DIMENSION_CUBE;
+   default:
+      unreachable("Invalid view type");
+   }
+}
+
+static void
+panvk_convert_swizzle(const VkComponentMapping *in, unsigned char *out)
+{
+   const VkComponentSwizzle *comp = &in->r;
+   for (unsigned i = 0; i < 4; i++) {
+      switch (comp[i]) {
+      case VK_COMPONENT_SWIZZLE_ZERO:
+         out[i] = PIPE_SWIZZLE_0;
+         break;
+      case VK_COMPONENT_SWIZZLE_ONE:
+         out[i] = PIPE_SWIZZLE_1;
+         break;
+      case VK_COMPONENT_SWIZZLE_R:
+         out[i] = PIPE_SWIZZLE_X;
+         break;
+      case VK_COMPONENT_SWIZZLE_G:
+         out[i] = PIPE_SWIZZLE_Y;
+         break;
+      case VK_COMPONENT_SWIZZLE_B:
+         out[i] = PIPE_SWIZZLE_Z;
+         break;
+      case VK_COMPONENT_SWIZZLE_A:
+         out[i] = PIPE_SWIZZLE_W;
+         break;
+      default:
+         unreachable("Invalid swizzle");
+      }
+   }
+}
+
+VkResult
+panvk_per_arch(CreateImageView)(VkDevice _device,
+                                const VkImageViewCreateInfo *pCreateInfo,
+                                const VkAllocationCallbacks *pAllocator,
+                                VkImageView *pView)
+{
+   VK_FROM_HANDLE(panvk_device, device, _device);
+   VK_FROM_HANDLE(panvk_image, image, pCreateInfo->image);
+   struct panvk_image_view *view;
+
+   view = vk_image_view_create(&device->vk, false, pCreateInfo, pAllocator,
+                               sizeof(*view));
+   if (view == NULL)
+      return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
+
+   view->pview = (struct pan_image_view){
+      .image = &image->pimage,
+      .format = vk_format_to_pipe_format(view->vk.view_format),
+      .dim = panvk_view_type_to_mali_tex_dim(view->vk.view_type),
+      .nr_samples = image->pimage.layout.nr_samples,
+      .first_level = view->vk.base_mip_level,
+      .last_level = view->vk.base_mip_level + view->vk.level_count - 1,
+      .first_layer = view->vk.base_array_layer,
+      .last_layer = view->vk.base_array_layer + view->vk.layer_count - 1,
+   };
+   panvk_convert_swizzle(&view->vk.swizzle, view->pview.swizzle);
+
+   struct panfrost_device *pdev = &device->physical_device->pdev;
+
+   if (view->vk.usage &
+       (VK_IMAGE_USAGE_SAMPLED_BIT | VK_IMAGE_USAGE_INPUT_ATTACHMENT_BIT)) {
+      unsigned bo_size =
+         GENX(panfrost_estimate_texture_payload_size)(&view->pview) +
+         pan_size(TEXTURE);
+
+      view->bo = panfrost_bo_create(pdev, bo_size, 0, "Texture descriptor");
+
+      STATIC_ASSERT(sizeof(view->descs.tex) >= pan_size(TEXTURE));
+      GENX(panfrost_new_texture)
+      (pdev, &view->pview, &view->descs.tex, &view->bo->ptr);
+   }
+
+   if (view->vk.usage & VK_IMAGE_USAGE_STORAGE_BIT) {
+      uint8_t *attrib_buf = (uint8_t *)view->descs.img_attrib_buf;
+      bool is_3d = image->pimage.layout.dim == MALI_TEXTURE_DIMENSION_3D;
+      unsigned offset = image->pimage.data.offset;
+      offset +=
+         panfrost_texture_offset(&image->pimage.layout, view->pview.first_level,
+                                 is_3d ? 0 : view->pview.first_layer,
+                                 is_3d ? view->pview.first_layer : 0);
+
+      pan_pack(attrib_buf, ATTRIBUTE_BUFFER, cfg) {
+         cfg.type = image->pimage.layout.modifier == DRM_FORMAT_MOD_LINEAR
+                       ? MALI_ATTRIBUTE_TYPE_3D_LINEAR
+                       : MALI_ATTRIBUTE_TYPE_3D_INTERLEAVED;
+         cfg.pointer = image->pimage.data.bo->ptr.gpu + offset;
+         cfg.stride = util_format_get_blocksize(view->pview.format);
+         cfg.size = image->pimage.data.bo->size - offset;
+      }
+
+      attrib_buf += pan_size(ATTRIBUTE_BUFFER);
+      pan_pack(attrib_buf, ATTRIBUTE_BUFFER_CONTINUATION_3D, cfg) {
+         unsigned level = view->pview.first_level;
+
+         cfg.s_dimension = u_minify(image->pimage.layout.width, level);
+         cfg.t_dimension = u_minify(image->pimage.layout.height, level);
+         cfg.r_dimension =
+            view->pview.dim == MALI_TEXTURE_DIMENSION_3D
+               ? u_minify(image->pimage.layout.depth, level)
+               : (view->pview.last_layer - view->pview.first_layer + 1);
+         cfg.row_stride = image->pimage.layout.slices[level].row_stride;
+         if (cfg.r_dimension > 1) {
+            cfg.slice_stride =
+               panfrost_get_layer_stride(&image->pimage.layout, level);
+         }
+      }
+   }
+
+   *pView = panvk_image_view_to_handle(view);
+   return VK_SUCCESS;
+}
+
+VkResult
+panvk_per_arch(CreateBufferView)(VkDevice _device,
+                                 const VkBufferViewCreateInfo *pCreateInfo,
+                                 const VkAllocationCallbacks *pAllocator,
+                                 VkBufferView *pView)
+{
+   VK_FROM_HANDLE(panvk_device, device, _device);
+   VK_FROM_HANDLE(panvk_buffer, buffer, pCreateInfo->buffer);
+
+   struct panvk_buffer_view *view = vk_object_zalloc(
+      &device->vk, pAllocator, sizeof(*view), VK_OBJECT_TYPE_BUFFER_VIEW);
+
+   if (!view)
+      return vk_error(device->instance, VK_ERROR_OUT_OF_HOST_MEMORY);
+
+   view->fmt = vk_format_to_pipe_format(pCreateInfo->format);
+
+   struct panfrost_device *pdev = &device->physical_device->pdev;
+   mali_ptr address = panvk_buffer_gpu_ptr(buffer, pCreateInfo->offset);
+   unsigned size =
+      panvk_buffer_range(buffer, pCreateInfo->offset, pCreateInfo->range);
+   unsigned blksz = util_format_get_blocksize(view->fmt);
+   view->elems = size / blksz;
+
+   assert(!(address & 63));
+
+   if (buffer->vk.usage & VK_BUFFER_USAGE_UNIFORM_TEXEL_BUFFER_BIT) {
+      unsigned bo_size = pan_size(SURFACE_WITH_STRIDE);
+      view->bo = panfrost_bo_create(pdev, bo_size, 0, "Texture descriptor");
+
+      pan_pack(view->bo->ptr.cpu, SURFACE_WITH_STRIDE, cfg) {
+         cfg.pointer = address;
+      }
+
+      pan_pack(view->descs.tex, TEXTURE, cfg) {
+         cfg.dimension = MALI_TEXTURE_DIMENSION_1D;
+         cfg.format = pdev->formats[view->fmt].hw;
+         cfg.width = view->elems;
+         cfg.depth = cfg.height = 1;
+         cfg.swizzle = PAN_V6_SWIZZLE(R, G, B, A);
+         cfg.texel_ordering = MALI_TEXTURE_LAYOUT_LINEAR;
+         cfg.levels = 1;
+         cfg.array_size = 1;
+         cfg.surfaces = view->bo->ptr.gpu;
+         cfg.maximum_lod = cfg.minimum_lod = 0;
+      }
+   }
+
+   if (buffer->vk.usage & VK_BUFFER_USAGE_STORAGE_TEXEL_BUFFER_BIT) {
+      uint8_t *attrib_buf = (uint8_t *)view->descs.img_attrib_buf;
+
+      pan_pack(attrib_buf, ATTRIBUTE_BUFFER, cfg) {
+         cfg.type = MALI_ATTRIBUTE_TYPE_3D_LINEAR;
+         cfg.pointer = address;
+         cfg.stride = blksz;
+         cfg.size = view->elems * blksz;
+      }
+
+      attrib_buf += pan_size(ATTRIBUTE_BUFFER);
+      pan_pack(attrib_buf, ATTRIBUTE_BUFFER_CONTINUATION_3D, cfg) {
+         cfg.s_dimension = view->elems;
+         cfg.t_dimension = 1;
+         cfg.r_dimension = 1;
+         cfg.row_stride = view->elems * blksz;
+      }
+   }
+
+   *pView = panvk_buffer_view_to_handle(view);
+   return VK_SUCCESS;
+}
diff -up mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_meta_blit.c.16~ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_meta_blit.c
--- mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_meta_blit.c.16~	2023-07-29 15:51:45.093980518 +0100
+++ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_meta_blit.c	2023-07-29 15:51:45.083980485 +0100
@@ -0,0 +1,237 @@
+/*
+ * Copyright © 2021 Collabora Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#include "gen_macros.h"
+
+#include "pan_blitter.h"
+
+#include "panvk_private.h"
+
+static void
+panvk_meta_blit(struct panvk_cmd_buffer *cmdbuf,
+                const struct pan_blit_info *blitinfo)
+{
+   struct panfrost_device *pdev = &cmdbuf->device->physical_device->pdev;
+   struct pan_fb_info *fbinfo = &cmdbuf->state.fb.info;
+   struct pan_blit_context ctx;
+   struct pan_image_view views[2] = {
+      {
+         .format = blitinfo->dst.planes[0].format,
+         .dim = MALI_TEXTURE_DIMENSION_2D,
+         .image = blitinfo->dst.planes[0].image,
+         .nr_samples = blitinfo->dst.planes[0].image->layout.nr_samples,
+         .first_level = blitinfo->dst.level,
+         .last_level = blitinfo->dst.level,
+         .swizzle = {PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_Z,
+                     PIPE_SWIZZLE_W},
+      },
+   };
+
+   *fbinfo = (struct pan_fb_info){
+      .width = u_minify(blitinfo->dst.planes[0].image->layout.width,
+                        blitinfo->dst.level),
+      .height = u_minify(blitinfo->dst.planes[0].image->layout.height,
+                         blitinfo->dst.level),
+      .extent =
+         {
+            .minx = MAX2(MIN2(blitinfo->dst.start.x, blitinfo->dst.end.x), 0),
+            .miny = MAX2(MIN2(blitinfo->dst.start.y, blitinfo->dst.end.y), 0),
+            .maxx = MAX2(blitinfo->dst.start.x, blitinfo->dst.end.x),
+            .maxy = MAX2(blitinfo->dst.start.y, blitinfo->dst.end.y),
+         },
+      .nr_samples = blitinfo->dst.planes[0].image->layout.nr_samples,
+   };
+
+   fbinfo->extent.maxx = MIN2(fbinfo->extent.maxx, fbinfo->width - 1);
+   fbinfo->extent.maxy = MIN2(fbinfo->extent.maxy, fbinfo->height - 1);
+
+   /* TODO: don't force preloads of dst resources if unneeded */
+
+   const struct util_format_description *fdesc =
+      util_format_description(blitinfo->dst.planes[0].image->layout.format);
+
+   if (util_format_has_depth(fdesc)) {
+      /* We want the image format here, otherwise we might lose one of the
+       * component.
+       */
+      views[0].format = blitinfo->dst.planes[0].image->layout.format;
+      fbinfo->zs.view.zs = &views[0];
+      fbinfo->zs.preload.z = true;
+      fbinfo->zs.preload.s = util_format_has_stencil(fdesc);
+   } else if (util_format_has_stencil(fdesc)) {
+      fbinfo->zs.view.s = &views[0];
+      fbinfo->zs.preload.s = true;
+   } else {
+      fbinfo->rt_count = 1;
+      fbinfo->rts[0].view = &views[0];
+      fbinfo->rts[0].preload = true;
+      cmdbuf->state.fb.crc_valid[0] = false;
+      fbinfo->rts[0].crc_valid = &cmdbuf->state.fb.crc_valid[0];
+   }
+
+   if (blitinfo->dst.planes[1].format != PIPE_FORMAT_NONE) {
+      /* TODO: don't force preloads of dst resources if unneeded */
+      views[1].format = blitinfo->dst.planes[1].format;
+      views[1].dim = MALI_TEXTURE_DIMENSION_2D;
+      views[1].image = blitinfo->dst.planes[1].image;
+      views[1].nr_samples = blitinfo->dst.planes[1].image->layout.nr_samples;
+      views[1].first_level = blitinfo->dst.level;
+      views[1].last_level = blitinfo->dst.level;
+      views[1].swizzle[0] = PIPE_SWIZZLE_X;
+      views[1].swizzle[1] = PIPE_SWIZZLE_Y;
+      views[1].swizzle[2] = PIPE_SWIZZLE_Z;
+      views[1].swizzle[3] = PIPE_SWIZZLE_W;
+      fbinfo->zs.view.s = &views[1];
+   }
+
+   panvk_per_arch(cmd_close_batch)(cmdbuf);
+
+   GENX(pan_blit_ctx_init)(pdev, blitinfo, &cmdbuf->desc_pool.base, &ctx);
+   do {
+      if (ctx.dst.cur_layer < 0)
+         continue;
+
+      struct panvk_batch *batch = panvk_cmd_open_batch(cmdbuf);
+      mali_ptr tsd, tiler;
+
+      views[0].first_layer = views[0].last_layer = ctx.dst.cur_layer;
+      views[1].first_layer = views[1].last_layer = views[0].first_layer;
+      batch->blit.src = blitinfo->src.planes[0].image->data.bo;
+      batch->blit.dst = blitinfo->dst.planes[0].image->data.bo;
+      panvk_per_arch(cmd_alloc_tls_desc)(cmdbuf, true);
+      panvk_per_arch(cmd_alloc_fb_desc)(cmdbuf);
+      panvk_per_arch(cmd_prepare_tiler_context)(cmdbuf);
+
+      tsd = batch->tls.gpu;
+      tiler = batch->tiler.descs.gpu;
+
+      struct panfrost_ptr job = GENX(pan_blit)(&ctx, &cmdbuf->desc_pool.base,
+                                               &batch->scoreboard, tsd, tiler);
+      util_dynarray_append(&batch->jobs, void *, job.cpu);
+      panvk_per_arch(cmd_close_batch)(cmdbuf);
+   } while (pan_blit_next_surface(&ctx));
+}
+
+void
+panvk_per_arch(CmdBlitImage2)(VkCommandBuffer commandBuffer,
+                              const VkBlitImageInfo2 *pBlitImageInfo)
+{
+   VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
+   VK_FROM_HANDLE(panvk_image, src, pBlitImageInfo->srcImage);
+   VK_FROM_HANDLE(panvk_image, dst, pBlitImageInfo->dstImage);
+
+   for (unsigned i = 0; i < pBlitImageInfo->regionCount; i++) {
+      const VkImageBlit2 *region = &pBlitImageInfo->pRegions[i];
+      struct pan_blit_info info = {
+         .src =
+            {
+               .planes[0].image = &src->pimage,
+               .planes[0].format = src->pimage.layout.format,
+               .level = region->srcSubresource.mipLevel,
+               .start =
+                  {
+                     region->srcOffsets[0].x,
+                     region->srcOffsets[0].y,
+                     region->srcOffsets[0].z,
+                     region->srcSubresource.baseArrayLayer,
+                  },
+               .end =
+                  {
+                     region->srcOffsets[1].x,
+                     region->srcOffsets[1].y,
+                     region->srcOffsets[1].z,
+                     region->srcSubresource.baseArrayLayer +
+                        region->srcSubresource.layerCount - 1,
+                  },
+            },
+         .dst =
+            {
+               .planes[0].image = &dst->pimage,
+               .planes[0].format = dst->pimage.layout.format,
+               .level = region->dstSubresource.mipLevel,
+               .start =
+                  {
+                     region->dstOffsets[0].x,
+                     region->dstOffsets[0].y,
+                     region->dstOffsets[0].z,
+                     region->dstSubresource.baseArrayLayer,
+                  },
+               .end =
+                  {
+                     region->dstOffsets[1].x,
+                     region->dstOffsets[1].y,
+                     region->dstOffsets[1].z,
+                     region->dstSubresource.baseArrayLayer +
+                        region->dstSubresource.layerCount - 1,
+                  },
+            },
+         .nearest = pBlitImageInfo->filter == VK_FILTER_NEAREST,
+      };
+
+      if (region->srcSubresource.aspectMask == VK_IMAGE_ASPECT_STENCIL_BIT)
+         info.src.planes[0].format =
+            util_format_stencil_only(info.src.planes[0].format);
+      else if (region->srcSubresource.aspectMask == VK_IMAGE_ASPECT_DEPTH_BIT)
+         info.src.planes[0].format =
+            util_format_get_depth_only(info.src.planes[0].format);
+
+      if (region->dstSubresource.aspectMask == VK_IMAGE_ASPECT_STENCIL_BIT)
+         info.dst.planes[0].format =
+            util_format_stencil_only(info.dst.planes[0].format);
+      else if (region->dstSubresource.aspectMask == VK_IMAGE_ASPECT_DEPTH_BIT)
+         info.dst.planes[0].format =
+            util_format_get_depth_only(info.dst.planes[0].format);
+
+      panvk_meta_blit(cmdbuf, &info);
+   }
+}
+
+void
+panvk_per_arch(CmdResolveImage2)(VkCommandBuffer commandBuffer,
+                                 const VkResolveImageInfo2 *pResolveImageInfo)
+{
+   panvk_stub();
+}
+
+void
+panvk_per_arch(meta_blit_init)(struct panvk_physical_device *dev)
+{
+   panvk_pool_init(&dev->meta.blitter.bin_pool, &dev->pdev, NULL,
+                   PAN_BO_EXECUTE, 16 * 1024, "panvk_meta blitter binary pool",
+                   false);
+   panvk_pool_init(&dev->meta.blitter.desc_pool, &dev->pdev, NULL, 0, 16 * 1024,
+                   "panvk_meta blitter descriptor pool", false);
+   pan_blend_shaders_init(&dev->pdev);
+   GENX(pan_blitter_init)
+   (&dev->pdev, &dev->meta.blitter.bin_pool.base,
+    &dev->meta.blitter.desc_pool.base);
+}
+
+void
+panvk_per_arch(meta_blit_cleanup)(struct panvk_physical_device *dev)
+{
+   GENX(pan_blitter_cleanup)(&dev->pdev);
+   pan_blend_shaders_cleanup(&dev->pdev);
+   panvk_pool_cleanup(&dev->meta.blitter.desc_pool);
+   panvk_pool_cleanup(&dev->meta.blitter.bin_pool);
+}
diff -up mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_meta.c.16~ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_meta.c
--- mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_meta.c.16~	2023-07-29 15:51:45.093980518 +0100
+++ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_meta.c	2023-07-29 15:51:45.083980485 +0100
@@ -0,0 +1,68 @@
+/*
+ * Copyright © 2021 Collabora Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#include "genxml/gen_macros.h"
+
+#include "nir/nir_builder.h"
+#include "pan_encoder.h"
+#include "pan_shader.h"
+
+#include "panvk_private.h"
+
+#include "vk_format.h"
+
+mali_ptr
+panvk_per_arch(meta_emit_viewport)(struct pan_pool *pool, uint16_t minx,
+                                   uint16_t miny, uint16_t maxx, uint16_t maxy)
+{
+   struct panfrost_ptr vp = pan_pool_alloc_desc(pool, VIEWPORT);
+
+   pan_pack(vp.cpu, VIEWPORT, cfg) {
+      cfg.scissor_minimum_x = minx;
+      cfg.scissor_minimum_y = miny;
+      cfg.scissor_maximum_x = maxx;
+      cfg.scissor_maximum_y = maxy;
+   }
+
+   return vp.gpu;
+}
+
+void
+panvk_per_arch(meta_init)(struct panvk_physical_device *dev)
+{
+   panvk_pool_init(&dev->meta.bin_pool, &dev->pdev, NULL, PAN_BO_EXECUTE,
+                   16 * 1024, "panvk_meta binary pool", false);
+   panvk_pool_init(&dev->meta.desc_pool, &dev->pdev, NULL, 0, 16 * 1024,
+                   "panvk_meta descriptor pool", false);
+   panvk_per_arch(meta_blit_init)(dev);
+   panvk_per_arch(meta_copy_init)(dev);
+   panvk_per_arch(meta_clear_init)(dev);
+}
+
+void
+panvk_per_arch(meta_cleanup)(struct panvk_physical_device *dev)
+{
+   panvk_per_arch(meta_blit_cleanup)(dev);
+   panvk_pool_cleanup(&dev->meta.desc_pool);
+   panvk_pool_cleanup(&dev->meta.bin_pool);
+}
diff -up mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_meta_clear.c.16~ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_meta_clear.c
--- mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_meta_clear.c.16~	2023-07-29 15:51:45.093980518 +0100
+++ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_meta_clear.c	2023-07-29 15:51:45.083980485 +0100
@@ -0,0 +1,516 @@
+/*
+ * Copyright © 2021 Collabora Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#include "nir/nir_builder.h"
+#include "pan_blitter.h"
+#include "pan_encoder.h"
+#include "pan_shader.h"
+
+#include "panvk_private.h"
+#include "panvk_vX_meta.h"
+
+#include "vk_format.h"
+
+static mali_ptr
+panvk_meta_clear_color_attachment_shader(struct panfrost_device *pdev,
+                                         struct pan_pool *bin_pool,
+                                         enum glsl_base_type base_type,
+                                         struct pan_shader_info *shader_info)
+{
+   nir_builder b = nir_builder_init_simple_shader(
+      MESA_SHADER_FRAGMENT, GENX(pan_shader_get_compiler_options)(),
+      "panvk_meta_clear_attachment(base_type=%d)", base_type);
+
+   const struct glsl_type *out_type = glsl_vector_type(base_type, 4);
+   nir_variable *out =
+      nir_variable_create(b.shader, nir_var_shader_out, out_type, "out");
+   out->data.location = FRAG_RESULT_DATA0;
+
+   nir_ssa_def *clear_values =
+      nir_load_push_constant(&b, 4, 32, nir_imm_int(&b, 0), .range = ~0);
+   nir_store_var(&b, out, clear_values, 0xff);
+
+   struct panfrost_compile_inputs inputs = {
+      .gpu_id = pdev->gpu_id,
+      .is_blit = true,
+      .no_ubo_to_push = true,
+   };
+
+   struct util_dynarray binary;
+
+   util_dynarray_init(&binary, NULL);
+   pan_shader_preprocess(b.shader, inputs.gpu_id);
+   GENX(pan_shader_compile)(b.shader, &inputs, &binary, shader_info);
+
+   shader_info->push.count = 4;
+
+   mali_ptr shader =
+      pan_pool_upload_aligned(bin_pool, binary.data, binary.size, 128);
+
+   util_dynarray_fini(&binary);
+   ralloc_free(b.shader);
+
+   return shader;
+}
+
+static mali_ptr
+panvk_meta_clear_color_attachment_emit_rsd(struct panfrost_device *pdev,
+                                           struct pan_pool *desc_pool,
+                                           enum pipe_format format, unsigned rt,
+                                           struct pan_shader_info *shader_info,
+                                           mali_ptr shader)
+{
+   struct panfrost_ptr rsd_ptr = pan_pool_alloc_desc_aggregate(
+      desc_pool, PAN_DESC(RENDERER_STATE), PAN_DESC_ARRAY(rt + 1, BLEND));
+
+   pan_pack(rsd_ptr.cpu, RENDERER_STATE, cfg) {
+      pan_shader_prepare_rsd(shader_info, shader, &cfg);
+
+      cfg.properties.depth_source = MALI_DEPTH_SOURCE_FIXED_FUNCTION;
+      cfg.multisample_misc.sample_mask = UINT16_MAX;
+      cfg.multisample_misc.depth_function = MALI_FUNC_ALWAYS;
+      cfg.properties.allow_forward_pixel_to_be_killed = true;
+      cfg.properties.allow_forward_pixel_to_kill = true;
+      cfg.properties.zs_update_operation = MALI_PIXEL_KILL_WEAK_EARLY;
+      cfg.properties.pixel_kill_operation = MALI_PIXEL_KILL_WEAK_EARLY;
+   }
+
+   void *bd = rsd_ptr.cpu + pan_size(RENDERER_STATE);
+
+   pan_pack(bd, BLEND, cfg) {
+      cfg.round_to_fb_precision = true;
+      cfg.load_destination = false;
+      cfg.equation.rgb.a = MALI_BLEND_OPERAND_A_SRC;
+      cfg.equation.rgb.b = MALI_BLEND_OPERAND_B_SRC;
+      cfg.equation.rgb.c = MALI_BLEND_OPERAND_C_ZERO;
+      cfg.equation.alpha.a = MALI_BLEND_OPERAND_A_SRC;
+      cfg.equation.alpha.b = MALI_BLEND_OPERAND_B_SRC;
+      cfg.equation.alpha.c = MALI_BLEND_OPERAND_C_ZERO;
+      cfg.internal.mode = MALI_BLEND_MODE_OPAQUE;
+      cfg.equation.color_mask = 0xf;
+      cfg.internal.fixed_function.num_comps = 4;
+      cfg.internal.fixed_function.rt = rt;
+      cfg.internal.fixed_function.conversion.memory_format =
+         panfrost_format_to_bifrost_blend(pdev, format, false);
+      cfg.internal.fixed_function.conversion.register_format =
+         shader_info->bifrost.blend[0].format;
+   }
+
+   return rsd_ptr.gpu;
+}
+
+static mali_ptr
+panvk_meta_clear_zs_attachment_emit_rsd(struct panfrost_device *pdev,
+                                        struct pan_pool *desc_pool,
+                                        VkImageAspectFlags mask,
+                                        VkClearDepthStencilValue value)
+{
+   struct panfrost_ptr rsd_ptr = pan_pool_alloc_desc(desc_pool, RENDERER_STATE);
+
+   pan_pack(rsd_ptr.cpu, RENDERER_STATE, cfg) {
+      cfg.properties.depth_source = MALI_DEPTH_SOURCE_FIXED_FUNCTION;
+      cfg.multisample_misc.sample_mask = UINT16_MAX;
+
+      if (mask & VK_IMAGE_ASPECT_DEPTH_BIT) {
+         cfg.multisample_misc.depth_write_mask = true;
+         cfg.multisample_misc.depth_function = MALI_FUNC_NOT_EQUAL;
+
+         if (value.depth != 0.0) {
+            cfg.stencil_mask_misc.front_facing_depth_bias = true;
+            cfg.stencil_mask_misc.back_facing_depth_bias = true;
+            cfg.depth_units = INFINITY;
+            cfg.depth_bias_clamp = value.depth;
+         }
+      }
+
+      if (mask & VK_IMAGE_ASPECT_STENCIL_BIT) {
+         cfg.stencil_mask_misc.stencil_enable = true;
+         cfg.stencil_mask_misc.stencil_mask_front = 0xFF;
+         cfg.stencil_mask_misc.stencil_mask_back = 0xFF;
+
+         cfg.stencil_front.compare_function = (mask & VK_IMAGE_ASPECT_DEPTH_BIT)
+                                                 ? MALI_FUNC_ALWAYS
+                                                 : MALI_FUNC_NOT_EQUAL;
+
+         cfg.stencil_front.stencil_fail = MALI_STENCIL_OP_KEEP;
+         cfg.stencil_front.depth_fail = MALI_STENCIL_OP_REPLACE;
+         cfg.stencil_front.depth_pass = MALI_STENCIL_OP_REPLACE;
+         cfg.stencil_front.reference_value = value.stencil;
+         cfg.stencil_front.mask = 0xFF;
+         cfg.stencil_back = cfg.stencil_front;
+      }
+
+      cfg.properties.allow_forward_pixel_to_be_killed = true;
+      cfg.properties.zs_update_operation = MALI_PIXEL_KILL_WEAK_EARLY;
+      cfg.properties.pixel_kill_operation = MALI_PIXEL_KILL_WEAK_EARLY;
+   }
+
+   return rsd_ptr.gpu;
+}
+
+static void
+panvk_meta_clear_attachment_emit_dcd(struct pan_pool *pool, mali_ptr coords,
+                                     mali_ptr push_constants, mali_ptr vpd,
+                                     mali_ptr tsd, mali_ptr rsd, void *out)
+{
+   pan_pack(out, DRAW, cfg) {
+      cfg.thread_storage = tsd;
+      cfg.state = rsd;
+      cfg.push_uniforms = push_constants;
+      cfg.position = coords;
+      cfg.viewport = vpd;
+   }
+}
+
+static struct panfrost_ptr
+panvk_meta_clear_attachment_emit_tiler_job(struct pan_pool *desc_pool,
+                                           struct pan_scoreboard *scoreboard,
+                                           mali_ptr coords,
+                                           mali_ptr push_constants,
+                                           mali_ptr vpd, mali_ptr rsd,
+                                           mali_ptr tsd, mali_ptr tiler)
+{
+   struct panfrost_ptr job = pan_pool_alloc_desc(desc_pool, TILER_JOB);
+
+   panvk_meta_clear_attachment_emit_dcd(
+      desc_pool, coords, push_constants, vpd, tsd, rsd,
+      pan_section_ptr(job.cpu, TILER_JOB, DRAW));
+
+   pan_section_pack(job.cpu, TILER_JOB, PRIMITIVE, cfg) {
+      cfg.draw_mode = MALI_DRAW_MODE_TRIANGLE_STRIP;
+      cfg.index_count = 4;
+      cfg.job_task_split = 6;
+   }
+
+   pan_section_pack(job.cpu, TILER_JOB, PRIMITIVE_SIZE, cfg) {
+      cfg.constant = 1.0f;
+   }
+
+   void *invoc = pan_section_ptr(job.cpu, TILER_JOB, INVOCATION);
+   panfrost_pack_work_groups_compute(invoc, 1, 4, 1, 1, 1, 1, true, false);
+
+   pan_section_pack(job.cpu, TILER_JOB, PADDING, cfg)
+      ;
+   pan_section_pack(job.cpu, TILER_JOB, TILER, cfg) {
+      cfg.address = tiler;
+   }
+
+   panfrost_add_job(desc_pool, scoreboard, MALI_JOB_TYPE_TILER, false, false, 0,
+                    0, &job, false);
+   return job;
+}
+
+static enum glsl_base_type
+panvk_meta_get_format_type(enum pipe_format format)
+{
+   const struct util_format_description *desc = util_format_description(format);
+   int i;
+
+   i = util_format_get_first_non_void_channel(format);
+   assert(i >= 0);
+
+   if (desc->channel[i].normalized)
+      return GLSL_TYPE_FLOAT;
+
+   switch (desc->channel[i].type) {
+
+   case UTIL_FORMAT_TYPE_UNSIGNED:
+      return GLSL_TYPE_UINT;
+
+   case UTIL_FORMAT_TYPE_SIGNED:
+      return GLSL_TYPE_INT;
+
+   case UTIL_FORMAT_TYPE_FLOAT:
+      return GLSL_TYPE_FLOAT;
+
+   default:
+      unreachable("Unhandled format");
+      return GLSL_TYPE_FLOAT;
+   }
+}
+
+static void
+panvk_meta_clear_attachment(struct panvk_cmd_buffer *cmdbuf,
+                            unsigned attachment, unsigned rt,
+                            VkImageAspectFlags mask,
+                            const VkClearValue *clear_value,
+                            const VkClearRect *clear_rect)
+{
+   struct panvk_physical_device *dev = cmdbuf->device->physical_device;
+   struct panfrost_device *pdev = &dev->pdev;
+   struct panvk_meta *meta = &cmdbuf->device->physical_device->meta;
+   struct panvk_batch *batch = cmdbuf->state.batch;
+   const struct panvk_render_pass *pass = cmdbuf->state.pass;
+   const struct panvk_render_pass_attachment *att =
+      &pass->attachments[attachment];
+   unsigned minx = MAX2(clear_rect->rect.offset.x, 0);
+   unsigned miny = MAX2(clear_rect->rect.offset.y, 0);
+   unsigned maxx =
+      MAX2(clear_rect->rect.offset.x + clear_rect->rect.extent.width - 1, 0);
+   unsigned maxy =
+      MAX2(clear_rect->rect.offset.y + clear_rect->rect.extent.height - 1, 0);
+
+   panvk_per_arch(cmd_alloc_fb_desc)(cmdbuf);
+   panvk_per_arch(cmd_alloc_tls_desc)(cmdbuf, true);
+   panvk_per_arch(cmd_prepare_tiler_context)(cmdbuf);
+
+   mali_ptr vpd = panvk_per_arch(meta_emit_viewport)(&cmdbuf->desc_pool.base,
+                                                     minx, miny, maxx, maxy);
+
+   float rect[] = {
+      minx, miny,     0.0, 1.0, maxx + 1, miny,     0.0, 1.0,
+      minx, maxy + 1, 0.0, 1.0, maxx + 1, maxy + 1, 0.0, 1.0,
+   };
+   mali_ptr coordinates =
+      pan_pool_upload_aligned(&cmdbuf->desc_pool.base, rect, sizeof(rect), 64);
+
+   enum glsl_base_type base_type = panvk_meta_get_format_type(att->format);
+
+   mali_ptr tiler = batch->tiler.descs.gpu;
+   mali_ptr tsd = batch->tls.gpu;
+
+   mali_ptr pushconsts = 0, rsd = 0;
+
+   if (mask & VK_IMAGE_ASPECT_COLOR_BIT) {
+      mali_ptr shader = meta->clear_attachment.color[base_type].shader;
+      struct pan_shader_info *shader_info =
+         &meta->clear_attachment.color[base_type].shader_info;
+
+      pushconsts = pan_pool_upload_aligned(&cmdbuf->desc_pool.base, clear_value,
+                                           sizeof(*clear_value), 16);
+
+      rsd = panvk_meta_clear_color_attachment_emit_rsd(
+         pdev, &cmdbuf->desc_pool.base, att->format, rt, shader_info, shader);
+   } else {
+      rsd = panvk_meta_clear_zs_attachment_emit_rsd(
+         pdev, &cmdbuf->desc_pool.base, mask, clear_value->depthStencil);
+   }
+
+   struct panfrost_ptr job;
+
+   job = panvk_meta_clear_attachment_emit_tiler_job(
+      &cmdbuf->desc_pool.base, &batch->scoreboard, coordinates, pushconsts, vpd,
+      rsd, tsd, tiler);
+
+   util_dynarray_append(&batch->jobs, void *, job.cpu);
+}
+
+static void
+panvk_meta_clear_color_img(struct panvk_cmd_buffer *cmdbuf,
+                           struct panvk_image *img,
+                           const VkClearColorValue *color,
+                           const VkImageSubresourceRange *range)
+{
+   struct pan_fb_info *fbinfo = &cmdbuf->state.fb.info;
+   struct pan_image_view view = {
+      .format = img->pimage.layout.format,
+      .dim = MALI_TEXTURE_DIMENSION_2D,
+      .image = &img->pimage,
+      .nr_samples = img->pimage.layout.nr_samples,
+      .swizzle = {PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_Z,
+                  PIPE_SWIZZLE_W},
+   };
+
+   cmdbuf->state.fb.crc_valid[0] = false;
+   *fbinfo = (struct pan_fb_info){
+      .nr_samples = img->pimage.layout.nr_samples,
+      .rt_count = 1,
+      .rts[0].view = &view,
+      .rts[0].clear = true,
+      .rts[0].crc_valid = &cmdbuf->state.fb.crc_valid[0],
+   };
+
+   uint32_t clearval[4];
+   pan_pack_color(clearval, (union pipe_color_union *)color,
+                  img->pimage.layout.format, false);
+   memcpy(fbinfo->rts[0].clear_value, clearval,
+          sizeof(fbinfo->rts[0].clear_value));
+
+   unsigned level_count = vk_image_subresource_level_count(&img->vk, range);
+   unsigned layer_count = vk_image_subresource_layer_count(&img->vk, range);
+
+   for (unsigned level = range->baseMipLevel;
+        level < range->baseMipLevel + level_count; level++) {
+      view.first_level = view.last_level = level;
+      fbinfo->width = u_minify(img->pimage.layout.width, level);
+      fbinfo->height = u_minify(img->pimage.layout.height, level);
+      fbinfo->extent.maxx = fbinfo->width - 1;
+      fbinfo->extent.maxy = fbinfo->height - 1;
+
+      for (unsigned layer = range->baseArrayLayer;
+           layer < range->baseArrayLayer + layer_count; layer++) {
+         view.first_layer = view.last_layer = layer;
+         panvk_cmd_open_batch(cmdbuf);
+         panvk_per_arch(cmd_alloc_fb_desc)(cmdbuf);
+         panvk_per_arch(cmd_close_batch)(cmdbuf);
+      }
+   }
+}
+
+void
+panvk_per_arch(CmdClearColorImage)(VkCommandBuffer commandBuffer, VkImage image,
+                                   VkImageLayout imageLayout,
+                                   const VkClearColorValue *pColor,
+                                   uint32_t rangeCount,
+                                   const VkImageSubresourceRange *pRanges)
+{
+   VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
+   VK_FROM_HANDLE(panvk_image, img, image);
+
+   panvk_per_arch(cmd_close_batch)(cmdbuf);
+
+   for (unsigned i = 0; i < rangeCount; i++)
+      panvk_meta_clear_color_img(cmdbuf, img, pColor, &pRanges[i]);
+}
+
+static void
+panvk_meta_clear_zs_img(struct panvk_cmd_buffer *cmdbuf,
+                        struct panvk_image *img,
+                        const VkClearDepthStencilValue *value,
+                        const VkImageSubresourceRange *range)
+{
+   struct pan_fb_info *fbinfo = &cmdbuf->state.fb.info;
+   struct pan_image_view view = {
+      .format = img->pimage.layout.format,
+      .dim = MALI_TEXTURE_DIMENSION_2D,
+      .image = &img->pimage,
+      .nr_samples = img->pimage.layout.nr_samples,
+      .swizzle = {PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_Z,
+                  PIPE_SWIZZLE_W},
+   };
+
+   cmdbuf->state.fb.crc_valid[0] = false;
+   *fbinfo = (struct pan_fb_info){
+      .nr_samples = img->pimage.layout.nr_samples,
+      .rt_count = 1,
+      .zs.clear_value.depth = value->depth,
+      .zs.clear_value.stencil = value->stencil,
+      .zs.clear.z = range->aspectMask & VK_IMAGE_ASPECT_DEPTH_BIT,
+      .zs.clear.s = range->aspectMask & VK_IMAGE_ASPECT_STENCIL_BIT};
+
+   const struct util_format_description *fdesc =
+      util_format_description(view.format);
+
+   if (util_format_has_depth(fdesc)) {
+      fbinfo->zs.view.zs = &view;
+      if (util_format_has_stencil(fdesc)) {
+         fbinfo->zs.preload.z = !fbinfo->zs.clear.z;
+         fbinfo->zs.preload.s = !fbinfo->zs.clear.s;
+      }
+   } else {
+      fbinfo->zs.view.s = &view;
+   }
+
+   unsigned level_count = vk_image_subresource_level_count(&img->vk, range);
+   unsigned layer_count = vk_image_subresource_layer_count(&img->vk, range);
+
+   for (unsigned level = range->baseMipLevel;
+        level < range->baseMipLevel + level_count; level++) {
+      view.first_level = view.last_level = level;
+      fbinfo->width = u_minify(img->pimage.layout.width, level);
+      fbinfo->height = u_minify(img->pimage.layout.height, level);
+      fbinfo->extent.maxx = fbinfo->width - 1;
+      fbinfo->extent.maxy = fbinfo->height - 1;
+
+      for (unsigned layer = range->baseArrayLayer;
+           layer < range->baseArrayLayer + layer_count; layer++) {
+         view.first_layer = view.last_layer = layer;
+         panvk_cmd_open_batch(cmdbuf);
+         panvk_per_arch(cmd_alloc_fb_desc)(cmdbuf);
+         panvk_per_arch(cmd_close_batch)(cmdbuf);
+      }
+   }
+
+   memset(fbinfo, 0, sizeof(*fbinfo));
+}
+
+void
+panvk_per_arch(CmdClearDepthStencilImage)(
+   VkCommandBuffer commandBuffer, VkImage image, VkImageLayout imageLayout,
+   const VkClearDepthStencilValue *pDepthStencil, uint32_t rangeCount,
+   const VkImageSubresourceRange *pRanges)
+{
+   VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
+   VK_FROM_HANDLE(panvk_image, img, image);
+
+   panvk_per_arch(cmd_close_batch)(cmdbuf);
+
+   for (unsigned i = 0; i < rangeCount; i++)
+      panvk_meta_clear_zs_img(cmdbuf, img, pDepthStencil, &pRanges[i]);
+}
+
+void
+panvk_per_arch(CmdClearAttachments)(VkCommandBuffer commandBuffer,
+                                    uint32_t attachmentCount,
+                                    const VkClearAttachment *pAttachments,
+                                    uint32_t rectCount,
+                                    const VkClearRect *pRects)
+{
+   VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
+   const struct panvk_subpass *subpass = cmdbuf->state.subpass;
+
+   for (unsigned i = 0; i < attachmentCount; i++) {
+      for (unsigned j = 0; j < rectCount; j++) {
+
+         uint32_t attachment, rt = 0;
+         if (pAttachments[i].aspectMask & VK_IMAGE_ASPECT_COLOR_BIT) {
+            rt = pAttachments[i].colorAttachment;
+            attachment = subpass->color_attachments[rt].idx;
+         } else {
+            attachment = subpass->zs_attachment.idx;
+         }
+
+         if (attachment == VK_ATTACHMENT_UNUSED)
+            continue;
+
+         panvk_meta_clear_attachment(cmdbuf, attachment, rt,
+                                     pAttachments[i].aspectMask,
+                                     &pAttachments[i].clearValue, &pRects[j]);
+      }
+   }
+}
+
+static void
+panvk_meta_clear_attachment_init(struct panvk_physical_device *dev)
+{
+   dev->meta.clear_attachment.color[GLSL_TYPE_UINT].shader =
+      panvk_meta_clear_color_attachment_shader(
+         &dev->pdev, &dev->meta.bin_pool.base, GLSL_TYPE_UINT,
+         &dev->meta.clear_attachment.color[GLSL_TYPE_UINT].shader_info);
+
+   dev->meta.clear_attachment.color[GLSL_TYPE_INT].shader =
+      panvk_meta_clear_color_attachment_shader(
+         &dev->pdev, &dev->meta.bin_pool.base, GLSL_TYPE_INT,
+         &dev->meta.clear_attachment.color[GLSL_TYPE_INT].shader_info);
+
+   dev->meta.clear_attachment.color[GLSL_TYPE_FLOAT].shader =
+      panvk_meta_clear_color_attachment_shader(
+         &dev->pdev, &dev->meta.bin_pool.base, GLSL_TYPE_FLOAT,
+         &dev->meta.clear_attachment.color[GLSL_TYPE_FLOAT].shader_info);
+}
+
+void
+panvk_per_arch(meta_clear_init)(struct panvk_physical_device *dev)
+{
+   panvk_meta_clear_attachment_init(dev);
+}
diff -up mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_meta_copy.c.16~ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_meta_copy.c
--- mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_meta_copy.c.16~	2023-07-29 15:51:45.093980518 +0100
+++ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_meta_copy.c	2023-07-29 15:51:45.093980518 +0100
@@ -0,0 +1,1946 @@
+/*
+ * Copyright © 2021 Collabora Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#include "gen_macros.h"
+
+#include "nir/nir_builder.h"
+#include "pan_encoder.h"
+#include "pan_shader.h"
+
+#include "panvk_private.h"
+
+static mali_ptr
+panvk_meta_copy_img_emit_texture(struct panfrost_device *pdev,
+                                 struct pan_pool *desc_pool,
+                                 const struct pan_image_view *view)
+{
+   struct panfrost_ptr texture = pan_pool_alloc_desc(desc_pool, TEXTURE);
+   size_t payload_size = GENX(panfrost_estimate_texture_payload_size)(view);
+   struct panfrost_ptr surfaces = pan_pool_alloc_aligned(
+      desc_pool, payload_size, pan_alignment(SURFACE_WITH_STRIDE));
+
+   GENX(panfrost_new_texture)(pdev, view, texture.cpu, &surfaces);
+
+   return texture.gpu;
+}
+
+static mali_ptr
+panvk_meta_copy_img_emit_sampler(struct panfrost_device *pdev,
+                                 struct pan_pool *desc_pool)
+{
+   struct panfrost_ptr sampler = pan_pool_alloc_desc(desc_pool, SAMPLER);
+
+   pan_pack(sampler.cpu, SAMPLER, cfg) {
+      cfg.seamless_cube_map = false;
+      cfg.normalized_coordinates = false;
+      cfg.minify_nearest = true;
+      cfg.magnify_nearest = true;
+   }
+
+   return sampler.gpu;
+}
+
+static void
+panvk_meta_copy_emit_varying(struct pan_pool *pool, mali_ptr coordinates,
+                             mali_ptr *varying_bufs, mali_ptr *varyings)
+{
+   struct panfrost_ptr varying = pan_pool_alloc_desc(pool, ATTRIBUTE);
+   struct panfrost_ptr varying_buffer =
+      pan_pool_alloc_desc_array(pool, 2, ATTRIBUTE_BUFFER);
+
+   pan_pack(varying_buffer.cpu, ATTRIBUTE_BUFFER, cfg) {
+      cfg.pointer = coordinates;
+      cfg.stride = 4 * sizeof(uint32_t);
+      cfg.size = cfg.stride * 4;
+   }
+
+   /* Bifrost needs an empty desc to mark end of prefetching */
+   pan_pack(varying_buffer.cpu + pan_size(ATTRIBUTE_BUFFER), ATTRIBUTE_BUFFER,
+            cfg)
+      ;
+
+   pan_pack(varying.cpu, ATTRIBUTE, cfg) {
+      cfg.buffer_index = 0;
+      cfg.format = pool->dev->formats[PIPE_FORMAT_R32G32B32_FLOAT].hw;
+   }
+
+   *varyings = varying.gpu;
+   *varying_bufs = varying_buffer.gpu;
+}
+
+static void
+panvk_meta_copy_emit_dcd(struct pan_pool *pool, mali_ptr src_coords,
+                         mali_ptr dst_coords, mali_ptr texture,
+                         mali_ptr sampler, mali_ptr vpd, mali_ptr tsd,
+                         mali_ptr rsd, mali_ptr push_constants, void *out)
+{
+   pan_pack(out, DRAW, cfg) {
+      cfg.thread_storage = tsd;
+      cfg.state = rsd;
+      cfg.push_uniforms = push_constants;
+      cfg.position = dst_coords;
+      if (src_coords) {
+         panvk_meta_copy_emit_varying(pool, src_coords, &cfg.varying_buffers,
+                                      &cfg.varyings);
+      }
+      cfg.viewport = vpd;
+      cfg.textures = texture;
+      cfg.samplers = sampler;
+   }
+}
+
+static struct panfrost_ptr
+panvk_meta_copy_emit_tiler_job(struct pan_pool *desc_pool,
+                               struct pan_scoreboard *scoreboard,
+                               mali_ptr src_coords, mali_ptr dst_coords,
+                               mali_ptr texture, mali_ptr sampler,
+                               mali_ptr push_constants, mali_ptr vpd,
+                               mali_ptr rsd, mali_ptr tsd, mali_ptr tiler)
+{
+   struct panfrost_ptr job = pan_pool_alloc_desc(desc_pool, TILER_JOB);
+
+   panvk_meta_copy_emit_dcd(desc_pool, src_coords, dst_coords, texture, sampler,
+                            vpd, tsd, rsd, push_constants,
+                            pan_section_ptr(job.cpu, TILER_JOB, DRAW));
+
+   pan_section_pack(job.cpu, TILER_JOB, PRIMITIVE, cfg) {
+      cfg.draw_mode = MALI_DRAW_MODE_TRIANGLE_STRIP;
+      cfg.index_count = 4;
+      cfg.job_task_split = 6;
+   }
+
+   pan_section_pack(job.cpu, TILER_JOB, PRIMITIVE_SIZE, cfg) {
+      cfg.constant = 1.0f;
+   }
+
+   void *invoc = pan_section_ptr(job.cpu, TILER_JOB, INVOCATION);
+   panfrost_pack_work_groups_compute(invoc, 1, 4, 1, 1, 1, 1, true, false);
+
+   pan_section_pack(job.cpu, TILER_JOB, PADDING, cfg)
+      ;
+   pan_section_pack(job.cpu, TILER_JOB, TILER, cfg) {
+      cfg.address = tiler;
+   }
+
+   panfrost_add_job(desc_pool, scoreboard, MALI_JOB_TYPE_TILER, false, false, 0,
+                    0, &job, false);
+   return job;
+}
+
+static struct panfrost_ptr
+panvk_meta_copy_emit_compute_job(struct pan_pool *desc_pool,
+                                 struct pan_scoreboard *scoreboard,
+                                 const struct pan_compute_dim *num_wg,
+                                 const struct pan_compute_dim *wg_sz,
+                                 mali_ptr texture, mali_ptr sampler,
+                                 mali_ptr push_constants, mali_ptr rsd,
+                                 mali_ptr tsd)
+{
+   struct panfrost_ptr job = pan_pool_alloc_desc(desc_pool, COMPUTE_JOB);
+
+   void *invoc = pan_section_ptr(job.cpu, COMPUTE_JOB, INVOCATION);
+   panfrost_pack_work_groups_compute(invoc, num_wg->x, num_wg->y, num_wg->z,
+                                     wg_sz->x, wg_sz->y, wg_sz->z, false,
+                                     false);
+
+   pan_section_pack(job.cpu, COMPUTE_JOB, PARAMETERS, cfg) {
+      cfg.job_task_split = 8;
+   }
+
+   panvk_meta_copy_emit_dcd(desc_pool, 0, 0, texture, sampler, 0, tsd, rsd,
+                            push_constants,
+                            pan_section_ptr(job.cpu, COMPUTE_JOB, DRAW));
+
+   panfrost_add_job(desc_pool, scoreboard, MALI_JOB_TYPE_COMPUTE, false, false,
+                    0, 0, &job, false);
+   return job;
+}
+
+static uint32_t
+panvk_meta_copy_img_bifrost_raw_format(unsigned texelsize)
+{
+   switch (texelsize) {
+   case 6:
+      return MALI_RGB16UI << 12;
+   case 8:
+      return MALI_RG32UI << 12;
+   case 12:
+      return MALI_RGB32UI << 12;
+   case 16:
+      return MALI_RGBA32UI << 12;
+   default:
+      unreachable("Invalid texel size\n");
+   }
+}
+
+static mali_ptr
+panvk_meta_copy_to_img_emit_rsd(struct panfrost_device *pdev,
+                                struct pan_pool *desc_pool, mali_ptr shader,
+                                const struct pan_shader_info *shader_info,
+                                enum pipe_format fmt, unsigned wrmask,
+                                bool from_img)
+{
+   struct panfrost_ptr rsd_ptr = pan_pool_alloc_desc_aggregate(
+      desc_pool, PAN_DESC(RENDERER_STATE), PAN_DESC_ARRAY(1, BLEND));
+
+   bool raw = util_format_get_blocksize(fmt) > 4;
+   unsigned fullmask = (1 << util_format_get_nr_components(fmt)) - 1;
+   bool partialwrite = fullmask != wrmask && !raw;
+   bool readstb = fullmask != wrmask && raw;
+
+   pan_pack(rsd_ptr.cpu, RENDERER_STATE, cfg) {
+      pan_shader_prepare_rsd(shader_info, shader, &cfg);
+      if (from_img) {
+         cfg.shader.varying_count = 1;
+         cfg.shader.texture_count = 1;
+         cfg.shader.sampler_count = 1;
+      }
+      cfg.properties.depth_source = MALI_DEPTH_SOURCE_FIXED_FUNCTION;
+      cfg.multisample_misc.sample_mask = UINT16_MAX;
+      cfg.multisample_misc.depth_function = MALI_FUNC_ALWAYS;
+      cfg.stencil_mask_misc.stencil_mask_front = 0xFF;
+      cfg.stencil_mask_misc.stencil_mask_back = 0xFF;
+      cfg.stencil_front.compare_function = MALI_FUNC_ALWAYS;
+      cfg.stencil_front.stencil_fail = MALI_STENCIL_OP_REPLACE;
+      cfg.stencil_front.depth_fail = MALI_STENCIL_OP_REPLACE;
+      cfg.stencil_front.depth_pass = MALI_STENCIL_OP_REPLACE;
+      cfg.stencil_front.mask = 0xFF;
+      cfg.stencil_back = cfg.stencil_front;
+
+      cfg.properties.allow_forward_pixel_to_be_killed = true;
+      cfg.properties.allow_forward_pixel_to_kill = !partialwrite && !readstb;
+      cfg.properties.zs_update_operation = MALI_PIXEL_KILL_STRONG_EARLY;
+      cfg.properties.pixel_kill_operation = MALI_PIXEL_KILL_FORCE_EARLY;
+   }
+
+   pan_pack(rsd_ptr.cpu + pan_size(RENDERER_STATE), BLEND, cfg) {
+      cfg.round_to_fb_precision = true;
+      cfg.load_destination = partialwrite;
+      cfg.equation.rgb.a = MALI_BLEND_OPERAND_A_SRC;
+      cfg.equation.rgb.b = MALI_BLEND_OPERAND_B_SRC;
+      cfg.equation.rgb.c = MALI_BLEND_OPERAND_C_ZERO;
+      cfg.equation.alpha.a = MALI_BLEND_OPERAND_A_SRC;
+      cfg.equation.alpha.b = MALI_BLEND_OPERAND_B_SRC;
+      cfg.equation.alpha.c = MALI_BLEND_OPERAND_C_ZERO;
+      cfg.internal.mode =
+         partialwrite ? MALI_BLEND_MODE_FIXED_FUNCTION : MALI_BLEND_MODE_OPAQUE;
+      cfg.equation.color_mask = partialwrite ? wrmask : 0xf;
+      cfg.internal.fixed_function.num_comps = 4;
+      if (!raw) {
+         cfg.internal.fixed_function.conversion.memory_format =
+            panfrost_format_to_bifrost_blend(pdev, fmt, false);
+         cfg.internal.fixed_function.conversion.register_format =
+            MALI_REGISTER_FILE_FORMAT_F32;
+      } else {
+         unsigned imgtexelsz = util_format_get_blocksize(fmt);
+
+         cfg.internal.fixed_function.conversion.memory_format =
+            panvk_meta_copy_img_bifrost_raw_format(imgtexelsz);
+         cfg.internal.fixed_function.conversion.register_format =
+            (imgtexelsz & 2) ? MALI_REGISTER_FILE_FORMAT_U16
+                             : MALI_REGISTER_FILE_FORMAT_U32;
+      }
+   }
+
+   return rsd_ptr.gpu;
+}
+
+static mali_ptr
+panvk_meta_copy_to_buf_emit_rsd(struct panfrost_device *pdev,
+                                struct pan_pool *desc_pool, mali_ptr shader,
+                                const struct pan_shader_info *shader_info,
+                                bool from_img)
+{
+   struct panfrost_ptr rsd_ptr =
+      pan_pool_alloc_desc_aggregate(desc_pool, PAN_DESC(RENDERER_STATE));
+
+   pan_pack(rsd_ptr.cpu, RENDERER_STATE, cfg) {
+      pan_shader_prepare_rsd(shader_info, shader, &cfg);
+      if (from_img) {
+         cfg.shader.texture_count = 1;
+         cfg.shader.sampler_count = 1;
+      }
+   }
+
+   return rsd_ptr.gpu;
+}
+
+static mali_ptr
+panvk_meta_copy_img2img_shader(struct panfrost_device *pdev,
+                               struct pan_pool *bin_pool,
+                               enum pipe_format srcfmt, enum pipe_format dstfmt,
+                               unsigned dstmask, unsigned texdim,
+                               bool texisarray, bool is_ms,
+                               struct pan_shader_info *shader_info)
+{
+   nir_builder b = nir_builder_init_simple_shader(
+      MESA_SHADER_FRAGMENT, GENX(pan_shader_get_compiler_options)(),
+      "panvk_meta_copy_img2img(srcfmt=%s,dstfmt=%s,%dD%s%s)",
+      util_format_name(srcfmt), util_format_name(dstfmt), texdim,
+      texisarray ? "[]" : "", is_ms ? ",ms" : "");
+
+   nir_variable *coord_var = nir_variable_create(
+      b.shader, nir_var_shader_in,
+      glsl_vector_type(GLSL_TYPE_FLOAT, texdim + texisarray), "coord");
+   coord_var->data.location = VARYING_SLOT_VAR0;
+   nir_ssa_def *coord = nir_f2u32(&b, nir_load_var(&b, coord_var));
+
+   nir_tex_instr *tex = nir_tex_instr_create(b.shader, is_ms ? 2 : 1);
+   tex->op = is_ms ? nir_texop_txf_ms : nir_texop_txf;
+   tex->texture_index = 0;
+   tex->is_array = texisarray;
+   tex->dest_type =
+      util_format_is_unorm(srcfmt) ? nir_type_float32 : nir_type_uint32;
+
+   switch (texdim) {
+   case 1:
+      tex->sampler_dim = GLSL_SAMPLER_DIM_1D;
+      break;
+   case 2:
+      tex->sampler_dim = GLSL_SAMPLER_DIM_2D;
+      break;
+   case 3:
+      tex->sampler_dim = GLSL_SAMPLER_DIM_3D;
+      break;
+   default:
+      unreachable("Invalid texture dimension");
+   }
+
+   tex->src[0] = nir_tex_src_for_ssa(nir_tex_src_coord, coord);
+   tex->coord_components = texdim + texisarray;
+
+   if (is_ms) {
+      tex->src[1] =
+         nir_tex_src_for_ssa(nir_tex_src_ms_index, nir_load_sample_id(&b));
+   }
+
+   nir_ssa_dest_init(&tex->instr, &tex->dest, 4,
+                     nir_alu_type_get_type_size(tex->dest_type));
+   nir_builder_instr_insert(&b, &tex->instr);
+
+   nir_ssa_def *texel = &tex->dest.ssa;
+
+   unsigned dstcompsz =
+      util_format_get_component_bits(dstfmt, UTIL_FORMAT_COLORSPACE_RGB, 0);
+   unsigned ndstcomps = util_format_get_nr_components(dstfmt);
+   const struct glsl_type *outtype = NULL;
+
+   if (srcfmt == PIPE_FORMAT_R5G6B5_UNORM && dstfmt == PIPE_FORMAT_R8G8_UNORM) {
+      nir_ssa_def *rgb = nir_f2u32(
+         &b, nir_fmul(&b, texel,
+                      nir_vec3(&b, nir_imm_float(&b, 31), nir_imm_float(&b, 63),
+                               nir_imm_float(&b, 31))));
+      nir_ssa_def *rg = nir_vec2(
+         &b,
+         nir_ior(&b, nir_channel(&b, rgb, 0),
+                 nir_ishl(&b, nir_channel(&b, rgb, 1), nir_imm_int(&b, 5))),
+         nir_ior(&b, nir_ushr_imm(&b, nir_channel(&b, rgb, 1), 3),
+                 nir_ishl(&b, nir_channel(&b, rgb, 2), nir_imm_int(&b, 3))));
+      rg = nir_iand_imm(&b, rg, 255);
+      texel = nir_fmul_imm(&b, nir_u2f32(&b, rg), 1.0 / 255);
+      outtype = glsl_vector_type(GLSL_TYPE_FLOAT, 2);
+   } else if (srcfmt == PIPE_FORMAT_R8G8_UNORM &&
+              dstfmt == PIPE_FORMAT_R5G6B5_UNORM) {
+      nir_ssa_def *rg = nir_f2u32(&b, nir_fmul_imm(&b, texel, 255));
+      nir_ssa_def *rgb = nir_vec3(
+         &b, nir_channel(&b, rg, 0),
+         nir_ior(&b, nir_ushr_imm(&b, nir_channel(&b, rg, 0), 5),
+                 nir_ishl(&b, nir_channel(&b, rg, 1), nir_imm_int(&b, 3))),
+         nir_ushr_imm(&b, nir_channel(&b, rg, 1), 3));
+      rgb = nir_iand(&b, rgb,
+                     nir_vec3(&b, nir_imm_int(&b, 31), nir_imm_int(&b, 63),
+                              nir_imm_int(&b, 31)));
+      texel = nir_fmul(
+         &b, nir_u2f32(&b, rgb),
+         nir_vec3(&b, nir_imm_float(&b, 1.0 / 31), nir_imm_float(&b, 1.0 / 63),
+                  nir_imm_float(&b, 1.0 / 31)));
+      outtype = glsl_vector_type(GLSL_TYPE_FLOAT, 3);
+   } else {
+      assert(srcfmt == dstfmt);
+      enum glsl_base_type basetype;
+      if (util_format_is_unorm(dstfmt)) {
+         basetype = GLSL_TYPE_FLOAT;
+      } else if (dstcompsz == 16) {
+         basetype = GLSL_TYPE_UINT16;
+      } else {
+         assert(dstcompsz == 32);
+         basetype = GLSL_TYPE_UINT;
+      }
+
+      if (dstcompsz == 16)
+         texel = nir_u2u16(&b, texel);
+
+      texel = nir_trim_vector(&b, texel, ndstcomps);
+      outtype = glsl_vector_type(basetype, ndstcomps);
+   }
+
+   nir_variable *out =
+      nir_variable_create(b.shader, nir_var_shader_out, outtype, "out");
+   out->data.location = FRAG_RESULT_DATA0;
+
+   unsigned fullmask = (1 << ndstcomps) - 1;
+   if (dstcompsz > 8 && dstmask != fullmask) {
+      nir_ssa_def *oldtexel = nir_load_var(&b, out);
+      nir_ssa_def *dstcomps[4];
+
+      for (unsigned i = 0; i < ndstcomps; i++) {
+         if (dstmask & BITFIELD_BIT(i))
+            dstcomps[i] = nir_channel(&b, texel, i);
+         else
+            dstcomps[i] = nir_channel(&b, oldtexel, i);
+      }
+
+      texel = nir_vec(&b, dstcomps, ndstcomps);
+   }
+
+   nir_store_var(&b, out, texel, 0xff);
+
+   struct panfrost_compile_inputs inputs = {
+      .gpu_id = pdev->gpu_id,
+      .is_blit = true,
+      .no_ubo_to_push = true,
+   };
+
+   struct util_dynarray binary;
+
+   util_dynarray_init(&binary, NULL);
+   pan_shader_preprocess(b.shader, inputs.gpu_id);
+   NIR_PASS_V(b.shader, GENX(pan_inline_rt_conversion), pdev, &dstfmt);
+   GENX(pan_shader_compile)(b.shader, &inputs, &binary, shader_info);
+
+   shader_info->fs.sample_shading = is_ms;
+
+   mali_ptr shader =
+      pan_pool_upload_aligned(bin_pool, binary.data, binary.size, 128);
+
+   util_dynarray_fini(&binary);
+   ralloc_free(b.shader);
+
+   return shader;
+}
+
+static enum pipe_format
+panvk_meta_copy_img_format(enum pipe_format fmt)
+{
+   /* We can't use a non-compressed format when handling a tiled/AFBC
+    * compressed format because the tile size differ (4x4 blocks for
+    * compressed formats and 16x16 texels for non-compressed ones).
+    */
+   assert(!util_format_is_compressed(fmt));
+
+   /* Pick blendable formats when we can, otherwise pick the UINT variant
+    * matching the texel size.
+    */
+   switch (util_format_get_blocksize(fmt)) {
+   case 16:
+      return PIPE_FORMAT_R32G32B32A32_UINT;
+   case 12:
+      return PIPE_FORMAT_R32G32B32_UINT;
+   case 8:
+      return PIPE_FORMAT_R32G32_UINT;
+   case 6:
+      return PIPE_FORMAT_R16G16B16_UINT;
+   case 4:
+      return PIPE_FORMAT_R8G8B8A8_UNORM;
+   case 2:
+      return (fmt == PIPE_FORMAT_R5G6B5_UNORM ||
+              fmt == PIPE_FORMAT_B5G6R5_UNORM)
+                ? PIPE_FORMAT_R5G6B5_UNORM
+                : PIPE_FORMAT_R8G8_UNORM;
+   case 1:
+      return PIPE_FORMAT_R8_UNORM;
+   default:
+      unreachable("Unsupported format\n");
+   }
+}
+
+struct panvk_meta_copy_img2img_format_info {
+   enum pipe_format srcfmt;
+   enum pipe_format dstfmt;
+   unsigned dstmask;
+} PACKED;
+
+static const struct panvk_meta_copy_img2img_format_info
+   panvk_meta_copy_img2img_fmts[] = {
+      {PIPE_FORMAT_R8_UNORM, PIPE_FORMAT_R8_UNORM, 0x1},
+      {PIPE_FORMAT_R5G6B5_UNORM, PIPE_FORMAT_R5G6B5_UNORM, 0x7},
+      {PIPE_FORMAT_R5G6B5_UNORM, PIPE_FORMAT_R8G8_UNORM, 0x3},
+      {PIPE_FORMAT_R8G8_UNORM, PIPE_FORMAT_R5G6B5_UNORM, 0x7},
+      {PIPE_FORMAT_R8G8_UNORM, PIPE_FORMAT_R8G8_UNORM, 0x3},
+      /* Z24S8(depth) */
+      {PIPE_FORMAT_R8G8B8A8_UNORM, PIPE_FORMAT_R8G8B8A8_UNORM, 0x7},
+      /* Z24S8(stencil) */
+      {PIPE_FORMAT_R8G8B8A8_UNORM, PIPE_FORMAT_R8G8B8A8_UNORM, 0x8},
+      {PIPE_FORMAT_R8G8B8A8_UNORM, PIPE_FORMAT_R8G8B8A8_UNORM, 0xf},
+      {PIPE_FORMAT_R16G16B16_UINT, PIPE_FORMAT_R16G16B16_UINT, 0x7},
+      {PIPE_FORMAT_R32G32_UINT, PIPE_FORMAT_R32G32_UINT, 0x3},
+      /* Z32S8X24(depth) */
+      {PIPE_FORMAT_R32G32_UINT, PIPE_FORMAT_R32G32_UINT, 0x1},
+      /* Z32S8X24(stencil) */
+      {PIPE_FORMAT_R32G32_UINT, PIPE_FORMAT_R32G32_UINT, 0x2},
+      {PIPE_FORMAT_R32G32B32_UINT, PIPE_FORMAT_R32G32B32_UINT, 0x7},
+      {PIPE_FORMAT_R32G32B32A32_UINT, PIPE_FORMAT_R32G32B32A32_UINT, 0xf},
+};
+
+static unsigned
+panvk_meta_copy_img2img_format_idx(
+   struct panvk_meta_copy_img2img_format_info key)
+{
+   STATIC_ASSERT(ARRAY_SIZE(panvk_meta_copy_img2img_fmts) ==
+                 PANVK_META_COPY_IMG2IMG_NUM_FORMATS);
+
+   for (unsigned i = 0; i < ARRAY_SIZE(panvk_meta_copy_img2img_fmts); i++) {
+      if (!memcmp(&key, &panvk_meta_copy_img2img_fmts[i], sizeof(key)))
+         return i;
+   }
+
+   unreachable("Invalid image format\n");
+}
+
+static unsigned
+panvk_meta_copy_img_mask(enum pipe_format imgfmt, VkImageAspectFlags aspectMask)
+{
+   if (aspectMask != VK_IMAGE_ASPECT_DEPTH_BIT &&
+       aspectMask != VK_IMAGE_ASPECT_STENCIL_BIT) {
+      enum pipe_format outfmt = panvk_meta_copy_img_format(imgfmt);
+
+      return (1 << util_format_get_nr_components(outfmt)) - 1;
+   }
+
+   switch (imgfmt) {
+   case PIPE_FORMAT_S8_UINT:
+      return 1;
+   case PIPE_FORMAT_Z16_UNORM:
+      return 3;
+   case PIPE_FORMAT_Z16_UNORM_S8_UINT:
+      return aspectMask == VK_IMAGE_ASPECT_DEPTH_BIT ? 3 : 8;
+   case PIPE_FORMAT_Z24_UNORM_S8_UINT:
+      return aspectMask == VK_IMAGE_ASPECT_DEPTH_BIT ? 7 : 8;
+   case PIPE_FORMAT_Z24X8_UNORM:
+      assert(aspectMask == VK_IMAGE_ASPECT_DEPTH_BIT);
+      return 7;
+   case PIPE_FORMAT_Z32_FLOAT:
+      return 0xf;
+   case PIPE_FORMAT_Z32_FLOAT_S8X24_UINT:
+      return aspectMask == VK_IMAGE_ASPECT_DEPTH_BIT ? 1 : 2;
+   default:
+      unreachable("Invalid depth format\n");
+   }
+}
+
+static void
+panvk_meta_copy_img2img(struct panvk_cmd_buffer *cmdbuf,
+                        const struct panvk_image *src,
+                        const struct panvk_image *dst,
+                        const VkImageCopy2 *region)
+{
+   struct panfrost_device *pdev = &cmdbuf->device->physical_device->pdev;
+   struct pan_fb_info *fbinfo = &cmdbuf->state.fb.info;
+   struct panvk_meta_copy_img2img_format_info key = {
+      .srcfmt = panvk_meta_copy_img_format(src->pimage.layout.format),
+      .dstfmt = panvk_meta_copy_img_format(dst->pimage.layout.format),
+      .dstmask = panvk_meta_copy_img_mask(dst->pimage.layout.format,
+                                          region->dstSubresource.aspectMask),
+   };
+
+   assert(src->pimage.layout.nr_samples == dst->pimage.layout.nr_samples);
+
+   unsigned texdimidx = panvk_meta_copy_tex_type(
+      src->pimage.layout.dim, src->pimage.layout.array_size > 1);
+   unsigned fmtidx = panvk_meta_copy_img2img_format_idx(key);
+   unsigned ms = dst->pimage.layout.nr_samples > 1 ? 1 : 0;
+
+   mali_ptr rsd =
+      cmdbuf->device->physical_device->meta.copy.img2img[ms][texdimidx][fmtidx]
+         .rsd;
+
+   struct pan_image_view srcview = {
+      .format = key.srcfmt,
+      .dim = src->pimage.layout.dim == MALI_TEXTURE_DIMENSION_CUBE
+                ? MALI_TEXTURE_DIMENSION_2D
+                : src->pimage.layout.dim,
+      .image = &src->pimage,
+      .nr_samples = src->pimage.layout.nr_samples,
+      .first_level = region->srcSubresource.mipLevel,
+      .last_level = region->srcSubresource.mipLevel,
+      .first_layer = region->srcSubresource.baseArrayLayer,
+      .last_layer = region->srcSubresource.baseArrayLayer +
+                    region->srcSubresource.layerCount - 1,
+      .swizzle = {PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_Z,
+                  PIPE_SWIZZLE_W},
+   };
+
+   struct pan_image_view dstview = {
+      .format = key.dstfmt,
+      .dim = MALI_TEXTURE_DIMENSION_2D,
+      .image = &dst->pimage,
+      .nr_samples = dst->pimage.layout.nr_samples,
+      .first_level = region->dstSubresource.mipLevel,
+      .last_level = region->dstSubresource.mipLevel,
+      .swizzle = {PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_Z,
+                  PIPE_SWIZZLE_W},
+   };
+
+   unsigned minx = MAX2(region->dstOffset.x, 0);
+   unsigned miny = MAX2(region->dstOffset.y, 0);
+   unsigned maxx = MAX2(region->dstOffset.x + region->extent.width - 1, 0);
+   unsigned maxy = MAX2(region->dstOffset.y + region->extent.height - 1, 0);
+
+   mali_ptr vpd = panvk_per_arch(meta_emit_viewport)(&cmdbuf->desc_pool.base,
+                                                     minx, miny, maxx, maxy);
+
+   float dst_rect[] = {
+      minx, miny,     0.0, 1.0, maxx + 1, miny,     0.0, 1.0,
+      minx, maxy + 1, 0.0, 1.0, maxx + 1, maxy + 1, 0.0, 1.0,
+   };
+
+   mali_ptr dst_coords = pan_pool_upload_aligned(
+      &cmdbuf->desc_pool.base, dst_rect, sizeof(dst_rect), 64);
+
+   /* TODO: don't force preloads of dst resources if unneeded */
+
+   unsigned width =
+      u_minify(dst->pimage.layout.width, region->dstSubresource.mipLevel);
+   unsigned height =
+      u_minify(dst->pimage.layout.height, region->dstSubresource.mipLevel);
+   cmdbuf->state.fb.crc_valid[0] = false;
+   *fbinfo = (struct pan_fb_info){
+      .width = width,
+      .height = height,
+      .extent.minx = minx & ~31,
+      .extent.miny = miny & ~31,
+      .extent.maxx = MIN2(ALIGN_POT(maxx + 1, 32), width) - 1,
+      .extent.maxy = MIN2(ALIGN_POT(maxy + 1, 32), height) - 1,
+      .nr_samples = dst->pimage.layout.nr_samples,
+      .rt_count = 1,
+      .rts[0].view = &dstview,
+      .rts[0].preload = true,
+      .rts[0].crc_valid = &cmdbuf->state.fb.crc_valid[0],
+   };
+
+   mali_ptr texture =
+      panvk_meta_copy_img_emit_texture(pdev, &cmdbuf->desc_pool.base, &srcview);
+   mali_ptr sampler =
+      panvk_meta_copy_img_emit_sampler(pdev, &cmdbuf->desc_pool.base);
+
+   panvk_per_arch(cmd_close_batch)(cmdbuf);
+
+   minx = MAX2(region->srcOffset.x, 0);
+   miny = MAX2(region->srcOffset.y, 0);
+   maxx = MAX2(region->srcOffset.x + region->extent.width - 1, 0);
+   maxy = MAX2(region->srcOffset.y + region->extent.height - 1, 0);
+   assert(region->dstOffset.z >= 0);
+
+   unsigned first_src_layer = MAX2(0, region->srcOffset.z);
+   unsigned first_dst_layer =
+      MAX2(region->dstSubresource.baseArrayLayer, region->dstOffset.z);
+   unsigned nlayers =
+      MAX2(region->dstSubresource.layerCount, region->extent.depth);
+   for (unsigned l = 0; l < nlayers; l++) {
+      unsigned src_l = l + first_src_layer;
+      float src_rect[] = {
+         minx, miny,     src_l, 1.0, maxx + 1, miny,     src_l, 1.0,
+         minx, maxy + 1, src_l, 1.0, maxx + 1, maxy + 1, src_l, 1.0,
+      };
+
+      mali_ptr src_coords = pan_pool_upload_aligned(
+         &cmdbuf->desc_pool.base, src_rect, sizeof(src_rect), 64);
+
+      struct panvk_batch *batch = panvk_cmd_open_batch(cmdbuf);
+
+      dstview.first_layer = dstview.last_layer = l + first_dst_layer;
+      batch->blit.src = src->pimage.data.bo;
+      batch->blit.dst = dst->pimage.data.bo;
+      panvk_per_arch(cmd_alloc_tls_desc)(cmdbuf, true);
+      panvk_per_arch(cmd_alloc_fb_desc)(cmdbuf);
+      panvk_per_arch(cmd_prepare_tiler_context)(cmdbuf);
+
+      mali_ptr tsd, tiler;
+
+      tsd = batch->tls.gpu;
+      tiler = batch->tiler.descs.gpu;
+
+      struct panfrost_ptr job;
+
+      job = panvk_meta_copy_emit_tiler_job(
+         &cmdbuf->desc_pool.base, &batch->scoreboard, src_coords, dst_coords,
+         texture, sampler, 0, vpd, rsd, tsd, tiler);
+
+      util_dynarray_append(&batch->jobs, void *, job.cpu);
+      panvk_per_arch(cmd_close_batch)(cmdbuf);
+   }
+}
+
+static void
+panvk_meta_copy_img2img_init(struct panvk_physical_device *dev, bool is_ms)
+{
+   STATIC_ASSERT(ARRAY_SIZE(panvk_meta_copy_img2img_fmts) ==
+                 PANVK_META_COPY_IMG2IMG_NUM_FORMATS);
+
+   for (unsigned i = 0; i < ARRAY_SIZE(panvk_meta_copy_img2img_fmts); i++) {
+      for (unsigned texdim = 1; texdim <= 3; texdim++) {
+         unsigned texdimidx = panvk_meta_copy_tex_type(texdim, false);
+         assert(texdimidx < ARRAY_SIZE(dev->meta.copy.img2img[0]));
+
+         /* No MSAA on 3D textures */
+         if (texdim == 3 && is_ms)
+            continue;
+
+         struct pan_shader_info shader_info;
+         mali_ptr shader = panvk_meta_copy_img2img_shader(
+            &dev->pdev, &dev->meta.bin_pool.base,
+            panvk_meta_copy_img2img_fmts[i].srcfmt,
+            panvk_meta_copy_img2img_fmts[i].dstfmt,
+            panvk_meta_copy_img2img_fmts[i].dstmask, texdim, false, is_ms,
+            &shader_info);
+         dev->meta.copy.img2img[is_ms][texdimidx][i].rsd =
+            panvk_meta_copy_to_img_emit_rsd(
+               &dev->pdev, &dev->meta.desc_pool.base, shader, &shader_info,
+               panvk_meta_copy_img2img_fmts[i].dstfmt,
+               panvk_meta_copy_img2img_fmts[i].dstmask, true);
+         if (texdim == 3)
+            continue;
+
+         memset(&shader_info, 0, sizeof(shader_info));
+         texdimidx = panvk_meta_copy_tex_type(texdim, true);
+         assert(texdimidx < ARRAY_SIZE(dev->meta.copy.img2img[0]));
+         shader = panvk_meta_copy_img2img_shader(
+            &dev->pdev, &dev->meta.bin_pool.base,
+            panvk_meta_copy_img2img_fmts[i].srcfmt,
+            panvk_meta_copy_img2img_fmts[i].dstfmt,
+            panvk_meta_copy_img2img_fmts[i].dstmask, texdim, true, is_ms,
+            &shader_info);
+         dev->meta.copy.img2img[is_ms][texdimidx][i].rsd =
+            panvk_meta_copy_to_img_emit_rsd(
+               &dev->pdev, &dev->meta.desc_pool.base, shader, &shader_info,
+               panvk_meta_copy_img2img_fmts[i].dstfmt,
+               panvk_meta_copy_img2img_fmts[i].dstmask, true);
+      }
+   }
+}
+
+void
+panvk_per_arch(CmdCopyImage2)(VkCommandBuffer commandBuffer,
+                              const VkCopyImageInfo2 *pCopyImageInfo)
+{
+   VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
+   VK_FROM_HANDLE(panvk_image, dst, pCopyImageInfo->dstImage);
+   VK_FROM_HANDLE(panvk_image, src, pCopyImageInfo->srcImage);
+
+   for (unsigned i = 0; i < pCopyImageInfo->regionCount; i++) {
+      panvk_meta_copy_img2img(cmdbuf, src, dst, &pCopyImageInfo->pRegions[i]);
+   }
+}
+
+static unsigned
+panvk_meta_copy_buf_texelsize(enum pipe_format imgfmt, unsigned mask)
+{
+   unsigned imgtexelsz = util_format_get_blocksize(imgfmt);
+   unsigned nbufcomps = util_bitcount(mask);
+
+   if (nbufcomps == util_format_get_nr_components(imgfmt))
+      return imgtexelsz;
+
+   /* Special case for Z24 buffers which are not tightly packed */
+   if (mask == 7 && imgtexelsz == 4)
+      return 4;
+
+   /* Special case for S8 extraction from Z32_S8X24 */
+   if (mask == 2 && imgtexelsz == 8)
+      return 1;
+
+   unsigned compsz =
+      util_format_get_component_bits(imgfmt, UTIL_FORMAT_COLORSPACE_RGB, 0);
+
+   assert(!(compsz % 8));
+
+   return nbufcomps * compsz / 8;
+}
+
+static enum pipe_format
+panvk_meta_copy_buf2img_format(enum pipe_format imgfmt)
+{
+   /* Pick blendable formats when we can, and the FLOAT variant matching the
+    * texelsize otherwise.
+    */
+   switch (util_format_get_blocksize(imgfmt)) {
+   case 1:
+      return PIPE_FORMAT_R8_UNORM;
+   /* AFBC stores things differently for RGB565,
+    * we can't simply map to R8G8 in that case */
+   case 2:
+      return (imgfmt == PIPE_FORMAT_R5G6B5_UNORM ||
+              imgfmt == PIPE_FORMAT_B5G6R5_UNORM)
+                ? PIPE_FORMAT_R5G6B5_UNORM
+                : PIPE_FORMAT_R8G8_UNORM;
+   case 4:
+      return PIPE_FORMAT_R8G8B8A8_UNORM;
+   case 6:
+      return PIPE_FORMAT_R16G16B16_UINT;
+   case 8:
+      return PIPE_FORMAT_R32G32_UINT;
+   case 12:
+      return PIPE_FORMAT_R32G32B32_UINT;
+   case 16:
+      return PIPE_FORMAT_R32G32B32A32_UINT;
+   default:
+      unreachable("Invalid format\n");
+   }
+}
+
+struct panvk_meta_copy_format_info {
+   enum pipe_format imgfmt;
+   unsigned mask;
+} PACKED;
+
+static const struct panvk_meta_copy_format_info panvk_meta_copy_buf2img_fmts[] =
+   {
+      {PIPE_FORMAT_R8_UNORM, 0x1},
+      {PIPE_FORMAT_R8G8_UNORM, 0x3},
+      {PIPE_FORMAT_R5G6B5_UNORM, 0x7},
+      {PIPE_FORMAT_R8G8B8A8_UNORM, 0xf},
+      {PIPE_FORMAT_R16G16B16_UINT, 0x7},
+      {PIPE_FORMAT_R32G32_UINT, 0x3},
+      {PIPE_FORMAT_R32G32B32_UINT, 0x7},
+      {PIPE_FORMAT_R32G32B32A32_UINT, 0xf},
+      /* S8 -> Z24S8 */
+      {PIPE_FORMAT_R8G8B8A8_UNORM, 0x8},
+      /* S8 -> Z32_S8X24 */
+      {PIPE_FORMAT_R32G32_UINT, 0x2},
+      /* Z24X8 -> Z24S8 */
+      {PIPE_FORMAT_R8G8B8A8_UNORM, 0x7},
+      /* Z32 -> Z32_S8X24 */
+      {PIPE_FORMAT_R32G32_UINT, 0x1},
+};
+
+struct panvk_meta_copy_buf2img_info {
+   struct {
+      mali_ptr ptr;
+      struct {
+         unsigned line;
+         unsigned surf;
+      } stride;
+   } buf;
+} PACKED;
+
+#define panvk_meta_copy_buf2img_get_info_field(b, field)                       \
+   nir_load_push_constant(                                                     \
+      (b), 1, sizeof(((struct panvk_meta_copy_buf2img_info *)0)->field) * 8,   \
+      nir_imm_int(b, 0),                                                       \
+      .base = offsetof(struct panvk_meta_copy_buf2img_info, field),            \
+      .range = ~0)
+
+static mali_ptr
+panvk_meta_copy_buf2img_shader(struct panfrost_device *pdev,
+                               struct pan_pool *bin_pool,
+                               struct panvk_meta_copy_format_info key,
+                               struct pan_shader_info *shader_info)
+{
+   nir_builder b = nir_builder_init_simple_shader(
+      MESA_SHADER_FRAGMENT, GENX(pan_shader_get_compiler_options)(),
+      "panvk_meta_copy_buf2img(imgfmt=%s,mask=%x)",
+      util_format_name(key.imgfmt), key.mask);
+
+   nir_variable *coord_var =
+      nir_variable_create(b.shader, nir_var_shader_in,
+                          glsl_vector_type(GLSL_TYPE_FLOAT, 3), "coord");
+   coord_var->data.location = VARYING_SLOT_VAR0;
+   nir_ssa_def *coord = nir_load_var(&b, coord_var);
+
+   coord = nir_f2u32(&b, coord);
+
+   nir_ssa_def *bufptr = panvk_meta_copy_buf2img_get_info_field(&b, buf.ptr);
+   nir_ssa_def *buflinestride =
+      panvk_meta_copy_buf2img_get_info_field(&b, buf.stride.line);
+   nir_ssa_def *bufsurfstride =
+      panvk_meta_copy_buf2img_get_info_field(&b, buf.stride.surf);
+
+   unsigned imgtexelsz = util_format_get_blocksize(key.imgfmt);
+   unsigned buftexelsz = panvk_meta_copy_buf_texelsize(key.imgfmt, key.mask);
+   unsigned writemask = key.mask;
+
+   nir_ssa_def *offset =
+      nir_imul(&b, nir_channel(&b, coord, 0), nir_imm_int(&b, buftexelsz));
+   offset = nir_iadd(&b, offset,
+                     nir_imul(&b, nir_channel(&b, coord, 1), buflinestride));
+   offset = nir_iadd(&b, offset,
+                     nir_imul(&b, nir_channel(&b, coord, 2), bufsurfstride));
+   bufptr = nir_iadd(&b, bufptr, nir_u2u64(&b, offset));
+
+   unsigned imgcompsz =
+      (imgtexelsz <= 4 && key.imgfmt != PIPE_FORMAT_R5G6B5_UNORM)
+         ? 1
+         : MIN2(1 << (ffs(imgtexelsz) - 1), 4);
+
+   unsigned nimgcomps = imgtexelsz / imgcompsz;
+   unsigned bufcompsz = MIN2(buftexelsz, imgcompsz);
+   unsigned nbufcomps = buftexelsz / bufcompsz;
+
+   assert(bufcompsz == 1 || bufcompsz == 2 || bufcompsz == 4);
+   assert(nbufcomps <= 4 && nimgcomps <= 4);
+
+   nir_ssa_def *texel =
+      nir_load_global(&b, bufptr, bufcompsz, nbufcomps, bufcompsz * 8);
+
+   enum glsl_base_type basetype;
+   if (key.imgfmt == PIPE_FORMAT_R5G6B5_UNORM) {
+      texel = nir_vec3(
+         &b, nir_iand_imm(&b, texel, BITFIELD_MASK(5)),
+         nir_iand_imm(&b, nir_ushr_imm(&b, texel, 5), BITFIELD_MASK(6)),
+         nir_iand_imm(&b, nir_ushr_imm(&b, texel, 11), BITFIELD_MASK(5)));
+      texel = nir_fmul(
+         &b, nir_u2f32(&b, texel),
+         nir_vec3(&b, nir_imm_float(&b, 1.0f / 31),
+                  nir_imm_float(&b, 1.0f / 63), nir_imm_float(&b, 1.0f / 31)));
+      nimgcomps = 3;
+      basetype = GLSL_TYPE_FLOAT;
+   } else if (imgcompsz == 1) {
+      assert(bufcompsz == 1);
+      /* Blendable formats are unorm and the fixed-function blend unit
+       * takes float values.
+       */
+      texel = nir_fmul_imm(&b, nir_u2f32(&b, texel), 1.0f / 255);
+      basetype = GLSL_TYPE_FLOAT;
+   } else {
+      texel = nir_u2uN(&b, texel, imgcompsz * 8);
+      basetype = imgcompsz == 2 ? GLSL_TYPE_UINT16 : GLSL_TYPE_UINT;
+   }
+
+   /* We always pass the texel using 32-bit regs for now */
+   nir_variable *out =
+      nir_variable_create(b.shader, nir_var_shader_out,
+                          glsl_vector_type(basetype, nimgcomps), "out");
+   out->data.location = FRAG_RESULT_DATA0;
+
+   uint16_t fullmask = (1 << nimgcomps) - 1;
+
+   assert(fullmask >= writemask);
+
+   if (fullmask != writemask) {
+      unsigned first_written_comp = ffs(writemask) - 1;
+      nir_ssa_def *oldtexel = NULL;
+      if (imgcompsz > 1)
+         oldtexel = nir_load_var(&b, out);
+
+      nir_ssa_def *texel_comps[4];
+      for (unsigned i = 0; i < nimgcomps; i++) {
+         if (writemask & BITFIELD_BIT(i))
+            texel_comps[i] = nir_channel(&b, texel, i - first_written_comp);
+         else if (imgcompsz > 1)
+            texel_comps[i] = nir_channel(&b, oldtexel, i);
+         else
+            texel_comps[i] = nir_imm_intN_t(&b, 0, texel->bit_size);
+      }
+
+      texel = nir_vec(&b, texel_comps, nimgcomps);
+   }
+
+   nir_store_var(&b, out, texel, 0xff);
+
+   struct panfrost_compile_inputs inputs = {
+      .gpu_id = pdev->gpu_id,
+      .is_blit = true,
+      .no_ubo_to_push = true,
+   };
+
+   struct util_dynarray binary;
+
+   util_dynarray_init(&binary, NULL);
+   pan_shader_preprocess(b.shader, inputs.gpu_id);
+
+   enum pipe_format rt_formats[8] = {key.imgfmt};
+   NIR_PASS_V(b.shader, GENX(pan_inline_rt_conversion), pdev, rt_formats);
+
+   GENX(pan_shader_compile)(b.shader, &inputs, &binary, shader_info);
+   shader_info->push.count =
+      DIV_ROUND_UP(sizeof(struct panvk_meta_copy_buf2img_info), 4);
+
+   mali_ptr shader =
+      pan_pool_upload_aligned(bin_pool, binary.data, binary.size, 128);
+
+   util_dynarray_fini(&binary);
+   ralloc_free(b.shader);
+
+   return shader;
+}
+
+static unsigned
+panvk_meta_copy_buf2img_format_idx(struct panvk_meta_copy_format_info key)
+{
+   for (unsigned i = 0; i < ARRAY_SIZE(panvk_meta_copy_buf2img_fmts); i++) {
+      if (!memcmp(&key, &panvk_meta_copy_buf2img_fmts[i], sizeof(key)))
+         return i;
+   }
+
+   unreachable("Invalid image format\n");
+}
+
+static void
+panvk_meta_copy_buf2img(struct panvk_cmd_buffer *cmdbuf,
+                        const struct panvk_buffer *buf,
+                        const struct panvk_image *img,
+                        const VkBufferImageCopy2 *region)
+{
+   struct pan_fb_info *fbinfo = &cmdbuf->state.fb.info;
+   unsigned minx = MAX2(region->imageOffset.x, 0);
+   unsigned miny = MAX2(region->imageOffset.y, 0);
+   unsigned maxx =
+      MAX2(region->imageOffset.x + region->imageExtent.width - 1, 0);
+   unsigned maxy =
+      MAX2(region->imageOffset.y + region->imageExtent.height - 1, 0);
+
+   mali_ptr vpd = panvk_per_arch(meta_emit_viewport)(&cmdbuf->desc_pool.base,
+                                                     minx, miny, maxx, maxy);
+
+   float dst_rect[] = {
+      minx, miny,     0.0, 1.0, maxx + 1, miny,     0.0, 1.0,
+      minx, maxy + 1, 0.0, 1.0, maxx + 1, maxy + 1, 0.0, 1.0,
+   };
+   mali_ptr dst_coords = pan_pool_upload_aligned(
+      &cmdbuf->desc_pool.base, dst_rect, sizeof(dst_rect), 64);
+
+   struct panvk_meta_copy_format_info key = {
+      .imgfmt = panvk_meta_copy_buf2img_format(img->pimage.layout.format),
+      .mask = panvk_meta_copy_img_mask(img->pimage.layout.format,
+                                       region->imageSubresource.aspectMask),
+   };
+
+   unsigned fmtidx = panvk_meta_copy_buf2img_format_idx(key);
+
+   mali_ptr rsd =
+      cmdbuf->device->physical_device->meta.copy.buf2img[fmtidx].rsd;
+
+   const struct vk_image_buffer_layout buflayout =
+      vk_image_buffer_copy_layout(&img->vk, region);
+   struct panvk_meta_copy_buf2img_info info = {
+      .buf.ptr = panvk_buffer_gpu_ptr(buf, region->bufferOffset),
+      .buf.stride.line = buflayout.row_stride_B,
+      .buf.stride.surf = buflayout.image_stride_B,
+   };
+
+   mali_ptr pushconsts =
+      pan_pool_upload_aligned(&cmdbuf->desc_pool.base, &info, sizeof(info), 16);
+
+   struct pan_image_view view = {
+      .format = key.imgfmt,
+      .dim = MALI_TEXTURE_DIMENSION_2D,
+      .image = &img->pimage,
+      .nr_samples = img->pimage.layout.nr_samples,
+      .first_level = region->imageSubresource.mipLevel,
+      .last_level = region->imageSubresource.mipLevel,
+      .swizzle = {PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_Z,
+                  PIPE_SWIZZLE_W},
+   };
+
+   /* TODO: don't force preloads of dst resources if unneeded */
+   cmdbuf->state.fb.crc_valid[0] = false;
+   *fbinfo = (struct pan_fb_info){
+      .width =
+         u_minify(img->pimage.layout.width, region->imageSubresource.mipLevel),
+      .height =
+         u_minify(img->pimage.layout.height, region->imageSubresource.mipLevel),
+      .extent.minx = minx,
+      .extent.maxx = maxx,
+      .extent.miny = miny,
+      .extent.maxy = maxy,
+      .nr_samples = 1,
+      .rt_count = 1,
+      .rts[0].view = &view,
+      .rts[0].preload = true,
+      .rts[0].crc_valid = &cmdbuf->state.fb.crc_valid[0],
+   };
+
+   panvk_per_arch(cmd_close_batch)(cmdbuf);
+
+   assert(region->imageSubresource.layerCount == 1 ||
+          region->imageExtent.depth == 1);
+   assert(region->imageOffset.z >= 0);
+   unsigned first_layer =
+      MAX2(region->imageSubresource.baseArrayLayer, region->imageOffset.z);
+   unsigned nlayers =
+      MAX2(region->imageSubresource.layerCount, region->imageExtent.depth);
+   for (unsigned l = 0; l < nlayers; l++) {
+      float src_rect[] = {
+         0,
+         0,
+         l,
+         1.0,
+         region->imageExtent.width,
+         0,
+         l,
+         1.0,
+         0,
+         region->imageExtent.height,
+         l,
+         1.0,
+         region->imageExtent.width,
+         region->imageExtent.height,
+         l,
+         1.0,
+      };
+
+      mali_ptr src_coords = pan_pool_upload_aligned(
+         &cmdbuf->desc_pool.base, src_rect, sizeof(src_rect), 64);
+
+      struct panvk_batch *batch = panvk_cmd_open_batch(cmdbuf);
+
+      view.first_layer = view.last_layer = l + first_layer;
+      batch->blit.src = buf->bo;
+      batch->blit.dst = img->pimage.data.bo;
+      panvk_per_arch(cmd_alloc_tls_desc)(cmdbuf, true);
+      panvk_per_arch(cmd_alloc_fb_desc)(cmdbuf);
+      panvk_per_arch(cmd_prepare_tiler_context)(cmdbuf);
+
+      mali_ptr tsd, tiler;
+
+      tsd = batch->tls.gpu;
+      tiler = batch->tiler.descs.gpu;
+
+      struct panfrost_ptr job;
+
+      job = panvk_meta_copy_emit_tiler_job(
+         &cmdbuf->desc_pool.base, &batch->scoreboard, src_coords, dst_coords, 0,
+         0, pushconsts, vpd, rsd, tsd, tiler);
+
+      util_dynarray_append(&batch->jobs, void *, job.cpu);
+      panvk_per_arch(cmd_close_batch)(cmdbuf);
+   }
+}
+
+static void
+panvk_meta_copy_buf2img_init(struct panvk_physical_device *dev)
+{
+   STATIC_ASSERT(ARRAY_SIZE(panvk_meta_copy_buf2img_fmts) ==
+                 PANVK_META_COPY_BUF2IMG_NUM_FORMATS);
+
+   for (unsigned i = 0; i < ARRAY_SIZE(panvk_meta_copy_buf2img_fmts); i++) {
+      struct pan_shader_info shader_info;
+      mali_ptr shader = panvk_meta_copy_buf2img_shader(
+         &dev->pdev, &dev->meta.bin_pool.base, panvk_meta_copy_buf2img_fmts[i],
+         &shader_info);
+      dev->meta.copy.buf2img[i].rsd = panvk_meta_copy_to_img_emit_rsd(
+         &dev->pdev, &dev->meta.desc_pool.base, shader, &shader_info,
+         panvk_meta_copy_buf2img_fmts[i].imgfmt,
+         panvk_meta_copy_buf2img_fmts[i].mask, false);
+   }
+}
+
+void
+panvk_per_arch(CmdCopyBufferToImage2)(
+   VkCommandBuffer commandBuffer,
+   const VkCopyBufferToImageInfo2 *pCopyBufferToImageInfo)
+{
+   VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
+   VK_FROM_HANDLE(panvk_buffer, buf, pCopyBufferToImageInfo->srcBuffer);
+   VK_FROM_HANDLE(panvk_image, img, pCopyBufferToImageInfo->dstImage);
+
+   for (unsigned i = 0; i < pCopyBufferToImageInfo->regionCount; i++) {
+      panvk_meta_copy_buf2img(cmdbuf, buf, img,
+                              &pCopyBufferToImageInfo->pRegions[i]);
+   }
+}
+
+static const struct panvk_meta_copy_format_info panvk_meta_copy_img2buf_fmts[] =
+   {
+      {PIPE_FORMAT_R8_UINT, 0x1},
+      {PIPE_FORMAT_R8G8_UINT, 0x3},
+      {PIPE_FORMAT_R5G6B5_UNORM, 0x7},
+      {PIPE_FORMAT_R8G8B8A8_UINT, 0xf},
+      {PIPE_FORMAT_R16G16B16_UINT, 0x7},
+      {PIPE_FORMAT_R32G32_UINT, 0x3},
+      {PIPE_FORMAT_R32G32B32_UINT, 0x7},
+      {PIPE_FORMAT_R32G32B32A32_UINT, 0xf},
+      /* S8 -> Z24S8 */
+      {PIPE_FORMAT_R8G8B8A8_UINT, 0x8},
+      /* S8 -> Z32_S8X24 */
+      {PIPE_FORMAT_R32G32_UINT, 0x2},
+      /* Z24X8 -> Z24S8 */
+      {PIPE_FORMAT_R8G8B8A8_UINT, 0x7},
+      /* Z32 -> Z32_S8X24 */
+      {PIPE_FORMAT_R32G32_UINT, 0x1},
+};
+
+static enum pipe_format
+panvk_meta_copy_img2buf_format(enum pipe_format imgfmt)
+{
+   /* Pick blendable formats when we can, and the FLOAT variant matching the
+    * texelsize otherwise.
+    */
+   switch (util_format_get_blocksize(imgfmt)) {
+   case 1:
+      return PIPE_FORMAT_R8_UINT;
+   /* AFBC stores things differently for RGB565,
+    * we can't simply map to R8G8 in that case */
+   case 2:
+      return (imgfmt == PIPE_FORMAT_R5G6B5_UNORM ||
+              imgfmt == PIPE_FORMAT_B5G6R5_UNORM)
+                ? PIPE_FORMAT_R5G6B5_UNORM
+                : PIPE_FORMAT_R8G8_UINT;
+   case 4:
+      return PIPE_FORMAT_R8G8B8A8_UINT;
+   case 6:
+      return PIPE_FORMAT_R16G16B16_UINT;
+   case 8:
+      return PIPE_FORMAT_R32G32_UINT;
+   case 12:
+      return PIPE_FORMAT_R32G32B32_UINT;
+   case 16:
+      return PIPE_FORMAT_R32G32B32A32_UINT;
+   default:
+      unreachable("Invalid format\n");
+   }
+}
+
+struct panvk_meta_copy_img2buf_info {
+   struct {
+      mali_ptr ptr;
+      struct {
+         unsigned line;
+         unsigned surf;
+      } stride;
+   } buf;
+   struct {
+      struct {
+         unsigned x, y, z;
+      } offset;
+      struct {
+         unsigned minx, miny, maxx, maxy;
+      } extent;
+   } img;
+} PACKED;
+
+#define panvk_meta_copy_img2buf_get_info_field(b, field)                       \
+   nir_load_push_constant(                                                     \
+      (b), 1, sizeof(((struct panvk_meta_copy_img2buf_info *)0)->field) * 8,   \
+      nir_imm_int(b, 0),                                                       \
+      .base = offsetof(struct panvk_meta_copy_img2buf_info, field),            \
+      .range = ~0)
+
+static mali_ptr
+panvk_meta_copy_img2buf_shader(struct panfrost_device *pdev,
+                               struct pan_pool *bin_pool,
+                               struct panvk_meta_copy_format_info key,
+                               unsigned texdim, unsigned texisarray,
+                               struct pan_shader_info *shader_info)
+{
+   unsigned imgtexelsz = util_format_get_blocksize(key.imgfmt);
+   unsigned buftexelsz = panvk_meta_copy_buf_texelsize(key.imgfmt, key.mask);
+
+   /* FIXME: Won't work on compute queues, but we can't do that with
+    * a compute shader if the destination is an AFBC surface.
+    */
+   nir_builder b = nir_builder_init_simple_shader(
+      MESA_SHADER_COMPUTE, GENX(pan_shader_get_compiler_options)(),
+      "panvk_meta_copy_img2buf(dim=%dD%s,imgfmt=%s,mask=%x)", texdim,
+      texisarray ? "[]" : "", util_format_name(key.imgfmt), key.mask);
+
+   nir_ssa_def *coord = nir_load_global_invocation_id(&b, 32);
+   nir_ssa_def *bufptr = panvk_meta_copy_img2buf_get_info_field(&b, buf.ptr);
+   nir_ssa_def *buflinestride =
+      panvk_meta_copy_img2buf_get_info_field(&b, buf.stride.line);
+   nir_ssa_def *bufsurfstride =
+      panvk_meta_copy_img2buf_get_info_field(&b, buf.stride.surf);
+
+   nir_ssa_def *imgminx =
+      panvk_meta_copy_img2buf_get_info_field(&b, img.extent.minx);
+   nir_ssa_def *imgminy =
+      panvk_meta_copy_img2buf_get_info_field(&b, img.extent.miny);
+   nir_ssa_def *imgmaxx =
+      panvk_meta_copy_img2buf_get_info_field(&b, img.extent.maxx);
+   nir_ssa_def *imgmaxy =
+      panvk_meta_copy_img2buf_get_info_field(&b, img.extent.maxy);
+
+   nir_ssa_def *imgcoords, *inbounds;
+
+   switch (texdim + texisarray) {
+   case 1:
+      imgcoords =
+         nir_iadd(&b, nir_channel(&b, coord, 0),
+                  panvk_meta_copy_img2buf_get_info_field(&b, img.offset.x));
+      inbounds =
+         nir_iand(&b, nir_uge(&b, imgmaxx, nir_channel(&b, imgcoords, 0)),
+                  nir_uge(&b, nir_channel(&b, imgcoords, 0), imgminx));
+      break;
+   case 2:
+      imgcoords = nir_vec2(
+         &b,
+         nir_iadd(&b, nir_channel(&b, coord, 0),
+                  panvk_meta_copy_img2buf_get_info_field(&b, img.offset.x)),
+         nir_iadd(&b, nir_channel(&b, coord, 1),
+                  panvk_meta_copy_img2buf_get_info_field(&b, img.offset.y)));
+      inbounds = nir_iand(
+         &b,
+         nir_iand(&b, nir_uge(&b, imgmaxx, nir_channel(&b, imgcoords, 0)),
+                  nir_uge(&b, imgmaxy, nir_channel(&b, imgcoords, 1))),
+         nir_iand(&b, nir_uge(&b, nir_channel(&b, imgcoords, 0), imgminx),
+                  nir_uge(&b, nir_channel(&b, imgcoords, 1), imgminy)));
+      break;
+   case 3:
+      imgcoords = nir_vec3(
+         &b,
+         nir_iadd(&b, nir_channel(&b, coord, 0),
+                  panvk_meta_copy_img2buf_get_info_field(&b, img.offset.x)),
+         nir_iadd(&b, nir_channel(&b, coord, 1),
+                  panvk_meta_copy_img2buf_get_info_field(&b, img.offset.y)),
+         nir_iadd(&b, nir_channel(&b, coord, 2),
+                  panvk_meta_copy_img2buf_get_info_field(&b, img.offset.y)));
+      inbounds = nir_iand(
+         &b,
+         nir_iand(&b, nir_uge(&b, imgmaxx, nir_channel(&b, imgcoords, 0)),
+                  nir_uge(&b, imgmaxy, nir_channel(&b, imgcoords, 1))),
+         nir_iand(&b, nir_uge(&b, nir_channel(&b, imgcoords, 0), imgminx),
+                  nir_uge(&b, nir_channel(&b, imgcoords, 1), imgminy)));
+      break;
+   default:
+      unreachable("Invalid texture dimension\n");
+   }
+
+   nir_push_if(&b, inbounds);
+
+   /* FIXME: doesn't work for tiled+compressed formats since blocks are 4x4
+    * blocks instead of 16x16 texels in that case, and there's nothing we can
+    * do to force the tile size to 4x4 in the render path.
+    * This being said, compressed textures are not compatible with AFBC, so we
+    * could use a compute shader arranging the blocks properly.
+    */
+   nir_ssa_def *offset =
+      nir_imul(&b, nir_channel(&b, coord, 0), nir_imm_int(&b, buftexelsz));
+   offset = nir_iadd(&b, offset,
+                     nir_imul(&b, nir_channel(&b, coord, 1), buflinestride));
+   offset = nir_iadd(&b, offset,
+                     nir_imul(&b, nir_channel(&b, coord, 2), bufsurfstride));
+   bufptr = nir_iadd(&b, bufptr, nir_u2u64(&b, offset));
+
+   unsigned imgcompsz =
+      imgtexelsz <= 4 ? 1 : MIN2(1 << (ffs(imgtexelsz) - 1), 4);
+   unsigned nimgcomps = imgtexelsz / imgcompsz;
+   assert(nimgcomps <= 4);
+
+   nir_tex_instr *tex = nir_tex_instr_create(b.shader, 1);
+   tex->op = nir_texop_txf;
+   tex->texture_index = 0;
+   tex->is_array = texisarray;
+   tex->dest_type =
+      util_format_is_unorm(key.imgfmt) ? nir_type_float32 : nir_type_uint32;
+
+   switch (texdim) {
+   case 1:
+      tex->sampler_dim = GLSL_SAMPLER_DIM_1D;
+      break;
+   case 2:
+      tex->sampler_dim = GLSL_SAMPLER_DIM_2D;
+      break;
+   case 3:
+      tex->sampler_dim = GLSL_SAMPLER_DIM_3D;
+      break;
+   default:
+      unreachable("Invalid texture dimension");
+   }
+
+   tex->src[0] = nir_tex_src_for_ssa(nir_tex_src_coord, imgcoords);
+   tex->coord_components = texdim + texisarray;
+   nir_ssa_dest_init(&tex->instr, &tex->dest, 4,
+                     nir_alu_type_get_type_size(tex->dest_type));
+   nir_builder_instr_insert(&b, &tex->instr);
+
+   nir_ssa_def *texel = &tex->dest.ssa;
+
+   unsigned fullmask = (1 << util_format_get_nr_components(key.imgfmt)) - 1;
+   unsigned nbufcomps = util_bitcount(fullmask);
+   if (key.mask != fullmask) {
+      nir_ssa_def *bufcomps[4];
+      nbufcomps = 0;
+      for (unsigned i = 0; i < nimgcomps; i++) {
+         if (key.mask & BITFIELD_BIT(i))
+            bufcomps[nbufcomps++] = nir_channel(&b, texel, i);
+      }
+
+      texel = nir_vec(&b, bufcomps, nbufcomps);
+   }
+
+   unsigned bufcompsz = buftexelsz / nbufcomps;
+
+   if (key.imgfmt == PIPE_FORMAT_R5G6B5_UNORM) {
+      texel = nir_fmul(&b, texel,
+                       nir_vec3(&b, nir_imm_float(&b, 31),
+                                nir_imm_float(&b, 63), nir_imm_float(&b, 31)));
+      texel = nir_f2u16(&b, texel);
+      texel = nir_ior(
+         &b, nir_channel(&b, texel, 0),
+         nir_ior(&b,
+                 nir_ishl(&b, nir_channel(&b, texel, 1), nir_imm_int(&b, 5)),
+                 nir_ishl(&b, nir_channel(&b, texel, 2), nir_imm_int(&b, 11))));
+      imgcompsz = 2;
+      bufcompsz = 2;
+      nbufcomps = 1;
+      nimgcomps = 1;
+   } else if (imgcompsz == 1) {
+      nir_ssa_def *packed = nir_channel(&b, texel, 0);
+      for (unsigned i = 1; i < nbufcomps; i++) {
+         packed = nir_ior(
+            &b, packed,
+            nir_ishl(&b, nir_iand_imm(&b, nir_channel(&b, texel, i), 0xff),
+                     nir_imm_int(&b, i * 8)));
+      }
+      texel = packed;
+
+      bufcompsz = nbufcomps == 3 ? 4 : nbufcomps;
+      nbufcomps = 1;
+   }
+
+   assert(bufcompsz == 1 || bufcompsz == 2 || bufcompsz == 4);
+   assert(nbufcomps <= 4 && nimgcomps <= 4);
+   texel = nir_u2uN(&b, texel, bufcompsz * 8);
+
+   nir_store_global(&b, bufptr, bufcompsz, texel, (1 << nbufcomps) - 1);
+   nir_pop_if(&b, NULL);
+
+   struct panfrost_compile_inputs inputs = {
+      .gpu_id = pdev->gpu_id,
+      .is_blit = true,
+      .no_ubo_to_push = true,
+   };
+
+   struct util_dynarray binary;
+
+   util_dynarray_init(&binary, NULL);
+   pan_shader_preprocess(b.shader, inputs.gpu_id);
+   GENX(pan_shader_compile)(b.shader, &inputs, &binary, shader_info);
+
+   shader_info->push.count =
+      DIV_ROUND_UP(sizeof(struct panvk_meta_copy_img2buf_info), 4);
+
+   mali_ptr shader =
+      pan_pool_upload_aligned(bin_pool, binary.data, binary.size, 128);
+
+   util_dynarray_fini(&binary);
+   ralloc_free(b.shader);
+
+   return shader;
+}
+
+static unsigned
+panvk_meta_copy_img2buf_format_idx(struct panvk_meta_copy_format_info key)
+{
+   for (unsigned i = 0; i < ARRAY_SIZE(panvk_meta_copy_img2buf_fmts); i++) {
+      if (!memcmp(&key, &panvk_meta_copy_img2buf_fmts[i], sizeof(key)))
+         return i;
+   }
+
+   unreachable("Invalid texel size\n");
+}
+
+static void
+panvk_meta_copy_img2buf(struct panvk_cmd_buffer *cmdbuf,
+                        const struct panvk_buffer *buf,
+                        const struct panvk_image *img,
+                        const VkBufferImageCopy2 *region)
+{
+   struct panfrost_device *pdev = &cmdbuf->device->physical_device->pdev;
+   struct panvk_meta_copy_format_info key = {
+      .imgfmt = panvk_meta_copy_img2buf_format(img->pimage.layout.format),
+      .mask = panvk_meta_copy_img_mask(img->pimage.layout.format,
+                                       region->imageSubresource.aspectMask),
+   };
+   unsigned buftexelsz = panvk_meta_copy_buf_texelsize(key.imgfmt, key.mask);
+   unsigned texdimidx = panvk_meta_copy_tex_type(
+      img->pimage.layout.dim, img->pimage.layout.array_size > 1);
+   unsigned fmtidx = panvk_meta_copy_img2buf_format_idx(key);
+
+   mali_ptr rsd =
+      cmdbuf->device->physical_device->meta.copy.img2buf[texdimidx][fmtidx].rsd;
+
+   struct panvk_meta_copy_img2buf_info info = {
+      .buf.ptr = panvk_buffer_gpu_ptr(buf, region->bufferOffset),
+      .buf.stride.line =
+         (region->bufferRowLength ?: region->imageExtent.width) * buftexelsz,
+      .img.offset.x = MAX2(region->imageOffset.x & ~15, 0),
+      .img.extent.minx = MAX2(region->imageOffset.x, 0),
+      .img.extent.maxx =
+         MAX2(region->imageOffset.x + region->imageExtent.width - 1, 0),
+   };
+
+   if (img->pimage.layout.dim == MALI_TEXTURE_DIMENSION_1D) {
+      info.img.extent.maxy = region->imageSubresource.layerCount - 1;
+   } else {
+      info.img.offset.y = MAX2(region->imageOffset.y & ~15, 0);
+      info.img.offset.z = MAX2(region->imageOffset.z, 0);
+      info.img.extent.miny = MAX2(region->imageOffset.y, 0);
+      info.img.extent.maxy =
+         MAX2(region->imageOffset.y + region->imageExtent.height - 1, 0);
+   }
+
+   info.buf.stride.surf =
+      (region->bufferImageHeight ?: region->imageExtent.height) *
+      info.buf.stride.line;
+
+   mali_ptr pushconsts =
+      pan_pool_upload_aligned(&cmdbuf->desc_pool.base, &info, sizeof(info), 16);
+
+   struct pan_image_view view = {
+      .format = key.imgfmt,
+      .dim = img->pimage.layout.dim == MALI_TEXTURE_DIMENSION_CUBE
+                ? MALI_TEXTURE_DIMENSION_2D
+                : img->pimage.layout.dim,
+      .image = &img->pimage,
+      .nr_samples = img->pimage.layout.nr_samples,
+      .first_level = region->imageSubresource.mipLevel,
+      .last_level = region->imageSubresource.mipLevel,
+      .first_layer = region->imageSubresource.baseArrayLayer,
+      .last_layer = region->imageSubresource.baseArrayLayer +
+                    region->imageSubresource.layerCount - 1,
+      .swizzle = {PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_Z,
+                  PIPE_SWIZZLE_W},
+   };
+
+   mali_ptr texture =
+      panvk_meta_copy_img_emit_texture(pdev, &cmdbuf->desc_pool.base, &view);
+   mali_ptr sampler =
+      panvk_meta_copy_img_emit_sampler(pdev, &cmdbuf->desc_pool.base);
+
+   panvk_per_arch(cmd_close_batch)(cmdbuf);
+
+   struct panvk_batch *batch = panvk_cmd_open_batch(cmdbuf);
+
+   struct pan_tls_info tlsinfo = {0};
+
+   batch->blit.src = img->pimage.data.bo;
+   batch->blit.dst = buf->bo;
+   batch->tls = pan_pool_alloc_desc(&cmdbuf->desc_pool.base, LOCAL_STORAGE);
+   GENX(pan_emit_tls)(&tlsinfo, batch->tls.cpu);
+
+   mali_ptr tsd = batch->tls.gpu;
+
+   struct pan_compute_dim wg_sz = {
+      16,
+      img->pimage.layout.dim == MALI_TEXTURE_DIMENSION_1D ? 1 : 16,
+      1,
+   };
+
+   struct pan_compute_dim num_wg = {
+      (ALIGN_POT(info.img.extent.maxx + 1, 16) - info.img.offset.x) / 16,
+      img->pimage.layout.dim == MALI_TEXTURE_DIMENSION_1D
+         ? region->imageSubresource.layerCount
+         : (ALIGN_POT(info.img.extent.maxy + 1, 16) - info.img.offset.y) / 16,
+      img->pimage.layout.dim != MALI_TEXTURE_DIMENSION_1D
+         ? MAX2(region->imageSubresource.layerCount, region->imageExtent.depth)
+         : 1,
+   };
+
+   struct panfrost_ptr job = panvk_meta_copy_emit_compute_job(
+      &cmdbuf->desc_pool.base, &batch->scoreboard, &num_wg, &wg_sz, texture,
+      sampler, pushconsts, rsd, tsd);
+
+   util_dynarray_append(&batch->jobs, void *, job.cpu);
+
+   panvk_per_arch(cmd_close_batch)(cmdbuf);
+}
+
+static void
+panvk_meta_copy_img2buf_init(struct panvk_physical_device *dev)
+{
+   STATIC_ASSERT(ARRAY_SIZE(panvk_meta_copy_img2buf_fmts) ==
+                 PANVK_META_COPY_IMG2BUF_NUM_FORMATS);
+
+   for (unsigned i = 0; i < ARRAY_SIZE(panvk_meta_copy_img2buf_fmts); i++) {
+      for (unsigned texdim = 1; texdim <= 3; texdim++) {
+         unsigned texdimidx = panvk_meta_copy_tex_type(texdim, false);
+         assert(texdimidx < ARRAY_SIZE(dev->meta.copy.img2buf));
+
+         struct pan_shader_info shader_info;
+         mali_ptr shader = panvk_meta_copy_img2buf_shader(
+            &dev->pdev, &dev->meta.bin_pool.base,
+            panvk_meta_copy_img2buf_fmts[i], texdim, false, &shader_info);
+         dev->meta.copy.img2buf[texdimidx][i].rsd =
+            panvk_meta_copy_to_buf_emit_rsd(&dev->pdev,
+                                            &dev->meta.desc_pool.base, shader,
+                                            &shader_info, true);
+
+         if (texdim == 3)
+            continue;
+
+         memset(&shader_info, 0, sizeof(shader_info));
+         texdimidx = panvk_meta_copy_tex_type(texdim, true);
+         assert(texdimidx < ARRAY_SIZE(dev->meta.copy.img2buf));
+         shader = panvk_meta_copy_img2buf_shader(
+            &dev->pdev, &dev->meta.bin_pool.base,
+            panvk_meta_copy_img2buf_fmts[i], texdim, true, &shader_info);
+         dev->meta.copy.img2buf[texdimidx][i].rsd =
+            panvk_meta_copy_to_buf_emit_rsd(&dev->pdev,
+                                            &dev->meta.desc_pool.base, shader,
+                                            &shader_info, true);
+      }
+   }
+}
+
+void
+panvk_per_arch(CmdCopyImageToBuffer2)(
+   VkCommandBuffer commandBuffer,
+   const VkCopyImageToBufferInfo2 *pCopyImageToBufferInfo)
+{
+   VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
+   VK_FROM_HANDLE(panvk_buffer, buf, pCopyImageToBufferInfo->dstBuffer);
+   VK_FROM_HANDLE(panvk_image, img, pCopyImageToBufferInfo->srcImage);
+
+   for (unsigned i = 0; i < pCopyImageToBufferInfo->regionCount; i++) {
+      panvk_meta_copy_img2buf(cmdbuf, buf, img,
+                              &pCopyImageToBufferInfo->pRegions[i]);
+   }
+}
+
+struct panvk_meta_copy_buf2buf_info {
+   mali_ptr src;
+   mali_ptr dst;
+} PACKED;
+
+#define panvk_meta_copy_buf2buf_get_info_field(b, field)                       \
+   nir_load_push_constant(                                                     \
+      (b), 1, sizeof(((struct panvk_meta_copy_buf2buf_info *)0)->field) * 8,   \
+      nir_imm_int(b, 0),                                                       \
+      .base = offsetof(struct panvk_meta_copy_buf2buf_info, field),            \
+      .range = ~0)
+
+static mali_ptr
+panvk_meta_copy_buf2buf_shader(struct panfrost_device *pdev,
+                               struct pan_pool *bin_pool, unsigned blksz,
+                               struct pan_shader_info *shader_info)
+{
+   /* FIXME: Won't work on compute queues, but we can't do that with
+    * a compute shader if the destination is an AFBC surface.
+    */
+   nir_builder b = nir_builder_init_simple_shader(
+      MESA_SHADER_COMPUTE, GENX(pan_shader_get_compiler_options)(),
+      "panvk_meta_copy_buf2buf(blksz=%d)", blksz);
+
+   nir_ssa_def *coord = nir_load_global_invocation_id(&b, 32);
+
+   nir_ssa_def *offset = nir_u2u64(
+      &b, nir_imul(&b, nir_channel(&b, coord, 0), nir_imm_int(&b, blksz)));
+   nir_ssa_def *srcptr =
+      nir_iadd(&b, panvk_meta_copy_buf2buf_get_info_field(&b, src), offset);
+   nir_ssa_def *dstptr =
+      nir_iadd(&b, panvk_meta_copy_buf2buf_get_info_field(&b, dst), offset);
+
+   unsigned compsz = blksz < 4 ? blksz : 4;
+   unsigned ncomps = blksz / compsz;
+   nir_store_global(&b, dstptr, blksz,
+                    nir_load_global(&b, srcptr, blksz, ncomps, compsz * 8),
+                    (1 << ncomps) - 1);
+
+   struct panfrost_compile_inputs inputs = {
+      .gpu_id = pdev->gpu_id,
+      .is_blit = true,
+      .no_ubo_to_push = true,
+   };
+
+   struct util_dynarray binary;
+
+   util_dynarray_init(&binary, NULL);
+   pan_shader_preprocess(b.shader, inputs.gpu_id);
+   GENX(pan_shader_compile)(b.shader, &inputs, &binary, shader_info);
+
+   shader_info->push.count =
+      DIV_ROUND_UP(sizeof(struct panvk_meta_copy_buf2buf_info), 4);
+
+   mali_ptr shader =
+      pan_pool_upload_aligned(bin_pool, binary.data, binary.size, 128);
+
+   util_dynarray_fini(&binary);
+   ralloc_free(b.shader);
+
+   return shader;
+}
+
+static void
+panvk_meta_copy_buf2buf_init(struct panvk_physical_device *dev)
+{
+   for (unsigned i = 0; i < ARRAY_SIZE(dev->meta.copy.buf2buf); i++) {
+      struct pan_shader_info shader_info;
+      mali_ptr shader = panvk_meta_copy_buf2buf_shader(
+         &dev->pdev, &dev->meta.bin_pool.base, 1 << i, &shader_info);
+      dev->meta.copy.buf2buf[i].rsd = panvk_meta_copy_to_buf_emit_rsd(
+         &dev->pdev, &dev->meta.desc_pool.base, shader, &shader_info, false);
+   }
+}
+
+static void
+panvk_meta_copy_buf2buf(struct panvk_cmd_buffer *cmdbuf,
+                        const struct panvk_buffer *src,
+                        const struct panvk_buffer *dst,
+                        const VkBufferCopy2 *region)
+{
+   struct panvk_meta_copy_buf2buf_info info = {
+      .src = panvk_buffer_gpu_ptr(src, region->srcOffset),
+      .dst = panvk_buffer_gpu_ptr(dst, region->dstOffset),
+   };
+
+   unsigned alignment = ffs((info.src | info.dst | region->size) & 15);
+   unsigned log2blksz = alignment ? alignment - 1 : 4;
+
+   assert(log2blksz <
+          ARRAY_SIZE(cmdbuf->device->physical_device->meta.copy.buf2buf));
+   mali_ptr rsd =
+      cmdbuf->device->physical_device->meta.copy.buf2buf[log2blksz].rsd;
+
+   mali_ptr pushconsts =
+      pan_pool_upload_aligned(&cmdbuf->desc_pool.base, &info, sizeof(info), 16);
+
+   panvk_per_arch(cmd_close_batch)(cmdbuf);
+
+   struct panvk_batch *batch = panvk_cmd_open_batch(cmdbuf);
+
+   panvk_per_arch(cmd_alloc_tls_desc)(cmdbuf, false);
+
+   mali_ptr tsd = batch->tls.gpu;
+
+   unsigned nblocks = region->size >> log2blksz;
+   struct pan_compute_dim num_wg = {nblocks, 1, 1};
+   struct pan_compute_dim wg_sz = {1, 1, 1};
+   struct panfrost_ptr job = panvk_meta_copy_emit_compute_job(
+      &cmdbuf->desc_pool.base, &batch->scoreboard, &num_wg, &wg_sz, 0, 0,
+      pushconsts, rsd, tsd);
+
+   util_dynarray_append(&batch->jobs, void *, job.cpu);
+
+   batch->blit.src = src->bo;
+   batch->blit.dst = dst->bo;
+   panvk_per_arch(cmd_close_batch)(cmdbuf);
+}
+
+void
+panvk_per_arch(CmdCopyBuffer2)(VkCommandBuffer commandBuffer,
+                               const VkCopyBufferInfo2 *pCopyBufferInfo)
+{
+   VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
+   VK_FROM_HANDLE(panvk_buffer, src, pCopyBufferInfo->srcBuffer);
+   VK_FROM_HANDLE(panvk_buffer, dst, pCopyBufferInfo->dstBuffer);
+
+   for (unsigned i = 0; i < pCopyBufferInfo->regionCount; i++) {
+      panvk_meta_copy_buf2buf(cmdbuf, src, dst, &pCopyBufferInfo->pRegions[i]);
+   }
+}
+
+struct panvk_meta_fill_buf_info {
+   mali_ptr start;
+   uint32_t val;
+} PACKED;
+
+#define panvk_meta_fill_buf_get_info_field(b, field)                           \
+   nir_load_push_constant(                                                     \
+      (b), 1, sizeof(((struct panvk_meta_fill_buf_info *)0)->field) * 8,       \
+      nir_imm_int(b, 0),                                                       \
+      .base = offsetof(struct panvk_meta_fill_buf_info, field), .range = ~0)
+
+static mali_ptr
+panvk_meta_fill_buf_shader(struct panfrost_device *pdev,
+                           struct pan_pool *bin_pool,
+                           struct pan_shader_info *shader_info)
+{
+   /* FIXME: Won't work on compute queues, but we can't do that with
+    * a compute shader if the destination is an AFBC surface.
+    */
+   nir_builder b = nir_builder_init_simple_shader(
+      MESA_SHADER_COMPUTE, GENX(pan_shader_get_compiler_options)(),
+      "panvk_meta_fill_buf()");
+
+   nir_ssa_def *coord = nir_load_global_invocation_id(&b, 32);
+
+   nir_ssa_def *offset =
+      nir_u2u64(&b, nir_imul(&b, nir_channel(&b, coord, 0),
+                             nir_imm_int(&b, sizeof(uint32_t))));
+   nir_ssa_def *ptr =
+      nir_iadd(&b, panvk_meta_fill_buf_get_info_field(&b, start), offset);
+   nir_ssa_def *val = panvk_meta_fill_buf_get_info_field(&b, val);
+
+   nir_store_global(&b, ptr, sizeof(uint32_t), val, 1);
+
+   struct panfrost_compile_inputs inputs = {
+      .gpu_id = pdev->gpu_id,
+      .is_blit = true,
+      .no_ubo_to_push = true,
+   };
+
+   struct util_dynarray binary;
+
+   util_dynarray_init(&binary, NULL);
+   pan_shader_preprocess(b.shader, inputs.gpu_id);
+   GENX(pan_shader_compile)(b.shader, &inputs, &binary, shader_info);
+
+   shader_info->push.count =
+      DIV_ROUND_UP(sizeof(struct panvk_meta_fill_buf_info), 4);
+
+   mali_ptr shader =
+      pan_pool_upload_aligned(bin_pool, binary.data, binary.size, 128);
+
+   util_dynarray_fini(&binary);
+   ralloc_free(b.shader);
+
+   return shader;
+}
+
+static mali_ptr
+panvk_meta_fill_buf_emit_rsd(struct panfrost_device *pdev,
+                             struct pan_pool *bin_pool,
+                             struct pan_pool *desc_pool)
+{
+   struct pan_shader_info shader_info;
+
+   mali_ptr shader = panvk_meta_fill_buf_shader(pdev, bin_pool, &shader_info);
+
+   struct panfrost_ptr rsd_ptr =
+      pan_pool_alloc_desc_aggregate(desc_pool, PAN_DESC(RENDERER_STATE));
+
+   pan_pack(rsd_ptr.cpu, RENDERER_STATE, cfg) {
+      pan_shader_prepare_rsd(&shader_info, shader, &cfg);
+   }
+
+   return rsd_ptr.gpu;
+}
+
+static void
+panvk_meta_fill_buf_init(struct panvk_physical_device *dev)
+{
+   dev->meta.copy.fillbuf.rsd = panvk_meta_fill_buf_emit_rsd(
+      &dev->pdev, &dev->meta.bin_pool.base, &dev->meta.desc_pool.base);
+}
+
+static void
+panvk_meta_fill_buf(struct panvk_cmd_buffer *cmdbuf,
+                    const struct panvk_buffer *dst, VkDeviceSize size,
+                    VkDeviceSize offset, uint32_t val)
+{
+   struct panvk_meta_fill_buf_info info = {
+      .start = panvk_buffer_gpu_ptr(dst, offset),
+      .val = val,
+   };
+   size = panvk_buffer_range(dst, offset, size);
+
+   /* From the Vulkan spec:
+    *
+    *    "size is the number of bytes to fill, and must be either a multiple
+    *    of 4, or VK_WHOLE_SIZE to fill the range from offset to the end of
+    *    the buffer. If VK_WHOLE_SIZE is used and the remaining size of the
+    *    buffer is not a multiple of 4, then the nearest smaller multiple is
+    *    used."
+    */
+   size &= ~3ull;
+
+   assert(!(offset & 3) && !(size & 3));
+
+   unsigned nwords = size / sizeof(uint32_t);
+   mali_ptr rsd = cmdbuf->device->physical_device->meta.copy.fillbuf.rsd;
+
+   mali_ptr pushconsts =
+      pan_pool_upload_aligned(&cmdbuf->desc_pool.base, &info, sizeof(info), 16);
+
+   panvk_per_arch(cmd_close_batch)(cmdbuf);
+
+   struct panvk_batch *batch = panvk_cmd_open_batch(cmdbuf);
+
+   panvk_per_arch(cmd_alloc_tls_desc)(cmdbuf, false);
+
+   mali_ptr tsd = batch->tls.gpu;
+
+   struct pan_compute_dim num_wg = {nwords, 1, 1};
+   struct pan_compute_dim wg_sz = {1, 1, 1};
+   struct panfrost_ptr job = panvk_meta_copy_emit_compute_job(
+      &cmdbuf->desc_pool.base, &batch->scoreboard, &num_wg, &wg_sz, 0, 0,
+      pushconsts, rsd, tsd);
+
+   util_dynarray_append(&batch->jobs, void *, job.cpu);
+
+   batch->blit.dst = dst->bo;
+   panvk_per_arch(cmd_close_batch)(cmdbuf);
+}
+
+void
+panvk_per_arch(CmdFillBuffer)(VkCommandBuffer commandBuffer, VkBuffer dstBuffer,
+                              VkDeviceSize dstOffset, VkDeviceSize fillSize,
+                              uint32_t data)
+{
+   VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
+   VK_FROM_HANDLE(panvk_buffer, dst, dstBuffer);
+
+   panvk_meta_fill_buf(cmdbuf, dst, fillSize, dstOffset, data);
+}
+
+static void
+panvk_meta_update_buf(struct panvk_cmd_buffer *cmdbuf,
+                      const struct panvk_buffer *dst, VkDeviceSize offset,
+                      VkDeviceSize size, const void *data)
+{
+   struct panvk_meta_copy_buf2buf_info info = {
+      .src = pan_pool_upload_aligned(&cmdbuf->desc_pool.base, data, size, 4),
+      .dst = panvk_buffer_gpu_ptr(dst, offset),
+   };
+
+   unsigned log2blksz = ffs(sizeof(uint32_t)) - 1;
+
+   mali_ptr rsd =
+      cmdbuf->device->physical_device->meta.copy.buf2buf[log2blksz].rsd;
+
+   mali_ptr pushconsts =
+      pan_pool_upload_aligned(&cmdbuf->desc_pool.base, &info, sizeof(info), 16);
+
+   panvk_per_arch(cmd_close_batch)(cmdbuf);
+
+   struct panvk_batch *batch = panvk_cmd_open_batch(cmdbuf);
+
+   panvk_per_arch(cmd_alloc_tls_desc)(cmdbuf, false);
+
+   mali_ptr tsd = batch->tls.gpu;
+
+   unsigned nblocks = size >> log2blksz;
+   struct pan_compute_dim num_wg = {nblocks, 1, 1};
+   struct pan_compute_dim wg_sz = {1, 1, 1};
+   struct panfrost_ptr job = panvk_meta_copy_emit_compute_job(
+      &cmdbuf->desc_pool.base, &batch->scoreboard, &num_wg, &wg_sz, 0, 0,
+      pushconsts, rsd, tsd);
+
+   util_dynarray_append(&batch->jobs, void *, job.cpu);
+
+   batch->blit.dst = dst->bo;
+   panvk_per_arch(cmd_close_batch)(cmdbuf);
+}
+
+void
+panvk_per_arch(CmdUpdateBuffer)(VkCommandBuffer commandBuffer,
+                                VkBuffer dstBuffer, VkDeviceSize dstOffset,
+                                VkDeviceSize dataSize, const void *pData)
+{
+   VK_FROM_HANDLE(panvk_cmd_buffer, cmdbuf, commandBuffer);
+   VK_FROM_HANDLE(panvk_buffer, dst, dstBuffer);
+
+   panvk_meta_update_buf(cmdbuf, dst, dstOffset, dataSize, pData);
+}
+
+void
+panvk_per_arch(meta_copy_init)(struct panvk_physical_device *dev)
+{
+   panvk_meta_copy_img2img_init(dev, false);
+   panvk_meta_copy_img2img_init(dev, true);
+   panvk_meta_copy_buf2img_init(dev);
+   panvk_meta_copy_img2buf_init(dev);
+   panvk_meta_copy_buf2buf_init(dev);
+   panvk_meta_fill_buf_init(dev);
+}
diff -up mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_meta.h.16~ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_meta.h
--- mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_meta.h.16~	2023-07-29 15:51:45.093980518 +0100
+++ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_meta.h	2023-07-29 15:51:45.083980485 +0100
@@ -0,0 +1,46 @@
+/*
+ * Copyright (C) 2021 Collabora Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#ifndef PANVK_PRIVATE_H
+#error "Must be included from panvk_private.h"
+#endif
+
+#ifndef PAN_ARCH
+#error "no arch"
+#endif
+
+void panvk_per_arch(meta_init)(struct panvk_physical_device *dev);
+
+void panvk_per_arch(meta_cleanup)(struct panvk_physical_device *dev);
+
+mali_ptr panvk_per_arch(meta_emit_viewport)(struct pan_pool *pool,
+                                            uint16_t minx, uint16_t miny,
+                                            uint16_t maxx, uint16_t maxy);
+
+void panvk_per_arch(meta_clear_init)(struct panvk_physical_device *dev);
+
+void panvk_per_arch(meta_blit_init)(struct panvk_physical_device *dev);
+
+void panvk_per_arch(meta_blit_cleanup)(struct panvk_physical_device *dev);
+
+void panvk_per_arch(meta_copy_init)(struct panvk_physical_device *dev);
diff -up mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_nir_lower_descriptors.c.16~ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_nir_lower_descriptors.c
--- mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_nir_lower_descriptors.c.16~	2023-07-29 15:51:45.093980518 +0100
+++ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_nir_lower_descriptors.c	2023-07-29 15:51:45.093980518 +0100
@@ -0,0 +1,593 @@
+/*
+ * Copyright © 2021 Collabora Ltd.
+ *
+ * Derived from tu_shader.c which is:
+ * Copyright © 2019 Google LLC
+ *
+ * Also derived from anv_pipeline.c which is
+ * Copyright © 2015 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#include "panvk_private.h"
+
+#include "nir.h"
+#include "nir_builder.h"
+
+struct apply_descriptors_ctx {
+   const struct panvk_pipeline_layout *layout;
+   bool add_bounds_checks;
+   bool has_img_access;
+   nir_address_format desc_addr_format;
+   nir_address_format ubo_addr_format;
+   nir_address_format ssbo_addr_format;
+};
+
+static nir_address_format
+addr_format_for_desc_type(VkDescriptorType desc_type,
+                          const struct apply_descriptors_ctx *ctx)
+{
+   switch (desc_type) {
+   case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER:
+   case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC:
+      return ctx->ubo_addr_format;
+
+   case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER:
+   case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER_DYNAMIC:
+      return ctx->ssbo_addr_format;
+
+   default:
+      unreachable("Unsupported descriptor type");
+   }
+}
+
+static const struct panvk_descriptor_set_layout *
+get_set_layout(uint32_t set, const struct apply_descriptors_ctx *ctx)
+{
+   return vk_to_panvk_descriptor_set_layout(ctx->layout->vk.set_layouts[set]);
+}
+
+static const struct panvk_descriptor_set_binding_layout *
+get_binding_layout(uint32_t set, uint32_t binding,
+                   const struct apply_descriptors_ctx *ctx)
+{
+   return &get_set_layout(set, ctx)->bindings[binding];
+}
+
+/** Build a Vulkan resource index
+ *
+ * A "resource index" is the term used by our SPIR-V parser and the relevant
+ * NIR intrinsics for a reference into a descriptor set.  It acts much like a
+ * deref in NIR except that it accesses opaque descriptors instead of memory.
+ *
+ * Coming out of SPIR-V, both the resource indices (in the form of
+ * vulkan_resource_[re]index intrinsics) and the memory derefs (in the form
+ * of nir_deref_instr) use the same vector component/bit size.  The meaning
+ * of those values for memory derefs (nir_deref_instr) is given by the
+ * nir_address_format associated with the descriptor type.  For resource
+ * indices, it's an entirely internal to panvk encoding which describes, in
+ * some sense, the address of the descriptor.  Thanks to the NIR/SPIR-V rules,
+ * it must be packed into the same size SSA values as a memory address.  For
+ * this reason, the actual encoding may depend both on the address format for
+ * memory derefs and the descriptor address format.
+ *
+ * The load_vulkan_descriptor intrinsic exists to provide a transition point
+ * between these two forms of derefs: descriptor and memory.
+ */
+static nir_ssa_def *
+build_res_index(nir_builder *b, uint32_t set, uint32_t binding,
+                nir_ssa_def *array_index, nir_address_format addr_format,
+                const struct apply_descriptors_ctx *ctx)
+{
+   const struct panvk_descriptor_set_layout *set_layout =
+      get_set_layout(set, ctx);
+   const struct panvk_descriptor_set_binding_layout *bind_layout =
+      &set_layout->bindings[binding];
+
+   uint32_t array_size = bind_layout->array_size;
+
+   switch (bind_layout->type) {
+   case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER:
+   case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC: {
+      assert(addr_format == nir_address_format_32bit_index_offset);
+
+      const unsigned ubo_idx =
+         panvk_pipeline_layout_ubo_index(ctx->layout, set, binding, 0);
+
+      const uint32_t packed = (array_size - 1) << 16 | ubo_idx;
+
+      return nir_vec2(b, nir_imm_int(b, packed), array_index);
+   }
+
+   case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER: {
+      assert(addr_format == nir_address_format_64bit_bounded_global ||
+             addr_format == nir_address_format_64bit_global_32bit_offset);
+
+      const unsigned set_ubo_idx =
+         panvk_pipeline_layout_ubo_start(ctx->layout, set, false) +
+         set_layout->desc_ubo_index;
+
+      const uint32_t packed =
+         (bind_layout->desc_ubo_stride << 16) | set_ubo_idx;
+
+      return nir_vec4(b, nir_imm_int(b, packed),
+                      nir_imm_int(b, bind_layout->desc_ubo_offset),
+                      nir_imm_int(b, array_size - 1), array_index);
+   }
+
+   case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER_DYNAMIC: {
+      assert(addr_format == nir_address_format_64bit_bounded_global ||
+             addr_format == nir_address_format_64bit_global_32bit_offset);
+
+      const unsigned dyn_ssbo_idx =
+         ctx->layout->sets[set].dyn_ssbo_offset + bind_layout->dyn_ssbo_idx;
+
+      const unsigned ubo_idx = PANVK_SYSVAL_UBO_INDEX;
+      const unsigned desc_stride = sizeof(struct panvk_ssbo_addr);
+      const uint32_t ubo_offset =
+         offsetof(struct panvk_sysvals, dyn_ssbos) + dyn_ssbo_idx * desc_stride;
+
+      const uint32_t packed = (desc_stride << 16) | ubo_idx;
+
+      return nir_vec4(b, nir_imm_int(b, packed), nir_imm_int(b, ubo_offset),
+                      nir_imm_int(b, array_size - 1), array_index);
+   }
+
+   default:
+      unreachable("Unsupported descriptor type");
+   }
+}
+
+/** Adjust a Vulkan resource index
+ *
+ * This is the equivalent of nir_deref_type_ptr_as_array for resource indices.
+ * For array descriptors, it allows us to adjust the array index.  Thanks to
+ * variable pointers, we cannot always fold this re-index operation into the
+ * vulkan_resource_index intrinsic and we have to do it based on nothing but
+ * the address format.
+ */
+static nir_ssa_def *
+build_res_reindex(nir_builder *b, nir_ssa_def *orig, nir_ssa_def *delta,
+                  nir_address_format addr_format)
+{
+   switch (addr_format) {
+   case nir_address_format_32bit_index_offset:
+      return nir_vec2(b, nir_channel(b, orig, 0),
+                      nir_iadd(b, nir_channel(b, orig, 1), delta));
+
+   case nir_address_format_64bit_bounded_global:
+   case nir_address_format_64bit_global_32bit_offset:
+      return nir_vec4(b, nir_channel(b, orig, 0), nir_channel(b, orig, 1),
+                      nir_channel(b, orig, 2),
+                      nir_iadd(b, nir_channel(b, orig, 3), delta));
+
+   default:
+      unreachable("Unhandled address format");
+   }
+}
+
+/** Convert a Vulkan resource index into a buffer address
+ *
+ * In some cases, this does a  memory load from the descriptor set and, in
+ * others, it simply converts from one form to another.
+ *
+ * See build_res_index for details about each resource index format.
+ */
+static nir_ssa_def *
+build_buffer_addr_for_res_index(nir_builder *b, nir_ssa_def *res_index,
+                                nir_address_format addr_format,
+                                const struct apply_descriptors_ctx *ctx)
+{
+   switch (addr_format) {
+   case nir_address_format_32bit_index_offset: {
+      nir_ssa_def *packed = nir_channel(b, res_index, 0);
+      nir_ssa_def *array_index = nir_channel(b, res_index, 1);
+      nir_ssa_def *surface_index =
+         nir_extract_u16(b, packed, nir_imm_int(b, 0));
+      nir_ssa_def *array_max = nir_extract_u16(b, packed, nir_imm_int(b, 1));
+
+      if (ctx->add_bounds_checks)
+         array_index = nir_umin(b, array_index, array_max);
+
+      return nir_vec2(b, nir_iadd(b, surface_index, array_index),
+                      nir_imm_int(b, 0));
+   }
+
+   case nir_address_format_64bit_bounded_global:
+   case nir_address_format_64bit_global_32bit_offset: {
+      nir_ssa_def *packed = nir_channel(b, res_index, 0);
+      nir_ssa_def *desc_ubo_offset = nir_channel(b, res_index, 1);
+      nir_ssa_def *array_max = nir_channel(b, res_index, 2);
+      nir_ssa_def *array_index = nir_channel(b, res_index, 3);
+
+      nir_ssa_def *desc_ubo_idx = nir_extract_u16(b, packed, nir_imm_int(b, 0));
+      nir_ssa_def *desc_ubo_stride =
+         nir_extract_u16(b, packed, nir_imm_int(b, 1));
+
+      if (ctx->add_bounds_checks)
+         array_index = nir_umin(b, array_index, array_max);
+
+      desc_ubo_offset = nir_iadd(b, desc_ubo_offset,
+                                 nir_imul(b, array_index, desc_ubo_stride));
+
+      nir_ssa_def *desc = nir_load_ubo(b, 4, 32, desc_ubo_idx, desc_ubo_offset,
+                                       .align_mul = 16, .range = ~0);
+
+      /* The offset in the descriptor is guaranteed to be zero when it's
+       * written into the descriptor set.  This lets us avoid some unnecessary
+       * adds.
+       */
+      return nir_vec4(b, nir_channel(b, desc, 0), nir_channel(b, desc, 1),
+                      nir_channel(b, desc, 2), nir_imm_int(b, 0));
+   }
+
+   default:
+      unreachable("Unhandled address format");
+   }
+}
+
+static bool
+lower_res_intrinsic(nir_builder *b, nir_intrinsic_instr *intrin,
+                    const struct apply_descriptors_ctx *ctx)
+{
+   b->cursor = nir_before_instr(&intrin->instr);
+
+   const VkDescriptorType desc_type = nir_intrinsic_desc_type(intrin);
+   nir_address_format addr_format = addr_format_for_desc_type(desc_type, ctx);
+
+   nir_ssa_def *res;
+   switch (intrin->intrinsic) {
+   case nir_intrinsic_vulkan_resource_index:
+      assert(intrin->src[0].is_ssa);
+      res = build_res_index(b, nir_intrinsic_desc_set(intrin),
+                            nir_intrinsic_binding(intrin), intrin->src[0].ssa,
+                            addr_format, ctx);
+      break;
+
+   case nir_intrinsic_vulkan_resource_reindex:
+      assert(intrin->src[0].is_ssa && intrin->src[1].is_ssa);
+      res = build_res_reindex(b, intrin->src[0].ssa, intrin->src[1].ssa,
+                              addr_format);
+      break;
+
+   case nir_intrinsic_load_vulkan_descriptor:
+      assert(intrin->src[0].is_ssa);
+      res = build_buffer_addr_for_res_index(b, intrin->src[0].ssa, addr_format,
+                                            ctx);
+      break;
+
+   default:
+      unreachable("Unhandled resource intrinsic");
+   }
+
+   assert(intrin->dest.is_ssa);
+   assert(intrin->dest.ssa.bit_size == res->bit_size);
+   assert(intrin->dest.ssa.num_components == res->num_components);
+   nir_ssa_def_rewrite_uses(&intrin->dest.ssa, res);
+   nir_instr_remove(&intrin->instr);
+
+   return true;
+}
+
+static void
+get_resource_deref_binding(nir_deref_instr *deref, uint32_t *set,
+                           uint32_t *binding, uint32_t *index_imm,
+                           nir_ssa_def **index_ssa)
+{
+   *index_imm = 0;
+   *index_ssa = NULL;
+
+   if (deref->deref_type == nir_deref_type_array) {
+      assert(deref->arr.index.is_ssa);
+      if (index_imm != NULL && nir_src_is_const(deref->arr.index))
+         *index_imm = nir_src_as_uint(deref->arr.index);
+      else
+         *index_ssa = deref->arr.index.ssa;
+
+      deref = nir_deref_instr_parent(deref);
+   }
+
+   assert(deref->deref_type == nir_deref_type_var);
+   nir_variable *var = deref->var;
+
+   *set = var->data.descriptor_set;
+   *binding = var->data.binding;
+}
+
+static nir_ssa_def *
+load_resource_deref_desc(nir_builder *b, nir_deref_instr *deref,
+                         unsigned desc_offset, unsigned num_components,
+                         unsigned bit_size,
+                         const struct apply_descriptors_ctx *ctx)
+{
+   uint32_t set, binding, index_imm;
+   nir_ssa_def *index_ssa;
+   get_resource_deref_binding(deref, &set, &binding, &index_imm, &index_ssa);
+
+   const struct panvk_descriptor_set_layout *set_layout =
+      get_set_layout(set, ctx);
+   const struct panvk_descriptor_set_binding_layout *bind_layout =
+      &set_layout->bindings[binding];
+
+   assert(index_ssa == NULL || index_imm == 0);
+   if (index_ssa == NULL)
+      index_ssa = nir_imm_int(b, index_imm);
+
+   const unsigned set_ubo_idx =
+      panvk_pipeline_layout_ubo_start(ctx->layout, set, false) +
+      set_layout->desc_ubo_index;
+
+   nir_ssa_def *desc_ubo_offset =
+      nir_iadd_imm(b, nir_imul_imm(b, index_ssa, bind_layout->desc_ubo_stride),
+                   bind_layout->desc_ubo_offset + desc_offset);
+
+   assert(bind_layout->desc_ubo_stride > 0);
+   unsigned desc_align = (1 << (ffs(bind_layout->desc_ubo_stride) - 1));
+   desc_align = MIN2(desc_align, 16);
+
+   return nir_load_ubo(b, num_components, bit_size, nir_imm_int(b, set_ubo_idx),
+                       desc_ubo_offset, .align_mul = desc_align,
+                       .align_offset = (desc_offset % desc_align), .range = ~0);
+}
+
+static nir_ssa_def *
+load_tex_img_size(nir_builder *b, nir_deref_instr *deref,
+                  enum glsl_sampler_dim dim,
+                  const struct apply_descriptors_ctx *ctx)
+{
+   if (dim == GLSL_SAMPLER_DIM_BUF) {
+      return load_resource_deref_desc(b, deref, 0, 1, 32, ctx);
+   } else {
+      nir_ssa_def *desc = load_resource_deref_desc(b, deref, 0, 4, 16, ctx);
+
+      /* The sizes are provided as 16-bit values with 1 subtracted so
+       * convert to 32-bit and add 1.
+       */
+      return nir_iadd_imm(b, nir_u2u32(b, desc), 1);
+   }
+}
+
+static nir_ssa_def *
+load_tex_img_levels(nir_builder *b, nir_deref_instr *deref,
+                    enum glsl_sampler_dim dim,
+                    const struct apply_descriptors_ctx *ctx)
+{
+   assert(dim != GLSL_SAMPLER_DIM_BUF);
+   nir_ssa_def *desc = load_resource_deref_desc(b, deref, 0, 4, 16, ctx);
+   return nir_u2u32(b, nir_iand_imm(b, nir_channel(b, desc, 3), 0xff));
+}
+
+static nir_ssa_def *
+load_tex_img_samples(nir_builder *b, nir_deref_instr *deref,
+                     enum glsl_sampler_dim dim,
+                     const struct apply_descriptors_ctx *ctx)
+{
+   assert(dim != GLSL_SAMPLER_DIM_BUF);
+   nir_ssa_def *desc = load_resource_deref_desc(b, deref, 0, 4, 16, ctx);
+   return nir_u2u32(b, nir_ushr_imm(b, nir_channel(b, desc, 3), 8));
+}
+
+static bool
+lower_tex(nir_builder *b, nir_tex_instr *tex,
+          const struct apply_descriptors_ctx *ctx)
+{
+   bool progress = false;
+
+   b->cursor = nir_before_instr(&tex->instr);
+
+   if (tex->op == nir_texop_txs || tex->op == nir_texop_query_levels ||
+       tex->op == nir_texop_texture_samples) {
+      int tex_src_idx = nir_tex_instr_src_index(tex, nir_tex_src_texture_deref);
+      assert(tex_src_idx >= 0);
+      nir_deref_instr *deref = nir_src_as_deref(tex->src[tex_src_idx].src);
+
+      const enum glsl_sampler_dim dim = tex->sampler_dim;
+
+      nir_ssa_def *res;
+      switch (tex->op) {
+      case nir_texop_txs:
+         res = nir_channels(b, load_tex_img_size(b, deref, dim, ctx),
+                            nir_component_mask(tex->dest.ssa.num_components));
+         break;
+      case nir_texop_query_levels:
+         assert(tex->dest.ssa.num_components == 1);
+         res = load_tex_img_levels(b, deref, dim, ctx);
+         break;
+      case nir_texop_texture_samples:
+         assert(tex->dest.ssa.num_components == 1);
+         res = load_tex_img_samples(b, deref, dim, ctx);
+         break;
+      default:
+         unreachable("Unsupported texture query op");
+      }
+
+      nir_ssa_def_rewrite_uses(&tex->dest.ssa, res);
+      nir_instr_remove(&tex->instr);
+      return true;
+   }
+
+   int sampler_src_idx =
+      nir_tex_instr_src_index(tex, nir_tex_src_sampler_deref);
+   if (sampler_src_idx >= 0) {
+      nir_deref_instr *deref = nir_src_as_deref(tex->src[sampler_src_idx].src);
+      nir_tex_instr_remove_src(tex, sampler_src_idx);
+
+      uint32_t set, binding, index_imm;
+      nir_ssa_def *index_ssa;
+      get_resource_deref_binding(deref, &set, &binding, &index_imm, &index_ssa);
+
+      const struct panvk_descriptor_set_binding_layout *bind_layout =
+         get_binding_layout(set, binding, ctx);
+
+      tex->sampler_index = ctx->layout->sets[set].sampler_offset +
+                           bind_layout->sampler_idx + index_imm;
+
+      if (index_ssa != NULL) {
+         nir_tex_instr_add_src(tex, nir_tex_src_sampler_offset,
+                               nir_src_for_ssa(index_ssa));
+      }
+      progress = true;
+   }
+
+   int tex_src_idx = nir_tex_instr_src_index(tex, nir_tex_src_texture_deref);
+   if (tex_src_idx >= 0) {
+      nir_deref_instr *deref = nir_src_as_deref(tex->src[tex_src_idx].src);
+      nir_tex_instr_remove_src(tex, tex_src_idx);
+
+      uint32_t set, binding, index_imm;
+      nir_ssa_def *index_ssa;
+      get_resource_deref_binding(deref, &set, &binding, &index_imm, &index_ssa);
+
+      const struct panvk_descriptor_set_binding_layout *bind_layout =
+         get_binding_layout(set, binding, ctx);
+
+      tex->texture_index =
+         ctx->layout->sets[set].tex_offset + bind_layout->tex_idx + index_imm;
+
+      if (index_ssa != NULL) {
+         nir_tex_instr_add_src(tex, nir_tex_src_texture_offset,
+                               nir_src_for_ssa(index_ssa));
+      }
+      progress = true;
+   }
+
+   return progress;
+}
+
+static nir_ssa_def *
+get_img_index(nir_builder *b, nir_deref_instr *deref,
+              const struct apply_descriptors_ctx *ctx)
+{
+   uint32_t set, binding, index_imm;
+   nir_ssa_def *index_ssa;
+   get_resource_deref_binding(deref, &set, &binding, &index_imm, &index_ssa);
+
+   const struct panvk_descriptor_set_binding_layout *bind_layout =
+      get_binding_layout(set, binding, ctx);
+   assert(bind_layout->type == VK_DESCRIPTOR_TYPE_STORAGE_IMAGE ||
+          bind_layout->type == VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER ||
+          bind_layout->type == VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER);
+
+   unsigned img_offset =
+      ctx->layout->sets[set].img_offset + bind_layout->img_idx;
+
+   if (index_ssa == NULL) {
+      return nir_imm_int(b, img_offset + index_imm);
+   } else {
+      assert(index_imm == 0);
+      return nir_iadd_imm(b, index_ssa, img_offset);
+   }
+}
+
+static bool
+lower_img_intrinsic(nir_builder *b, nir_intrinsic_instr *intr,
+                    struct apply_descriptors_ctx *ctx)
+{
+   b->cursor = nir_before_instr(&intr->instr);
+   nir_deref_instr *deref = nir_src_as_deref(intr->src[0]);
+
+   if (intr->intrinsic == nir_intrinsic_image_deref_size ||
+       intr->intrinsic == nir_intrinsic_image_deref_samples) {
+      assert(intr->dest.is_ssa);
+
+      const enum glsl_sampler_dim dim = nir_intrinsic_image_dim(intr);
+
+      nir_ssa_def *res;
+      switch (intr->intrinsic) {
+      case nir_intrinsic_image_deref_size:
+         res = nir_channels(b, load_tex_img_size(b, deref, dim, ctx),
+                            nir_component_mask(intr->dest.ssa.num_components));
+         break;
+      case nir_intrinsic_image_deref_samples:
+         res = load_tex_img_samples(b, deref, dim, ctx);
+         break;
+      default:
+         unreachable("Unsupported image query op");
+      }
+
+      nir_ssa_def_rewrite_uses(&intr->dest.ssa, res);
+      nir_instr_remove(&intr->instr);
+   } else {
+      nir_rewrite_image_intrinsic(intr, get_img_index(b, deref, ctx), false);
+      ctx->has_img_access = true;
+   }
+
+   return true;
+}
+
+static bool
+lower_intrinsic(nir_builder *b, nir_intrinsic_instr *intr,
+                struct apply_descriptors_ctx *ctx)
+{
+   switch (intr->intrinsic) {
+   case nir_intrinsic_vulkan_resource_index:
+   case nir_intrinsic_vulkan_resource_reindex:
+   case nir_intrinsic_load_vulkan_descriptor:
+      return lower_res_intrinsic(b, intr, ctx);
+   case nir_intrinsic_image_deref_store:
+   case nir_intrinsic_image_deref_load:
+   case nir_intrinsic_image_deref_atomic:
+   case nir_intrinsic_image_deref_atomic_swap:
+   case nir_intrinsic_image_deref_size:
+   case nir_intrinsic_image_deref_samples:
+      return lower_img_intrinsic(b, intr, ctx);
+   default:
+      return false;
+   }
+}
+
+static bool
+lower_descriptors_instr(nir_builder *b, nir_instr *instr, void *data)
+{
+   struct apply_descriptors_ctx *ctx = data;
+
+   switch (instr->type) {
+   case nir_instr_type_tex:
+      return lower_tex(b, nir_instr_as_tex(instr), ctx);
+   case nir_instr_type_intrinsic:
+      return lower_intrinsic(b, nir_instr_as_intrinsic(instr), ctx);
+   default:
+      return false;
+   }
+}
+
+bool
+panvk_per_arch(nir_lower_descriptors)(nir_shader *nir, struct panvk_device *dev,
+                                      const struct panvk_pipeline_layout *layout,
+                                      bool *has_img_access_out)
+{
+   struct apply_descriptors_ctx ctx = {
+      .layout = layout,
+      .desc_addr_format = nir_address_format_32bit_index_offset,
+      .ubo_addr_format = nir_address_format_32bit_index_offset,
+      .ssbo_addr_format = dev->vk.enabled_features.robustBufferAccess
+                             ? nir_address_format_64bit_bounded_global
+                             : nir_address_format_64bit_global_32bit_offset,
+   };
+
+   bool progress = nir_shader_instructions_pass(
+      nir, lower_descriptors_instr,
+      nir_metadata_block_index | nir_metadata_dominance, (void *)&ctx);
+   if (has_img_access_out)
+      *has_img_access_out = ctx.has_img_access;
+
+   return progress;
+}
diff -up mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_pipeline.c.16~ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_pipeline.c
--- mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_pipeline.c.16~	2023-07-29 15:51:45.093980518 +0100
+++ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_pipeline.c	2023-07-29 15:51:45.093980518 +0100
@@ -0,0 +1,1097 @@
+/*
+ * Copyright © 2021 Collabora Ltd.
+ *
+ * Derived from tu_pipeline.c which is:
+ * Copyright © 2016 Red Hat.
+ * Copyright © 2016 Bas Nieuwenhuizen
+ * Copyright © 2015 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#include "panvk_cs.h"
+#include "panvk_private.h"
+
+#include "pan_bo.h"
+
+#include "nir/nir.h"
+#include "nir/nir_builder.h"
+#include "spirv/nir_spirv.h"
+#include "util/mesa-sha1.h"
+#include "util/u_atomic.h"
+#include "util/u_debug.h"
+#include "vk_format.h"
+#include "vk_util.h"
+
+#include "panfrost/util/pan_lower_framebuffer.h"
+
+struct panvk_pipeline_builder {
+   struct panvk_device *device;
+   struct panvk_pipeline_cache *cache;
+   const VkAllocationCallbacks *alloc;
+   struct {
+      const VkGraphicsPipelineCreateInfo *gfx;
+      const VkComputePipelineCreateInfo *compute;
+   } create_info;
+   const struct panvk_pipeline_layout *layout;
+
+   struct panvk_shader *shaders[MESA_SHADER_STAGES];
+   struct {
+      uint32_t shader_offset;
+      uint32_t rsd_offset;
+   } stages[MESA_SHADER_STAGES];
+   uint32_t blend_shader_offsets[MAX_RTS];
+   uint32_t shader_total_size;
+   uint32_t static_state_size;
+   uint32_t vpd_offset;
+
+   bool rasterizer_discard;
+   /* these states are affectd by rasterizer_discard */
+   VkSampleCountFlagBits samples;
+   bool use_depth_stencil_attachment;
+   uint8_t active_color_attachments;
+   enum pipe_format color_attachment_formats[MAX_RTS];
+};
+
+static VkResult
+panvk_pipeline_builder_create_pipeline(struct panvk_pipeline_builder *builder,
+                                       struct panvk_pipeline **out_pipeline)
+{
+   struct panvk_device *dev = builder->device;
+
+   struct panvk_pipeline *pipeline = vk_object_zalloc(
+      &dev->vk, builder->alloc, sizeof(*pipeline), VK_OBJECT_TYPE_PIPELINE);
+   if (!pipeline)
+      return VK_ERROR_OUT_OF_HOST_MEMORY;
+
+   pipeline->layout = builder->layout;
+   *out_pipeline = pipeline;
+   return VK_SUCCESS;
+}
+
+static void
+panvk_pipeline_builder_finish(struct panvk_pipeline_builder *builder)
+{
+   for (uint32_t i = 0; i < MESA_SHADER_STAGES; i++) {
+      if (!builder->shaders[i])
+         continue;
+      panvk_shader_destroy(builder->device, builder->shaders[i],
+                           builder->alloc);
+   }
+}
+
+static bool
+panvk_pipeline_static_state(struct panvk_pipeline *pipeline, uint32_t id)
+{
+   return !(pipeline->dynamic_state_mask & (1 << id));
+}
+
+static VkResult
+panvk_pipeline_builder_compile_shaders(struct panvk_pipeline_builder *builder,
+                                       struct panvk_pipeline *pipeline)
+{
+   const VkPipelineShaderStageCreateInfo *stage_infos[MESA_SHADER_STAGES] = {
+      NULL};
+   const VkPipelineShaderStageCreateInfo *stages =
+      builder->create_info.gfx ? builder->create_info.gfx->pStages
+                               : &builder->create_info.compute->stage;
+   unsigned stage_count =
+      builder->create_info.gfx ? builder->create_info.gfx->stageCount : 1;
+
+   for (uint32_t i = 0; i < stage_count; i++) {
+      gl_shader_stage stage = vk_to_mesa_shader_stage(stages[i].stage);
+      stage_infos[stage] = &stages[i];
+   }
+
+   /* compile shaders in reverse order */
+   for (gl_shader_stage stage = MESA_SHADER_STAGES - 1;
+        stage > MESA_SHADER_NONE; stage--) {
+      const VkPipelineShaderStageCreateInfo *stage_info = stage_infos[stage];
+      if (!stage_info)
+         continue;
+
+      struct panvk_shader *shader;
+
+      shader = panvk_per_arch(shader_create)(
+         builder->device, stage, stage_info, builder->layout,
+         PANVK_SYSVAL_UBO_INDEX, &pipeline->blend.state,
+         panvk_pipeline_static_state(pipeline,
+                                     VK_DYNAMIC_STATE_BLEND_CONSTANTS),
+         builder->alloc);
+      if (!shader)
+         return VK_ERROR_OUT_OF_HOST_MEMORY;
+
+      builder->shaders[stage] = shader;
+      builder->shader_total_size = ALIGN_POT(builder->shader_total_size, 128);
+      builder->stages[stage].shader_offset = builder->shader_total_size;
+      builder->shader_total_size +=
+         util_dynarray_num_elements(&shader->binary, uint8_t);
+   }
+
+   return VK_SUCCESS;
+}
+
+static VkResult
+panvk_pipeline_builder_upload_shaders(struct panvk_pipeline_builder *builder,
+                                      struct panvk_pipeline *pipeline)
+{
+   /* In some cases, the optimized shader is empty. Don't bother allocating
+    * anything in this case.
+    */
+   if (builder->shader_total_size == 0)
+      return VK_SUCCESS;
+
+   struct panfrost_bo *bin_bo =
+      panfrost_bo_create(&builder->device->physical_device->pdev,
+                         builder->shader_total_size, PAN_BO_EXECUTE, "Shader");
+
+   pipeline->binary_bo = bin_bo;
+   panfrost_bo_mmap(bin_bo);
+
+   for (uint32_t i = 0; i < MESA_SHADER_STAGES; i++) {
+      const struct panvk_shader *shader = builder->shaders[i];
+      if (!shader)
+         continue;
+
+      memcpy(pipeline->binary_bo->ptr.cpu + builder->stages[i].shader_offset,
+             util_dynarray_element(&shader->binary, uint8_t, 0),
+             util_dynarray_num_elements(&shader->binary, uint8_t));
+   }
+
+   return VK_SUCCESS;
+}
+
+static void
+panvk_pipeline_builder_alloc_static_state_bo(
+   struct panvk_pipeline_builder *builder, struct panvk_pipeline *pipeline)
+{
+   struct panfrost_device *pdev = &builder->device->physical_device->pdev;
+   unsigned bo_size = 0;
+
+   for (uint32_t i = 0; i < MESA_SHADER_STAGES; i++) {
+      const struct panvk_shader *shader = builder->shaders[i];
+      if (!shader && i != MESA_SHADER_FRAGMENT)
+         continue;
+
+      if (pipeline->fs.dynamic_rsd && i == MESA_SHADER_FRAGMENT)
+         continue;
+
+      bo_size = ALIGN_POT(bo_size, pan_alignment(RENDERER_STATE));
+      builder->stages[i].rsd_offset = bo_size;
+      bo_size += pan_size(RENDERER_STATE);
+      if (i == MESA_SHADER_FRAGMENT)
+         bo_size += pan_size(BLEND) * MAX2(pipeline->blend.state.rt_count, 1);
+   }
+
+   if (builder->create_info.gfx &&
+       panvk_pipeline_static_state(pipeline, VK_DYNAMIC_STATE_VIEWPORT) &&
+       panvk_pipeline_static_state(pipeline, VK_DYNAMIC_STATE_SCISSOR)) {
+      bo_size = ALIGN_POT(bo_size, pan_alignment(VIEWPORT));
+      builder->vpd_offset = bo_size;
+      bo_size += pan_size(VIEWPORT);
+   }
+
+   if (bo_size) {
+      pipeline->state_bo =
+         panfrost_bo_create(pdev, bo_size, 0, "Pipeline descriptors");
+      panfrost_bo_mmap(pipeline->state_bo);
+   }
+}
+
+static void
+panvk_pipeline_builder_init_sysvals(struct panvk_pipeline_builder *builder,
+                                    struct panvk_pipeline *pipeline,
+                                    gl_shader_stage stage)
+{
+   const struct panvk_shader *shader = builder->shaders[stage];
+
+   pipeline->sysvals[stage].ubo_idx = shader->sysval_ubo;
+}
+
+static void
+panvk_pipeline_builder_init_shaders(struct panvk_pipeline_builder *builder,
+                                    struct panvk_pipeline *pipeline)
+{
+   for (uint32_t i = 0; i < MESA_SHADER_STAGES; i++) {
+      const struct panvk_shader *shader = builder->shaders[i];
+      if (!shader)
+         continue;
+
+      pipeline->tls_size = MAX2(pipeline->tls_size, shader->info.tls_size);
+      pipeline->wls_size = MAX2(pipeline->wls_size, shader->info.wls_size);
+
+      if (shader->has_img_access)
+         pipeline->img_access_mask |= BITFIELD_BIT(i);
+
+      if (i == MESA_SHADER_VERTEX && shader->info.vs.writes_point_size) {
+         VkPrimitiveTopology topology =
+            builder->create_info.gfx->pInputAssemblyState->topology;
+         bool points = (topology == VK_PRIMITIVE_TOPOLOGY_POINT_LIST);
+
+         /* Even if the vertex shader writes point size, we only consider the
+          * pipeline to write point size when we're actually drawing points.
+          * Otherwise the point size write would conflict with wide lines.
+          */
+         pipeline->ia.writes_point_size = points;
+      }
+
+      mali_ptr shader_ptr = 0;
+
+      /* Handle empty shaders gracefully */
+      if (util_dynarray_num_elements(&builder->shaders[i]->binary, uint8_t)) {
+         shader_ptr =
+            pipeline->binary_bo->ptr.gpu + builder->stages[i].shader_offset;
+      }
+
+      if (i != MESA_SHADER_FRAGMENT) {
+         void *rsd =
+            pipeline->state_bo->ptr.cpu + builder->stages[i].rsd_offset;
+         mali_ptr gpu_rsd =
+            pipeline->state_bo->ptr.gpu + builder->stages[i].rsd_offset;
+
+         panvk_per_arch(emit_non_fs_rsd)(builder->device, &shader->info,
+                                         shader_ptr, rsd);
+         pipeline->rsds[i] = gpu_rsd;
+      }
+
+      panvk_pipeline_builder_init_sysvals(builder, pipeline, i);
+
+      if (i == MESA_SHADER_COMPUTE)
+         pipeline->cs.local_size = shader->local_size;
+   }
+
+   if (builder->create_info.gfx && !pipeline->fs.dynamic_rsd) {
+      void *rsd = pipeline->state_bo->ptr.cpu +
+                  builder->stages[MESA_SHADER_FRAGMENT].rsd_offset;
+      mali_ptr gpu_rsd = pipeline->state_bo->ptr.gpu +
+                         builder->stages[MESA_SHADER_FRAGMENT].rsd_offset;
+      void *bd = rsd + pan_size(RENDERER_STATE);
+
+      panvk_per_arch(emit_base_fs_rsd)(builder->device, pipeline, rsd);
+      for (unsigned rt = 0; rt < pipeline->blend.state.rt_count; rt++) {
+         panvk_per_arch(emit_blend)(builder->device, pipeline, rt, bd);
+         bd += pan_size(BLEND);
+      }
+
+      pipeline->rsds[MESA_SHADER_FRAGMENT] = gpu_rsd;
+   } else if (builder->create_info.gfx) {
+      panvk_per_arch(emit_base_fs_rsd)(builder->device, pipeline,
+                                       &pipeline->fs.rsd_template);
+      for (unsigned rt = 0; rt < MAX2(pipeline->blend.state.rt_count, 1);
+           rt++) {
+         panvk_per_arch(emit_blend)(builder->device, pipeline, rt,
+                                    &pipeline->blend.bd_template[rt]);
+      }
+   }
+
+   pipeline->num_ubos = PANVK_NUM_BUILTIN_UBOS + builder->layout->num_ubos +
+                        builder->layout->num_dyn_ubos;
+}
+
+static void
+panvk_pipeline_builder_parse_viewport(struct panvk_pipeline_builder *builder,
+                                      struct panvk_pipeline *pipeline)
+{
+   /* The spec says:
+    *
+    *    pViewportState is a pointer to an instance of the
+    *    VkPipelineViewportStateCreateInfo structure, and is ignored if the
+    *    pipeline has rasterization disabled.
+    */
+   if (!builder->rasterizer_discard &&
+       panvk_pipeline_static_state(pipeline, VK_DYNAMIC_STATE_VIEWPORT) &&
+       panvk_pipeline_static_state(pipeline, VK_DYNAMIC_STATE_SCISSOR)) {
+      void *vpd = pipeline->state_bo->ptr.cpu + builder->vpd_offset;
+      panvk_per_arch(emit_viewport)(
+         builder->create_info.gfx->pViewportState->pViewports,
+         builder->create_info.gfx->pViewportState->pScissors, vpd);
+      pipeline->vpd = pipeline->state_bo->ptr.gpu + builder->vpd_offset;
+   }
+   if (panvk_pipeline_static_state(pipeline, VK_DYNAMIC_STATE_VIEWPORT))
+      pipeline->viewport =
+         builder->create_info.gfx->pViewportState->pViewports[0];
+
+   if (panvk_pipeline_static_state(pipeline, VK_DYNAMIC_STATE_SCISSOR))
+      pipeline->scissor =
+         builder->create_info.gfx->pViewportState->pScissors[0];
+}
+
+static void
+panvk_pipeline_builder_parse_dynamic(struct panvk_pipeline_builder *builder,
+                                     struct panvk_pipeline *pipeline)
+{
+   const VkPipelineDynamicStateCreateInfo *dynamic_info =
+      builder->create_info.gfx->pDynamicState;
+
+   if (!dynamic_info)
+      return;
+
+   for (uint32_t i = 0; i < dynamic_info->dynamicStateCount; i++) {
+      VkDynamicState state = dynamic_info->pDynamicStates[i];
+      switch (state) {
+      case VK_DYNAMIC_STATE_VIEWPORT ... VK_DYNAMIC_STATE_STENCIL_REFERENCE:
+         pipeline->dynamic_state_mask |= 1 << state;
+         break;
+      default:
+         unreachable("unsupported dynamic state");
+      }
+   }
+}
+
+static enum mali_draw_mode
+translate_prim_topology(VkPrimitiveTopology in)
+{
+   switch (in) {
+   case VK_PRIMITIVE_TOPOLOGY_POINT_LIST:
+      return MALI_DRAW_MODE_POINTS;
+   case VK_PRIMITIVE_TOPOLOGY_LINE_LIST:
+      return MALI_DRAW_MODE_LINES;
+   case VK_PRIMITIVE_TOPOLOGY_LINE_STRIP:
+      return MALI_DRAW_MODE_LINE_STRIP;
+   case VK_PRIMITIVE_TOPOLOGY_TRIANGLE_LIST:
+      return MALI_DRAW_MODE_TRIANGLES;
+   case VK_PRIMITIVE_TOPOLOGY_TRIANGLE_STRIP:
+      return MALI_DRAW_MODE_TRIANGLE_STRIP;
+   case VK_PRIMITIVE_TOPOLOGY_TRIANGLE_FAN:
+      return MALI_DRAW_MODE_TRIANGLE_FAN;
+   case VK_PRIMITIVE_TOPOLOGY_LINE_LIST_WITH_ADJACENCY:
+   case VK_PRIMITIVE_TOPOLOGY_LINE_STRIP_WITH_ADJACENCY:
+   case VK_PRIMITIVE_TOPOLOGY_TRIANGLE_LIST_WITH_ADJACENCY:
+   case VK_PRIMITIVE_TOPOLOGY_TRIANGLE_STRIP_WITH_ADJACENCY:
+   case VK_PRIMITIVE_TOPOLOGY_PATCH_LIST:
+   default:
+      unreachable("Invalid primitive type");
+   }
+}
+
+static void
+panvk_pipeline_builder_parse_input_assembly(
+   struct panvk_pipeline_builder *builder, struct panvk_pipeline *pipeline)
+{
+   pipeline->ia.primitive_restart =
+      builder->create_info.gfx->pInputAssemblyState->primitiveRestartEnable;
+   pipeline->ia.topology = translate_prim_topology(
+      builder->create_info.gfx->pInputAssemblyState->topology);
+}
+
+static enum pipe_logicop
+translate_logicop(VkLogicOp in)
+{
+   switch (in) {
+   case VK_LOGIC_OP_CLEAR:
+      return PIPE_LOGICOP_CLEAR;
+   case VK_LOGIC_OP_AND:
+      return PIPE_LOGICOP_AND;
+   case VK_LOGIC_OP_AND_REVERSE:
+      return PIPE_LOGICOP_AND_REVERSE;
+   case VK_LOGIC_OP_COPY:
+      return PIPE_LOGICOP_COPY;
+   case VK_LOGIC_OP_AND_INVERTED:
+      return PIPE_LOGICOP_AND_INVERTED;
+   case VK_LOGIC_OP_NO_OP:
+      return PIPE_LOGICOP_NOOP;
+   case VK_LOGIC_OP_XOR:
+      return PIPE_LOGICOP_XOR;
+   case VK_LOGIC_OP_OR:
+      return PIPE_LOGICOP_OR;
+   case VK_LOGIC_OP_NOR:
+      return PIPE_LOGICOP_NOR;
+   case VK_LOGIC_OP_EQUIVALENT:
+      return PIPE_LOGICOP_EQUIV;
+   case VK_LOGIC_OP_INVERT:
+      return PIPE_LOGICOP_INVERT;
+   case VK_LOGIC_OP_OR_REVERSE:
+      return PIPE_LOGICOP_OR_REVERSE;
+   case VK_LOGIC_OP_COPY_INVERTED:
+      return PIPE_LOGICOP_COPY_INVERTED;
+   case VK_LOGIC_OP_OR_INVERTED:
+      return PIPE_LOGICOP_OR_INVERTED;
+   case VK_LOGIC_OP_NAND:
+      return PIPE_LOGICOP_NAND;
+   case VK_LOGIC_OP_SET:
+      return PIPE_LOGICOP_SET;
+   default:
+      unreachable("Invalid logicop");
+   }
+}
+
+static enum blend_func
+translate_blend_op(VkBlendOp in)
+{
+   switch (in) {
+   case VK_BLEND_OP_ADD:
+      return BLEND_FUNC_ADD;
+   case VK_BLEND_OP_SUBTRACT:
+      return BLEND_FUNC_SUBTRACT;
+   case VK_BLEND_OP_REVERSE_SUBTRACT:
+      return BLEND_FUNC_REVERSE_SUBTRACT;
+   case VK_BLEND_OP_MIN:
+      return BLEND_FUNC_MIN;
+   case VK_BLEND_OP_MAX:
+      return BLEND_FUNC_MAX;
+   default:
+      unreachable("Invalid blend op");
+   }
+}
+
+static enum blend_factor
+translate_blend_factor(VkBlendFactor in, bool dest_has_alpha)
+{
+   switch (in) {
+   case VK_BLEND_FACTOR_ZERO:
+   case VK_BLEND_FACTOR_ONE:
+      return BLEND_FACTOR_ZERO;
+   case VK_BLEND_FACTOR_SRC_COLOR:
+   case VK_BLEND_FACTOR_ONE_MINUS_SRC_COLOR:
+      return BLEND_FACTOR_SRC_COLOR;
+   case VK_BLEND_FACTOR_DST_COLOR:
+   case VK_BLEND_FACTOR_ONE_MINUS_DST_COLOR:
+      return BLEND_FACTOR_DST_COLOR;
+   case VK_BLEND_FACTOR_SRC_ALPHA:
+   case VK_BLEND_FACTOR_ONE_MINUS_SRC_ALPHA:
+      return BLEND_FACTOR_SRC_ALPHA;
+   case VK_BLEND_FACTOR_DST_ALPHA:
+   case VK_BLEND_FACTOR_ONE_MINUS_DST_ALPHA:
+      return dest_has_alpha ? BLEND_FACTOR_DST_ALPHA : BLEND_FACTOR_ZERO;
+   case VK_BLEND_FACTOR_CONSTANT_COLOR:
+   case VK_BLEND_FACTOR_ONE_MINUS_CONSTANT_COLOR:
+      return BLEND_FACTOR_CONSTANT_COLOR;
+   case VK_BLEND_FACTOR_CONSTANT_ALPHA:
+   case VK_BLEND_FACTOR_ONE_MINUS_CONSTANT_ALPHA:
+      return BLEND_FACTOR_CONSTANT_ALPHA;
+   case VK_BLEND_FACTOR_SRC1_COLOR:
+   case VK_BLEND_FACTOR_ONE_MINUS_SRC1_COLOR:
+      return BLEND_FACTOR_SRC1_COLOR;
+   case VK_BLEND_FACTOR_SRC1_ALPHA:
+   case VK_BLEND_FACTOR_ONE_MINUS_SRC1_ALPHA:
+      return BLEND_FACTOR_SRC1_ALPHA;
+   case VK_BLEND_FACTOR_SRC_ALPHA_SATURATE:
+      return BLEND_FACTOR_SRC_ALPHA_SATURATE;
+   default:
+      unreachable("Invalid blend factor");
+   }
+}
+
+static bool
+inverted_blend_factor(VkBlendFactor in, bool dest_has_alpha)
+{
+   switch (in) {
+   case VK_BLEND_FACTOR_ONE:
+   case VK_BLEND_FACTOR_ONE_MINUS_SRC_COLOR:
+   case VK_BLEND_FACTOR_ONE_MINUS_DST_COLOR:
+   case VK_BLEND_FACTOR_ONE_MINUS_SRC_ALPHA:
+   case VK_BLEND_FACTOR_ONE_MINUS_CONSTANT_COLOR:
+   case VK_BLEND_FACTOR_ONE_MINUS_CONSTANT_ALPHA:
+   case VK_BLEND_FACTOR_ONE_MINUS_SRC1_COLOR:
+   case VK_BLEND_FACTOR_ONE_MINUS_SRC1_ALPHA:
+      return true;
+   case VK_BLEND_FACTOR_ONE_MINUS_DST_ALPHA:
+      return dest_has_alpha ? true : false;
+   case VK_BLEND_FACTOR_DST_ALPHA:
+      return !dest_has_alpha ? true : false;
+   default:
+      return false;
+   }
+}
+
+bool
+panvk_per_arch(blend_needs_lowering)(const struct panfrost_device *dev,
+                                     const struct pan_blend_state *state,
+                                     unsigned rt)
+{
+   /* LogicOp requires a blend shader */
+   if (state->logicop_enable)
+      return true;
+
+   /* Not all formats can be blended by fixed-function hardware */
+   if (!panfrost_blendable_formats_v7[state->rts[rt].format].internal)
+      return true;
+
+   unsigned constant_mask = pan_blend_constant_mask(state->rts[rt].equation);
+
+   /* v6 doesn't support blend constants in FF blend equations.
+    * v7 only uses the constant from RT 0 (TODO: what if it's the same
+    * constant? or a constant is shared?)
+    */
+   if (constant_mask && (PAN_ARCH == 6 || (PAN_ARCH == 7 && rt > 0)))
+      return true;
+
+   if (!pan_blend_is_homogenous_constant(constant_mask, state->constants))
+      return true;
+
+   bool supports_2src = pan_blend_supports_2src(dev->arch);
+   return !pan_blend_can_fixed_function(state->rts[rt].equation, supports_2src);
+}
+
+static void
+panvk_pipeline_builder_parse_color_blend(struct panvk_pipeline_builder *builder,
+                                         struct panvk_pipeline *pipeline)
+{
+   struct panfrost_device *pdev = &builder->device->physical_device->pdev;
+   pipeline->blend.state.logicop_enable =
+      builder->create_info.gfx->pColorBlendState->logicOpEnable;
+   pipeline->blend.state.logicop_func =
+      translate_logicop(builder->create_info.gfx->pColorBlendState->logicOp);
+   pipeline->blend.state.rt_count =
+      util_last_bit(builder->active_color_attachments);
+   memcpy(pipeline->blend.state.constants,
+          builder->create_info.gfx->pColorBlendState->blendConstants,
+          sizeof(pipeline->blend.state.constants));
+
+   for (unsigned i = 0; i < pipeline->blend.state.rt_count; i++) {
+      const VkPipelineColorBlendAttachmentState *in =
+         &builder->create_info.gfx->pColorBlendState->pAttachments[i];
+      struct pan_blend_rt_state *out = &pipeline->blend.state.rts[i];
+
+      out->format = builder->color_attachment_formats[i];
+
+      bool dest_has_alpha = util_format_has_alpha(out->format);
+
+      out->nr_samples =
+         builder->create_info.gfx->pMultisampleState->rasterizationSamples;
+      out->equation.blend_enable = in->blendEnable;
+      out->equation.color_mask = in->colorWriteMask;
+      out->equation.rgb_func = translate_blend_op(in->colorBlendOp);
+      out->equation.rgb_src_factor =
+         translate_blend_factor(in->srcColorBlendFactor, dest_has_alpha);
+      out->equation.rgb_invert_src_factor =
+         inverted_blend_factor(in->srcColorBlendFactor, dest_has_alpha);
+      out->equation.rgb_dst_factor =
+         translate_blend_factor(in->dstColorBlendFactor, dest_has_alpha);
+      out->equation.rgb_invert_dst_factor =
+         inverted_blend_factor(in->dstColorBlendFactor, dest_has_alpha);
+      out->equation.alpha_func = translate_blend_op(in->alphaBlendOp);
+      out->equation.alpha_src_factor =
+         translate_blend_factor(in->srcAlphaBlendFactor, dest_has_alpha);
+      out->equation.alpha_invert_src_factor =
+         inverted_blend_factor(in->srcAlphaBlendFactor, dest_has_alpha);
+      out->equation.alpha_dst_factor =
+         translate_blend_factor(in->dstAlphaBlendFactor, dest_has_alpha);
+      out->equation.alpha_invert_dst_factor =
+         inverted_blend_factor(in->dstAlphaBlendFactor, dest_has_alpha);
+
+      pipeline->blend.reads_dest |= pan_blend_reads_dest(out->equation);
+
+      unsigned constant_mask =
+         panvk_per_arch(blend_needs_lowering)(pdev, &pipeline->blend.state, i)
+            ? 0
+            : pan_blend_constant_mask(out->equation);
+      pipeline->blend.constant[i].index = ffs(constant_mask) - 1;
+      if (constant_mask) {
+         /* On Bifrost, the blend constant is expressed with a UNORM of the
+          * size of the target format. The value is then shifted such that
+          * used bits are in the MSB. Here we calculate the factor at pipeline
+          * creation time so we only have to do a
+          *   hw_constant = float_constant * factor;
+          * at descriptor emission time.
+          */
+         const struct util_format_description *format_desc =
+            util_format_description(out->format);
+         unsigned chan_size = 0;
+         for (unsigned c = 0; c < format_desc->nr_channels; c++)
+            chan_size = MAX2(format_desc->channel[c].size, chan_size);
+         pipeline->blend.constant[i].bifrost_factor = ((1 << chan_size) - 1)
+                                                      << (16 - chan_size);
+      }
+   }
+}
+
+static void
+panvk_pipeline_builder_parse_multisample(struct panvk_pipeline_builder *builder,
+                                         struct panvk_pipeline *pipeline)
+{
+   unsigned nr_samples = MAX2(
+      builder->create_info.gfx->pMultisampleState->rasterizationSamples, 1);
+
+   pipeline->ms.rast_samples =
+      builder->create_info.gfx->pMultisampleState->rasterizationSamples;
+   pipeline->ms.sample_mask =
+      builder->create_info.gfx->pMultisampleState->pSampleMask
+         ? builder->create_info.gfx->pMultisampleState->pSampleMask[0]
+         : UINT16_MAX;
+   pipeline->ms.min_samples =
+      MAX2(builder->create_info.gfx->pMultisampleState->minSampleShading *
+              nr_samples,
+           1);
+}
+
+static enum mali_stencil_op
+translate_stencil_op(VkStencilOp in)
+{
+   switch (in) {
+   case VK_STENCIL_OP_KEEP:
+      return MALI_STENCIL_OP_KEEP;
+   case VK_STENCIL_OP_ZERO:
+      return MALI_STENCIL_OP_ZERO;
+   case VK_STENCIL_OP_REPLACE:
+      return MALI_STENCIL_OP_REPLACE;
+   case VK_STENCIL_OP_INCREMENT_AND_CLAMP:
+      return MALI_STENCIL_OP_INCR_SAT;
+   case VK_STENCIL_OP_DECREMENT_AND_CLAMP:
+      return MALI_STENCIL_OP_DECR_SAT;
+   case VK_STENCIL_OP_INCREMENT_AND_WRAP:
+      return MALI_STENCIL_OP_INCR_WRAP;
+   case VK_STENCIL_OP_DECREMENT_AND_WRAP:
+      return MALI_STENCIL_OP_DECR_WRAP;
+   case VK_STENCIL_OP_INVERT:
+      return MALI_STENCIL_OP_INVERT;
+   default:
+      unreachable("Invalid stencil op");
+   }
+}
+
+static void
+panvk_pipeline_builder_parse_zs(struct panvk_pipeline_builder *builder,
+                                struct panvk_pipeline *pipeline)
+{
+   if (!builder->use_depth_stencil_attachment)
+      return;
+
+   pipeline->zs.z_test =
+      builder->create_info.gfx->pDepthStencilState->depthTestEnable;
+
+   /* The Vulkan spec says:
+    *
+    *    depthWriteEnable controls whether depth writes are enabled when
+    *    depthTestEnable is VK_TRUE. Depth writes are always disabled when
+    *    depthTestEnable is VK_FALSE.
+    *
+    * The hardware does not make this distinction, though, so we AND in the
+    * condition ourselves.
+    */
+   pipeline->zs.z_write =
+      pipeline->zs.z_test &&
+      builder->create_info.gfx->pDepthStencilState->depthWriteEnable;
+
+   pipeline->zs.z_compare_func = panvk_per_arch(translate_compare_func)(
+      builder->create_info.gfx->pDepthStencilState->depthCompareOp);
+   pipeline->zs.s_test =
+      builder->create_info.gfx->pDepthStencilState->stencilTestEnable;
+   pipeline->zs.s_front.fail_op = translate_stencil_op(
+      builder->create_info.gfx->pDepthStencilState->front.failOp);
+   pipeline->zs.s_front.pass_op = translate_stencil_op(
+      builder->create_info.gfx->pDepthStencilState->front.passOp);
+   pipeline->zs.s_front.z_fail_op = translate_stencil_op(
+      builder->create_info.gfx->pDepthStencilState->front.depthFailOp);
+   pipeline->zs.s_front.compare_func = panvk_per_arch(translate_compare_func)(
+      builder->create_info.gfx->pDepthStencilState->front.compareOp);
+   pipeline->zs.s_front.compare_mask =
+      builder->create_info.gfx->pDepthStencilState->front.compareMask;
+   pipeline->zs.s_front.write_mask =
+      builder->create_info.gfx->pDepthStencilState->front.writeMask;
+   pipeline->zs.s_front.ref =
+      builder->create_info.gfx->pDepthStencilState->front.reference;
+   pipeline->zs.s_back.fail_op = translate_stencil_op(
+      builder->create_info.gfx->pDepthStencilState->back.failOp);
+   pipeline->zs.s_back.pass_op = translate_stencil_op(
+      builder->create_info.gfx->pDepthStencilState->back.passOp);
+   pipeline->zs.s_back.z_fail_op = translate_stencil_op(
+      builder->create_info.gfx->pDepthStencilState->back.depthFailOp);
+   pipeline->zs.s_back.compare_func = panvk_per_arch(translate_compare_func)(
+      builder->create_info.gfx->pDepthStencilState->back.compareOp);
+   pipeline->zs.s_back.compare_mask =
+      builder->create_info.gfx->pDepthStencilState->back.compareMask;
+   pipeline->zs.s_back.write_mask =
+      builder->create_info.gfx->pDepthStencilState->back.writeMask;
+   pipeline->zs.s_back.ref =
+      builder->create_info.gfx->pDepthStencilState->back.reference;
+}
+
+static void
+panvk_pipeline_builder_parse_rast(struct panvk_pipeline_builder *builder,
+                                  struct panvk_pipeline *pipeline)
+{
+   pipeline->rast.clamp_depth =
+      builder->create_info.gfx->pRasterizationState->depthClampEnable;
+   pipeline->rast.depth_bias.enable =
+      builder->create_info.gfx->pRasterizationState->depthBiasEnable;
+   pipeline->rast.depth_bias.constant_factor =
+      builder->create_info.gfx->pRasterizationState->depthBiasConstantFactor;
+   pipeline->rast.depth_bias.clamp =
+      builder->create_info.gfx->pRasterizationState->depthBiasClamp;
+   pipeline->rast.depth_bias.slope_factor =
+      builder->create_info.gfx->pRasterizationState->depthBiasSlopeFactor;
+   pipeline->rast.front_ccw =
+      builder->create_info.gfx->pRasterizationState->frontFace ==
+      VK_FRONT_FACE_COUNTER_CLOCKWISE;
+   pipeline->rast.cull_front_face =
+      builder->create_info.gfx->pRasterizationState->cullMode &
+      VK_CULL_MODE_FRONT_BIT;
+   pipeline->rast.cull_back_face =
+      builder->create_info.gfx->pRasterizationState->cullMode &
+      VK_CULL_MODE_BACK_BIT;
+   pipeline->rast.line_width =
+      builder->create_info.gfx->pRasterizationState->lineWidth;
+   pipeline->rast.enable =
+      !builder->create_info.gfx->pRasterizationState->rasterizerDiscardEnable;
+}
+
+static bool
+panvk_fs_required(struct panvk_pipeline *pipeline)
+{
+   const struct pan_shader_info *info = &pipeline->fs.info;
+
+   /* If we generally have side effects */
+   if (info->fs.sidefx)
+      return true;
+
+   /* If colour is written we need to execute */
+   const struct pan_blend_state *blend = &pipeline->blend.state;
+   for (unsigned i = 0; i < blend->rt_count; ++i) {
+      if (blend->rts[i].equation.color_mask)
+         return true;
+   }
+
+   /* If depth is written and not implied we need to execute.
+    * TODO: Predicate on Z/S writes being enabled */
+   return (info->fs.writes_depth || info->fs.writes_stencil);
+}
+
+#define PANVK_DYNAMIC_FS_RSD_MASK                                              \
+   ((1 << VK_DYNAMIC_STATE_DEPTH_BIAS) |                                       \
+    (1 << VK_DYNAMIC_STATE_BLEND_CONSTANTS) |                                  \
+    (1 << VK_DYNAMIC_STATE_STENCIL_COMPARE_MASK) |                             \
+    (1 << VK_DYNAMIC_STATE_STENCIL_WRITE_MASK) |                               \
+    (1 << VK_DYNAMIC_STATE_STENCIL_REFERENCE))
+
+static void
+panvk_pipeline_builder_init_fs_state(struct panvk_pipeline_builder *builder,
+                                     struct panvk_pipeline *pipeline)
+{
+   if (!builder->shaders[MESA_SHADER_FRAGMENT])
+      return;
+
+   pipeline->fs.dynamic_rsd =
+      pipeline->dynamic_state_mask & PANVK_DYNAMIC_FS_RSD_MASK;
+   pipeline->fs.address = pipeline->binary_bo->ptr.gpu +
+                          builder->stages[MESA_SHADER_FRAGMENT].shader_offset;
+   pipeline->fs.info = builder->shaders[MESA_SHADER_FRAGMENT]->info;
+   pipeline->fs.rt_mask = builder->active_color_attachments;
+   pipeline->fs.required = panvk_fs_required(pipeline);
+}
+
+static void
+panvk_pipeline_update_varying_slot(struct panvk_varyings_info *varyings,
+                                   gl_shader_stage stage,
+                                   const struct pan_shader_varying *varying,
+                                   bool input)
+{
+   gl_varying_slot loc = varying->location;
+   enum panvk_varying_buf_id buf_id = panvk_varying_buf_id(loc);
+
+   varyings->stage[stage].loc[varyings->stage[stage].count++] = loc;
+
+   assert(loc < ARRAY_SIZE(varyings->varying));
+
+   enum pipe_format new_fmt = varying->format;
+   enum pipe_format old_fmt = varyings->varying[loc].format;
+
+   BITSET_SET(varyings->active, loc);
+
+   /* We expect inputs to either be set by a previous stage or be built
+    * in, skip the entry if that's not the case, we'll emit a const
+    * varying returning zero for those entries.
+    */
+   if (input && old_fmt == PIPE_FORMAT_NONE)
+      return;
+
+   unsigned new_size = util_format_get_blocksize(new_fmt);
+   unsigned old_size = util_format_get_blocksize(old_fmt);
+
+   if (old_size < new_size)
+      varyings->varying[loc].format = new_fmt;
+
+   /* Type (float or not) information is only known in the fragment shader, so
+    * override for that
+    */
+   if (input) {
+      assert(stage == MESA_SHADER_FRAGMENT && "no geom/tess on Bifrost");
+      varyings->varying[loc].format = new_fmt;
+   }
+
+   varyings->buf_mask |= 1 << buf_id;
+}
+
+static void
+panvk_pipeline_builder_collect_varyings(struct panvk_pipeline_builder *builder,
+                                        struct panvk_pipeline *pipeline)
+{
+   for (uint32_t s = 0; s < MESA_SHADER_STAGES; s++) {
+      if (!builder->shaders[s])
+         continue;
+
+      const struct pan_shader_info *info = &builder->shaders[s]->info;
+
+      for (unsigned i = 0; i < info->varyings.input_count; i++) {
+         panvk_pipeline_update_varying_slot(&pipeline->varyings, s,
+                                            &info->varyings.input[i], true);
+      }
+
+      for (unsigned i = 0; i < info->varyings.output_count; i++) {
+         panvk_pipeline_update_varying_slot(&pipeline->varyings, s,
+                                            &info->varyings.output[i], false);
+      }
+   }
+
+   /* TODO: Xfb */
+   gl_varying_slot loc;
+   BITSET_FOREACH_SET(loc, pipeline->varyings.active, VARYING_SLOT_MAX) {
+      if (pipeline->varyings.varying[loc].format == PIPE_FORMAT_NONE)
+         continue;
+
+      enum panvk_varying_buf_id buf_id = panvk_varying_buf_id(loc);
+      unsigned buf_idx = panvk_varying_buf_index(&pipeline->varyings, buf_id);
+      unsigned varying_sz = panvk_varying_size(&pipeline->varyings, loc);
+
+      pipeline->varyings.varying[loc].buf = buf_idx;
+      pipeline->varyings.varying[loc].offset =
+         pipeline->varyings.buf[buf_idx].stride;
+      pipeline->varyings.buf[buf_idx].stride += varying_sz;
+   }
+}
+
+static void
+panvk_pipeline_builder_parse_vertex_input(
+   struct panvk_pipeline_builder *builder, struct panvk_pipeline *pipeline)
+{
+   struct panvk_attribs_info *attribs = &pipeline->attribs;
+   const VkPipelineVertexInputStateCreateInfo *info =
+      builder->create_info.gfx->pVertexInputState;
+
+   const VkPipelineVertexInputDivisorStateCreateInfoEXT *div_info =
+      vk_find_struct_const(info->pNext,
+                           PIPELINE_VERTEX_INPUT_DIVISOR_STATE_CREATE_INFO_EXT);
+
+   for (unsigned i = 0; i < info->vertexBindingDescriptionCount; i++) {
+      const VkVertexInputBindingDescription *desc =
+         &info->pVertexBindingDescriptions[i];
+      attribs->buf_count = MAX2(desc->binding + 1, attribs->buf_count);
+      attribs->buf[desc->binding].stride = desc->stride;
+      attribs->buf[desc->binding].per_instance =
+         desc->inputRate == VK_VERTEX_INPUT_RATE_INSTANCE;
+      attribs->buf[desc->binding].instance_divisor = 1;
+      attribs->buf[desc->binding].special = false;
+   }
+
+   if (div_info) {
+      for (unsigned i = 0; i < div_info->vertexBindingDivisorCount; i++) {
+         const VkVertexInputBindingDivisorDescriptionEXT *div =
+            &div_info->pVertexBindingDivisors[i];
+         attribs->buf[div->binding].instance_divisor = div->divisor;
+      }
+   }
+
+   const struct pan_shader_info *vs =
+      &builder->shaders[MESA_SHADER_VERTEX]->info;
+
+   for (unsigned i = 0; i < info->vertexAttributeDescriptionCount; i++) {
+      const VkVertexInputAttributeDescription *desc =
+         &info->pVertexAttributeDescriptions[i];
+
+      unsigned attrib = desc->location + VERT_ATTRIB_GENERIC0;
+      unsigned slot =
+         util_bitcount64(vs->attributes_read & BITFIELD64_MASK(attrib));
+
+      attribs->attrib[slot].buf = desc->binding;
+      attribs->attrib[slot].format = vk_format_to_pipe_format(desc->format);
+      attribs->attrib[slot].offset = desc->offset;
+   }
+
+   if (vs->attribute_count >= PAN_VERTEX_ID) {
+      attribs->buf[attribs->buf_count].special = true;
+      attribs->buf[attribs->buf_count].special_id = PAN_VERTEX_ID;
+      attribs->attrib[PAN_VERTEX_ID].buf = attribs->buf_count++;
+      attribs->attrib[PAN_VERTEX_ID].format = PIPE_FORMAT_R32_UINT;
+   }
+
+   if (vs->attribute_count >= PAN_INSTANCE_ID) {
+      attribs->buf[attribs->buf_count].special = true;
+      attribs->buf[attribs->buf_count].special_id = PAN_INSTANCE_ID;
+      attribs->attrib[PAN_INSTANCE_ID].buf = attribs->buf_count++;
+      attribs->attrib[PAN_INSTANCE_ID].format = PIPE_FORMAT_R32_UINT;
+   }
+
+   attribs->attrib_count = MAX2(attribs->attrib_count, vs->attribute_count);
+}
+
+static VkResult
+panvk_pipeline_builder_build(struct panvk_pipeline_builder *builder,
+                             struct panvk_pipeline **pipeline)
+{
+   VkResult result = panvk_pipeline_builder_create_pipeline(builder, pipeline);
+   if (result != VK_SUCCESS)
+      return result;
+
+   /* TODO: make those functions return a result and handle errors */
+   if (builder->create_info.gfx) {
+      panvk_pipeline_builder_parse_dynamic(builder, *pipeline);
+      panvk_pipeline_builder_parse_color_blend(builder, *pipeline);
+      panvk_pipeline_builder_compile_shaders(builder, *pipeline);
+      panvk_pipeline_builder_collect_varyings(builder, *pipeline);
+      panvk_pipeline_builder_parse_input_assembly(builder, *pipeline);
+      panvk_pipeline_builder_parse_multisample(builder, *pipeline);
+      panvk_pipeline_builder_parse_zs(builder, *pipeline);
+      panvk_pipeline_builder_parse_rast(builder, *pipeline);
+      panvk_pipeline_builder_parse_vertex_input(builder, *pipeline);
+      panvk_pipeline_builder_upload_shaders(builder, *pipeline);
+      panvk_pipeline_builder_init_fs_state(builder, *pipeline);
+      panvk_pipeline_builder_alloc_static_state_bo(builder, *pipeline);
+      panvk_pipeline_builder_init_shaders(builder, *pipeline);
+      panvk_pipeline_builder_parse_viewport(builder, *pipeline);
+   } else {
+      panvk_pipeline_builder_compile_shaders(builder, *pipeline);
+      panvk_pipeline_builder_upload_shaders(builder, *pipeline);
+      panvk_pipeline_builder_alloc_static_state_bo(builder, *pipeline);
+      panvk_pipeline_builder_init_shaders(builder, *pipeline);
+   }
+
+   return VK_SUCCESS;
+}
+
+static void
+panvk_pipeline_builder_init_graphics(
+   struct panvk_pipeline_builder *builder, struct panvk_device *dev,
+   struct panvk_pipeline_cache *cache,
+   const VkGraphicsPipelineCreateInfo *create_info,
+   const VkAllocationCallbacks *alloc)
+{
+   VK_FROM_HANDLE(panvk_pipeline_layout, layout, create_info->layout);
+   assert(layout);
+   *builder = (struct panvk_pipeline_builder){
+      .device = dev,
+      .cache = cache,
+      .layout = layout,
+      .create_info.gfx = create_info,
+      .alloc = alloc,
+   };
+
+   builder->rasterizer_discard =
+      create_info->pRasterizationState->rasterizerDiscardEnable;
+
+   if (builder->rasterizer_discard) {
+      builder->samples = VK_SAMPLE_COUNT_1_BIT;
+   } else {
+      builder->samples = create_info->pMultisampleState->rasterizationSamples;
+
+      const struct panvk_render_pass *pass =
+         panvk_render_pass_from_handle(create_info->renderPass);
+      const struct panvk_subpass *subpass =
+         &pass->subpasses[create_info->subpass];
+
+      builder->use_depth_stencil_attachment =
+         subpass->zs_attachment.idx != VK_ATTACHMENT_UNUSED;
+
+      assert(subpass->color_count <=
+             create_info->pColorBlendState->attachmentCount);
+      builder->active_color_attachments = 0;
+      for (uint32_t i = 0; i < subpass->color_count; i++) {
+         uint32_t idx = subpass->color_attachments[i].idx;
+         if (idx == VK_ATTACHMENT_UNUSED)
+            continue;
+
+         builder->active_color_attachments |= 1 << i;
+         builder->color_attachment_formats[i] = pass->attachments[idx].format;
+      }
+   }
+}
+
+VkResult
+panvk_per_arch(CreateGraphicsPipelines)(
+   VkDevice device, VkPipelineCache pipelineCache, uint32_t count,
+   const VkGraphicsPipelineCreateInfo *pCreateInfos,
+   const VkAllocationCallbacks *pAllocator, VkPipeline *pPipelines)
+{
+   VK_FROM_HANDLE(panvk_device, dev, device);
+   VK_FROM_HANDLE(panvk_pipeline_cache, cache, pipelineCache);
+
+   for (uint32_t i = 0; i < count; i++) {
+      struct panvk_pipeline_builder builder;
+      panvk_pipeline_builder_init_graphics(&builder, dev, cache,
+                                           &pCreateInfos[i], pAllocator);
+
+      struct panvk_pipeline *pipeline;
+      VkResult result = panvk_pipeline_builder_build(&builder, &pipeline);
+      panvk_pipeline_builder_finish(&builder);
+
+      if (result != VK_SUCCESS) {
+         for (uint32_t j = 0; j < i; j++) {
+            panvk_DestroyPipeline(device, pPipelines[j], pAllocator);
+            pPipelines[j] = VK_NULL_HANDLE;
+         }
+
+         return result;
+      }
+
+      pPipelines[i] = panvk_pipeline_to_handle(pipeline);
+   }
+
+   return VK_SUCCESS;
+}
+
+static void
+panvk_pipeline_builder_init_compute(
+   struct panvk_pipeline_builder *builder, struct panvk_device *dev,
+   struct panvk_pipeline_cache *cache,
+   const VkComputePipelineCreateInfo *create_info,
+   const VkAllocationCallbacks *alloc)
+{
+   VK_FROM_HANDLE(panvk_pipeline_layout, layout, create_info->layout);
+   assert(layout);
+   *builder = (struct panvk_pipeline_builder){
+      .device = dev,
+      .cache = cache,
+      .layout = layout,
+      .create_info.compute = create_info,
+      .alloc = alloc,
+   };
+}
+
+VkResult
+panvk_per_arch(CreateComputePipelines)(
+   VkDevice device, VkPipelineCache pipelineCache, uint32_t count,
+   const VkComputePipelineCreateInfo *pCreateInfos,
+   const VkAllocationCallbacks *pAllocator, VkPipeline *pPipelines)
+{
+   VK_FROM_HANDLE(panvk_device, dev, device);
+   VK_FROM_HANDLE(panvk_pipeline_cache, cache, pipelineCache);
+
+   for (uint32_t i = 0; i < count; i++) {
+      struct panvk_pipeline_builder builder;
+      panvk_pipeline_builder_init_compute(&builder, dev, cache,
+                                          &pCreateInfos[i], pAllocator);
+
+      struct panvk_pipeline *pipeline;
+      VkResult result = panvk_pipeline_builder_build(&builder, &pipeline);
+      panvk_pipeline_builder_finish(&builder);
+
+      if (result != VK_SUCCESS) {
+         for (uint32_t j = 0; j < i; j++) {
+            panvk_DestroyPipeline(device, pPipelines[j], pAllocator);
+            pPipelines[j] = VK_NULL_HANDLE;
+         }
+
+         return result;
+      }
+
+      pPipelines[i] = panvk_pipeline_to_handle(pipeline);
+   }
+
+   return VK_SUCCESS;
+}
diff -up mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_shader.c.16~ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_shader.c
--- mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_shader.c.16~	2023-07-29 15:51:45.093980518 +0100
+++ mesa-23.2.0/src/panfrost/vulkan/bifrost/panvk_vX_shader.c	2023-07-29 15:51:45.093980518 +0100
@@ -0,0 +1,406 @@
+/*
+ * Copyright © 2021 Collabora Ltd.
+ *
+ * Derived from tu_shader.c which is:
+ * Copyright © 2019 Google LLC
+ *
+ * Also derived from anv_pipeline.c which is
+ * Copyright © 2015 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#include "genxml/gen_macros.h"
+
+#include "panvk_private.h"
+
+#include "spirv/nir_spirv.h"
+#include "util/mesa-sha1.h"
+#include "nir_builder.h"
+#include "nir_conversion_builder.h"
+#include "nir_deref.h"
+#include "nir_lower_blend.h"
+#include "vk_shader_module.h"
+
+#include "compiler/bifrost_nir.h"
+#include "util/pan_lower_framebuffer.h"
+#include "pan_shader.h"
+
+#include "vk_util.h"
+
+static nir_ssa_def *
+load_sysval_from_ubo(nir_builder *b, nir_intrinsic_instr *intr, unsigned offset)
+{
+   return nir_load_ubo(
+      b, nir_dest_num_components(intr->dest), nir_dest_bit_size(intr->dest),
+      nir_imm_int(b, PANVK_SYSVAL_UBO_INDEX), nir_imm_int(b, offset),
+      .align_mul = nir_dest_bit_size(intr->dest) / 8, .align_offset = 0,
+      .range_base = offset, .range = nir_dest_bit_size(intr->dest) / 8);
+}
+
+struct sysval_options {
+   /* If non-null, a vec4 of blend constants known at pipeline compile time. If
+    * null, blend constants are dynamic.
+    */
+   float *static_blend_constants;
+};
+
+static bool
+panvk_lower_sysvals(nir_builder *b, nir_instr *instr, void *data)
+{
+   if (instr->type != nir_instr_type_intrinsic)
+      return false;
+
+   struct sysval_options *opts = data;
+   nir_intrinsic_instr *intr = nir_instr_as_intrinsic(instr);
+   nir_ssa_def *val = NULL;
+   b->cursor = nir_before_instr(instr);
+
+#define SYSVAL(name) offsetof(struct panvk_sysvals, name)
+   switch (intr->intrinsic) {
+   case nir_intrinsic_load_num_workgroups:
+      val = load_sysval_from_ubo(b, intr, SYSVAL(num_work_groups));
+      break;
+   case nir_intrinsic_load_workgroup_size:
+      val = load_sysval_from_ubo(b, intr, SYSVAL(local_group_size));
+      break;
+   case nir_intrinsic_load_viewport_scale:
+      val = load_sysval_from_ubo(b, intr, SYSVAL(viewport_scale));
+      break;
+   case nir_intrinsic_load_viewport_offset:
+      val = load_sysval_from_ubo(b, intr, SYSVAL(viewport_offset));
+      break;
+   case nir_intrinsic_load_first_vertex:
+      val = load_sysval_from_ubo(b, intr, SYSVAL(first_vertex));
+      break;
+   case nir_intrinsic_load_base_vertex:
+      val = load_sysval_from_ubo(b, intr, SYSVAL(base_vertex));
+      break;
+   case nir_intrinsic_load_base_instance:
+      val = load_sysval_from_ubo(b, intr, SYSVAL(base_instance));
+      break;
+   case nir_intrinsic_load_blend_const_color_rgba:
+      if (opts->static_blend_constants) {
+         const nir_const_value constants[4] = {
+            {.f32 = opts->static_blend_constants[0]},
+            {.f32 = opts->static_blend_constants[1]},
+            {.f32 = opts->static_blend_constants[2]},
+            {.f32 = opts->static_blend_constants[3]},
+         };
+
+         val = nir_build_imm(b, 4, 32, constants);
+      } else {
+         val = load_sysval_from_ubo(b, intr, SYSVAL(blend_constants));
+      }
+      break;
+   default:
+      return false;
+   }
+#undef SYSVAL
+
+   b->cursor = nir_after_instr(instr);
+   nir_ssa_def_rewrite_uses(&intr->dest.ssa, val);
+   return true;
+}
+
+static void
+panvk_lower_blend(struct panfrost_device *pdev, nir_shader *nir,
+                  struct panfrost_compile_inputs *inputs,
+                  struct pan_blend_state *blend_state)
+{
+   nir_lower_blend_options options = {
+      .logicop_enable = blend_state->logicop_enable,
+      .logicop_func = blend_state->logicop_func,
+   };
+
+   bool lower_blend = false;
+
+   for (unsigned rt = 0; rt < blend_state->rt_count; rt++) {
+      struct pan_blend_rt_state *rt_state = &blend_state->rts[rt];
+
+      if (!panvk_per_arch(blend_needs_lowering)(pdev, blend_state, rt))
+         continue;
+
+      enum pipe_format fmt = rt_state->format;
+
+      options.format[rt] = fmt;
+      options.rt[rt].colormask = rt_state->equation.color_mask;
+
+      if (!rt_state->equation.blend_enable) {
+         static const nir_lower_blend_channel replace = {
+            .func = BLEND_FUNC_ADD,
+            .src_factor = BLEND_FACTOR_ZERO,
+            .invert_src_factor = true,
+            .dst_factor = BLEND_FACTOR_ZERO,
+            .invert_dst_factor = false,
+         };
+
+         options.rt[rt].rgb = replace;
+         options.rt[rt].alpha = replace;
+      } else {
+         options.rt[rt].rgb.func = rt_state->equation.rgb_func;
+         options.rt[rt].rgb.src_factor = rt_state->equation.rgb_src_factor;
+         options.rt[rt].rgb.invert_src_factor =
+            rt_state->equation.rgb_invert_src_factor;
+         options.rt[rt].rgb.dst_factor = rt_state->equation.rgb_dst_factor;
+         options.rt[rt].rgb.invert_dst_factor =
+            rt_state->equation.rgb_invert_dst_factor;
+         options.rt[rt].alpha.func = rt_state->equation.alpha_func;
+         options.rt[rt].alpha.src_factor = rt_state->equation.alpha_src_factor;
+         options.rt[rt].alpha.invert_src_factor =
+            rt_state->equation.alpha_invert_src_factor;
+         options.rt[rt].alpha.dst_factor = rt_state->equation.alpha_dst_factor;
+         options.rt[rt].alpha.invert_dst_factor =
+            rt_state->equation.alpha_invert_dst_factor;
+      }
+
+      /* Update the equation to force a color replacement */
+      rt_state->equation.color_mask = 0xf;
+      rt_state->equation.rgb_func = BLEND_FUNC_ADD;
+      rt_state->equation.rgb_src_factor = BLEND_FACTOR_ZERO;
+      rt_state->equation.rgb_invert_src_factor = true;
+      rt_state->equation.rgb_dst_factor = BLEND_FACTOR_ZERO;
+      rt_state->equation.rgb_invert_dst_factor = false;
+      rt_state->equation.alpha_func = BLEND_FUNC_ADD;
+      rt_state->equation.alpha_src_factor = BLEND_FACTOR_ZERO;
+      rt_state->equation.alpha_invert_src_factor = true;
+      rt_state->equation.alpha_dst_factor = BLEND_FACTOR_ZERO;
+      rt_state->equation.alpha_invert_dst_factor = false;
+      lower_blend = true;
+   }
+
+   if (lower_blend) {
+      NIR_PASS_V(nir, nir_lower_blend, &options);
+      NIR_PASS_V(nir, bifrost_nir_lower_load_output);
+   }
+}
+
+static bool
+panvk_lower_load_push_constant(nir_builder *b, nir_instr *instr, void *data)
+{
+   if (instr->type != nir_instr_type_intrinsic)
+      return false;
+
+   nir_intrinsic_instr *intr = nir_instr_as_intrinsic(instr);
+   if (intr->intrinsic != nir_intrinsic_load_push_constant)
+      return false;
+
+   b->cursor = nir_before_instr(instr);
+   nir_ssa_def *ubo_load = nir_load_ubo(
+      b, nir_dest_num_components(intr->dest), nir_dest_bit_size(intr->dest),
+      nir_imm_int(b, PANVK_PUSH_CONST_UBO_INDEX), intr->src[0].ssa,
+      .align_mul = nir_dest_bit_size(intr->dest) / 8, .align_offset = 0,
+      .range_base = nir_intrinsic_base(intr),
+      .range = nir_intrinsic_range(intr));
+   nir_ssa_def_rewrite_uses(&intr->dest.ssa, ubo_load);
+   nir_instr_remove(instr);
+   return true;
+}
+
+static void
+shared_type_info(const struct glsl_type *type, unsigned *size, unsigned *align)
+{
+   assert(glsl_type_is_vector_or_scalar(type));
+
+   uint32_t comp_size =
+      glsl_type_is_boolean(type) ? 4 : glsl_get_bit_size(type) / 8;
+   unsigned length = glsl_get_vector_elements(type);
+   *size = comp_size * length, *align = comp_size * (length == 3 ? 4 : length);
+}
+
+struct panvk_shader *
+panvk_per_arch(shader_create)(struct panvk_device *dev, gl_shader_stage stage,
+                              const VkPipelineShaderStageCreateInfo *stage_info,
+                              const struct panvk_pipeline_layout *layout,
+                              unsigned sysval_ubo,
+                              struct pan_blend_state *blend_state,
+                              bool static_blend_constants,
+                              const VkAllocationCallbacks *alloc)
+{
+   VK_FROM_HANDLE(vk_shader_module, module, stage_info->module);
+   struct panfrost_device *pdev = &dev->physical_device->pdev;
+   struct panvk_shader *shader;
+
+   shader = vk_zalloc2(&dev->vk.alloc, alloc, sizeof(*shader), 8,
+                       VK_SYSTEM_ALLOCATION_SCOPE_COMMAND);
+   if (!shader)
+      return NULL;
+
+   util_dynarray_init(&shader->binary, NULL);
+
+   /* TODO these are made-up */
+   const struct spirv_to_nir_options spirv_options = {
+      .caps =
+         {
+            .variable_pointers = true,
+         },
+      .ubo_addr_format = nir_address_format_32bit_index_offset,
+      .ssbo_addr_format = dev->vk.enabled_features.robustBufferAccess
+                             ? nir_address_format_64bit_bounded_global
+                             : nir_address_format_64bit_global_32bit_offset,
+   };
+
+   nir_shader *nir;
+   VkResult result = vk_shader_module_to_nir(
+      &dev->vk, module, stage, stage_info->pName,
+      stage_info->pSpecializationInfo, &spirv_options,
+      GENX(pan_shader_get_compiler_options)(), NULL, &nir);
+   if (result != VK_SUCCESS) {
+      vk_free2(&dev->vk.alloc, alloc, shader);
+      return NULL;
+   }
+
+   NIR_PASS_V(nir, nir_lower_io_to_temporaries, nir_shader_get_entrypoint(nir),
+              true, true);
+
+   struct panfrost_compile_inputs inputs = {
+      .gpu_id = pdev->gpu_id,
+      .no_ubo_to_push = true,
+      .no_idvs = true, /* TODO */
+   };
+
+   NIR_PASS_V(nir, nir_lower_indirect_derefs,
+              nir_var_shader_in | nir_var_shader_out, UINT32_MAX);
+
+   NIR_PASS_V(nir, nir_opt_copy_prop_vars);
+   NIR_PASS_V(nir, nir_opt_combine_stores, nir_var_all);
+   NIR_PASS_V(nir, nir_opt_trivial_continues);
+
+   /* Do texture lowering here.  Yes, it's a duplication of the texture
+    * lowering in bifrost_compile.  However, we need to lower texture stuff
+    * now, before we call panvk_per_arch(nir_lower_descriptors)() because some
+    * of the texture lowering generates nir_texop_txs which we handle as part
+    * of descriptor lowering.
+    *
+    * TODO: We really should be doing this in common code, not dpulicated in
+    * panvk.  In order to do that, we need to rework the panfrost compile
+    * flow to look more like the Intel flow:
+    *
+    *  1. Compile SPIR-V to NIR and maybe do a tiny bit of lowering that needs
+    *     to be done really early.
+    *
+    *  2. bi_preprocess_nir: Does common lowering and runs the optimization
+    *     loop.  Nothing here should be API-specific.
+    *
+    *  3. Do additional lowering in panvk
+    *
+    *  4. bi_postprocess_nir: Does final lowering and runs the optimization
+    *     loop again.  This can happen as part of the final compile.
+    *
+    * This would give us a better place to do panvk-specific lowering.
+    */
+   nir_lower_tex_options lower_tex_options = {
+      .lower_txs_lod = true,
+      .lower_txp = ~0,
+      .lower_tg4_broadcom_swizzle = true,
+      .lower_txd = true,
+      .lower_invalid_implicit_lod = true,
+   };
+   NIR_PASS_V(nir, nir_lower_tex, &lower_tex_options);
+
+   NIR_PASS_V(nir, panvk_per_arch(nir_lower_descriptors), dev, layout,
+              &shader->has_img_access);
+
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_ubo,
+              nir_address_format_32bit_index_offset);
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_ssbo,
+              spirv_options.ssbo_addr_format);
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_push_const,
+              nir_address_format_32bit_offset);
+
+   if (gl_shader_stage_uses_workgroup(stage)) {
+      if (!nir->info.shared_memory_explicit_layout) {
+         NIR_PASS_V(nir, nir_lower_vars_to_explicit_types, nir_var_mem_shared,
+                    shared_type_info);
+      }
+
+      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_shared,
+                 nir_address_format_32bit_offset);
+   }
+
+   NIR_PASS_V(nir, nir_shader_instructions_pass, panvk_lower_load_push_constant,
+              nir_metadata_block_index | nir_metadata_dominance,
+              (void *)layout);
+
+   NIR_PASS_V(nir, nir_lower_system_values);
+   NIR_PASS_V(nir, nir_lower_compute_system_values, NULL);
+
+   NIR_PASS_V(nir, nir_split_var_copies);
+   NIR_PASS_V(nir, nir_lower_var_copies);
+
+   nir_assign_io_var_locations(nir, nir_var_shader_in, &nir->num_inputs, stage);
+   nir_assign_io_var_locations(nir, nir_var_shader_out, &nir->num_outputs,
+                               stage);
+
+   /* Needed to turn shader_temp into function_temp since the backend only
+    * handles the latter for now.
+    */
+   NIR_PASS_V(nir, nir_lower_global_vars_to_local);
+
+   nir_shader_gather_info(nir, nir_shader_get_entrypoint(nir));
+   if (unlikely(dev->physical_device->instance->debug_flags &
+                PANVK_DEBUG_NIR)) {
+      fprintf(stderr, "translated nir:\n");
+      nir_print_shader(nir, stderr);
+   }
+
+   pan_shader_preprocess(nir, inputs.gpu_id);
+
+   if (stage == MESA_SHADER_FRAGMENT) {
+      panvk_lower_blend(pdev, nir, &inputs, blend_state);
+   }
+
+   struct sysval_options sysval_options = {
+      .static_blend_constants =
+         static_blend_constants ? blend_state->constants : NULL,
+   };
+
+   NIR_PASS_V(nir, nir_shader_instructions_pass, panvk_lower_sysvals,
+              nir_metadata_block_index | nir_metadata_dominance,
+              &sysval_options);
+
+   if (stage == MESA_SHADER_FRAGMENT) {
+      enum pipe_format rt_formats[MAX_RTS] = {PIPE_FORMAT_NONE};
+
+      for (unsigned rt = 0; rt < MAX_RTS; ++rt)
+         rt_formats[rt] = blend_state->rts[rt].format;
+
+      NIR_PASS_V(nir, GENX(pan_inline_rt_conversion), pdev, rt_formats);
+   }
+
+   GENX(pan_shader_compile)(nir, &inputs, &shader->binary, &shader->info);
+
+   /* Patch the descriptor count */
+   shader->info.ubo_count =
+      PANVK_NUM_BUILTIN_UBOS + layout->num_ubos + layout->num_dyn_ubos;
+   shader->info.sampler_count = layout->num_samplers;
+   shader->info.texture_count = layout->num_textures;
+   if (shader->has_img_access)
+      shader->info.attribute_count += layout->num_imgs;
+
+   shader->sysval_ubo = sysval_ubo;
+   shader->local_size.x = nir->info.workgroup_size[0];
+   shader->local_size.y = nir->info.workgroup_size[1];
+   shader->local_size.z = nir->info.workgroup_size[2];
+
+   ralloc_free(nir);
+
+   return shader;
+}
diff -up mesa-23.2.0/src/panfrost/vulkan/meson.build.16~ mesa-23.2.0/src/panfrost/vulkan/meson.build
--- mesa-23.2.0/src/panfrost/vulkan/meson.build.16~	2023-07-29 11:05:19.000000000 +0100
+++ mesa-23.2.0/src/panfrost/vulkan/meson.build	2023-07-29 15:53:25.244311451 +0100
@@ -28,6 +28,7 @@ panvk_entrypoints = custom_target(
   command : [
     prog_python, '@INPUT0@', '--xml', '@INPUT1@', '--proto', '--weak',
     '--out-h', '@OUTPUT0@', '--out-c', '@OUTPUT1@', '--prefix', 'panvk',
+    '--device-prefix', 'panvk_bifrost',
     '--device-prefix', 'panvk_v6', '--device-prefix', 'panvk_v7',
     '--beta', with_vulkan_beta.to_string()
   ],
@@ -35,10 +36,10 @@ panvk_entrypoints = custom_target(
 )
 
 libpanvk_files = files(
+  'bifrost/panvk_cs.c',
+  'bifrost/panvk_descriptor_set.c',
   'panvk_cmd_buffer.c',
-  'panvk_cs.c',
   'panvk_device.c',
-  'panvk_descriptor_set.c',
   'panvk_formats.c',
   'panvk_image.c',
   'panvk_mempool.c',
@@ -60,18 +61,18 @@ foreach arch : ['6', '7']
     'panvk_v@0@'.format(arch),
     [
       panvk_entrypoints[0],
-      'panvk_vX_cmd_buffer.c',
-      'panvk_vX_cs.c',
-      'panvk_vX_descriptor_set.c',
-      'panvk_vX_device.c',
-      'panvk_vX_image.c',
-      'panvk_vX_meta.c',
-      'panvk_vX_meta_blit.c',
-      'panvk_vX_meta_copy.c',
-      'panvk_vX_meta_clear.c',
-      'panvk_vX_nir_lower_descriptors.c',
-      'panvk_vX_pipeline.c',
-      'panvk_vX_shader.c',
+      'bifrost/panvk_vX_cmd_buffer.c',
+      'bifrost/panvk_vX_cs.c',
+      'bifrost/panvk_vX_descriptor_set.c',
+      'bifrost/panvk_vX_device.c',
+      'bifrost/panvk_vX_image.c',
+      'bifrost/panvk_vX_meta.c',
+      'bifrost/panvk_vX_meta_blit.c',
+      'bifrost/panvk_vX_meta_copy.c',
+      'bifrost/panvk_vX_meta_clear.c',
+      'bifrost/panvk_vX_nir_lower_descriptors.c',
+      'bifrost/panvk_vX_pipeline.c',
+      'bifrost/panvk_vX_shader.c',
     ],
     include_directories : [
       inc_include,
diff -up mesa-23.2.0/src/panfrost/vulkan/panvk_cs.c.16~ mesa-23.2.0/src/panfrost/vulkan/panvk_cs.c
diff -up mesa-23.2.0/src/panfrost/vulkan/panvk_descriptor_set.c.16~ mesa-23.2.0/src/panfrost/vulkan/panvk_descriptor_set.c
diff -up mesa-23.2.0/src/panfrost/vulkan/panvk_device.c.16~ mesa-23.2.0/src/panfrost/vulkan/panvk_device.c
--- mesa-23.2.0/src/panfrost/vulkan/panvk_device.c.16~	2023-07-29 11:05:19.000000000 +0100
+++ mesa-23.2.0/src/panfrost/vulkan/panvk_device.c	2023-07-29 15:51:45.093980518 +0100
@@ -841,16 +841,18 @@ panvk_CreateDevice(VkPhysicalDevice phys
    if (!device)
       return vk_error(physical_device, VK_ERROR_OUT_OF_HOST_MEMORY);
 
-   const struct vk_device_entrypoint_table *dev_entrypoints;
+   const struct vk_device_entrypoint_table *base_dev_entrypoints, *dev_entrypoints;
    const struct vk_command_buffer_ops *cmd_buffer_ops;
    struct vk_device_dispatch_table dispatch_table;
 
    switch (physical_device->pdev.arch) {
    case 6:
+      base_dev_entrypoints = &panvk_bifrost_device_entrypoints;
       dev_entrypoints = &panvk_v6_device_entrypoints;
       cmd_buffer_ops = &panvk_v6_cmd_buffer_ops;
       break;
    case 7:
+      base_dev_entrypoints = &panvk_bifrost_device_entrypoints;
       dev_entrypoints = &panvk_v7_device_entrypoints;
       cmd_buffer_ops = &panvk_v7_cmd_buffer_ops;
       break;
@@ -867,6 +869,8 @@ panvk_CreateDevice(VkPhysicalDevice phys
 
    vk_device_dispatch_table_from_entrypoints(&dispatch_table, dev_entrypoints,
                                              false);
+   vk_device_dispatch_table_from_entrypoints(&dispatch_table, base_dev_entrypoints,
+                                             false);
    vk_device_dispatch_table_from_entrypoints(&dispatch_table,
                                              &panvk_device_entrypoints, false);
    vk_device_dispatch_table_from_entrypoints(&dispatch_table,
diff -up mesa-23.2.0/src/panfrost/vulkan/panvk_private.h.16~ mesa-23.2.0/src/panfrost/vulkan/panvk_private.h
--- mesa-23.2.0/src/panfrost/vulkan/panvk_private.h.16~	2023-07-29 11:05:19.000000000 +0100
+++ mesa-23.2.0/src/panfrost/vulkan/panvk_private.h	2023-07-29 15:51:45.093980518 +0100
@@ -1070,25 +1070,25 @@ VK_DEFINE_NONDISP_HANDLE_CASTS(panvk_sam
 #elif PAN_ARCH == 7
 #define panvk_per_arch(name) panvk_arch_name(name, v7)
 #endif
-#include "panvk_vX_cmd_buffer.h"
-#include "panvk_vX_cs.h"
-#include "panvk_vX_device.h"
-#include "panvk_vX_meta.h"
+#include "bifrost/panvk_vX_cmd_buffer.h"
+#include "bifrost/panvk_vX_cs.h"
+#include "bifrost/panvk_vX_device.h"
+#include "bifrost/panvk_vX_meta.h"
 #else
 #define PAN_ARCH             6
 #define panvk_per_arch(name) panvk_arch_name(name, v6)
-#include "panvk_vX_cmd_buffer.h"
-#include "panvk_vX_cs.h"
-#include "panvk_vX_device.h"
-#include "panvk_vX_meta.h"
+#include "bifrost/panvk_vX_cmd_buffer.h"
+#include "bifrost/panvk_vX_cs.h"
+#include "bifrost/panvk_vX_device.h"
+#include "bifrost/panvk_vX_meta.h"
 #undef PAN_ARCH
 #undef panvk_per_arch
 #define PAN_ARCH             7
 #define panvk_per_arch(name) panvk_arch_name(name, v7)
-#include "panvk_vX_cmd_buffer.h"
-#include "panvk_vX_cs.h"
-#include "panvk_vX_device.h"
-#include "panvk_vX_meta.h"
+#include "bifrost/panvk_vX_cmd_buffer.h"
+#include "bifrost/panvk_vX_cs.h"
+#include "bifrost/panvk_vX_device.h"
+#include "bifrost/panvk_vX_meta.h"
 #undef PAN_ARCH
 #undef panvk_per_arch
 #endif
diff -up mesa-23.2.0/src/panfrost/vulkan/panvk_vX_cmd_buffer.c.16~ mesa-23.2.0/src/panfrost/vulkan/panvk_vX_cmd_buffer.c
diff -up mesa-23.2.0/src/panfrost/vulkan/panvk_vX_cmd_buffer.h.16~ mesa-23.2.0/src/panfrost/vulkan/panvk_vX_cmd_buffer.h
diff -up mesa-23.2.0/src/panfrost/vulkan/panvk_vX_cs.c.16~ mesa-23.2.0/src/panfrost/vulkan/panvk_vX_cs.c
diff -up mesa-23.2.0/src/panfrost/vulkan/panvk_vX_cs.h.16~ mesa-23.2.0/src/panfrost/vulkan/panvk_vX_cs.h
diff -up mesa-23.2.0/src/panfrost/vulkan/panvk_vX_descriptor_set.c.16~ mesa-23.2.0/src/panfrost/vulkan/panvk_vX_descriptor_set.c
diff -up mesa-23.2.0/src/panfrost/vulkan/panvk_vX_device.c.16~ mesa-23.2.0/src/panfrost/vulkan/panvk_vX_device.c
diff -up mesa-23.2.0/src/panfrost/vulkan/panvk_vX_device.h.16~ mesa-23.2.0/src/panfrost/vulkan/panvk_vX_device.h
diff -up mesa-23.2.0/src/panfrost/vulkan/panvk_vX_image.c.16~ mesa-23.2.0/src/panfrost/vulkan/panvk_vX_image.c
diff -up mesa-23.2.0/src/panfrost/vulkan/panvk_vX_meta_blit.c.16~ mesa-23.2.0/src/panfrost/vulkan/panvk_vX_meta_blit.c
diff -up mesa-23.2.0/src/panfrost/vulkan/panvk_vX_meta.c.16~ mesa-23.2.0/src/panfrost/vulkan/panvk_vX_meta.c
diff -up mesa-23.2.0/src/panfrost/vulkan/panvk_vX_meta_clear.c.16~ mesa-23.2.0/src/panfrost/vulkan/panvk_vX_meta_clear.c
diff -up mesa-23.2.0/src/panfrost/vulkan/panvk_vX_meta_copy.c.16~ mesa-23.2.0/src/panfrost/vulkan/panvk_vX_meta_copy.c
diff -up mesa-23.2.0/src/panfrost/vulkan/panvk_vX_meta.h.16~ mesa-23.2.0/src/panfrost/vulkan/panvk_vX_meta.h
diff -up mesa-23.2.0/src/panfrost/vulkan/panvk_vX_nir_lower_descriptors.c.16~ mesa-23.2.0/src/panfrost/vulkan/panvk_vX_nir_lower_descriptors.c
diff -up mesa-23.2.0/src/panfrost/vulkan/panvk_vX_pipeline.c.16~ mesa-23.2.0/src/panfrost/vulkan/panvk_vX_pipeline.c
diff -up mesa-23.2.0/src/panfrost/vulkan/panvk_vX_shader.c.16~ mesa-23.2.0/src/panfrost/vulkan/panvk_vX_shader.c
